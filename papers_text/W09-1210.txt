Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 67?72,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficient Parsing of Syntactic and Semantic Dependency Structures
Bernd Bohnet
International Computer Science Institute
1947 Center Street
Berkeley 94704, California
bohnet@icsi.Berkeley.edu
Abstract
In this paper, we describe our system for the
2009 CoNLL shared task for joint parsing of
syntactic and semantic dependency structures
of multiple languages. Our system combines
and implements efficient parsing techniques to
get a high accuracy as well as very good pars-
ing and training time. For the applications
of syntactic and semantic parsing, the pars-
ing time and memory footprint are very im-
portant. We think that also the development of
systems can profit from this since one can per-
form more experiments in the given time. For
the subtask of syntactic dependency parsing,
we could reach the second place with an ac-
curacy in average of 85.68 which is only 0.09
points behind the first ranked system. For this
task, our system has the highest accuracy for
English with 89.88, German with 87.48 and
the out-of-domain data in average with 78.79.
The semantic role labeler works not as well as
our parser and we reached therefore the fourth
place (ranked by the macro F1 score) in the
joint task for syntactic and semantic depen-
dency parsing.
1 Introduction
Depedendency parsing and semantic role labeling
improved in the last years significantly. One of the
reasons are CoNLL shared tasks for syntactic de-
pendency parsing in the years 2006, 2007 (Buch-
holz and Marsi, 2006; Nivre et al, 2007) and the
CoNLL shared task for joint parsing of syntactic and
semantic dependencies in the year 2008 and of cause
this shared task in 2009, cf. (Surdeanu et al, 2008;
Hajic? et al, 2009). The CoNLL Shared Task 2009
is to parse syntactic and semantic dependencies of
seven languages. Therefore, training and develop-
ment data in form of annotated corpora for Cata-
lan, Chinese, Czech, English, German, Japanese and
Spanish is provided, cf. (Taule? et al, 2008; Palmer
and Xue, 2009; Hajic? et al, 2006; Surdeanu et al,
2008; Burchardt et al, 2006; Kawahara et al, 2002).
There are two main approaches to dependency
parsing: Maximum Spanning Tree (MST) based de-
pendency parsing and Transition based dependency
parsing, cf. (Eisner, 1996; Nivre et al, 2004; Mc-
Donald and Pereira, 2006). Our system uses the first
approach since we saw better chance to improve the
parsing speed and additionally, the MST had so far
slightly better parsing results. For the task of seman-
tic role labeling, we adopted a pipeline architecture
where we used for each step the same learning tech-
nique (SVM) since we opted for the possibility to
build a synchronous combined parser with one score
function.
2 Parsing Algorithm
We adopted the second order MST parsing algo-
rithm as outlined by Eisner (1996). This algorithm
has a higher accuracy compared to the first order
parsing algorithm since it considers also siblings and
grandchildren of a node. Eisner?s second order ap-
proach can compute a projective dependency tree
within cubic time (O(n3)).
Both algorithms are bottom up parsing algorithms
based on dynamic programming similar to the CKY
chart parsing algorithm. The score for a dependency
tree is the score of all edge scores. The following
67
equation describes this formally.
score(S, t) =??(i,j)?E score(i, j)
The score of the sentence S and a tree t over S
is defined as the sum of all edge scores where the
words of S are w0...w1. The tree consists of set of
nodes N and set of edges E = ?N ?N?. The word
indices (0..n) are the elements of the node set N .
The expression (i, j) ? E denotes an edge which is
going from the node i to the node j.
The edge score (score(i, j)) is computed as the
scalar product of a feature vector representation of
each edge ??fS(i, j) with a weight vector ??w where
i, j are the indices of the words in a sentence. The
feature vector fS might take not only into account
the words with indices i and j but also additional
values such as the words before and after the words
wi and wj . The following equation shows the score
function.
score(i, j) = ??fS(i, j) ? ??w
Many systems encode the features as strings and
map the strings to a number. The number becomes
the index of the feature in the feature vector and
weight vector. In order to compute the weight vec-
tor, we reimplemented the support vector machine
MIRA which implements online Margin Infused Re-
laxed Algorithm, cf. (Crammer et al, 2003).
3 Labeled Dependency Parsing
The second order parsing algorithm builds an un-
labeled dependency tree. However, all dependency
tree banks of the shared task provide trees with edge
labels. The following two approaches are common
to solve this problem. An additional algorithm la-
bels the edges or the parsing algorithm itself is ex-
tended and the labeling algorithm is integrated into
the parsing algorithm. McDonald et al (2006) use
an additional algorithm. Their two stage model has
a good computational complexity since the label-
ing algorithm contributes again only a cubic time
complexity to the algorithm and keeps therefore the
joint algorithm still cubic. The algorithm selects
the highest scored label due to the score function
score(wi, label) + score(wj , label) and inserts the
highest scored label into a matrix. The scores are
also used in the parsing algorithms and added to
the edge scores which improves the overall pars-
ing results as well. In the first order parsing sce-
nario, this procedure is sufficient since no combi-
nation of edges are considered by the parsing algo-
rithm. However, in the second order parsing sce-
nario where more than one edge are considered by
the parsing algorithm, combinations of two edges
might be more accurate.
Johansson and Nugues (2008) combines the edge
labeling with the second order parsing algorithm.
This adds an additional loop over the edge labels.
The complexity is therefore O(n4). However, they
could show that a system can gain accuracy of about
2-4% which is a lot.
4 Non-Projective Dependency Parsing
The dependency parser developed in the last years
use two different techniques for non-projective de-
pendency parsing.
Nivre and Nilsson (2005) uses tree rewriting
which is the most common technique. With this
technique, the training input to the parser is first pro-
jectivized by applying a minimal number of lifting
operations to the non-projective edges and encoding
information about these lifts in edge labels. After
these operations, the trees are projective and there-
fore a projective dependency parser can be applied.
During the training, the parser learns also to built
trees with the lifted edges and so indirect to built
non-projective dependency trees by applying the in-
verse operations to the lifting on the projective tree.
McDonald and Pereira (2006) developed a tech-
nique to rearrange edges in the tree in a postpro-
cessing step after the projective parsing has taken
place. Their Approximate Dependency Parsing Al-
gorithm searches first the highest scoring projective
parse tree and then it rearranges edges in the tree
until the rearrangements does not increase the score
for the tree anymore. This technique is computa-
tionally expensive for trees with a large number of
non-projective edges since it considers to re-attach
all edges to any other node until no higher scoring
trees can be found. Their argument for the algo-
rithm is that most edges in a tree even in language
with lot of non-projective sentences, the portion of
non-projective edges are still small and therefore by
starting with the highest scoring projective tree, typ-
68
ically the highest scoring non-projective tree is only
a small number of transformations away.
Our experiments showed that with the non-
projective Approximate Dependency Parsing Algo-
rithm and a threshold for the improvment of score
higher than about 0.7, the parsing accuracy improves
even for English slightly. With a threshold of 1.1, we
got the highest improvements.
5 Learning Framework
As learning technique, we use Margin Infused Re-
laxed Algorithm (MIRA) as developed by Crammer
et al (2003) and applied to dependency parsing by
McDonald et al (2005). The online Algorithm in
Figure 1 processes one training instance on each it-
eration, and updates the parameters accordingly.
Algorithm 1: MIRA
? = {Sx, Tx}Xx=1 // The set of training data consists
// of sentences and the corresponding dependency trees
??w (0) = 0,??v = 0
for n = 1 to N
for x = 1 to X
wi+1 = update wi according to instance (Sx, Tx)
v = v + wi+1
i = i+ 1
end for
end for
w = v/(N ?X)
The inner loop iterates over all sentences x of the
training set while the outer loop repeats the train i
times. The algorithm returns an averaged weight
vector and uses an auxiliary weight vector v that ac-
cumulates the values of w after each iteration. At
the end, the algorithm computes the average of all
weight vectors by dividing it by the number of train-
ing iterations and sentences. This helps to avoid
overfitting, cf. (Collins, 2002).
The update function computes the update to the
weight vector wi during the training so that wrong
classified edges of the training instances are possibly
correctly classified. This is computed by increasing
the weight for the correct features and decreasing the
weight for wrong features of the vectors for the tree
of the training set ??fTx ? wi and the vector for the
predicted dependency tree ??fT ?x ? wi.
The update function tries to keep the change to
the parameter vector wi as small as possible for cor-
rectly classifying the current instance with a differ-
ence at least as large as the loss of the incorrect clas-
sifications.
6 Selected Parsing Features
Table 1, 4 and 2 give an overview of the selected
features for our system. Similar to Johansson and
Nugues (2008), we add the edge labels to each fea-
tures. In the feature selection, we follow a bit more
McDonald and Pereira (2006) since we have in addi-
tion the lemmas, morphologic features and the dis-
tance between the word forms.
For the parsing and training speed, most impor-
tant is a fast feature extraction beside of a fast pars-
ing algorithm.
Standard Features
h-f/l h-f/l, d-pos
h-pos h-pos, d-f/l
d-f/l h-f/l, d-f/l
d-pos h-pos, d-pos
h-f/l,h-pos h-f/l, d-f/l, h-pos
d-f/l,d-pos h-f/l, d-f/l, d-pos
h-pos, d-pos, h-f/l
h-pos, d-pos, d-f/l
h-pos, d-pos, h-f/l, d-f/l
Table 1: Selected standard parsing features. h is the ab-
brevation for head, d for dependent, g for grandchild, and
s for sibling. Each feature contains also the edge label
which is not listed in order to save some space. Addi-
tional features are build by adding the direction and the
distance plus the direction. The direction is left if the de-
pendent is left of the head otherwise right. The distance
is the number of words between the head and the depen-
dent, if ?5, 6 if >5 and 11 if >10. ? means that an
additional feature is built with the previous part plus the
following part. f/l represent features that are built once
with the form and once with the lemma.
Selected morphologic parsing features.
? h-morpheme ? head-morphologic-feature-set do
? d-morpheme ? dependent-morphologic-feature-set do
build-feautre: h-pos, d-pos, h-morpheme, d-morpheme
7 Implementation Aspects
In this section, we provide implementation details
considering improvements of the parsing and train-
ing time. The training of our system (parser) has
69
Linear Features Grandchild Features Sibling Features
h-pos, d-pos, h-pos + 1 h-pos, d-pos, g-pos, dir(h,d), dir(d,g) d-f/l, s-f/l ? dir(d,s) ?dist(d,s)
h-pos, d-pos, h-pos - 1 h-f/l, g-f/l, dir(h,d), dir(d,g) d-pos, s-f/l ? dir(d,s) ?dist(d,s)
h-pos, d-pos, d-pos + 1 d-f/l, g-f/l, dir(h,d), dir(d,g) d-pos, s-f/l ? dir(d,s)+ ? dist(d,s)
h-pos, d-pos, d-pos - 1 h-pos, g-f/l, dir(h,d), dir(d,g) d-pos, s-pos ?dir(d,s)?dist(d,s)
h-pos, d-pos, h-pos - 1, d-pos - 1 d-pos, g-f/l, dir(h,d), dir(d,g) h-pos, d-pos, s-pos, dir(h,d), dir(d,s) ?dist(h,s)
h-f/l, g-pos, dir(h,d), dir(d,g) h-f/l, s-f/l, dir(h,d), dir(d,s)?dist(h,s) h-pos, s-f/l, dir(h,d), dir(d,s)?dist(h,s)
d-f/l, g-pos, dir(h,d), dir(d,g) d-f/l, s-f/l, dir(h,d), dir(d,s) ?dist(h,s) d-pos, s-f/l, dir(h,d), dir(d,s)?dist(h,s)
h-f/l, s-pos, dir(h,d), dir(d,s)?dist(h,s)
d-f/l, s-pos, dir(h,d), dir(d,s)?dist(h,s)
Table 2: Selected Features.
three passes. The goal of the first two passes is to
collect the set of possible features of the training
set. In order to determine the minimal description
length, the feature extractor collects in the first pass
all attributes that the features can contain. For each
attribute (labels, part-of-speech, etc.), the extractor
computes a mapping to a number which is continous
from 1 to the count of elements without duplicates.
We enumerate in the same way the feature pat-
terns (e.g. h-pos, d-pos) in order to distinguish the
patterns. In the second pass, the extractor builds the
features for all training examples which occur in the
train set. This means for all edges in the training
examples.
We create the features with a function that adds it-
eratively the attributes of a feature to a number rep-
resented with 64 bits and shifts it by the minimal
number of bits to encode the attribute and then enu-
merates and maps these numbers to 32 bit numbers
to save even more memory.
Beside this, the following list shows an overview
of the most important implementation details to im-
prove the speed:
1. We use as feature vector a custom array imple-
mentation with only a list of the features that
means without double floating point value.
2. We store the feature vectors for
f(label, wi, wj), f(label, wi, wj , wg),
f(label, wi, wj , ws) etc. in a compressed
file since otherwise it becomes the bottleneck.
3. After the training, we store only the parameters
of the support vector machine which are higher
than a threshold of 1?10?7
Table 3 compares system regarding their perfor-
mance and memory usage. For the shared task, we
System (1) (2) (3)
Type 2nd order 2nd order 2nd order
Labeling separate separate integrated
System baseline this this
Training 22 hours 3 hours 15 hours
7GB 1.5 GB 3 GB
Parsing 2000 ms 50 ms 610 ms
700 MB 1 GB
LAS 0.86 0.86 0.88
Table 3: Performance Comparison. For the baseline sys-
tem (1), we used the system of McDonald and Pereira
(2006) on a MacPro 2.8 Ghz as well for our implementa-
tion (2). For system (3), we use a computer with Intel i7
3.2 Ghz which is faster than the MacPro. For all systems,
we use 10 training iterations for the SVM Mira.
use the system (3) with integrated labeling.
8 Semantic Role Labeling
The semantic role labeler is implemented as a
pipeline architecture. The components of the
pipeline are predicate selection (PS), argument iden-
tification (AI), argument classification (AC), and
word sense disambiguation (WSD).
In order to select the predicates, we look up the
lemmas in the Prob Bank, Nom Bank, etc. if avail-
able, cf. (Palmer et al, 2005; Meyers et al, 2004).
For all other components, we use the support vec-
tor machine MIRA to select and classify the seman-
tic role labels as well as to disambiguate the word
senese.
The AI component identifies the arguments of
each predicate. It iterates over the predicates and
over the words of a sentence. In the case that the
score function is large or equal to zero the argument
is added to the set of arguments of the predicate in
question. Table 5 lists for the attribute identification
and semantic role labeling.
The argument classification algorithm labels each
70
Language Catalan Chinese Czech English German Japanese Spanish Czech English German
Development Set
LAS 86.69 76.77 80.75 87.97 86.46 92.37 86.53
Semantic Unlabeled 93.92 85.09 94.05 91.06 91.61 93.90 93.87
Semantic Labeled 74.98 75.94 78.07 78.79 72.66 72.86 73.01
Macro (F1) 80.84 76.48 79.42 84.52 79.56 82.63 79.77
Test Set Out-of-domain data
LAS 86.35 76.51 80.11 @89.88 @87.48 92.21 87.19 76.40 @82.64 @77.34
Semantic Labeled 74.53 75.29 79.02 80.39 75.72 72.76 74.31 78.01 68.44 63.36
Macro (F1) 80.44 75.91 79.57 85.14 81.60 82.51 80.75 77.20 75.55 70.35
Table 4: Syntactic and Semantic Scores. @ indicate values that are the highest scores of all systems.
Features with part-of-speech tags Features with lemmas Features with rels
arg, path-len arg, p-lemma ? dir ? path-len arg, a-rel ? path-len
arg, p-pos arg, a-lemma, path, dir arg, a-pos, p-pos, p-rel ? path-len
arg, sub-cat, a-pos, p-pos arg, p-lemma - 1, a-pos, path-len, dir arg, p-rel, a-pos, lms-lemma
arg, p-pos, a-pos, a-rmc-pos arg, p-lemma + 1, a-lemma, path, dir arg, a-pos, p-pos, a-rel
arg, p-pos, a-pos, a-lmc-pos arg, p-lemma - 1, a-lemma, path, dir arg, path-rel
arg, p-pos, a-pos, a-lemma-1 arg, p-lemma - 2, a-lemma, path, dir arg, p-lemma, a-pos, path-rel
arg, sub-cat, a-lemma, dir, path-len arg, p-lemma, a-lemma, pathPos, dir
arg, a-pos, a-lemma + 1 arg, p-lemma, p-lemma + 1
arg, a-pos, a-lemma + 2 arg, p-lemma, p-lemma - 1, a-pos ? dir ? path-len
arg, a-pos, a-lemma-lmc arg, p-lemma, a-lms-lemma, a-pos ? dir ? path-len
arg, p-sub-cat, p-pos ? dir arg, p-lemma, path-len ? dir
arg, p-pos, path, p-parent-lemma arg, p-lemma, path-len ? dir
arg, p-pos, path, p-parent-pos ? dir arg, a-pos, path
arg, p-pos, a-pos, familyship(p,a) arg, p-pos, p-lemma, familyship(p,a)
arg, path-pos arg, a-pos, p-lemma, familyship(p,a)
arg, p-pos, a-lemma, familyship(p,a)
Table 5: Argument identification and semantic role labeling Features. p is the abbrivation for predicate and a for
argument. For the AI component, the attribute arg is either the value yes and no and for the SRL component, ars is
the role label. path is the path in terms of up?s and down?s. pathPos is a path plus the part-of-speech on the path. dir
is left, if the argument is left of the predicate, equal if the predicate and argument are equal, otherwise right. rmc is
the abbrivation for right most child, lmc for left most child, and lms left most sibling. familiship(x,y) is a function that
computes the relation between two words: self, parent, child, ancestor, decendant and none.
identified argument with a semantic role label. The
argument classification algorithm selects with a
beam search algorithm the combination of argu-
ments with the highest score.
The last component of our pipeline is the word
sense disambiguation. We put this against the in-
tuition at the end of our pipeline since experiments
showed that other components could not profit from
disambiguated word senses but on the other hand
the word sense disambiguation could profit from the
argument identification and argument classification.
In order to disambiguate, we iterate over the words
in the corpus that have more than one sense and take
the sense with the highest score.
The average time to execute the SRL pipeline on
a sentence is less than 0.15 seconds and the training
time for all languages less than 2 hours.
9 Conclusion
We provided a fast implementation with good pars-
ing time and memory footprint. Even if we traded
off a lot of the speed improvement by using a
more expensive decoder and more attributes to get
a higher accuracy.
For some languages, features are not provided or
the parser does not profit from using these features.
For instance, the English parser does not profit from
the lemmas and the Chinese as well as the Japanese
71
corpus does not have lemmas different from the
word forms, etc. Therefore, a possible further ac-
curacy and parsing speed improvement would be to
select different features sets for different languages
or to leave out some features.
Acknowledgments
This work was supported by the German Academic
Exchange Service (DAAD). We gratefully acknowl-
edge this support.
References
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing. In
In Proc. of CoNLL, pages 149?164.
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In EMNLP.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2003. Online Passive-Aggressive Al-
gorithms. In Sixteenth Annual Conference on Neural
Information Processing Systems (NIPS).
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340?345,
Copenhaen.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zden?k ?Zabokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu?is
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan ?Ste?pa?nek, Pavel Stran?a?k, Miahi Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Dependen-
cies in Multiple Languages. In Proceedings of the 13th
CoNLL-2009, June 4-5, Boulder, Colorado, USA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008, Manchester,
UK.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. In In Proc. of EACL, pages 81?88.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-margin Training of Dependency
Parsers. In Proc. ACL, pages 91?98.
Ryan McDonald, Kevin Lerman, Koby Crammer, and
Fernando Pereira. 2006. Multilingual Dependency
Parsing with a Two-Stage Discriminative Parser. In
Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 91?98.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The nombank project: An interim
report. In A. Meyers, editor, HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May 2 - May 7. Associ-
ation for Computational Linguistics.
Joakim. Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 99?106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-Based Dependency Parsing. In Proceed-
ings of the 8th CoNLL, pages 49?56, Boston, Mas-
sachusetts.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Rayn McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The conll 2007 shared task on dependency pars-
ing. In Proc. of the CoNLL 2007 Shared Task. Joint
Conf. on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), June.
Martha Palmer and Nianwen Xue. 2009. Adding Seman-
tic Roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Corpus
of Semantic Roles. volume 31, pages 71?106.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the 12th
CoNLL-2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the LREC-
2008, Marrakesh, Morroco.
72
