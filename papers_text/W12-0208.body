Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 49?53,
Avignon, France, April 23 - 24 2012. c?2012 Association for Computational Linguistics
Similarity Patterns in Words
Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, Alberta, Canada, T6G 2E8
gkondrak@ualberta.ca
Abstract
Words are important both in historical lin-
guistics and natural language processing.
They are not indivisible abstract atoms;
much can be gained by considering smaller
units such as morphemes, phonemes, syl-
lables, and letters. In this presentation,
I attempt to sketch the similarity pat-
terns among a number of diverse research
projects in which I participated.
1 Introduction
Languages are made up of words, which con-
tinuously change their form and meaning. Lan-
guages that are related contain cognates ? re-
flexes of proto-words that survive in some form
in the daughter languages. Sets of cognates reg-
ularly exhibit recurrent sound correspondences.
Together, cognates and recurrent sound corre-
spondences provide evidence of a common origin
of languages.
Although I consider myself more a computer
scientist than a linguist, I am deeply interested
in words. Even though many NLP algorithms
treat words as indivisible abstract atoms, I think
that much can be gained by considering smaller
units: morphemes, phonemes, syllables, and let-
ters. Words that are similar at the sub-word level
often exhibit similarities on the syntactic and se-
mantic level as well. Even more important, as we
move beyond written text towards speech and pro-
nunciation, the make-up of words cannot be ig-
nored anymore.
I commenced my NLP research by investigat-
ing ways of developing computer programs for
various stages of the language reconstruction pro-
cess (Kondrak, 2002a). From the very start, I
aimed at proposing language-independent solu-
tions grounded in the current advances in NLP,
bioinformatics, and computer science in general.
The algorithms were evaluated on authentic lin-
guistic data and compared quantitatively to pre-
vious proposals. The projects directly related to
language histories still form an important part of
my research. In Section 2, I refer to several of my
publications on the subject, while in Section 3,
I focus on other NLP applications contributions
that originate from my research on diachronic lin-
guistics.
2 Diachronic NLP
The comparative method is the technique applied
by linguists for reconstructing proto-languages. It
consists of several stages, which include the iden-
tification of cognates by semantic and phonetic
similarity, the alignment of cognates, the deter-
mination of recurrent sound correspondences, and
finally the reconstruction of the proto-forms. The
results of later steps are used to refine the judg-
ments made in earlier ones. The comparative
method is not an algorithm, but rather a collection
of heuristics, which involve intuitive criteria and
broad domain knowledge. As such, it is a very
time-consuming process that has yet to be accom-
plished for many language families.
Since the comparative method involves detec-
tion of regularities in large amounts of data, it is
natural to investigate whether it can be performed
by a computer program. In this section, I dis-
cuss methods for implementing several steps of
the comparative method that are outlined above.
The ordering of projects is roughly chronologi-
cal. For an article-length summary see (Kondrak,
2009).
49
2.1 Alignment
Identification of the corresponding segments in
sequences of phonemes is a necessary step in
many applications in both diachronic and syn-
chronic phonology. ALINE (Kondrak, 2000) was
originally developed for aligning corresponding
phonemes in cognate pairs. It combines a dy-
namic programming alignment algorithm with a
scoring scheme based on multi-valued phonetic
features. ALINE has been shown to generate
more accurate alignments than comparable algo-
rithms (Kondrak, 2003b).
Bhargava and Kondrak (2009) propose a dif-
ferent method of alignment, which is an adapta-
tion of Profile Hidden Markov Models developed
for biological sequence analysis. They find that
Profile HMMs work well on the tasks of multiple
cognate alignment and cognate set matching.
2.2 Phonetic Similarity
In many applications, it is necessary to algorith-
mically quantify the similarity exhibited by two
strings composed of symbols from a finite al-
phabet. Probably the most well-known measure
of string similarity is the edit distance, which is
the number of insertions, deletions and substitu-
tions required to transform one string into another.
Other measures include the length of the longest
common subsequence, and the bigram Dice coef-
ficient. Kondrak (2005b) introduces a notion of n-
gram similarity and distance, and shows that edit
distance and the length of the longest common
subsequence are special cases of n-gram distance
and similarity, respectively.
Another class of similarity measures are specif-
ically for phonetic comparison. The ALINE algo-
rithm chooses the optimal alignment on the ba-
sis of a similarity score, and therefore can also be
used for computing phonetic similarity of words.
Kondrak (2001) shows that it performs well on the
task of cognate identification.
The above algorithms have the important ad-
vantage of not requiring training data, but they
cannot adapt to a specific task or language. Re-
searchers have therefore investigated adaptive
measures that are learned from a set of training
pairs. Mackay and Kondrak (2005) propose a sys-
tem for computing string similarity based on Pair
HMMs. The parameters of the model are auto-
matically learned from training data that consists
of pairs of strings that are known to be similar.
Kondrak and Sherif (2006) test representatives
of the two principal approaches to computing
phonetic similarity on the task of identifying cog-
nates among Indoeuropean languages, both in the
supervised and unsupervised context. Their re-
sults suggest that given a sufficiently large train-
ing set of positive examples, the learning algo-
rithms achieve higher accuracy than manually-
designed metrics.
Techniques such as Pair HMMs improve on
the baseline approaches by using a set of similar
words to re-weight the costs of edit operations or
the score of sequence matches. A more flexible
approach is to learn from both positive and nega-
tive examples of word pairs. Bergsma and Kon-
drak (2007a) propose such a discriminative al-
gorithm, which achieves exceptional performance
on the task of cognate identification.
2.3 Recurrent Sound Correspondences
An important phenomenon that allows us to dis-
tinguish between cognates and borrowings or
chance resemblances is the regularity of sound
change. The regularity principle states that a
change in pronunciation applies to sounds in a
given phonological context across all words in the
language. Regular sound changes tend to produce
recurrent sound correspondences of phonemes in
corresponding cognates.
Although it may not be immediately appar-
ent, there is a strong similarity between the task
of matching phonetic segments in a pair of cog-
nate words, and the task of matching words in
two sentences that are mutual translations. The
consistency with which a word in one language
is translated into a word in another language is
mirrored by the consistency of sound correspon-
dences. Kondrak (2002b) proposes to adapt an
algorithm for inducing word alignment between
words in bitexts (bilingual corpora) to the task
of identifying recurrent sound correspondences in
word lists. The method is able to determine corre-
spondences with high accuracy in bilingual word
lists in which less than a third the word pairs are
cognates.
Kondrak (2003a) extends the approach to the
identification of complex correspondences that in-
volve groups of phonemes by employing an algo-
rithm designed for extracting non-compositional
compounds from bitexts. In experimental evalu-
ation against a set of correspondences manually
50
identified by linguists, it achieves approximately
90% F-score on raw dictionary data.
2.4 Semantic Similarity
Only a fraction of all cognates can be detected
by analyzing Swadesh-type word lists, which are
usually limited to at most 200 basic meanings. A
more challenging task is identifying cognates di-
rectly in bilingual dictionaries, which define the
meanings of words in the form of glosses. The
main problem is how to quantify semantic simi-
larity of two words on the basis of their respective
glosses.
Kondrak (2001) proposes to compute similarity
of glosses by augmenting simple string-matching
with a syntactically-informed keyword extraction.
In addition, the concepts mentioned in glosses
are mapped to WordNet synsets in an attempt to
account for various types of diachronic seman-
tic change, such as generalization, specialization,
and synechdoche.
Kondrak (2004) presents a method of combin-
ing distinct types of cognation evidence, includ-
ing the phonetic and semantic similarity, as well
as simple and complex recurrent sound correspon-
dences. The method requires no manual parame-
ter tuning, and performs well when tested on cog-
nate identification in the Indoeuropean word lists
and Algonquian dictionaries.
2.5 Cognate Sets
When data from several related languages is avail-
able, it is preferable to identify cognate sets si-
multaneously across all languages rather than per-
form pairwise analysis. Kondrak et al. (2007) ap-
ply several of the algorithms described above to a
set of diverse dictionaries of languages belonging
to the Totonac-Tepehua family in Mexico. They
show that by combining expert linguistic knowl-
edge with computational analysis, it is possible to
quickly identify a large number of cognate sets
within the family, resulting in a basic comparative
dictionary. The dictionary subsequently served
as a starting point for generating lists of puta-
tive cognates between the Totonacan and Mixe-
Zoquean families. The project eventually culmi-
nated in a proposal for establishing a super-family
dubbed Totozoquean (Brown et al., 2011).
Bergsma and Kondrak (2007b) present a
method for identifying sets of cognates across
groups of languages using the global inference
framework of Integer Linear Programming. They
show improvements over simple clustering tech-
niques that do not inherently consider the transi-
tivity of cognate relations.
Hauer and Kondrak (2011) present a machine-
learning approach that automatically clusters
words in multilingual word lists into cognate sets.
The method incorporates a number of diverse
word similarity measures and features that encode
the degree of affinity between pairs of languages.
2.6 Phylogenetic Trees
Phylogenetic methods are used to build evolution-
ary trees of languages given data that may include
lexical, phonological, and morphological infor-
mation. Such data rarely admits a perfect phy-
logeny. Enright and Kondrak (2011) explore the
use of the more permissive conservative Dollo
phylogeny as an alternative approach that pro-
duces an output tree minimizing the number of
borrowing events directly from the data. The ap-
proach which is significantly faster than the more
commonly known perfect phylogeny, is shown to
produce plausible phylogenetic trees on three dif-
ferent datasets.
3 NLP Applications
In this section, I mention several NLP projects
which directly benefitted from insights gained in
my research on diachronic linguistics.
Statistical machine translation in its origi-
nal formulation disregarded the actual forms of
words, focusing instead exclusively on their co-
occurrence patterns. In contrast, Kondrak et al.
(2003) show that automatically identifying ortho-
graphically similar words in bitexts can improve
the quality of word alignment, which is an impor-
tant step in statistical machine translation. The
improved alignment leads to better translation
models, and, consequently, translations of higher
quality.
Kondrak (2005a) further investigates word
alignment in bitexts, focusing on on identifying
cognates on the basis of their orthographic sim-
ilarity. He concludes that word alignment links
can be used as a substitute for cognates for the
purpose of evaluating word similarity measures.
Many hundreds of drugs have names that ei-
ther look or sound so much alike that doctors,
nurses and pharmacists sometimes get them con-
fused, dispensing the wrong one in errors that may
51
injure or even kill patients. Kondrak and Dorr
(2004) apply anumber of similarity measures to
the task of identifying confusable drug names.
They find that a combination of several measures
outperforms all individual measures.
Cognate lists can also assist in second-
language learning, especially in vocabulary ex-
pansion and reading comprehension. On the other
hand, the learner needs to pay attention to false
friends, which are pairs of similar-looking words
that have different meanings. Inkpen et al. (2005)
propose a method to automatically classify pairs
of words as cognates or false friends, with focus
on French and English. The results show that it is
possible to achieve very good accuracy even with-
out any training data by employing orthographic
measures of word similarity.
Transliteration is the task of converting words
from one writing script to another. Transliteration
mining aims at automatically constructing bilin-
gual lists of names for the purpose of training
transliteration programs. The task of detecting
phonetically-similar words across different writ-
ing scripts is quite similar to that of identifying
cognates, Sherif and Kondrak (2007) applies sev-
eral methods, including ALINE, to the task of ex-
tracting transliterations from an English-Arabic
bitext, and show that it performs better than edit
distance, but not as well as a bootstrapping ap-
proach to training a memoriless stochastic trans-
ducer. Jiampojamarn et al. (2009) employ ALINE
for aligning transliterations from distinct scripts
by mapping every character to a phoneme that is
the most likely to be produced by that character.
They observe that even such an imprecise map-
ping is sufficient for ALINE to produce high qual-
ity alignments.
Dwyer and Kondrak (2009) apply the ALINE
algorithm to the task of grapheme-to-phoneme
conversion, which is the process of producing the
correct phoneme sequence for a word given its or-
thographic form. They find ALINE to be an excel-
lent substitute for the expectation-maximization
(EM) algorithm when the quantity of the training
data is small.
Jiampojamarn and Kondrak (2010) confirm
that ALINE is highly accurate on the task of letter-
phoneme alignment. When evaluated on a man-
ually aligned lexicon, its precision was very close
to the theoretical upper bound, with the number
of incorrect links less than one in a thousand.
Lastly, ALINE has also been used for the map-
ping of annotations, including syllable breaks
and stress marks, from the phonetic to ortho-
graphic forms (Bartlett et al., 2008; Dou et al.,
2009).
4 Conclusion
The problems involved in language reconstruction
are easy to state but surprisingly hard to solve. As
such, they lead to the development of new meth-
ods and insights that are not restricted in applica-
tion to historical linguistics. Although the goal of
developing a program that performs a fully auto-
matic reconstruction of a proto-language has yet
to been attained, the research conducted towards
this goal has been, and is likely to continue to in-
fluence other areas of NLP.
Acknowledgments
This paper refers to research projects that were
conducted jointly with the following colleagues:
Susan Bartlett, David Beck, Shane Bergsma,
Aditya Bhargava, Cecil Brown, Colin Cherry,
Philip Dilts, Bonnie Dorr, Qing Dou, Elan
Dresher, Ken Dwyer, Jessica Enright, Oana
Frunza, Bradley Hauer, Graeme Hirst, Diana
Inkpen, Sittichai Jiampojamarn, Kevin Knight,
Wesley Mackay, Daniel Marcu, and Tarek Sherif.
References
