Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 103?108, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level
Semantic Similarity Estimation
Nikolaos Malandrakis1, Elias Iosif2, Vassiliki Prokopi2, Alexandros Potamianos2,
Shrikanth Narayanan1
1Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA
2Department of ECE, Technical University of Crete, 73100 Chania, Greece
malandra@usc.edu, iosife@telecom.tuc.gr, vprokopi@isc.tuc.gr, potam@telecom.tuc.gr,
shri@sipi.usc.edu
Abstract
This paper describes our submission for the
*SEM shared task of Semantic Textual Sim-
ilarity. We estimate the semantic similarity
between two sentences using regression mod-
els with features: 1) n-gram hit rates (lexical
matches) between sentences, 2) lexical seman-
tic similarity between non-matching words, 3)
string similarity metrics, 4) affective content
similarity and 5) sentence length. Domain
adaptation is applied in the form of indepen-
dent models and a model selection strategy
achieving a mean correlation of 0.47.
1 Introduction
Text semantic similarity estimation has been an ac-
tive research area, thanks to a variety of potential ap-
plications and the wide availability of data afforded
by the world wide web. Semantic textual similar-
ity (STS) estimates can be used for information ex-
traction (Szpektor and Dagan, 2008), question an-
swering (Harabagiu and Hickl, 2006) and machine
translation (Mirkin et al, 2009). Term-level simi-
larity has been successfully applied to problems like
grammar induction (Meng and Siu, 2002) and affec-
tive text categorization (Malandrakis et al, 2011). In
this work, we built on previous research and our sub-
mission to SemEval?2012 (Malandrakis et al, 2012)
to create a sentence-level STS model for the shared
task of *SEM 2013 (Agirre et al, 2013).
Semantic similarity between words has been
well researched, with a variety of knowledge-based
(Miller, 1990; Budanitsky and Hirst, 2006) and
corpus-based (Baroni and Lenci, 2010; Iosif and
Potamianos, 2010) metrics proposed. Moving to
sentences increases the complexity exponentially
and as a result has led to measurements of simi-
larity at various levels: lexical (Malakasiotis and
Androutsopoulos, 2007), syntactic (Malakasiotis,
2009; Zanzotto et al, 2009), and semantic (Rinaldi
et al, 2003; Bos and Markert, 2005). Machine trans-
lation evaluation metrics can be used to estimate lex-
ical level similarity (Finch et al, 2005; Perez and
Alfonseca, 2005), including BLEU (Papineni et al,
2002), a metric using word n-gram hit rates. The pi-
lot task of sentence STS in SemEval 2012 (Agirre et
al., 2012) showed a similar trend towards multi-level
similarity, with the top performing systems utilizing
large amounts of partial similarity metrics and do-
main adaptation (the use of separate models for each
input domain) (Ba?r et al, 2012; S?aric? et al, 2012).
Our approach is originally motivated by BLEU
and primarily utilizes ?hard? and ?soft? n-gram hit
rates to estimate similarity. Compared to last year,
we utilize different alignment strategies (to decide
which n-grams should be compared with which).
We also include string similarities (at the token and
character level) and similarity of affective content,
expressed through the difference in sentence arousal
and valence ratings. Finally we added domain adap-
tation: the creation of separate models per domain
and a strategy to select the most appropriate model.
2 Model
Our model is based upon that submitted for the same
task in 2012 (Malandrakis et al, 2012). To esti-
mate semantic similarity metrics we use a super-
vised model with features extracted using corpus-
103
based word-level similarity metrics. To combine
these metrics into a sentence-level similarity score
we use a modification of BLEU (Papineni et al,
2002) that utilizes word-level semantic similarities,
string level comparisons and comparisons of affec-
tive content, detailed below.
2.1 Word level semantic similarity
Co-occurrence-based. The semantic similarity be-
tween two words, wi and wj , is estimated as their
pointwise mutual information (Church and Hanks,
1990): I(i, j) = log p?(i,j)p?(i)p?(j) , where p?(i) and p?(j) are
the occurrence probabilities of wi and wj , respec-
tively, while the probability of their co-occurrence
is denoted by p?(i, j). In our previous participation
in SemEval12-STS task (Malandrakis et al, 2012)
we employed a modification of the pointwise mutual
information based on the maximum sense similar-
ity assumption (Resnik, 1995) and the minimization
of the respective error in similarity estimation. In
particular, exponential weights ? were introduced in
order to reduce the overestimation of denominator
probabilities. The modified metric Ia(i, j), is de-
fined as:
Ia(i, j)=
1
2
[
log
p?(i, j)
p??(i)p?(j) + log
p?(i, j)
p?(i)p??(j)
]
. (1)
The weight ? was estimated on the corpus of (Iosif
and Potamianos, 2012) in order to maximize word
sense coverage in the semantic neighborhood of
each word. The Ia(i, j) metric using the estimated
value of ? = 0.8 was shown to significantly
outperform I(i, j) and to achieve state-of-the-art
results on standard semantic similarity datasets
(Rubenstein and Goodenough, 1965; Miller and
Charles, 1998; Finkelstein et al, 2002).
Context-based: The fundamental assumption
behind context-based metrics is that similarity
of context implies similarity of meaning (Harris,
1954). A contextual window of size 2H + 1 words
is centered on the word of interest wi and lexical
features are extracted. For every instance of wi
in the corpus the H words left and right of wi
formulate a feature vector vi. For a given value of
H the context-based semantic similarity between
two words, wi and wj , is computed as the cosine
of their feature vectors: QH(i, j) = vi.vj||vi|| ||vj || .
The elements of feature vectors can be weighted
according various schemes [(Iosif and Potamianos,
2010)], while, here we use a binary scheme.
Network-based: The aforementioned similarity
metrics were used for the definition of a semantic
network (Iosif and Potamianos, 2013; Iosif et al,
2013). A number of similarity metrics were pro-
posed under either the attributional similarity (Tur-
ney, 2006) or the maximum sense similarity (Resnik,
1995) assumptions of lexical semantics1.
2.2 Sentence level similarities
To utilize word-level semantic similarities in the
sentence-level task we use a modified version of
BLEU (Papineni et al, 2002). The model works in
two passes: the first pass identifies exact matches
(similar to baseline BLEU), the second pass com-
pares non-matched terms using semantic similarity.
Non-matched terms from the hypothesis sentence
are compared with all terms of the reference sen-
tence (regardless of whether they were matched dur-
ing the first pass). In the case of bigram and higher
order terms, the process is applied recursively: the
bigrams are decomposed into two words and the
similarity between them is estimated by applying the
same method to the words. All word similarity met-
rics used are peak-to-peak normalized in the [0,1]
range, so they serve as a ?degree-of-match?. The se-
mantic similarity scores from term pairs are summed
(just like n-gram hits) to obtain a BLEU-like hit-rate.
Alignment is performed via maximum similarity:
we iterate on the hypothesis n-grams, left-to-right,
and compare each with the most similar n-gram in
the reference. The features produced by this process
are ?soft? hit-rates (for 1-, 2-, 3-, 4-grams)2. We also
use the ?hard? hit rates produced by baseline BLEU
as features of the final model.
2.3 String similarities
We use the following string-based similarity fea-
tures: 1) Longest Common Subsequence Similarity
(LCSS) (Lin and Och, 2004) based on the Longest
Common Subsequence (LCS) character-based dy-
1The network-based metrics were applied only during the
training phase of the shared task, due to time limitations. They
exhibited almost identical performance as the metric defined by
(1), which was used in the test runs.
2Note that the features are computed twice on each sentence
pair and then averaged.
104
namic programming algorithm. LCSS represents the
length of the longest string (or strings) that is a sub-
string (or are substrings) of two or more strings. 2)
Skip bigram co-occurrence measures the overlap of
skip-bigrams between two sentences or phrases. A
skip-bigram is defined as any pair of words in the
sentence order, allowing for arbitrary gaps between
words (Lin and Och, 2004). 3) Containment is de-
fined as the percentage of a sentence that is con-
tained in another sentence. It is a number between
0 and 1, where 1 means the hypothesis sentence is
fully contained in the reference sentence (Broder,
1997). We express containment as the amount of n-
grams of a sentence contained in another. The con-
tainment metric is not symmetric and is calculated
as: c(X,Y ) = |S(X) ? S(Y )|/S(X), where S(X)
and S(Y ) are all the n-grams of sentences X and Y
respectively.
2.4 Affective similarity
We used the method proposed in (Malandrakis et al,
2011) to estimate affective features. Continuous (va-
lence and arousal) ratings in [?1, 1] of any term are
represented as a linear combination of a function of
its semantic similarities to a set of seed words and
the affective ratings of these words, as follows:
v?(wj) = a0 +
N
?
i=1
ai v(wi) dij , (2)
where wj is the term we mean to characterize,
w1...wN are the seed words, v(wi) is the valence rat-
ing for seed word wi, ai is the weight corresponding
to seed word wi (that is estimated as described next),
dij is a measure of semantic similarity between wi
andwj (for the purposes of this work, cosine similar-
ity between context vectors is used). The weights ai
are estimated over the Affective norms for English
Words (ANEW) (Bradley and Lang, 1999) corpus.
Using this model we generate affective ratings for
every content word (noun, verb, adjective or adverb)
of every sentence. We assume that these can ad-
equately describe the affective content of the sen-
tences. To create an ?affective similarity metric? we
use the difference of means of the word affective rat-
ings between two sentences.
d?affect = 2? |?(v?(s1))? ?(v?(s2))| (3)
where ?(v?(si)) the mean of content word ratings in-
cluded in sentence i.
2.5 Fusion
The aforementioned features are combined using
one of two possible models. The first model is a
Multiple Linear Regression (MLR) model
D?L = a0 +
k
?
n=1
an fk, (4)
where D?L is the estimated similarity, fk are the un-
supervised semantic similarity metrics and an are
the trainable parameters of the model.
The second model is motivated by an assumption
of cognitive scaling of similarity scores: we expect
that the perception of hit rates is non-linearly af-
fected by the length of the sentences. We call this the
hierarchical fusion scheme. It is a combination of
(overlapping) MLR models, each matching a range
of sentence lengths. The first model DL1 is trained
with sentences with length up to l1, i.e., l ? l1, the
second model DL2 up to length l2 etc. During test-
ing, sentences with length l ? [1, l1] are decoded
with DL1, sentences with length l ? (l1, l2] with
model DL2 etc. Each of these partial models is a
linear fusion model as shown in (4). In this work,
we use four models with l1 = 10, l2 = 20, l3 = 30,
l4 = ?.
Domain adaptation is employed, by creating sep-
arate models per domain (training data source). Be-
yond that, we also create a unified model, trained
on all data to be used as a fallback if an appropriate
model can not be decided upon during evaluation.
3 Experimental Procedure and Results
Initially all sentences are pre-processed by the
CoreNLP (Finkel et al, 2005; Toutanova et al,
2003) suite of tools, a process that includes named
entity recognition, normalization, part of speech tag-
ging, lemmatization and stemming. We evaluated
multiple types of preprocessing per unsupervised
metric and chose different ones depending on the
metric. Word-level semantic similarities, used for
soft comparisons and affective feature extraction,
were computed over a corpus of 116 million web
snippets collected by posing one query for every
word in the Aspell spellchecker (asp, ) vocabulary to
the Yahoo! search engine. Word-level emotional rat-
ings in continuous valence and arousal scales were
produced by a model trained on the ANEW dataset
105
and using contextual similarities. Finally, string sim-
ilarities were calculated over the original unmodified
sentences.
Next, results are reported in terms of correla-
tion between the generated scores and the ground
truth, for each corpus in the shared task, as well as
their weighted mean. Feature selection is applied
to the large candidate feature set using a wrapper-
based backward selection approach on the train-
ing data.The final feature set contains 15 features:
soft hit rates calculated over content word 1- to 4-
grams (4 features), soft hit rates calculated over un-
igrams per part-of-speech, for adjectives, nouns, ad-
verbs, verbs (4 features), BLEU unigram hit rates
for all words and content words (2 features), skip
and containment similarities, containment normal-
ized by sum of sentence lengths or product of sen-
tence lengths (3 features) and affective similarities
for arousal and valence (2 features).
Domain adaptation methods are the only dif-
ference between the three submitted runs. For all
three runs we train one linear model per training set
and a fallback model. For the first run, dubbed lin-
ear, the fallback model is linear and model selection
during evaluation is performed by file name, there-
fore results for the OnWN set are produced by a
model trained with OnWN data, while the rest are
produced by the fallback model. The second run,
dubbed length, uses a hierarchical fallback model
and model selection is performed by file name. The
third run, dubbed adapt, uses the same models as
the first run and each test set is assigned to a model
(i.e., the fallback model is never used). The test set -
model (training) mapping for this run is: OnWN ?
OnWN, headlines ? SMTnews, SMT ? Europarl
and FNWN? OnWN.
Table 1: Correlation performance for the linear model us-
ing lexical (L), string (S) and affect (A) features
Feature headl. OnWN FNWN SMT mean
L 0.68 0.51 0.23 0.25 0.46
L+S 0.69 0.49 0.23 0.26 0.46
L+S+A 0.69 0.51 0.27 0.28 0.47
Results are shown in Tables 1 and 2. Results for
the linear run using subsets of the final feature set
are shown in Table 1. Lexical features (hit rates) are
obviously the most valuable features. String similar-
ities provided us with an improvement in the train-
Table 2: Correlation performance on the evaluation set.
Run headl. OnWN FNWN SMT mean
linear 0.69 0.51 0.27 0.28 0.47
length 0.65 0.51 0.25 0.28 0.46
adapt 0.62 0.51 0.33 0.30 0.46
ing set which is not reflected in the test set. Af-
fect proved valuable, particularly in the most diffi-
cult sets of FNWN and SMT.
Results for the three submission runs are shown
in Table 2. Our best run was the simplest one, using
a purely linear model and effectively no adaptation.
Adding a more aggressive adaptation strategy im-
proved results in the FNWN and SMT sets, so there
is definitely some potential, however the improve-
ment observed is nowhere near that observed in the
training data or the same task of SemEval 2012. We
have to question whether this improvement is an ar-
tifact of the rating distributions of these two sets
(SMT contains virtually only high ratings, FNWN
contains virtually only low ratings): such wild mis-
matches in priors among training and test sets can
be mitigated using more elaborate machine learning
algorithms (rather than employing better semantic
similarity features or algorithms). Overall the sys-
tem performs well in the two sets containing large
similarity rating ranges.
4 Conclusions
We have improved over our previous model of sen-
tence semantic similarity. The inclusion of string-
based similarities and more so of affective content
measures proved significant, but domain adaptation
provided mixed results. While expanding the model
to include more layers of similarity estimates is
clearly a step in the right direction, further work is
required to include even more layers. Using syntac-
tic information and more levels of abstraction (e.g.
concepts) are obvious next steps.
5 Acknowledgements
The first four authors have been partially funded
by the PortDial project (Language Resources for
Portable Multilingual Spoken Dialog Systems) sup-
ported by the EU Seventh Framework Programme
(FP7), grant number 296170.
106
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.
2012. Semeval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proc. SemEval, pages 385?393.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In Proc. *SEM.
Gnu aspell. http://www.aspell.net.
D. Ba?r, C. Biemann, I. Gurevych, and T. Zesch. 2012.
Ukp: Computing semantic textual similarity by com-
bining multiple content similarity measures. In Proc.
SemEval, pages 435?440.
M. Baroni and A. Lenci. 2010. Distributional mem-
ory: A general framework for corpus-based semantics.
Computational Linguistics, 36(4):673?721.
J. Bos and K. Markert. 2005. Recognising textual en-
tailment with logical inference. In Proceedings of the
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, page 628635.
M. Bradley and P. Lang. 1999. Affective norms for En-
glish words (ANEW): Stimuli, instruction manual and
affective ratings. Technical report C-1. The Center for
Research in Psychophysiology, University of Florida.
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. In In Compression and Com-
plexity of Sequences (SEQUENCES97, pages 21?29.
IEEE Computer Society.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32:13?47.
K. W. Church and P. Hanks. 1990. Word association
norms, mutual information, and lexicography. Com-
putational Linguistics, 16(1):22?29.
A. Finch, S. Y. Hwang, and E. Sumita. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Proceedings
of the 3rd International Workshop on Paraphrasing,
page 1724.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 363?370.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116?131.
S. Harabagiu and A. Hickl. 2006. Methods for Us-
ing Textual Entailment in Open-Domain Question An-
swering. In Proceedings of the 21st InternationalCon-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 905?912.
Z. Harris. 1954. Distributional structure. Word,
10(23):146?162.
E. Iosif and A. Potamianos. 2010. Unsupervised seman-
tic similarity computation between terms using web
documents. IEEE Transactions on Knowledge and
Data Engineering, 22(11):1637?1647.
E. Iosif and A. Potamianos. 2012. Semsim: Resources
for normalized semantic similarity computation using
lexical networks. In Proc. Eighth International Con-
ference on Language Resources and Evaluation, pages
3499?3504.
Elias Iosif and Alexandros Potamianos. 2013. Similarity
Computation Using Semantic Networks Created From
Web-Harvested Data. Natural Language Engineering,
(submitted).
E. Iosif, A. Potamianos, M. Giannoudaki, and K. Zer-
vanou. 2013. Semantic similarity computation for ab-
stract and concrete nouns using network-based distri-
butional semantic models. In 10th International Con-
ference on Computational Semantics (IWCS), pages
328?334.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using longest
common subsequence and skip-bigram statistics. In
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
P. Malakasiotis and I. Androutsopoulos. 2007. Learn-
ing textual entailment using svms and string similar-
ity measures. In Proceedings of of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pages 42?47.
P. Malakasiotis. 2009. Paraphrase recognition using ma-
chine learning to combine similarity measures. In Pro-
ceedings of the 47th Annual Meeting of ACL and the
4th Int. Joint Conference on Natural Language Pro-
cessing of AFNLP, pages 42?47.
N. Malandrakis, A. Potamianos, E. Iosif, and
S. Narayanan. 2011. Kernel models for affec-
tive lexicon creation. In Proc. Interspeech, pages
2977?2980.
N. Malandrakis, E. Iosif, and A. Potamianos. 2012.
DeepPurple: Estimating sentence semantic similarity
using n-gram regression models and web snippets. In
Proc. Sixth International Workshop on Semantic Eval-
uation (SemEval) ? The First Joint Conference on
Lexical and Computational Semantics (*SEM), pages
565?570.
H. Meng and K.-C. Siu. 2002. Semi-automatic acquisi-
tion of semantic structures for understanding domain-
107
specific natural language queries. IEEE Transactions
on Knowledge and Data Engineering, 14(1):172?181.
G. Miller and W. Charles. 1998. Contextual correlates
of semantic similarity. Language and Cognitive Pro-
cesses, 6(1):1?28.
G. Miller. 1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235?312.
S. Mirkin, L. Specia, N. Cancedda, I. Dagan, M. Dymet-
man, and S. Idan. 2009. Source-language entailment
modeling for translating unknown terms. In Proceed-
ings of the 47th AnnualMeeting of ACL and the 4th Int.
Joint Conference on Natural Language Processing of
AFNLP, pages 791?799.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 311?318.
D. Perez and E. Alfonseca. 2005. Application of the
bleu algorithm for recognizing textual entailments. In
Proceedings of the PASCAL Challenges Worshop on
Recognising Textual Entailment.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity in a taxanomy. In Proc. of In-
ternational Joint Conference for Artificial Intelligence,
pages 448?453.
F. Rinaldi, J. Dowdall, K. Kaljurand, M. Hess, and
D. Molla. 2003. Exploiting paraphrases in a question
answering system. In Proceedings of the 2nd Interna-
tional Workshop on Paraphrasing, pages 25?32.
H. Rubenstein and J. B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proceedings of the 22nd
International Conference on Computational Linguis-
tics, pages 849?856.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology, pages 173?180.
P. Turney. 2006. Similarity of semantic relations. Com-
putational Linguistics, 32(3):379?416.
F. S?aric?, G. Glavas?, M. Karan, J. S?najder, and B. Dal-
belo Bas?ic?. 2012. Takelab: Systems for measuring
semantic text similarity. In Proc. SemEval, pages 441?
448.
F. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine-learning approach to textual en-
tailment recognition. Natural Language Engineering,
15(4):551582.
108
