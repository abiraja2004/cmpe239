Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 9?16,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
An Empirical Study of Vietnamese Noun Phrase Chunking with
Discriminative Sequence Models
Le Minh Nguyen
School of Information Science, JAIST
nguyenml@jaist.ac.jp
Huong Thao Nguyen and Phuong Thai Nguyen
College of Technology, VNU
{thaonth, thainp}@vnu.edu.vn
Tu Bao Ho and Akira Shimazu
Japan Advanced Institute of Science and Technology
{bao,shimazu}@jaist.ac.jp
Abstract
This paper presents an empirical work
for Vietnamese NP chunking task. We
show how to build an annotation corpus of
NP chunking and how discriminative se-
quence models are trained using the cor-
pus. Experiment results using 5 fold cross
validation test show that discriminative se-
quence learning are well suitable for Viet-
namese chunking. In addition, by em-
pirical experiments we show that the part
of speech information contribute signifi-
cantly to the performance of there learning
models.
1 Introduction
Many Natural Language Processing applications
(i.e machine translation) require syntactic infor-
mation and tools for syntactic analysis. However,
these linguistic resources are only available for
some languages(i.e English, Japanese, Chines). In
the case of Vietnamese, currently most researchers
have focused on word segmentation and part of
speech tagging. For example, Nghiem et al
(Nghiem, Dinh, Nguyen, 2008) has developed a
Vietnamese POS tagging. Tu (Tu, Phan, Nguyen,
Ha, 2006) (Nguyen, Romary, Rossignol, Vu,
2006)(Dien, Thuy, 2006) have developed Viet-
namese word segmentation.
The processing of building tools and annotated
data for other fundamental tasks such as chunk-
ing and syntactic parsing are currently developed.
This can be viewed as a bottleneck for develop-
ing NLP applications that require a deeper under-
standing of the language. The requirement of de-
veloping such tools motives us to develop a Viet-
namese chunking tool. For this goal, we have
been looking for an annotation corpus for conduct-
ing a Vietnamese chunking using machine learn-
ing methods. Unfortunately, at the moment, there
is still no common standard annotated corpus for
evaluation and comparison regarding Vietnamese
chunking.
In this paper, we aim at discussing on how
we can build annotated data for Vietnamese text
chunking and how to apply discriminative se-
quence learning for Vietnamese text chunking. We
choose discriminative sequence models for Viet-
namese text chunking because they have shown
very suitable methods for several languages(i.e
English, Japanese, Chinese) (Sha and Pereira,
2005)(Chen, Zhang, and Ishihara, 2006) (Kudo
and Matsumoto, 2001). These presentative dis-
criminative models which we choose for conduct-
ing empirical experiments including: Conditional
Random Fields (Lafferty, McCallum, and Pereira,
2001), Support Vector Machine (Vapnik, 1995)
and Online Prediction (Crammer et al 2006). In
other words, because Noun Phrase chunks appear
most frequently in sentences. So, in this paper
we focus mainly on empirical experiments for the
tasks of Vietnamese NP chunking.
We plan to answer several major questions by
using empirical experiments as follows.
? Whether or not the discriminative learning
models are suitable for Vietnamese chunking
problem?
? We want to know the difference of SVM,
Online Learning, and Conditional Random
Fields for Vietnamese chunking task.
? Which features are suitable for discriminative
learning models and how they contribute to
the performance of Vietnamese text chunk-
ing?
The rest of this paper is organized as follows:
Section 2 describes Vietnamese text chunking with
discriminative sequence learning models. Section
3 shows experimental results and Section 4 dis-
9
cusses the advantage of our method and describes
future work.
2 Vietnamese NP Chunking with
Discriminative Sequence Learning
Noun Phrase chunking is considered as the task
of grouping a consecutive sequence of words into
a NP chunk lablel. For example: ?[NP Anh Ay
(He)] [VP thich(likes)] [NP mot chiec oto(a car)]
?
Before describing NP chunking tasks, we
summarize the characteristic of Vietnamese lan-
guage and the background of Conditional Ran-
dom Fields, Support Vector Machine, and Online
Learning. Then, we present how to build the an-
notated corpus for the NP chunking task.
2.1 The characteristic of Vietnamese Words
Vietnamese syllables are elementary units that
have one way of pronunciation. In documents,
they are usually delimited by white-space. Be-
ing the elementary units, Vietnamese syllables are
not undivided elements but a structure. Generally,
each Vietnamese syllable has all five parts: first
consonant, secondary vowel, main vowel, last con-
sonant and a tone mark. For instance, the sylla-
ble tu.n (week) has a tone mark (grave accent), a
first consonant (t), a secondary vowel (u), a main
vowel () and a last consonant (n). However, except
for main vowel that is required for all syllables,
the other parts may be not present in some cases.
For example, the syllable anh (brother) has no tone
mark, no secondary vowel and no first consonant.
In other case, the syllable hoa (flower) has a sec-
ondary vowel (o) but no last consonant.
Words in Vietnamese are made of one or more
syllables which are combined in different ways.
Based on the way of constructing words from syl-
lables, we can classify them into three categories:
single words, complex words and reduplicative
words (Mai,Vu, Hoang, 1997).
The past of speechs (Pos) of each word in Viet-
namese are mainly sketched as follows.
A Noun Phrase (NP) in Vietnamese consists of
three main parts as follows: the noun center, the
prefix part, and the post fix part. The prefix and
postfix are used to support the meaning of the NP.
For example in the NP ?ba sinh vien nay?, the
noun center is ?sinh vien?, and the prefix is ?ba
(three)?, the postfix is ?nay?.
Vietnamese Tag Equivalent to English Tag
CC Coordinating conjunction)
CD Cardinal number)
DT Determiner)
V Verb
P Preposition
A Adjective
LS List item marker
MD Modal
N Noun
Table 1: Part of Speeches in Vietnamese
2.2 The Corpus
We have collected more than 9,000 sentences from
several web-sites through the internet. After that,
we then applied the segmentation tool (Tu, Phan,
Nguyen, Ha, 2006) to segment each sentences
into a sequence of tokens. Each sequence of
tokens are then represented using the format of
CONLL 2000. The details are sketched as follows.
Each line in the annotated data consists of
three columns: the token (a word or a punc-
tuation mark), the part-of-speech tag of the to-
ken, and the phrase type label (label for short)
of the token. The label of each token indicates
whether the token is outside a phrase (O), starts
a phrase (B-?PhraseType?), or continues a phrase
(I-?PhraseType?).
In order to save time for building annotated
data, we made a set of simple rules for automat-
ically generating the chunking data as follows. If
a word is not a ?noun?, ?adjective?, or ?article? it
should be assigned the label ?O?. The consecu-
tive words are NP if they is one of type as follows:
?noun noun?; ?article noun?, ?article noun adjec-
tive?. After generating such as data, we ask an
expert about Vietnamese linguistic to correct the
data. Finally, we got more than 9,000 sentences
which are annotated with NP chunking labels.
Figure 1 shows an example of the Vietnamese
chunking corpus.
2.3 Discriminative Sequence Learning
In this section, we briefly introduce three dis-
criminative sequence learning models for chunk-
ing problems.
2.3.1 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty,
McCallum, and Pereira, 2001) are undirected
graphical models used to calculate the conditional
10
Figure 1: An Example of the Vietnamese chunk-
ing corpus
probability of values on designated output nodes,
given values assigned to other designated input
nodes for data sequences. CRFs make a first-order
Markov independence assumption among output
nodes, and thus correspond to finite state machine
(FSMs).
Let o = (o1, o2, . . . , oT ) be some observed in-
put data sequence, such as a sequence of words in
a text (values on T input nodes of the graphical
model). Let S be a finite set of FSM states, each is
associated with a label l such as a clause start po-
sition. Let s = (s1, s2, . . . , sT ) be some sequences
of states (values on T output nodes). CRFs de-
fine the conditional probability of a state sequence
given an input sequence to be
P?(s|o) = 1Zo exp
( T?
t=1
F (s, o, t)
)
(1)
where Zo =
?
s exp
(?T
t=1 F (s, o, t)
)
is a nor-
malization factor over all state sequences. We de-
note ? to be the Kronecker-?. Let F (s, o, t) be the
sum of CRFs features at time position t:
?
i
?ifi(st?1, st, t) +
?
j
?jgj(o, st, t) (2)
where fi(st?1, st, t) = ?(st?1, l?)?(st, l) is a
transition feature function which represents se-
quential dependencies by combining the label l?
of the previous state st?1 and the label l of the
current state st, such as the previous label l? =
AV (adverb) and the current label l = JJ (adjec-
tive). gj(o, st, t) = ?(st, l)xk(o, t) is a per-state
feature function which combines the label l of cur-
rent state st and a context predicate, i.e., the binary
function xk(o, t) that captures a particular prop-
erty of the observation sequence o at time position
t. For instance, the current label is JJ and the cur-
rent word is ?conditional?.
Training CRFs is commonly performed by max-
imizing the likelihood function with respect to
the training data using advanced convex optimiza-
tion techniques like L-BFGS. Recently, there are
several works apply Stochastic Gradient Descent
(SGD) for training CRFs models. SGD has been
historically associated with back-propagation al-
gorithms in multilayer neural networks.
And inference in CRFs, i.e., searching the most
likely output label sequence of an input observa-
tion sequence, can be done using Viterbi algo-
rithm.
2.3.2 Support Vector Machines
Support vector machine (SVM)(Vapnik, 1995)
is a technique of machine learning based on sta-
tistical learning theory. The main idea behind
this method can be summarized as follows. Sup-
pose that we are given l training examples (xi, yi),
(1 ? i ? l), where xi is a feature vector in n di-
mensional feature space, and yi is the class label
{-1, +1 } of xi.
SVM finds a hyperplane w.x+b = 0 which cor-
rectly separates training examples and has maxi-
mum margin which is the distance between two
hyperplanes w ? x + b ? 1 and w ? x + b ? ?1.
Finally, the optimal hyperplane is formulated as
follows:
f(x) = sign
( l?
1
?iyiK(xi, x) + b
)
(3)
where ?i is the Lagrange multiple, and K(x?, x??)
is called a kernel function, which calculates sim-
ilarity between two arguments x? and x??. For in-
stance, the Polynomial kernel function is formu-
lated as follows:
K(x?, x??) = (x? ? x??)p (4)
SVMs estimate the label of an unknown example
x whether the sign of f(x) is positive or not.
Basically, SVMs are binary classifier, thus we
must extend SVMs to multi-class classifier in or-
11
der to classify three or more classes. The pair-
wise classifier is one of the most popular meth-
ods to extend the binary classification task to that
of K classes. Though, we leave the details to
(Kudo and Matsumoto, 2001), the idea of pairwise
classification is to build K.(K-1)/2 classifiers con-
sidering all pairs of classes, and final decision is
given by their weighted voting. The implementa-
tion of Vietnamese text chunking is based on Yam-
cha (V0.33)1.
2.3.3 Online Passive-Aggressive Learning
Online Passive-Aggressive Learning (PA) was
proposed by Crammer (Crammer et al 2006) as
an alternative learning algorithm to the maximize
margin algorithm. The Perceptron style for nat-
ural language processing problems as initially pro-
posed by (Collins, 2002) can provide to state of
the art results on various domains including text
segmentation, syntactic parsing, and dependency
parsing. The main drawback of the Perceptron
style algorithm is that it does not have a mech-
anism for attaining the maximize margin of the
training data. It may be difficult to obtain high
accuracy in dealing with hard learning data. The
online algorithm for chunking parsing in which
we can attain the maximize margin of the training
data without using an optimization technique. It
is thus much faster and easier to implement. The
details of PA algorithm for chunking parsing are
presented as follows.
Assume that we are given a set of sentences
xi and their chunks yi where i = 1, ..., n. Let
the feature mapping between a sentence x and
a sequence of chunk labels y be: ?(x, y) =
?1(x, y),?2(x, y), ...,?d(x, y) where each fea-
ture mapping ?j maps (x, y) to a real value. We
assume that each feature ?(x, y) is associated with
a weight value. The goal of PA learning for chunk-
ing parsing is to obtain a parameter w that min-
imizes the hinge-loss function and the margin of
learning data.
Algorithm 1 shows briefly the Online Learning
for chunking problem. The detail about this al-
gorithm can be referred to the work of (Crammer
et al 2006). In Line 7, the argmax value is com-
puted by using the Viterbi algorithm which is sim-
ilar to the one described in (Collins, 2002). Algo-
rithm 1 is terminated after T round.
1Yamcha is available at
http://chasen.org/ taku/software/yamcha/
Input: S = (xi; yi), i = 1, 2, ..., n in which1
xi is the sentence and yi is a sequence of
chunks
Aggressive parameter C2
Output: the model3
Initialize: w1 = (0, 0, ..., 0)4
for t=1, 2... do5
Receive an sentence xt6
Predict y?t = argmaxy?Y (wt.?(xt, yt))7
Suffer loss: lt =
wt.?(xt, y?t )? wt.?(xt, yt) +
??(yt, y?t )
Set:?t = lt||?(xt,y?t )??(xt,yt)||28
Update:9
wt+1 = wt + ?t(?(xt, yt)? ?(xt, y?t ))
end10
Algorithm 1: The Passive-Aggressive algo-
rithm for NP chunking.
2.3.4 Feature Set
Feature set is designed through features template
which is shown in Table 2. All edge features obey
the first-order Markov dependency that the label
(l) of the current state depends on the label (l?)
of the previous state (e.g., ?l = I-NP? and ?l? =
B-NP?). Each observation feature expresses how
much influence a statistic (x(o, i)) observed sur-
rounding the current position i has on the label
(l) of the current state. A statistic captures a par-
ticular property of the observation sequence. For
instance, the observation feature ?l = I-NP? and
?word?1 is the? indicates that the label of the cur-
rent state should be I-NP (i.e., continue a noun
phrase) if the previous word is the. Table 2 de-
scribes both edge and observation feature tem-
plates. Statistics for observation features are iden-
tities of words, POS tags surrounding the current
position, such as words and POS tags at ?2, ?1,
1, 2.
We also employ 2-order conjunctions of the cur-
rent word with the previous (w?1w0) or the next
word (w0w1), and 2-order and 3-order conjunc-
tions of two or three consecutive POS tags within
the current window to make use of the mutual de-
pendencies among singleton properties. With the
feature templates shown in Table 2 and the feature
rare threshold of 1 (i.e., only features with occur-
rence frequency larger than 1 are included into the
discriminative models)
12
Edge feature templates
Current state: si Previous state: si?1
l l?
Observation feature templates
Current state: si Statistic (or context predicate) templates: x(o, i)
l w?2; w?1; w0; w1; w2; w?1w0; w0w1;
t?2; t?1; t0; t1; t2;
t?2t?1; t?1t0; t0t1; t1t2; t?2t?1t0;
t?1t0t1; t0t1t2
Table 2: Feature templates for phrase chunking
3 Experimental Results
We evaluate the performance of using several se-
quence learning models for the Vietnamese NP
chunking problem. The data of more than 9,000
sentences is evaluated using an empirical experi-
ment with 5 fold cross validation test. It means
we used 1,800 and 7,200 sentences for testing
and training the discriminative sequence learning
models, respectively. Note that the evaluation
method is used the same as CONLL2000 did. We
used Precision, Recall, and F-Measure in which
Precision measures how many chunks found by
the algorithm are correct and the recall is per-
centage of chunks defined in the corpus that were
found by the chunking program.
Precision = #correct?chunk#numberofchunks
Recall = #correct?chunks#numerofchunksinthecorpus
F?measure =2? Precision? RecallPrecision + Recall
To compute the scores in our experiments, we
utilized the evaluation tool (conlleval.pl) which is
available in CONLL 2000 (Sang and Buchholz,
2000, ).
Figure 2 shows the precision scores of three
methods using 5 Folds cross validation test. It
reports that the CRF-LBFGS attain the highest
score. The SVMs and CRF-SGD are comparable
to CRF-LBFGS. The Online Learning achieved
the lowest score.
Figure 3 shows the recall scores of three CRFs-
LBFGS, CRFs-SGD, SVM, and Online Learning.
The results show that CRFs-SGD achieved the
highest score while the Online Learning obtained
the lowest score in comparison with others.
Figure 4 and Figure 5 show the F-measure and
accuracy scores using 5 Folds Cross-validation
Figure 2: Precision results in 5 Fold cross valida-
tion test
Test. Similar to these results of Precision and Re-
call, CRFs-LBFGS was superior to the other ones
while the Online Learning method obtained the
lowest result.
Table 3 shows the comparison of three discrim-
inative learning methods for Vietnamese Noun
Phrase chunking. We compared the three se-
quence learning methods including: CRFs using
the LBFGS method, CRFs with SGD, and On-
line Learning. Experiment results show that the
CRFs-LBFGS is the best in comparison with oth-
ers. However, the computational times when train-
ing the data is slower than either SGD or Online
Learning. The SGD is faster than CRF-LBFS ap-
proximately 6 times. The SVM model obtained a
comparable results with CRFs models and it was
superior to Online Learning. It yields results that
were 0.712% than Online Learning. However, the
SVM?s training process take slower than CRFs
and Online Learning. According to our empirical
investigation, it takes approximately slower than
CRF-SGF, CRF-LBFGS as well as Online Learn-
ing.
13
Figure 3: Recall result in 5 Fold cross validation
test
Figure 4: The F-measure results of 5 Folds Cross-
validation Test
Note that we used FlexCRFs (Phan, Nguyen,
Tu , 2005) for Conditional Random Fields us-
ing LBFGS, and for Stochastic Gradient Descent
(SGD) we used SGD1.3 which is developed by
Leon Bottou 2.
Methods Precision Recall F1
CRF-LBGS 80.85 81.034 80.86
CRF-SGD 80.74 80.66 80.58
Online-PA 80.034 80.13 79.89
SVM 80.412 80.982 80.638
Table 3: Vietnamese Noun Phrase chunking per-
formance using Discriminative Sequence Learn-
ing (CRFs, SVM, Online-PA)
In order to investigate which features are ma-
jor effect on the discriminative learning models for
Vietnamese Chunking problems, we conduct three
experiments as follows.
2http://leon.bottou.org/projects/sgd
Figure 5: The accuracy scores of four methods
with 5 Folds Cross-validation Test
? Cross validation test for three modes without
considering the edge features
? Cross validation test for three models without
using POS features
? Cross validation test for three models without
using lexical features
? Cross validation test for three models without
using ?edge features template? features
Note that the computational time of training
SVMs model is slow, so we skip considering fea-
ture selection for SVMs. We only consider feature
selection for CRFs and Online Learning.
Feature Set LBFGS SGD Online
Full-Features 80.86 80.58 79.89
Without-Edge 80.91 78.66 80.13
Without-Pos 62.264 62.626 59.572
Without-Lex 77.204 77.712 75.576
Table 4: Vietnamese Noun Phrase chunking per-
formance using Discriminative Sequence Learn-
ing (CRFs, Online-PA)
Table 4 shows that the Edge features have an
impact to the CRF-SGD model while it do not
affect to the performance of CRFs-LBFGS and
Online-PA learning. Table 4 also indicates that
the POS features are severed as important features
regarding to the performance of all discrimina-
tive sequence learning models. As we can see,
if one do not use POS features the F1-score of
each model is decreased more than 20%. We also
remark that the lexical features contribute an im-
portant role to the performance of Vietnamese text
14
Figure 6: F-measures of three methods with different feature set
chunking. If we do not use lexical features the
F1-score of each model is decreased till approxi-
mately 3%. In conclusion, the POS features signif-
icantly effect on the performance of the discrimi-
native sequence models. This is similar to the note
of (Chen, Zhang, and Ishihara, 2006).
Figure 6 reports the F-Measures of using dif-
ferent feature set for each discriminative models.
Note that WPos, WLex, and WEdge mean without
using Pos features, without using lexical features,
and without using edge features, respectively. As
we can see, the CRF-LBFGs always achieved the
best scores in comparison with the other ones and
the Online Learning achieved the lowest scores.
4 Conclusions
In this paper, we report an investigation of devel-
oping a Vietnamese Chunking tool. We have con-
structed an annotation corpus of more than 9,000
sentences and exploiting discriminative learning
models for the NP chunking task. Experimen-
tal results using 5 Folds cross-validation test have
showed that the discriminative models are well
suitable for Vietnamese phrase chunking. Con-
ditional random fields show a better performance
in comparison with other methods. The part of
speech features are known as the most influence
features regarding to the performances of discrim-
inative models on Vietnamese phrases chunking.
What our contribution is expected to be useful
for the development of Vietnamese Natural Lan-
guage Processing. Our results and corpus can be
severed as a very good baseline for Natural Lan-
guage Processing community to develop the Viet-
namese chunking task.
There are still room for improving the perfor-
mance of Vietnamese chunking models. For ex-
ample, more attention on features selection is nec-
essary. We would like to solve this in future work.
Acknowledgments
The constructive comments and helpful sugges-
tions from three anonymous reviewers are greatly
appreciated. This paper is supported by JAIST
Grant for Research Associates and a part from a
national project named Building Basic Resources
and Tools for Vietnamese Language and Speech
Processing, KC01.01/06-10.
References
M. Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP 2002.
K. Crammer et al 2006. Online Passive-Aggressive
Algorithm. Journal of Machine Learning Research,
2006
W. Chen, Y. Zhang, and H. Ishihara 2006. An em-
pirical study of Chinese chunking. In Proceedings
COLING/ACL 2006
Dinh Dien, Vu Thuy 2006. A maximum entropy
approach for vietnamese word segmentation. In
Proceedings of the IEEE - International Conference
on Computing and Telecommunication Technolo-
gies RIVF 2006: 248-253
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In the proceed-
15
ings of International Conference on Machine Learn-
ing (ICML), pp.282-289, 2001
N.C. Mai, D.N. Vu, T.P. Hoang. 1997. Foundations
of linguistics and Vietnamese. Education Publisher
(1997) 142. 152
Thi Minh Huyen Nguyen, Laurent Romary, Mathias
Rossignol, Xuan Luong Vu. 2006. A lexicon
for Vietnamese language processing. Language Re-
seourse Evaluation (2006) 40:291-309.
Minh Nghiem, Dien Dinh, Mai Nguyen. 2008. Im-
proving Vietnamese POS tagging by integrating a
rich feature set and Support Vector Machines. In
Proceedings of the IEEE - International Conference
on Computing and Telecommunication Technolo-
gies RIVF 2008: 128?133.
X.H. Phan, M.L. Nguyen, C.T. Nguyen. Flex-
CRFs: Flexible Conditional Random Field Toolkit.
http://flexcrfs.sourceforge.net, 2005
T. Kudo and Y. Matsumoto. 2001. Chunking with
Support Vector Machines. The Second Meeting of
the North American Chapter of the Association for
Computational Linguistics (2001)
F. Sha and F. Pereira. 2005. Shallow Parsing with
Conditional Random Fields. Proceedings of HLT-
NAACL 2003 213-220 (2003)
C.T. Nguyen, T.K. Nguyen, X.H. Phan, L.M. Viet-
namese Word Segmentation with CRFs and SVMs:
An Investigation. 2006. The 20th Pacific Asia Con-
ference on Language, Information, and Computation
(PACLIC), 1-3 November, 2006, Wuhan, China
Tjong Kim Sang and Sabine Buchholz. 2000. Intro-
duction to the CoNLL-2000 Shared Task: Chunk-
ing. Proceedings of CoNLL-2000 , Lisbon, Portugal,
2000.
V. Vapnik. 1995. The Natural of Statistical Learning
Theory. New York: Springer-Verlag, 1995.
16
