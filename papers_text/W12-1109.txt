JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 77?90,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Enrichir et raisonner sur des espaces s?mantiques pour
l?attribution de mots-cl?s
Adil El Ghali1, 2 Daniel Hromada1 Kaoutar El Ghali
(1) LUTIN UserLab, 30, avenue Corentin Cariou, 75930 Paris cedex 19
(2) IBM CAS France, 9 rue de Verdun, 94253 Gentilly
elghali@lutin-userlab.fr
R?SUM?
Cet article pr?sent le syst?me hybride et multi-modulaire d?extraction des mots-cl?s ? partir
de corpus des articles scientifiques. Il s?agit d?un syst?me multi-modulaire car int?gre en soi
les traitements 1) morphosyntaxiques (lemmatization et chunking) 2) s?mantiques (Reflective
Random Indexing) ainsi que 3) pragmatiques (mod?lis?s par les r?gles de production). On
parle aussi d?un syst?me hybride car il ?tait utilis? -sans modification majeure- pour trouver des
solutions aux toutes les deux pistes du DEFT 2012. Pour la Piste 1 - o? une terminologie ?tait
fournie - nous obt?nmes le F-score de 0.9488 ; pour la Piste 2 ? o? aucune liste des mots cl?s
candidates n??tait pas fourni au pr?alable ? le F-score obtenu est 0.5874.
ABSTRACT
Enriching and reasoning on semantic spaces for keyword extraction
This article presents a multi-modular hybrid system for extraction of keywords from corpus of
scientific articles. System is multi-modular because it integrates components executing transfor-
mations on 1) morphosyntactic level (lemmatization and chunking) 2) semantic level (Reflected
Random Indexing), as well as upon more 3) ? pragmatic ? aspects of processed documents,
modeled by production rules. The system is hybrid because it was able to address both tracks of
DEFT2012 competition ? a ?reduced search-space? scenario of Track 1, whose objective was to
map the content of a scientific article upon one among the members of a ? terminological list ? ;
as well as more ? real-life ? scenario of Track2 within which no list was associated to documents
contained in the corpus. In both Tracks, the system hereby presented has obtained the an F-score
of 0.9488 for the Track1, and 0.5874 for the Track2.
MOTS-CL?S : Extraction de mots-cl?s, Espaces s?mantiques, RRI, R?seau bay?sien, R?gles de production,
Chunking.
KEYWORDS: Keyword extraction, Semantic spaces, RRI, Bayesian Network, Production Rules, Chunking.
1 Introduction
L??dition 2012 du d?fi fouille de textes (DEFT) a pour th?me l?identification automatique des
mots-cl?s indexant le contenu d?articles publi?s dans des revues scientifiques. Deux pistes ont ?t?
propos?es : dans la premi?re (Piste 1) la terminologie des mots-cl?s est fournie, alors que dans la
deuxi?me (Piste 2) l?attribution des mots-cl?s devait se faire sans terminologie.
77
Pour la r?alisation de cette t?che nous avons d?cid?, dans la continuit? de ce que nous avions
r?alis? en 2011 (El Ghali, 2011), de repr?senter le sens des termes et des documents du corpus
dans des espaces s?mantiques utilisant la variante Reflective Random Indexing (RRI). Le choix de
RRI une variante de Random Indexing (RI) (Sahlgren, 2006) est motiv? par les bonnes propri?t?s
de cette m?thode, h?rit?es de RI et qui sont largement d?crites dans la litt?rature (Cohen et al,
2010a). Mais une de ces propri?t?s moins connue et comment?e s?est r?v?l?e particuli?rement
pertinente pour le probl?me pos? dans le cadre de cette ?dition du DEFT, ? savoir l?uniformit? de
l?espace s?mantique : en effet, les vecteurs construits par RRI pour repr?senter les documents et
les termes du corpus sont ? comparables ?.
Dans la m?thode que nous avons d?velopp? pour cette ?dition du DEFT, nous avons voulu
r?pondre ? deux questions principales :
1. quel serait l?apport d?un pr?-traitement linguistique de surface aux espaces s?mantiques ? et
en quoi pourrait-on comparer ces pr?-traitements aux m?thodes de constructions d?espaces
s?mantiques permettant de capturer des ?l?ments de structure ?
2. peut-on am?liorer les m?thodes de scoring d?velopp?es dans les pr?c?dentes ?ditions
du DEFT en utilisant les derni?res avanc?es en Intelligence artificielle, notamment le
raisonnement ? base de r?gles et les graphes probabilistes, encodant respectivement des
r?gles g?n?rales sur le choix des mots-cl?s et des informations incertaines issues du corpus
d?apprentissage ?
La premi?re question s?imposait naturellement du fait qu?une grande partie des mots-cl?s qui ont
?t? fournis pour la Piste 1 sont en fait des groupes de mots et que leurs cat?gories morphosyn-
taxiques et grammaticales respectait des r?gles assez simples. Pour pouvoir traiter les mots-cl?s
compos?s de plusieurs mots, certaines m?thodes de repr?sentation de textes en espaces s?man-
tiques telles que BEAGLE (Jones et Mewhort, 2007), PSI (Cohen et al, 2009), ou encore RRI
avec des indexes positionnels (Widdows et Cohen, 2010), permettent d?encoder les informations
sur l?ordre des mots. La deuxi?me question est n?e du fait que l?on disposait d?informations de
nature diff?rentes qui pouvait aider ? attribuer correctement des mots-cl?s : sur la s?mantique,
sur la distribution des mots-cl?s, sur la structure, sur les revues dont sont issues les articles ...
Ces informations pouvaient ?tre difficilement encod?es dans un seul formalisme de d?cision.
Nous avons donc d?cid? de d?finir une proc?dure de d?cision pour l?attribution de mots-cl?s
qui combine des r?gles symboliques avec des r?seaux bay?siens, avec les R?gles de production
Probabilistes (A?t-Kaci et Bonnard, 2011).
Nous avons fait le choix d?aborder les deux pistes du d?fi de cette ann?e de mani?re sensiblement
identique, les m?mes m?thodes ont ?t? utilis?es pour les deux pistes. Pour ce faire, nous avons
construit une terminologie pour la Piste 2. Cette terminologie est une liste de mots-cl?s candidats
?tablie en utilisant un espace s?mantique et un pr?-traitement linguistique de surface.
L?article est organis? comme suit : nous commen?ons par pr?senter dans la section 2 une analyse
du corpus et des informations qui peuvent en ?tre extraite et qui sont utiles pour la t?che
d?attribution de mots-cl?s. Ensuite, dans la section 3, nous rappelons bri?vement le principe
de fonctionnement de RRI, puis nous d?crivons comment incorporer les informations issue
du pr?-traitement linguistique dans les espaces s?mantiques, mais aussi comment la liste des
candidats mots-cl?s pour la Piste 2 est construite. Dans la section 4 nous pr?sentons le principe
de fonctionnement de la proc?dure de d?cision pour l?attribution des mots-cl?s. Enfin, dans la
section 5 nous d?taillons les caract?ristiques de chacune des ex?cutions et discutons les r?sultats
avant de conclure.
78
2 Le Corpus
2.1 Statistiques g?n?rales de corpus d?apprentissage
2.1.1 Piste 1
Pour la Piste 1, il y a 140 documents dans le corpus d?apprentissage. Les documents proviennent
de 4 revues diff?rentes, l?identificateur de la revue ?tant encod? dans le nom du fichier XML
contenant l?article.
La liste terminologique ? i.e. la liste contenant tous les termes uniques choisies comme un mot
cl? pour un document dans le corpus - associ?e au corpus d?apprentissage contient Tappr = 666termes uniques.
Les nombres des mots-cl?s associ?s sont fournis pour chaque document du corpus d?apprentissage
aussi bien que du corpus de test. En somme, ?iNappri = 754. En moyenne, chaque article decorpus d?apprentissage a :
mean(Nappr) = 5.386 ; median(Nappr) = 5;min(Nappr) = 1;max(Nappr) = 13; sd(Nappr) = 1.344
Etant donn? que ?iNappri > Tappr , il est ?vident qu?il y a des termes qui sont d?finis comme motscl?s pour plusieurs articles. Le principe de bijection 1 terme ? 1 article n?est pas donc applicable.
Plus pr?cis?ment, pour le corpus d?apprentissage, 604 mots cl?s sont associ?s ? un seul article,
46 en sont associ?s ? deux, 10 ? trois, quatre mots cl?s (i.e. ? identit? ?, ? interpr?tation ?, ?
enseignement de la traduction ?, ? traduction ?) sont chacun associ?s ? quatre articles, tandis que
le terme ? humanitaire ? est d?fini comme mot cl? pour cinq articles et le terme ? mondialisation ?
pour sept articles.
On note aussi que parmi 62 termes qui sont associ?s ? plus qu?un article, seulement 26 (i.e.
41,9%) sont associ?s aux articles appartenants ? plus qu?une revue.
Les analyses fr?quentielles pr?liminaires montrent aussi que dans 141 parmi 740 cas, le mot cl?
ne se trouve pas dans le corps ni r?sum? d?article auquel il est associ?. En d?autres termes, pour
plus que 19% des mots cl?s, la fr?quence de leur occurrence dans l?article est z?ro, c?est donc
plus qu??vident qu?il faut aller au-del? des fr?quences ? brutes ? si on veut que notre syst?me
d?extraction des mots cl?s ait la pr?cision > 80% (la Figure 1 montre les fr?quences d?occurrence
des mots-cl?s dans les documents associ?s).
L?objectif de la Piste 1 est donc de concevoir le syst?me qui, partant de fichiers de corpus
d?apprentissage contenant Dappr ? Tappr = 140 * 666 = 93240 couplages (document, terme)serait capable ? d?terminer les couples ayant ?t? ?tablis par les auteurs de leurs documents.
2.1.2 Piste 2
Le corpus d?apprentissage contient 142 documents. Contrairement ? la Piste 1, aucune liste
terminologique n?est fournie, l?espace de recherche dans lequel on cherche les candidats cens?
d??tre les mots cl?s est donc beaucoup plus grande. Mais les quantit? des mots cl?s associ?s
au diff?rents articles sont pr?sents. Gr?ce ? ces quantit?s fournis dans la balise <nombre> des
documents XML, on sait sans regarder au fichier de r?f?rence que la distribution de ?iNappri = 763
79
FIGURE 1 ? Cca 19% (en rouge) des mots cl?s de corpus d?apprentissage ne figurent pas dans les
documents auxquels ils sont attribu?s
associations entre mots cl?s et articles dispose de propri?t?s suivantes :
mean(Nappr) = 5.411;median(Nappr) = 5;min(Nappr) = 3;max(Nappr) = 13; sd(Nappr) = 1.404.
L?analyse de fichier de r?f?rence r?v?le que parmi 681 termes qui couvrent l?ensemble de tous
les mots cl?s du corpus d?apprentissage de piste2 , 627 en sont associ?s ? un seul article, 37 ?
deux, 12 ? trois, deux termes ? (? humanitaire ? et ? didactique ?) ? quatre articles, les termes ?
identit? ? et ? culture ? ?tant associ? ? cinq articles et le terme ? traduction ? ? huit documents.
?tant donn? que l?information concernant l?appartenance d?un article ? une revue est pr?sente,
on sait aussi que parmi 54 termes associ?s ? plus qu?un article, seulement 18 (i.e. 33.3%) sont
associ?s ? plus qu?une revue.
L?analyse des fr?quences de mots cl?s dans les articles associ?s donne les r?sultats qui vont dans
le m?me sens que ceux de la Piste 1 : dans 145 cas (19%), les mots cl?s n?appara?ssent pas dans
l?article auquel ils ?taient associ?s !
2.2 Statistiques g?n?rales du corpus de test
2.2.1 Piste 1
Le corpus de test de la Piste 1 contient Dtest = 94 documents dans . La liste terminologique ducorpus de test contient 478 termes uniques. Parmi ces 478 termes-candidats, 435 en sont associ?s
avec un seul document, 34 aux deux documents diff?rentes, quatre termes sont associ?s aux
trois articles, et quatre termes aux quatre articles, le terme le plus r?ussi comme mot cl? ?tant ?
identit? ? lui-m?me associ? au six articles. Parmi les 43 termes associ?s ? plus d?un article, 20
(i.e. 46,5%) sont associ?s aux articles appartenants ? plus d?une revue.
La distribution de la somme du nombre des mots cl?s associ?s aux articles du corpus de test de la
80
Piste 1 ( ?iNtest i = 537) dispose de propri?t?s suivantes :
mean(Ntest) = 5.712;median(Ntest) = 5;min(Ntest) = 1;max(Ntest) = 12; sd(Ntest) = 1.701.
2.2.2 Piste 2
La distribution de ?iNtest i = 484 mots cl?s attribu?s aux 93 documents contenus dans le corpusde test de la Piste 2 est caract?ris? par les mesures suivantes :
mean(Ntest) = 5.204;median(Ntest) = 5;min(Ntest) = 2;max(Ntest) = 10; sd(Ntest) = 1.323.
La consultation des fichiers de r?f?rence obtenus apr?s la fin de la phase competitive de DEFT2012
nous permets ? savoir que parmi 35 termes associ?s ? plus qu?un article, seulement 10 (i.e. 28,6%)
sont associ?s aux articles appartenants ? plus d?une revue.
2.3 Que peut-on apprendre d?autre du corpus ?
Un rapide parcours du corpus de d?apprentissage et de la terminologie fournie pour la Piste 1,
nous montre qu?au del? des fr?quences, les mots-cl? choisis par les auteurs respectent quelques
r?gles :
? les mots-cl?s sont diff?rents entre eux : les auteurs n?utilisent que rarement des mots-cl?s tr?s
proches ;
? ils sont assez souvent repris dans l?introduction et la conclusion de l?article ;
? leur cat?gorie morphosyntaxique ou grammaticale est tr?s rarement ? verbale ?, les mot-cl?s
les plus utilis?s sont des noms (communs ou propres), des adjectifs ou des groupes nominaux ;
Par ailleurs, comme on pouvait s?y attendre les mots-cl?s sont fortement li?s s?mantiquement au
document, comme le montre la figure 2 :
FIGURE 2 ? Similarit?s document-mots-cl?s (min, max, mean) vs. document-terminologie (mean)
81
3 Espaces s?mantiques
Les mod?les de repr?sentation vectorielle de la s?mantique des mots sont une famille de mod?les
qui repr?sentent la similarit? s?mantique entre les mots en fonction de l?environnement textuel
dans lequel ces mots apparaissent. La distribution de co-occurrence des mots dans le corpus est
rassembl?e, analys?e puis transform?e en espace s?mantique dans lequel les mots sont repr?sent?s
comme des vecteurs dans un espace vectoriel de grande dimension. LSA (Landauer et Dumais,
1997), HAL (Lund et Burgess, 1996) et RI (Kanerva et al, 2000) en sont quelques exemples. Ces
mod?les sont bas?s sur l?hypoth?se distributionnelle de (Harris, 1968) qui affirme que les mots
qui apparaissent dans des contextes similaires ont un sens similaire. La caract?risation de l?unit?
de contexte est une probl?matique commune ? toutes ces m?thodes, sa d?finition est diff?rente
suivant les mod?les. Par exemple, LSA construit une matrice mot-document dans laquelle chaque
cellule ai j contient la fr?quence d?un mot i dans une unit? de contexte j. HAL d?finit une fen?treflottante de n mots qui parcourt chaque mot du corpus, puis construit une matrice mot-mot dans
laquelle chaque cellule ai j contient la fr?quence ? laquelle un mot i co-occure avec un mot jdans la fen?tre pr?c?demment d?finie.
Diff?rentes m?thodes math?matiques permettant d?extraire la signification des concepts, en
r?duisant la dimensionnalit? de l?espace de co-occurence, sont appliqu?es ? la distribution
des fr?quences stock?es dans la matrice mot-document ou mot-mot. Le premier objectif de
ces traitements math?matiques est d?extraire les ?patrons? qui rendent compte des variations
de fr?quences et qui permettent d??liminer ce qui peut ?tre consid?r? comme du ? bruit ?.
LSA emploie une m?thode g?n?rale de d?composition lin?aire d?une matrice en composantes
ind?pendantes : la d?composition de valeur singuli?re (SVD). Dans HAL la dimension de l?espace
est r?duite en maintenant un nombre restreint de composantes principales de la matrice de
co-occurrence. ? la fin de ce processus de r?duction de dimensionnalit?, la similitude entre deux
mots peut ?tre calcul?e selon diff?rentes m?thodes. Classiquement, la valeur du cosinus de l?angle
entre deux vecteurs correspondant ? deux mots ou ? deux groupes de mots est calcul?e afin
d?approximer leur similarit? s?mantique.
3.1 Reflective Random Indexing
La m?thode de construction d?espace s?mantique utilis?e est Reflective Random Indexing (RRI)
(Cohen et al, 2010a), c?est une nouvelle m?thode de construction d?espaces s?mantiques bas?e
sur la projection al?atoire qui est assez diff?rente des autres m?thodes de construction d?espaces
s?mantiques. Ses particularit?s sont (i) qu?elle ne construit pas de matrice de co-occurrence
et (ii) qu?elle ne n?cessite pas, contrairement aux autres mod?les vectoriels de repr?sentation
s?mantique, des traitements statistiques lourds comme la SVD pour LSA. RRI est bas?e sur la
projection al?atoire (Vempala, 2004; Bingham et Mannila, 2001), qui permet un meilleur passage
? l??chelle pour grand nombre des documents. La construction d?un espace s?mantique avec RRI
se d?roule comme suit :
? Cr?er une matrice A(d ? n), contenant des vecteurs indexes, o? d est le nombre de documents
ou de contextes et n le nombre de dimensions choisies par l?exp?rimentateur. Les vecteurs
indexes sont des vecteurs creux g?n?r?s al?atoirement.
? Cr?er une matrice B(t ? n), contenant des vecteurs termes, o? t est le nombre de termes
diff?rents dans le corpus. Initialiser tous ces vecteurs avec des valeurs nulles pour d?marrer la
82
construction de l?espace s?mantique.
? Pour tout document du corpus, chaque fois qu?un terme ? appara?t dans un document ?,
accumuler le vecteur index de ? au vecteur terme de ?.
? la fin du processus, les vecteurs termes qui appara?ssent dans des contextes similaires ont
accumul? des vecteurs indexes similaire.
L?aspect ? Reflective ? dans RRI consiste ? rejouer plusieurs cycles des trois ?tapes de l?algorithme
non plus ? partir de vecteurs al?atoires mais ? partir des vecteurs indexes obtenues pour
les documents. Ces cycles permettent de gommer l?aspect al?atoire de l?espace, le syst?me
convergeant g?n?ralement au bout d?un nombre r?duit de cycles.
3.1.1 Semantic Vectors
Plusieurs impl?mentations libre de RRI sont disponibles, nous utilisons la librairie Semantic
Vectors 1 (Widdows et Cohen, 2010). Semantic Vectors pr?sente un certain nombre d?avantages
par rapport aux autres librairies impl?mentant RRI, en particulier, parce qu?il offre, d?une part,
une impl?mentation de RRI bas? sur des indexes positionnels (Cohen et al, 2010a) qui construit
l?espace s?mantique non plus en se basant sur les occurrences d?un terme dans un document
mais dans une fen?tre glissante ? la mani?re de HAL, cette version de RRI permet de capturer
outre les informations sur la s?mantiques des termes, des informations structurelles sur leur
proximit?. D?autre part, Semantic Vectors implante un certain nombre de mesures de similarit?
entre des groupes de mots, en particulier (i) la ? disjonction quantique ? (Cohen et al, 2010b)
qui permet de construire un volume correspondant ? plusieurs termes dans l?espace s?mantique
et de calculer la distance entre ce volume et d?autres termes ou documents de l?espace ; (ii) ?
similarit? tensorielle ? qui prend en entr?e une suite ordonn?e de termes et calcule sa similarit?
avec d?autres suites ordonn?es, exploitant ainsi les informations d?ordre provenant des indexes
positionnels.
Semantic Vectors est utilis? dans nombre d?applications. Nous l?avons utilis? dans nos partici-
pations au DEFT depuis l??dition 2009. Dans des t?ches proches de celle qui nous occupe, la
librairie a ?t? utilis?e pour comparer RRI ? d?autres m?thodes d?espaces s?mantiques pour la
recherche de relations entre termes dans un corpus (Rangan, 2011).
3.2 Enrichir les espaces s?mantiques avec des informations linguistiques
Dans le probl?me d?attributions de mots-cl?s ? un texte, les termes utilis?s comme mots-cl?s
sont, pour une partie d?entre-eux, des groupes de mots. La s?mantique associ?e ? un groupe de
mots dans espace s?mantiques n?est pas aussi pr?cise que celle associ? ? un mot : elle comprend
des composantes de ce mots dans d?autres contextes. Pour pouvoir traiter la s?mantique de
ces groupes de mots, certaines m?thodes de repr?sentation du sens en espaces s?mantiques
telles que BEAGLE (Jones et Mewhort, 2007), PSI (Cohen et al, 2009), ou encore RRI avec des
indexes positionnels (Cohen et al, 2010b; Widdows et Cohen, 2010), permettent d?encoder les
informations sur l?ordre des mots. Nous avons voulu tester une autre m?thode bas?e sur une
analyse linguistique de surface du texte.
1. http://code.google.com/p/semanticvectors/
83
Le principe de cette m?thode est d?identifier des groupes de mots candidats dans le texte via
une phase de chunking (Abney, 1991) puis de construire des classes d??quivalence de chunks
qui regroupent une majorit? de mots identiques (apr?s lemmatisation des mots) et qui sont
s?mantiquement proches - en se basant sur la s?mantique, dans un espace s?mantique ?classique?,
des mots qu?ils contiennent -. Le corpus est alors transform? en rempla?ant tous les chunks d?une
m?me classe d??quivalence par un repr?sentant de la classe et un nouvel espace s?mantique est
construit ? partir de ce nouveau corpus, dans cet espace les repr?sentants des classes de chunks
sont consid?r?s comme des mots.
Pour les besoins de la Piste 1, le chunker a ?t? entrain? pour consid?rer comme chunk tous
les mots-cl?s compos?s de la terminologie fournie. Dans la Piste 2 ce m?me chunker, ainsi que
la proc?dure de construction de classes de chunks, sont utilis?s pour construire une liste de
mots-cl?s candidats.
4 Affectation de mots-cl?s comme proc?dure de d?cision
mixte
4.1 R?seau Bay?sien pour l?affectation de mots-cl?s
En analysant un corpus d?articles, nous cherchons, dans un premier temps, ? d?terminer la taille
des diff?rents mots-cl?s rattach?s ? un article donn?. Dans un second temps, nous nous effor?ons
d??tablir les probabilit?s d?appartenance de ces mots-cl?s ? une liste pr?-?tablie. Nous disposons
pour chaque document du corpus des informations suivantes :
? les longueurs du r?sum? l et du texte L ;
? la revue R dans laquelle l?article est paru ;
? le nombre de mots-cl?s n et leurs tailles respectives n1, . . . , nn (ie le nombre de mots lescomposant) ;
? les similarit?s avec la totalit? du lexique des mots-cl?s (d1, . . . , dN ) (N taille de la terminologie) ;? les mots-cl?s (kw1, . . . , kwn).
Il s?agit donc de trouver des relations entre les variables exog?nes (l, L, R, n, d1, . . . , dN ) permet-tant de pr?voir le comportement des variables endog?nes (n1, . . . , nn, kw1, . . . , kwn). A cette fin,il faut disposer d?un formalisme de mod?lisation des connaissances adapt?. Les r?seaux bay?-
siens (Barber, 2012), ?tant des mod?les graphiques auxquels sont associ?es des repr?sentations
probabilistes sous-jacentes, apparaissent comme particuli?rement adapt?s ? notre cas d??tude.
Un r?seau bay?sien B est un couple (G,?) o? G est un graphe acyclique dirig? dont les noeuds
repr?sentent un ensemble de variables al?atoires X = {X1, . . . , Xn} et ?i = [P(X i/C(X i))] est lamatrice des probabilit?s conditionnelles du n?ud i connaissant l??tat de ses parents C(X i).
L?int?r?t des r?seaux bay?siens est donc que leurs structures graphique et probabiliste permettent
de prendre en charge une repr?sentation modulaire des connaissances, une interpr?tation ? la
fois quantitative et qualitative des donn?es. En effet, le graphe d?un r?seau bay?sien permet ainsi
de repr?senter sch?matiquement les relations entre les variables du syst?me ? mod?liser et les
distributions de probabilit?s, elles, permettent de quantifier ces relations.
84
Le mod?le que l?on se propose de construire est un r?seau bay?sien ? variables discr?tes (le
nom de la revue R, les mots-cl?s kwi , leur nombre n, leurs tailles ni) et ? variables continues(longueurs du r?sum? l, de l?article L et les similarit? ? la terminologie). C?est un mod?le
mixte, appel? mod?le conditionnel gaussien, pour lequel la distribution des variables continues
conditionnellement aux variables discr?tes est une gaussienne multivari?e. Cela implique qu?il
peut y avoir des arcs partant de noeuds discrets vers des noeuds continus, mais pas l?inverse
hormis pour le cas o? les noeuds continus sont observables (ce qui est notre cas). Notons
?galement que le nombre de variables n1, . . . , nn et kw1, . . . , kwn varie selon le nombre de mots-cl?s n ; le nombre de noeuds dans un r?seau bay?sien ?tant fixe, nous nous proposons de poser
n1, . . . , n25, les tailles des diff?rents mots-cl?s avec ni = 0 si i > n et kw1, . . . , kw25 les diff?rentsmots-cl?s avec kwi = NU LL si i > n.
Pour r?sumer nous disposons des variables al?atoires suivantes repr?sent?es par les noeuds du
r?seau bay?sien que l?on cherche ? construire :
? R, le nom de la revue (variable discr?te pouvant prendre 4 valeurs) ;
? l, la longueur du r?sum? (variable continue) ;
? L, la longueur de l?article (variable continue) ;
? n, le nombre de mots-cl?s (variable discr?te pouvant prendre 25 valeurs) ;
? n1, . . . , n25, la taille des mots-cl?s (variable discr?te pouvant prendre 11 valeurs) ;? d1, . . . , d1062, les similarit?s ? l?ensemble des mots-cl?s (variable continue) ;? kw1, . . . , kw25, les mots-cl?s (variable discr?te pouvant prendre 1062 valeurs).
L?observation des distributions des documents entre les diff?rentes revues nous permet d?affirmer
que celles-ci sont similaires dans le corpus d?apprentissage et celui de test ; ce qui implique que le
biais qu?introduit cette distribution n?impactera pas les performances du mod?le ? construire.
Les moyennes des longueurs de r?sum? l et d?article L pr?sentent le m?me ordre de grandeur.
Ces moyennes ne sont certes pas similaires dans le corpus d?apprentissage et celui de test, mais
elles sont distribu?es de la m?me mani?re, ie que les longueurs de r?sum? (respectivement
d?article) sont ?gales dans le corpus d?apprentissage et dans celui de test au m?me facteur pr?s.
Notons ?galement que les longueurs d?article et de r?sum? ne sont pas distribu?es de la m?me
mani?re ; cela veut dire qu?en plus de la relation directe ?vidente entre ces deux variables, il
existe probablement une cause commune aux deux, ce qui se traduit dans la structure du r?seau
bay?sien par la pr?sence d?un parent commun.
Les distributions des nombres de mots par article (respectivement par r?sum?) peuvent ?tre
approxim?es par des m?langes de gaussiennes. Ces histogrammes sont similaires pour le corpus
entier et pour celui d?apprentissage. Ce qui nous montre que l??chantillon ?tudi? peut ?tre
consid?r? comme repr?sentatif du probl?me. Toutefois, la relative disparit? observ?e entre le
corpus de test et celui d?apprentissage cr?era probablement un probl?me de biais qu?il faudra
prendre en compte durant la construction du mod?le.
Les histogrammes des nombres de mots par article (respectivement par r?sum?) repr?sentent pour
les diff?rentes revues des distributions diff?rentes. Ces variables sont donc directement reli?es ?
la nature de la revue. Ces diff?rentes distributions ont des formes quelconques, cependant, nous
remarquons que l?on pourra les approximer par un m?lange de gaussiennes ; ce qui nous conforte
dans le choix d?un mod?le conditionnel gaussien pour repr?senter ces variables dans un r?seau
bay?sien.
85
En observant la monotonie des moyennes des similarit?s ? la terminologie des mots-cl?s sur
les diff?rentes parties du corpus, nous remarquons qu?elle pr?sente la m?me allure (et m?me
quasiment le m?me trac?) dans tous les cas (corpus entier, corpus d?apprentissage, revue en
particulier, . . . ). Cela nous permet de supposer que la s?lection de mots-cl?s se fait strictement
de la m?me mani?re partout, et donc l?id?e d?en faire un mod?le math?matique est parfaitement
coh?rente.
Sur la base de ces diff?rentes observations, prenons un exemple de structure de r?seau bay?sien
reliant les variables de notre probl?me. Par convention, les variables discr?tes sont repr?sent?es
par des noeuds carr?s, les variables continues par des noeuds ronds et les variables observables
par des noeuds ombr?s (figure 3).
FIGURE 3 ? Structure du r?seau bay?sien appris sur le corpus
4.2 Combiner des d?cisions statistiques avec du raisonnement ? base de
r?gles
Les r?cents travaux en intelligence artificielle sur la combinaison de m?thodes de d?cision
statistiques et de raisonnement ? base de r?gles de production, comme les R?gles de Production
Probabilistes (PPR) de (A?t-Kaci et Bonnard, 2011), nous offrent un cadre pour mod?liser une
proc?dure de d?cision qui prend en compte ce qui est appris par le r?seau bay?sien d?crit
ci-dessus, et les connaissances symboliques encod?es dans les r?gles sur le choix des mots-cl?s
dont nous avons donn? des exemples en 2.3.
86
Le principe de fonctionnement du syst?me de d?cision, construit en se basant sur PPR, est de
calculer un score pour chacun des mots-cl?s pour un document donn?. Ce calcul est r?alis? en
utilisant des r?gles pouvant faire appel au r?seau bay?sien. Par exemple, la r?gle ?les mots-cl?s
sont diff?rents entre eux? peut se traduire par la r?gle production ?si deux mots-cl?s sont proches
alors augmenter le score de celui qui est le plus haute probabilit? d??tre un mot-cl? du document et
r?duire l?autre? qui s??crit :
IF similarity(kw1, kw2) > seuil AND bnproba(kw1|doc) > bnproba(kw2|doc)
THEN increase-score(kw1, doc) AND decrease-score(kw2, doc)
Le syst?me de r?gles que nous avons utilis? contient une quinzaine de r?gles. Nous ne pouvons
pas les d?tailler ici par manque de place.
5 Les ex?cutions soumises
La table 1 r?sume les ex?cutions soumises par notre ?quipe. Ses r?sultats sont tr?s satisfaisants
pour toutes les approches que nous avons utilis?. La moyenne de F-score pour la Piste 1 pour
l?ensemble des participants ?tant de 0,3575 et pour la Piste 2 de 0,2045. On notera que les
premi?res ex?cutions pour les deux pistes (1.1 et 2.1) qui sont nos ex?cutions de base donnent
des r?sultats corrects en des temps relativement bas.
Run Precision Rappel F-score Temps (en s)
1.1 0.4618 0.4618 0.4618 2
1.2 0.9479 0.9497 0.9483 7590
1.3 0.7486 0.7486 0.7486 -
2.1 0.2438 0.2438 0.2438 26
2.2 0.3471 0.3471 0.3471 269
2.3 0.5879 0.5867 0.5873 12700
TABLE 1 ? R?sultats soumis : performance et temps d??xecution
5.1 Piste 1
5.1.1 Run 1.1 ? baseline : RRI et k-NN
Dans cette ex?cution qui constitue notre baseline, nous avons construit un espace s?mantique
RRI avec l?ensemble des documents du corpus (appr + test), un document ?tant constitu? par la
concat?nation du r?sum? et du corps de l?article. Puis pour chaque document d du corpus de test,
nous avons retenu comme mots-cl?s les k plus proches voisins du document dans la terminologie,
k ?tant le nombre de mots-cl?s pour le document d. Le vecteur pour un mot-cl? kwi compos?des mots w1, ..., wn ?tant obtenu en sommant les vecteurs des mots qu?il contient.
~kwi = ?i ~wi (1)
87
5.1.2 Run 1.2 ? RRI(chunks), BN et r?gles
Dans cette ex?cution, qui a obtenu le meilleur r?sultat, nous avons construit un espace s?mantique
?enrichi? comme nous l?avons d?crit dans la section 3.2, mais dans lequel un document ?tait
repr?sent? par quatre vecteurs, un pour le r?sum?, un pour le corps de l?article et deux vecteurs
pour le premier et le dernier paragraphe de l?article (que nous avons pris comme approximation
de l?introduction et la conclusion) . Nous avons ensuite appris le r?seau bay?sien d?crit en 4.1
en utilisant les distances entres les documents et les mots-cl?s obtenues sur cet espace. Enfin,
nous avons utilis? la proc?dure de d?cision d?crite en 4.2 pour affecter un score ? chacun des
mots-cl?s, les mots-cl?s retenus sont les k ayant les plus hauts scores (k ?tant le nombre de
mots-cl?s pour le document).
5.1.3 Run 1.3
Dans le cadre de ce run, on a combin? les r?sultats de run 1 et run 2, en donnant une l?g?re
pr?f?rence aux candidates-termes lesquels sont plus longues que d?autres termes-candidates. On
a donc combin?, par exemple, les termes-candidates de run1 :
Catalogne ; Narotzky ; conflit ; contexte ;district industriel ; femmes ; production
traductionnelle ; production ?crite ; r?seau
avec les termes-candidates de run 2 :
Espagne ; Narotzky ; anthropologie ?conomique ; district industriel ; f?minisme ;
histoire ; r?seaux de production ; ?conomie politique ; ?conomie r?gionale
pour obtenir la liste des candidates de run3 :
district industriel ; r?seaux de production ; ?conomie politique ; production traduc-
tionnelle ; anthropologie ?conomique ; Narotzky ; ?conomie r?gionale ; production ?crite ;
f?minisme
Le score du candidat ?tait calcul? par la formule :
score = Fr ? (l ? Fa) (2)
o? Fr est la fr?quence relative du terme-candidat dans l?article analys?, Fa est la fr?quenceabsolue du terme-candidat dans tous les articles du corpus et l est le nombre de caract?res du
terme-candidat.
5.2 Piste 2
5.2.1 Run 2.1 ? baseline : RRI et k-NN
Cette ex?cution est identique ? la premi?re ex?cution de la Piste 1 5.1.1, la terminologie obtenue
par la m?thode d?crite en 3.2 contient 3000 candidats mots-cl?s.
88
5.2.2 Run 2.2 ? RRI(PositionalIndex), Tensor Similarity et k-NN
Dans cette deuxi?me ex?cution, nous avons utilis? la m?me terminologie que pour 2.1, mais
l?espace s?mantique a ?t? construit en utilisant RRI sur des indexes positionnels. Le calcul des
vecteurs de mots-cl?s utilise l?op?rateur Tensoriel de Semantic Vectors. Les mots-cl?s retenus
pour un document d sont les k plus proches voisins du document d dans la terminologie, k ?tant
le nombre de mots-cl?s pour le document d.
5.2.3 Run 2.3 ? RRI(chunks), BN et r?gles
Cette ex?cution est identique ? la deuxi?me ex?cution de la Piste 1 d?crite en 5.1.2, la ter-
minologie obtenu par la m?thode d?crite en 3.2 ? laquelle on ajout? les mots-cl?s du corpus
d?apprentissage elle contenaint 3270 candidats mots-cl?s.
5.3 Discussion
Nous pouvons voir que les ex?cutions 1.2 et 2.3 sont celles qui obtiennent les meilleurs r?sultats,
ce qui nous conforte dans nos hypoth?ses de d?part. Les ex?cutions officielles nous ne permettent
pas de comparer les performances des espaces ?enrichis? par des chunks et des espaces RRI avec
indexes positionnels, nous avons effectu? une ex?cution 2.2bis avec un espace ?enrichi? et k-NN
le F-score obtenu est de 0.4186, le r?sultat est sensiblement meilleur que l?ex?cution 2.2.
Rappelons que pour le 1.3, on a combin? les r?sultats de 1.1 et 1.2 de en donnant plus de poids
aux candidates-termes longues (cette r?gle n?ayant pas ?t? incluse dans le syst?me de r?gles
d?crit en 4.2 ). Etant donn? que le F-score obtenu (0.7486) se trouve au mi-chemin entre le
F-score de 1.1 et de 1.2, nous ne pouvons pas r?ellement conclure quand ? la pertinence de cette
r?gle.
Conslusion
Dans cet article, nous avons pr?sent? un syst?me d?attribution de mots-cl?s ? des articles scien-
tifiques, qui se base sur des espaces s?mantiques construit en utilisant RRI. Puis nous avons
essay? d?am?liorer les performances du syst?mes par deux moyens : (i) en enrichissant les
espaces s?mantiques par des informations issues d?une analyse linguistique de surface, et (ii)
en d?finissant une proc?dure de d?cision bas?e sur une combinaison de r?seaux bay?siens et
de syst?mes ? base de r?gles. Les r?sultats obtenus montrent que ces deux hypoth?ses se sont
r?v?l?es payantes et qu?elles am?liorent sensiblement les r?sultats obtenus par une approche RRI
seul (qui obtient d?j? des r?sultats honorables).
89
R?f?rences
ABNEY, S. (1991). Principle-Based Parsing, chapitre Parsing By Chunks. Kluwer Academic
Publishers.
A?T-KACI, H. et BONNARD, P. (2011). Probabilistic production rules. Rapport technique, IBM.
BARBER, D. (2012). Bayesian Reasoning and Machine Learning. Cambridge University Press.
BINGHAM, E. et MANNILA, H. (2001). Random projection in dimensionality reduction : Applica-
tions to image and text data. In in Knowledge Discovery and Data Mining, pages 245?250. ACM
Press.
COHEN, T., SCHVANEVELDT, R. et RINDLESCH, T. (2009). Predication-based semantic indexing :
Permutations as a means to encode predications in semantic space. In Proceedings of the AMIA
Annual Symposium, pages 114?118.
COHEN, T., SCHVANEVELDT, R. et WIDDOWS, D. (2010a). Reflective random indexing and indirect
inference : A scalable method for the discovery of implicit connections. Biomed Inform, 43(2):
240?256.
COHEN, T., WIDDOWS, D., SCHVANEVELDT, R. et RINDLESCH, T. (2010b). Logical leaps and quantum
connectives : Forging paths through predication space. In Proceedings of the AAAI Fall 2010
symposium on Quantum Informatics for cognitive, social and semantic processes (QI-2010).
EL GHALI, A. (2011). Exp?rimentations autour des espaces s?mantiques hybrides. In Actes de
l?atelier DEFT?2011, Montpellier.
HARRIS, Z. (1968). Mathematical Structures of Language. John Wiley and Son, New York.
JONES, M. N. et MEWHORT, D. J. K. (2007). Representing word meaning and order information
in a composite holographic lexicon. Psychological Review, 114(1):1?37.
KANERVA, P., KRISTOFERSON, J. et HOLST, A. (2000). Random Indexing of Text Samples for
Latent Semantic Analysis. In GLEITMAN, L. et JOSH, A., ?diteurs : Proceedings of the 22nd Annual
Conference of the Cognitive Science Society, Mahwah. Lawrence Erlbaum Associates.
LANDAUER, T. K. et DUMAIS, S. T. (1997). A Solution to Plato?s Problem : The Latent Semantic
Analysis Theory of Acquisition, Induction and Representation of Knowledge. Psychological
Review, 104(2):211?240.
LUND, K. et BURGESS, C. (1996). Producing high-dimensional semantic space from lexical
co-occurence. Behavior research methods, instruments & computers, 28(2):203?208.
RANGAN, V. (2011). Discovery of related terms in a corpus using reflective random indexing. In
Proceedings of Workshop on Setting Standards for Searching Electronically Stored Information In
Discovery Proceedings (DESI-4).
SAHLGREN, M. (2006). The Word-Space Model : Using distributional analysis to represent syn-
tagmatic and paradigmatic relations between words in high-dimensional vector spaces. Th?se de
doctorat, Department of Linguistics Stockholm University.
VEMPALA, S. S. (2004). The Random Projection Method, volume 65 de DIMACS Series in Discrete
Mathematics and Theoretical Computer Science. American Mathematical Society.
WIDDOWS, D. et COHEN, T. (2010). The semantic vectors package : New algorithms and public
tools for distributional semantics. In Proceedings of the Fourth IEEE International Conference on
Semantic Computing (IEEE ICSC2010).
90
