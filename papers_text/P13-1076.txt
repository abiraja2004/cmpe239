Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 770?779,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Graph-based Semi-Supervised Model for Joint Chinese Word
Segmentation and Part-of-Speech Tagging
Xiaodong Zeng? Derek F. Wong? Lidia S. Chao? Isabel Trancoso?
?Department of Computer and Information Science, University of Macau
?INESC-ID / Instituto Superior Te?cnico, Lisboa, Portugal
nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,
isabel.trancoso@inesc-id.pt
Abstract
This paper introduces a graph-based semi-
supervised joint model of Chinese word
segmentation and part-of-speech tagging.
The proposed approach is based on a
graph-based label propagation technique.
One constructs a nearest-neighbor simi-
larity graph over all trigrams of labeled
and unlabeled data for propagating syn-
tactic information, i.e., label distribution-
s. The derived label distributions are re-
garded as virtual evidences to regular-
ize the learning of linear conditional ran-
dom fields (CRFs) on unlabeled data. An
inductive character-based joint model is
obtained eventually. Empirical results on
Chinese tree bank (CTB-7) and Microsoft
Research corpora (MSR) reveal that the
proposed model can yield better result-
s than the supervised baselines and other
competitive semi-supervised CRFs in this
task.
1 Introduction
Word segmentation and part-of-speech (POS) tag-
ging are two critical and necessary initial proce-
dures with respect to the majority of high-level
Chinese language processing tasks such as syn-
tax parsing, information extraction and machine
translation. The traditional way of segmentation
and tagging is performed in a pipeline approach,
first segmenting a sentence into words, and then
assigning each word a POS tag. The pipeline ap-
proach is very simple to implement, but frequently
causes error propagation, given that wrong seg-
mentations in the earlier stage harm the subse-
quent POS tagging (Ng and Low, 2004). The join-
t approaches of word segmentation and POS tag-
ging (joint S&T) are proposed to resolve these t-
wo tasks simultaneously. They effectively allevi-
ate the error propagation, because segmentation
and tagging have strong interaction, given that
most segmentation ambiguities cannot be resolved
without considering the surrounding grammatical
constructions encoded in a POS sequence (Qian
and Liu, 2012).
In the past years, several proposed supervised
joint models (Ng and Low, 2004; Zhang and
Clark, 2008; Jiang et al, 2009; Zhang and Clark,
2010) achieved reasonably accurate results, but the
outstanding problem among these models is that
they rely heavily on a large amount of labeled data,
i.e., segmented texts with POS tags. However, the
production of such labeled data is extremely time-
consuming and expensive (Jiao et al, 2006; Jiang
et al, 2009). Therefore, semi-supervised join-
t S&T appears to be a natural solution for easily in-
corporating accessible unlabeled data to improve
the joint S&T model. This study focuses on using
a graph-based label propagation method to build
a semi-supervised joint S&T model. Graph-based
label propagation methods have recently shown
they can outperform the state-of-the-art in sever-
al natural language processing (NLP) tasks, e.g.,
POS tagging (Subramanya et al, 2010), knowl-
edge acquisition (Talukdar et al, 2008), shallow
semantic parsing for unknown predicate (Das and
Smith, 2011). As far as we know, however, these
methods have not yet been applied to resolve
the problem of joint Chinese word segmentation
(CWS) and POS tagging.
Motivated by the works in (Subramanya et al,
2010; Das and Smith, 2011), for structured prob-
lems, graph-based label propagation can be em-
ployed to infer valuable syntactic information (n-
gram-level label distributions) from labeled data
to unlabeled data. This study extends this intui-
tion to construct a similarity graph for propagating
trigram-level label distributions. The derived label
distributions are regarded as prior knowledge to
regularize the learning of a sequential model, con-
ditional random fields (CRFs) in this case, on both
770
labeled and unlabeled data to achieve the semi-
supervised learning. The approach performs the
incorporation of the derived labeled distributions
by manipulating a ?virtual evidence? function as
described in (Li, 2009). Experiments on the da-
ta from the Chinese tree bank (CTB-7) and Mi-
crosoft Research (MSR) show that the proposed
model results in significant improvement over oth-
er comparative candidates in terms of F-score and
out-of-vocabulary (OOV) recall.
This paper is structured as follows: Section
2 points out the main differences with the re-
lated work of this study. Section 3 reviews the
background, including supervised character-based
joint S&T model based on CRFs and graph-based
label propagation. Section 4 presents the details of
the proposed approach. Section 5 reports the ex-
periment results. The conclusion is drawn in Sec-
tion 6.
2 Related Work
Prior supervised joint S&T models present ap-
proximate 0.2% - 1.3% improvement in F-score
over supervised pipeline ones. The state-of-the-
art joint models include reranking approaches (Shi
and Wang, 2007), hybrid approaches (Nakagawa
and Uchimoto, 2007; Jiang et al, 2008; Sun,
2011), and single-model approaches (Ng and Low,
2004; Zhang and Clark, 2008; Kruengkrai et al,
2009; Zhang and Clark, 2010). The proposed ap-
proach in this paper belongs to the single-model
type.
There are few explorations of semi-supervised
approaches for CWS or POS tagging in previ-
ous works. Xu et al (2008) described a Bayesian
semi-supervised CWS model by considering the
segmentation as the hidden variable in machine
translation. Unlike this model, the proposed ap-
proach is targeted at a general model, instead of
one oriented to machine translation task. Sun and
Xu (2011) enhanced a CWS model by interpolat-
ing statistical features of unlabeled data into the
CRFs model. Wang et al (2011) proposed a semi-
supervised pipeline S&T model by incorporating
n-gram and lexicon features derived from unla-
beled data. Different from their concern, our em-
phasis is to learn the semi-supervised model by
injecting the label information from a similarity
graph constructed from labeled and unlabeled da-
ta.
The induction method of the proposed approach
also differs from other semi-supervised CRFs al-
gorithms. Jiao et al (2006), extended by Mann
and McCallum (2007), reported a semi-supervised
CRFs model which aims to guide the learning
by minimizing the conditional entropy of unla-
beled data. The proposed approach regularizes the
CRFs by the graph information. Subramanya et
al. (2010) proposed a graph-based self-train style
semi-supervised CRFs algorithm. In the proposed
approach, an analogous way of graph construction
intuition is applied. But overall, our approach dif-
fers in three important aspects: first, novel feature
templates are defined for measuring the similari-
ty between vertices. Second, the critical property,
i.e., sparsity, is considered among label propaga-
tion. And third, the derived label information from
the graph is smoothed into the model by optimiz-
ing a modified objective function.
3 Background
3.1 Supervised Character-based Model
The character-based joint S&T approach is oper-
ated as a sequence labeling fashion that each Chi-
nese character, i.e., hanzi, in the sequence is as-
signed with a tag. To perform segmentation and
tagging simultaneously in a uniform framework,
according to Ng and Low (2004), the tag is com-
posed of a word boundary part, and a POS part,
e.g., ?B NN? refers to the first character in a word
with POS tag ?NN?. In this paper, 4 word bound-
ary tags are employed: B (beginning of a word),
M (middle part of a word), E (end of a word) and
S (single character). As for the POS tag, we shal-
l use the 33 tags in the Chinese tree bank. Thus,
the potential composite tags of joint S&T consist
of 132 (4?33) classes.
The first-order CRFs model (Lafferty et al,
2001) has been the most common one in this
task. Given a set of labeled examples Dl =
{(xi, yi)}li=1, where xi = x1ix2i ...xNi is the se-
quence of characters in the ith sentence, and yi =
y1i y2i ...yNi is the corresponding label sequence.
The goal is to learn a CRFs model in the form,
p(yi|xi; ?) =
1
Z(xi; ?)
exp{
N?
j=1
K?
k=1
?kfk(yj?1i , y
j
i , xi, j)}
(1)
where Z(xi; ?) is the partition function that nor-
malizes the exponential form to be a probability
distribution, and fk(yj?1i , yji , xi, j). In this study,
771
the baseline feature templates of joint S&T are
the ones used in (Ng and Low, 2004; Jiang et al,
2008), as shown in Table 1. ? = {?1?2...?K} ?
RK are the weight parameters to be learned. In su-
pervised training, the aim is to estimate the ? that
maximizes the conditional likelihood of the train-
ing data while regularizing model parameters:
L(?) =
l?
i=1
log p(yi|xi; ?)?R(?) (2)
R(?) can be any standard regularizer on parame-
ters, e.g., R(?) =? ? ? /2?2, to limit overfitting
on rare features and avoid degeneracy in the case
of correlated features. This objective function can
be optimized by the stochastic gradient method or
other numerical optimization methods.
Type Font Size
Unigram Cn(n = ?2,?1, 0, 1, 2)
Bigram CnCn+1(n = ?2,?1, 0, 1)
Date, Digit and
Alphabetic Letter
T (C?2)T (C?1)T (C0)
T (C1)T (C2)
Table 1: The feature templates of joint S&T.
3.2 Graph-based Label Propagation
Graph-based label propagation, a critical subclass
of semi-supervised learning (SSL), has been wide-
ly used and shown to outperform other SSL meth-
ods (Chapelle et al, 2006). Most of these algo-
rithms are transductive in nature, so they cannot
be used to predict an unseen test example in the fu-
ture (Belkin et al, 2006). Typically, graph-based
label propagation algorithms are run in two main
steps: graph construction and label propagation.
The graph construction provides a natural way to
represent data in a variety of target domains. One
constructs a graph whose vertices consist of la-
beled and unlabeled examples. Pairs of vertices
are connected by weighted edges which encode
the degree to which they are expected to have the
same label (Zhu et al, 2003). Popular graph con-
struction methods include k-nearest neighbors (k-
NN) (Bentley, 1980; Beygelzimer et al, 2006),
b-matching (Jebara et al, 2009) and local recon-
struction (Daitch et al, 2009). Label propaga-
tion operates on the constructed graph. The pri-
mary objective is to propagate labels from a few
labeled vertices to the entire graph by optimiz-
ing a loss function based on the constraints or
properties derived from the graph, e.g., smooth-
ness (Zhu et al, 2003; Subramanya et al, 2010;
Talukdar et al, 2008), or sparsity (Das and Smith,
2012). State-of-the-art label propagation algo-
rithms include LP-ZGL (Zhu et al, 2003), Ad-
sorption (Baluja et al, 2008), MAD (Talukdar
and Crammer, 2009) and Sparse Inducing Penal-
ties (Das and Smith, 2012).
4 Method
The emphasis of this work is on building a joint
S&T model based on two different kinds of data
sources, labeled and unlabeled data. In essence,
this learning problem can be treated as incorporat-
ing certain gainful information, e.g., prior knowl-
edge or label constraints, of unlabeled data into
the supervised model. The proposed approach em-
ploys a transductive graph-based label propagation
method to acquire such gainful information, i.e.,
label distributions from a similarity graph con-
structed over labeled and unlabeled data. Then,
the derived label distributions are injected as vir-
tual evidences for guiding the learning of CRFs.
Algorithm 1 semi-supervised joint S&T induction
Input:
Dl = {(xi, yi)}li=1 labeled sentences
Du = {(xi)}l+ui=l+1 unlabeled sentencesOutput:
?: a set of feature weights
1: Begin
2: {G} = construct graph (Dl,Du)
3: {q0} = init labelDist ({G})
4: {q} = propagate label ({G}, {q0})
5: {?} = train crf (Dl ? Du, {q})
6: End
The model induction includes the following
steps (see Algorithm 1): firstly, given labeled
and unlabeled data, i.e., Dl = {(xi, yi)}li=1
with l labeled sentences and Du = {(xi)}l+ui=l+1with u unlabeled sentences, a specific similarity
graph G representing Dl and Du is constructed
(construct graph). The vertices (Section 4.1) in
the constructed graph consist of all trigrams that
occur in labeled and unlabeled sentences, and edge
weights between vertices are computed using the
cosine distance between pointwise mutual infor-
mation (PMI) statistics. Afterwards, the estimated
label distributions q0 of vertices in the graph G are
randomly initialized (init labelDist). Subsequently,
772
the label propagation procedure (propagate label)
is conducted for projecting label distributions q
from labeled vertices to the entire graph, using
the algorithm of Sparse-Inducing Penalties (Das
and Smith, 2012) (Section 4.2). The final step
(train crf) of the induction is incorporating the in-
ferred trigram-level label distributions q into CRFs
model (Section 4.3).
4.1 Graph Construction
In most graph-based label propagation tasks, the
final effect depends heavily on the quality of
the graph. Graph construction thus plays a cen-
tral role in graph-based label propagation (Zhu et
al., 2003). For character-based joint S&T, unlike
the unstructured learning problem whose vertices
are formed directly by labeled and unlabeled in-
stances, the graph construction is non-trivial. Das
and Petrov (2011) mentioned that taking individu-
al characters as the vertices would result in various
ambiguities, whereas the similarity measurement
is still challenging if vertices corresponding to en-
tire sentences.
This study follows the intuitions of graph con-
struction from Subramanya et al (2010) in which
vertices are represented by character trigrams oc-
curring in labeled and unlabeled sentences. For-
mally, given a set of labeled sentences Dl, and un-
labeled onesDu, whereD , {Dl,Du}, the goal is
to form an undirected weighted graph G = (V,E),
where V is defined as the set of vertices which
covers all trigrams extracted from Dl and Du.
Here, V = Vl ? Vu, where Vl refers to trigrams
that occurs at least once in labeled sentences and
Vu refers to trigrams that occur only in unlabeled
sentences. The edges E ? Vl ? Vu, connect all
the vertices. This study makes use of a symmet-
ric k-NN graph (k = 5) and the edge weights are
measured by a symmetric similarity function (E-
quation (3)):
wi,j =
{
sim(xi, xj) if j ? K(i) or i ? K(j)
0 otherwise
(3)
where K(i) is the set of the k nearest neighbors of
xi(|K(i) = k, ?i|) and sim(xi, xj) is a similari-
ty measure between two vertices. The similarity
is computed based on the co-occurrence statistic-
s over the features in Table 2. Most features we
adopted are selected from those of (Subramanya
et al, 2010). Note that a novel feature in the last
row encodes the classes of surrounding character-
s, where four types are defined: number, punctu-
ation, alphabetic letter and other. It is especially
helpful for the graph to make connections with tri-
grams that may not have been seen in labeled data
but have similar label information. The pointwise
mutual information values between the trigram-
s and each feature instantiation that they have in
common are summed to sparse vectors, and their
cosine distances are computed as the similarities.
Description Feature
Trigram + Context x1x2x3x4x5
Trigram x2x3x4
Left Context x1x2
Right Context x4x5
Center Word x3
Trigram - Center Word x2x4
Left Word + Right Context x2x4x5
Right Word + Left Context x1x2x3
Type of Trigram: number,
punctuation, alphabetic letter
and other
t(x2)t(x3)t(x4)
Table 2: Features employed to measure the sim-
ilarity between two vertices, in a given tex-
t ?x1x2x3x4x5?, where the trigram is ?x2x3x4?.
The nature of the similarity graph enforces that
the connected trigrams with high weight appearing
in different texts should have similar syntax con-
figurations. Thus, the constructed graph is expect-
ed to provide additional information that cannot
be expressed directly in a sequence model (Subra-
manya et al, 2010). One primary benefit of this
property is on enriching vocabulary coverage. In
other words, the new features of various trigram-
s only occurring in unlabeled data can be discov-
ered. As the excerpt in Figure 1 shows, the trigram
????? (Tianjin port) has no any label informa-
tion, as it only occurs in unlabeled data, but for-
tunately its neighborhoods with similar syntax in-
formation, e.g., ????? (Shanghai port), ???
?? (Guangzhou port), can assist to infer the cor-
rect tag ?M NN?.
4.2 Label Propagation
In order to induce trigram-level label distributions
from the graph constructed by the previous step,
a label propagation algorithm, Sparsity-Inducing
Penalties, proposed by Das and Smith (2012), is
employed. This algorithm is used because it cap-
tures the property of sparsity that only a few labels
773
Figure 1: An excerpt from the similarity graph
over trigrams on labeled and unlabeled data.
are typically associated with a given instance. In
fact, the sparsity is also a common phenomenon
among character-based CWS and POS tagging.
The following convex objective is optimized on
the similarity graph in this case:
argmin
q
l?
j=1
? qj ? rj ?2
+?
l+u?
i=1,k?N (i)
wik ? qi ? qk ?2 +?
l+u?
i=1
? qi ?2
s.t. qi ? 0, ?i ? V
(4)
where rj denotes empirical label distributions of
labeled vertices, and qi denotes unnormalized es-
timate measures in every vertex. The wik refers to
the similarity between the ith trigram and the kth
trigram, and N (i) is a set of neighbors of the ith
trigram. ? and ? are two hyperparameters whose
values are discussed in Section 5. The squared-
loss criterion1 is used to formulate the objective
function. The first term in Equation (4) is the seed
match loss which penalizes the estimated label dis-
tributions qj , if they go too far away from the em-
pirical labeled distributions rj . The second term
is the edge smoothness loss that requires qi should
be smooth with respect to the graph, such that two
vertices connected by an edge with high weight
should be assigned similar labels. The final term
is a regularizer to incorporate the prior knowledge,
e.g., uniform distributions used in (Talukdar et al,
2008; Das and Smith, 2011). This study applies
the squared norm of q to encourage sparsity per
vertex. Note that the estimated label distribution
1It can be seen as a multi-class extension of quadratic cost
criterion (Bengio et al, 2006) or as a variant of the objective
in (Zhu et al, 2003). An entropic distance measure could also
be used, e.g., KL-divergence (Subramanya et al, 2010; Das
and Smith, 2012).
qi in Equation (4) is relaxed to be unnormalized,
which simplifies the optimization. Thus, the objec-
tive function can be optimized by L-BFGS-B (Zhu
et al, 1997), a generic quasi-Newton gradient-
based optimizer. The partial derivatives of Equa-
tion (4) are computed for each parameter of q and
then passed on to the optimizer that updates them
such that Equation (4) is maximized.
4.3 Semi-Supervised CRFs Training
The trigram-level label distributions inferred in the
propagation step can be viewed as a kind of valu-
able ?prior knowledge? to regularize the learning
on unlabeled data. The final step of the induc-
tion is thus to incorporate such prior knowledge
into CRFs. Li (2009) generalizes the use of vir-
tual evidence to undirected graphical models and,
in particular, to CRFs for incorporating external
knowledge. By extending the similar intuition, as
illustrated in Figure 2, we modify the structure of
a regular linear-chain CRFs on unlabeled data for
smoothing the derived label distributions, where
virtual evidences, i.e., q in our case, are donated
by {v1, v2, . . . , vT }, in parallel with the state vari-
ables {y1, y2, . . . , yT }. The modified CRFs model
allows us to flexibly define the interaction between
estimated state values and virtual evidences by po-
tential functions. Therefore, given labeled and un-
labeled data, the learning objective is defined as
follows:
L(?) +
l+u?
i=l+1
Ep(yi|xi,vi;?g)[log p(yi, vi|xi; ?)]
(5)
where the conditional probability in the second
term is denoted as
p(yi, vi|xi; ?) =
1
Z ?(xi; ?)
exp{
N?
j=1
K?
k=1
?kfk(yj?1i , y
j
i , xi, j)
+?
N?
t=1
s(yti , vti)}
(6)
The first term in Equation (5) is the same as E-
quation (2), which is the traditional CRFs learn-
ing objective function on the labeled data. The
second term is the expected conditional likelihood
of unlabeled data. It is directed to maximize the
conditional likelihood of hidden states with the
derived label distributions on unlabeled data, i.e.,
p(y, v|x), where y and v are jointly modeled but
774
the probability is still conditional on x. Here,
Z ?(x; ?) is the partition function of normalization
that is achieved by summing the numerator over
both y and v. A virtual evidence feature function
of s(yti , vti) with pre-defined weight ? is defined
to regularize the conditional distributions of states
over the derived label distributions. The learning
is impacted by the derived label distributions as E-
quation (7): firstly, if the trigram xt?1i xtixt+1i at
current position does have no corresponding de-
rived label distributions (vti = null), the value of
zero is assigned to all state hypotheses so that the
posteriors would not affected by the derived infor-
mation. Secondly, if it does have a derived label
distribution, since the virtual evidence in this case
is a distribution instead of a specific label, the la-
bel probability in the distribution under the current
state hypothesis is assigned. This means that the
values of state variables are constrained to agree
with the derived distributions.
s(yti , vti) =
{
qxt?1i xtixt+1i (y
t
i) if vti 6= null
0 else
(7)
The second term in Equation (5) can be op-
timized by using the expectation maximization
(EM) algorithm in the same fashion as in the
generative approach, following (Li, 2009). One
can iteratively optimize the Q function Q(?) =?
y p(yi|xi; ?g) log p(yi, vi|xi; ?), in which ?g is
the model estimated from the previous iteration.
Here the gradient of the Q function can be mea-
sured by:
?Q(?)
??k
=
?
t
?
yt?1i ,yti
fk(yt?1i , yti , xi, t).
(p(yt?1i , yti |xi, vi; ?)? p(yt?1i , yti |xi; ?))
(8)
The forward-backward algorithm is used to mea-
sure p(yt?1i , yti |xi, vi; ?) and p(yt?1i , yti |xi; ?).
Thus, the objective function Equation (5) is op-
timized as follows: for the instances i = 1, 2, ..., l,
the parameters ? are learned as the supervised
manner; for the instances i = l+1, l+2, ..., u+ l,
in the E-step, the expected value of Q function is
computed, based on the current model ?g. In the
M-step, the posteriors are fixed and updated ? that
maximizes Equation (5).
Figure 2: Modified linear-chain CRFs integrating
virtual evidences on unlabeled data.
5 Experiment
5.1 Setting
The experimental data are mainly taken from the
Chinese tree bank (CTB-7) and Microsoft Re-
search (MSR)2. CTB-7 consists of over one mil-
lion words of annotated and parsed text from Chi-
nese newswire, magazine news, various broadcast
news and broadcast conversation programs, web
newsgroups and weblogs. It is a segmented, POS
tagged3 and fully bracketed corpus. The train, de-
velopment and test sets4 from CTB-7 and their
corresponding statistics are reported in Table 3.
To satisfy the characteristic of the semi-supervised
learning problem, the train set, i.e., the labeled da-
ta, is formed by a relatively small amount of an-
notated texts sampled from CTB-7. For the un-
labeled data in this experiment, a greater amount
of texts is extracted from CTB-7 and MSR, which
contains 53,108 sentences with 2,418,690 charac-
ters.
The performance measurement indicators for
word segmentation and POS tagging (joint S&T)
are balance F-score, F = 2PR/(P+R), the harmon-
ic mean of precision (P) and recall (R), and out-
of-vocabulary recall (OOV-R). For segmentation,
a token is regarded to be correct if its boundaries
match the ones of a word in the gold standard.
For the POS tagging, it is correct only if both the
boundaries and the POS tags are perfect matches.
The experimental platform is implemented
based on two toolkits: Mallet (McCallum and
Kachites, 2002) and Junto (Talukdar and Pereira,
2010). Mallet is a java-based package for s-
tatistical natural language processing, which in-
cludes the CRFs implementation. Junto is a graph-
2It can be download at: www.sighan.org/bakeoff2005.
3There is a total of 33 POS tags in CTB-7.
4The extracted sentences in train, development and test set
were assigned with the composite tags as described in Section
3.1.
775
based label propagation toolkit that provides sev-
eral state-of-the-art algorithms.
Data #Sent #Word #Char #OOV
Train 17,968 374,697 596,360
Develop 1,659 46,637 79,283 0.074
Test 2,037 65,219 104,502 0.089
Table 3: Training, development and testing data.
5.2 Baseline and Proposed Models
In the experiment, the baseline supervised pipeline
and joint S&T models are built only on the train
data. The proposed model will also be compared
with the semi-supervised pipeline S&T model de-
scribed in (Wang et al, 2011). In addition, two
state-of-the-art semi-supervised CRFs algorithms,
Jiao?s CRFs (Jiao et al, 2006) and Subramanya?s
CRFs (Subramanya et al, 2010), are also used to
build joint S&T models. The corresponding set-
tings of the above candidates are listed below:
? Baseline I: a supervised CRFs pipeline S&T
model. The feature templates are from Zhao
et al (2006) and Wu et al (2008).
? Wang?s model: a semi-supervised CRFs
pipeline S&T model. The same feature tem-
plates in (Wang et al, 2011) are used, i.e.,
?+n-gram+cluster+lexicon?.
? Baseline II: a supervised CRFs joint S&T
model. The feature templates introduced in
Section 3.1 are used.
? Jiao?s model: a semi-supervised CRFs joint
S&T model trained using the entropy regular-
ization (ER) criteria (Jiao et al, 2006). The
optimization method proposed by Mann and
McCallum (2007) is applied.
? Subramanya?s model: a self-train style
semi-supervised CRFs joint S&T model
based on the same parameters used in (Sub-
ramanya et al, 2010).
? Our model: several parameters in our model
are needed to tune based on the development
set, e.g., ?, ? and ?.
In all the CRFs models above, the Gaussian reg-
ularizer and stochastic gradient descent method
are employed.
5.3 Main Results
This experiment yielded a similarity graph that
consists of 462,962 trigrams from labeled and un-
labeled data. The majority (317,677 trigrams) oc-
curred only in unlabeled data. Based on the de-
velopment data, the hyperparameters of our mod-
el were tuned among the following settings: for
the graph propagation, ? ? {0.2, 0.5, 0.8} and
? ? {0.1, 0.3, 0.5, 0.8}; for the CRFs training,
? ? {0.1, 0.3, 0.5, 0.7, 0.9}. The best performed
joint settings are ? = 0.5, ? = 0.3 and ? = 0.7.
With the chosen set of hyperparameters, the test
data was used to measure the final performance.
Model Segmentation POS TaggingF1 OOV-R F1 OOV-R
Baseline I 94.27 60.12 91.08 51.72
Wang?s 95.17 63.10 91.64 53.29
Baseline II 95.14 61.52 91.61 52.29
Jiao?s 95.58 63.05 92.11 53.27
Subramanya?s 96.30 67.12 92.46 57.15
Our model 96.85 68.09 92.89 58.36
Table 4: The performance of segmentation and
POS tagging on testing data.
Table 4 summarizes the performance of seg-
mentation and POS tagging on the test data, in
comparison with the other five models. First-
ly, as expected, for the two supervised baselines,
the joint model outperforms the pipeline one, e-
specially on segmentation. It obtains 0.92% and
2.32% increase in terms of F-score and OOV-R
respectively. This outcome verifies the commonly
accepted fact that the joint model can substantially
improve the pipeline one, since POS tags provide
additional information to word segmentation (Ng
and Low, 2004). Secondly, it is also noticed that
all four semi-supervised models are able to benefit
from unlabeled data and greatly improve the re-
sults with respect to the baselines. On the whole,
for segmentation, they achieve average improve-
ments of 1.02% and 6.8% in F-score and OOV-R;
whereas for POS tagging, the average increments
of F-sore and OOV-R are 0.87% and 6.45%. An
interesting phenomenon is found among the com-
parisons with baselines that the supervised joint
model (Baseline II) is even competitive with semi-
supervised pipeline one (Wang et al, 2011). This
illustrates the effects of error propagation in the
pipeline approach. Thirdly, in what concerns the
semi-supervised approaches, the three joint S&T
models, i.e., Jiao?s, Subramanya?s and our mod-
el, are superior to the pipeline model, i.e., Wang?s
776
model. Moreover, the two graph-based approach-
es, i.e., Subramanya?s and our model, outperform
the others. Most importantly, the boldface num-
bers in the last row illustrate that our model does
achieve the best performance. Overall, for word
segmentation, it obtains average improvements of
1.43% and 8.09% in F-score and OOV-R over oth-
ers; for POS tagging, it achieves average improve-
ments of 1.09% and 7.73%.
0 10,000 20,000 30,000 40,000 50,000
94.0
94.5
95.0
95.5
96.0
96.5
97.0
97.5
 Wang's
 Jiao's
 Subramanya's
 Our
F
-
s
c
o
r
e
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
91.0
91.5
92.0
92.5
93.0
93.5
 Wang's
 Jiao's
 Subramanya's
 Our
F
-
s
c
o
r
e
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
60.0
62.5
65.0
67.5
70.0
 Wang's
 Jiao's
 Subramanya's
 Our
O
O
V
-
R
Number of unlabeled sentences
0 10,000 20,000 30,000 40,000 50,000
51.0
52.5
54.0
55.5
57.0
58.5
 Wang's
 Jiao's
 Subramanya's
 Our
O
O
V
-
R
Number of unlabeled sentences
Figure 3: The learning curves of semi-supervised
models on unlabeled data, where left graphs are
segmentation and the right ones are tagging.
5.4 Learning Curve
An additional experiment was conducted to inves-
tigate the impact of unlabeled data for the four
semi-supervised models. Figure 3 illustrates the
curves of F-score and OOV-R for segmentation
and tagging respectively, as the unlabeled data
size is progressively increased in steps of 6,000
sentences. It can be clearly observed that al-
l curves of our model are able to mount up steadi-
ly and achieve better gains over others consistent-
ly. The most competitive performance of the oth-
er three candidates is achieved by Subramanya?s
model. This strongly reveals that the knowledge
derived from the similarity graph does effectively
strengthen the model. But in Subramanya?s mod-
el, when the unlabeled size ascends to approxi-
mately 30,000 sentences the curves become nearly
asymptotic. The semi-supervised pipeline model,
Wang?s model, presents a much slower growth on
all curves over the others and also begins to over-
fit with large unlabeled data sizes (>25,000 sen-
tences). The figure also shows an erratic fluctu-
ation of Jiao?s model. Since this approach aims
at minimizing conditional entropy over unlabeled
data and encourages finding putative labelings for
unlabeled data, it results in a data-sensitive mod-
el (Li et al, 2009).
5.5 Analysis & Discussion
A statistical analysis of the segmentation and tag-
ging results of the supervised joint model (Base-
line II) and our model is carried out to comprehend
the influence of the graph-based semi-supervised
behavior. For word segmentation, the most signif-
icant improvement of our model is mainly concen-
trated on two kinds of words which are known for
their difficulties in terms of CWS: a) named enti-
ties (NE), e.g., ????? (Tianjin port) and ???
?? (free tax zone); and b) Chinese numbers (CN),
e.g., ?????? (eight hundred and fifty million)
and ???????? (seventy two percent). Very
often, these words do not exist in the labeled data,
so the supervised model is hard to learn their fea-
tures. Part of these words, however, may occur in
the unlabeled data. The proposed semi-supervised
approach is able to discover their label information
with the help of a similarity graph. Specifically, it
learns the label distributions from similar words
(neighborhoods), e.g., ????? (Shanghai port),
????? (protection zone), ?????? (nine
hundred and seventy million). The statistics in Ta-
ble 5 demonstrate significant error reductions of
50.44% and 48.74% on test data, corresponding to
NE and CN respectively.
Type #word #baErr #gbErr ErrDec%
NE 471 226 112 50.44
CN 181 119 61 48.74
Table 5: The statistics of segmentation error for
named entities (NE) and Chinese numbers (CN)
in test data. #baErr and #gbErr denote the count
of segmentations by Baseline II and our model;
ErrDec% denotes the error reduction.
On the other hand, to better understand the tag-
ging results, we summarize the increase and de-
crease of the top five common tagging error pat-
terns of our model over Baseline II for the cor-
rectly segmented words, as shown in Table 6. The
error pattern is defined by ?A?B? that refers the
true tag of ?A? is annotated by a tag of ?B?. The
obvious improvement brought by our model oc-
curs with the tags ?NN?, ?CD?, ?NR?, ?JJ? and
?NR?, where errors are reduced 60.74% on aver-
777
Pattern #baErr ? Pattern #baErr ?
NN?VV 58 38 NN?NR 13 6
CD?NN 41 27 IJ?ON 9 5
NR?VV 29 17 VV?NN 4 3
JJ?NN 18 11 NR?NN 1 3
NR?VA 19 10 JJ?AD 1 2
Table 6: The statistics of POS tagging error pat-
terns in test data. #baErr denote the count of tag-
ging error by Baseline II, while ? and ? denotes
the number of error reduced or increased by our
model.
age. More impressively, there is a large portion of
fixed error pattern instances stemming from OOV
words. Meanwhile, it is also observed that the dis-
ambiguation of error patterns in the right portion
of the table slightly suffers from our approach. In
reality, it is impossible and unrealistic to request
a model to be ?no harms but only benefits? under
whatever circumstances.
6 Conclusion
This study introduces a novel semi-supervised ap-
proach for joint Chinese word segmentation and
POS tagging. The approach performs the semi-
supervised learning in the way that the trigram-
level distributions inferred from a similarity graph
are used to regularize the learning of CRFs model
on labeled and unlabeled data. The empirical re-
sults indicate that the similarity graph information
and the incorporation manner of virtual evidences
present a positive effect to the model induction.
Acknowledgments
The authors are grateful to the Science and Tech-
nology Development Fund of Macau and the Re-
search Committee of the University of Macau
for the funding support for our research, un-
der the reference No. 017/2009/A and RG060/09-
10S/CS/FST. The authors also wish to thank the
anonymous reviewers for many helpful comments.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi
Jing, Jay Yagnik, Shankar Kumar, Deepak Ravich,
and Mohamed Aly. 2008. Video suggestion and
discovery for youtube: taking random walks through
the view graph. In Proceedings of WWW, pages 895-
904, Beijing, China.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization. Journal of machine
learning research, 7:2399?2434.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le
Roux. 2006. Label propogation and quadratic crite-
rion. MIT Press.
Jon Louis Bentley. 1980. Multidimensional divide-and
-conquer. Communications of the ACM, 23(4):214 -
229.
Alina Beygelzimer, Sham Kakade, and John Langford.
2006. Cover trees for nearest neighbor. In Proceed-
ings of ICML, pages 97-104, New York, USA
Olivier Chapelle, Bernhard Scho? lkopf, and Alexander
Zien. 2006. Semi-supervised learning. MIT Press.
Samuel I. Daitch, Jonathan A. Kelner, and Daniel A.
Spielman. 2009. Fitting a graph to vector data. In
Proceedings of ICML, 201-208, NY, USA.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised framesemantic parsing for unknown
predicates. In Proceedings of ACL, pages 1435-
1444, Portland, Oregon, USA.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
Part-of-Speech Tagging with Bilingual Graph-based
Projections. In Proceedings of ACL, pages 1435-
1444, Portland, Oregon, USA.
Dipanjan Das and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings of NAACL, pages 677-687, Montre?al,
Canada.
Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009.
Graph construction and b-matching for semi-
supervised learning. In Proceedings of ICML, 441-
448, New York, USA.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging. In
Proceedings of ACL, pages 897-904, Columbus, O-
hio.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging ? A Case
Study. In Proceedings of he ACL and the 4th IJC-
NLP of the AFNLP, pages 522?530, Suntec, Singa-
pore.
Feng Jiao, Shaojun Wang, and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In In
Proceedings of ACL, pages 209?216, Sydney, Aus-
tralia.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hy-
brid model for joint Chinese word segmentation and
POS tagging. In Proceedings of ACL and IJCNLP
of the AFNLP, pages 513- 521, Suntec, Singapore
August.
778
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Field: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282-
289, Williams College, USA.
Xiao Li. 2009. On the use of virtual evidence in con-
ditional random fields. In Proceedings of EMNLP,
pages 1289-1297, Singapore.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields In Pro-
ceedings of ACM SIGIR, pages 572-579, Boston,
USA.
Gideon S. Mann and Andrew McCallum. 2007. Ef-
ficient computation of entropy gradient for semi-
supervised conditional random fields. In Proceed-
ings of NAACL, pages 109-112, New York, USA.
McCallum and Andrew Kachites. 2002. MALLET: A
Machine Learning for Language Toolkit. Software
at http://mallet.cs.umass.edu.
Tetsuji Nakagawa and Kiyotaka Uchimoto. 2007. A
hybrid approach to word segmentation and POS tag-
ging. In Proceedings of ACL Demo and Poster Ses-
sion, pages 217?220, Prague, Czech Republic.
Hwee Tou Ng and Jin Kiat Low 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP, Barcelona, Spain.
Xian Qian and Yang Liu. 2012. Joint Chinese Word
Segmentation, POS Tagging and Parsing. In Pro-
ceedings of EMNLP-CoNLL, pages 501-511, Jeju Is-
land, Korea.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer
CRF based joint decoding method for cascade seg-
mentation and labelling tasks. In Proceedings of IJ-
CAI, Hyderabad, India.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of EMNLP, pages 167-176, Mas-
sachusetts, USA.
Weiwei Sun. 2011. A Stacked Sub-Word Model
for Joint Chinese Word Segmentation and Part-of-
Speech Tagging. In Proceedings of ACL, pages
1385?1394, Portland, Oregon.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP, pages 970-979, Scotland, UK.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pas-
ca, Deepak Ravichandran, Rahul Bhagat, and Fer-
nando Pereira. 2008. Weakly Supervised Acquisi-
tion of Labeled Class Instances using Graph Ran-
dom Walks. In Proceedings of EMNLP, pages 582-
590, Hawaii, USA.
Partha Pratim Talukdar and Koby Crammer. 2009.
New Regularized Algorithms for Transductive
Learning. In Proceedings of ECML-PKDD, pages
442 - 457, Bled, Slovenia.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learn-
ing methods for class-instance acquisition. In Pro-
ceedings of ACL, pages 1473-1481, Uppsala, Swe-
den.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisa-
wa. 2011. Improving Chinese word segmentation
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of IJC-
NLP, pages 309?317, Chiang Mai, Thailand.
Yu-Chieh Wu Jie-Chi Yang, and Yue-Shi Lee. 2008.
Description of the NCU Chinese Word Segmenta-
tion and Part-of-Speech Tagging for SIGHAN Bake-
off. In Proceedings of the SIGHAN Workshop on
Chinese Language Processing, pages 161-166, Hy-
derabad, India.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Herman-
n Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING, pages 1017-1024,
Manchester, UK.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of EMNLP, pages 888-896,
Columbus, Ohio.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of
EMNLP, pages 843-852, Massachusetts, USA.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, pages 87-94,
Wuhan, China.
Xiaojin Zhu, Zoubin Ghahramani, and John Laffer-
ty. 2003. Semi-supervised learning using Gaussian
fields and harmonic functions. In Proceedings of
ICML, pages 912?919, Washington DC, USA.
Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 1997. L-BFGS-B: Fortran subroutines for
large scale bound constrained optimization. ACM
Transactions on Mathematical Software, 23:550-
560.
779
