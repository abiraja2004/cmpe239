A Comparative Study of Mixture Models for Automatic Topic Segmentation
of Multiparty Dialogues
Maria Georgescul
ISSCO/TIM, ETI
University of Geneva
maria.georgescul@eti.unige.ch
Alexander Clark
Department of Computer Science
Royal Holloway University of London
alexc@cs.rhul.ac.uk
Susan Armstrong
ISSCO/TIM, ETI
University of Geneva
susan.armstrong@issco.unige.ch
Abstract
In this article we address the task of auto-
matic text structuring into linear and non-
overlapping thematic episodes at a coarse
level of granularity. In particular, we
deal with topic segmentation on multi-party
meeting recording transcripts, which pose
specific challenges for topic segmentation
models. We present a comparative study
of two probabilistic mixture models. Based
on lexical features, we use these models in
parallel in order to generate a low dimen-
sional input representation for topic segmen-
tation. Our experiments demonstrate that in
this manner important information is cap-
tured from the data through less features.
1 Introduction
Some of the earliest research related to the prob-
lem of text segmentation into thematic episodes used
the word distribution as an intrinsic feature of texts
(Morris and Hirst, 1991). The studies of (Reynar,
1994; Hearst, 1997; Choi, 2000) continued in this
vein. While having quite different emphasis at dif-
ferent levels of detail (basically from the point of
view of the employed term weighting and/or the
adopted inter-block similarity measure), these stud-
ies analyzed the word distribution inside the texts
through the instrumentality of merely one feature,
i.e. the one-dimensional inter-block similarity.
More recent work use techniques from graph the-
ory (Malioutov and Barzilay, 2006) and machine
learning (Galley et al, 2003; Georgescul et al,
2006; Purver et al, 2006) in order to find patterns
in vocabulary use.
We investigate new approaches for topic segmen-
tation on corpora containing multi-party dialogues,
which currently represents a relatively less explored
domain. Compared to other types of audio content
(e.g. broadcast news recordings), meeting record-
ings are less structured, often exhibiting a high de-
gree of participants spontaneity and there may be
overlap in finishing one topic while introducing an-
other. Moreover while ending the discussion on a
certain topic, there can be numerous new attempts
to introduce a new topic before it becomes the fo-
cus of the dialogue. Therefore, the task of automatic
topic segmentation of meeting recordings is more
difficult and requires a more refined analysis. (Gal-
ley et al, 2003; Georgescul et al, 2007) dealt with
the problem of topic segmentation of multiparty di-
alogues by combining various features based on cue
phrases, syntactic and prosodic information. In this
article, our investigation is based on using merely
lexical features.
We study mixture models in order to group the
words co-occurring in texts into a small number
of semantic concepts in an automatic unsupervised
way. The intuition behind these models is that a
text document has an underlying structure of ?la-
tent? topics, which is hidden. In order to reveal
these latent topics, the basic assumption made is that
words related to a semantic concept tend to occur in
the proximity of each other. The notion of proxim-
ity between semantically related words can vary for
various tasks. For instance, bigrams can be consid-
ered to capture correlation between words at a very
925
short distance. At the other extreme, in the domain
of document classification, it is often assumed that
the whole document is concerned with one specific
topic and in this sense all words in a document are
considered to be semantically related. We consider
for our application that words occurring in the same
thematic episode are semantically related.
In the following, the major issues we will discuss
include the formulations of two probabilistic mix-
ture approaches, their methodology, aspects of their
implementation and the results obtained when ap-
plied in the topic segmentation context. Section 2
presents our approach on using probabilistic mix-
ture models for topic segmentation and shows com-
parisons between these techniques. In Section 3 we
discuss our empirical evaluation of these models for
topic segmentation. Finally, some conclusions are
drawn in Section 4.
2 Probabilistic Mixture Models
The probabilistic latent models described in the fol-
lowing exploit hierarchical Bayesian frameworks.
Based on prior distributions of word rate variability
acquired from a training corpus, we will compute a
density function to further analyze the text content in
order to perform topic segmentation at a coarse level
of granularity. In this model, we will be working
with ?blocks? of text which consist of a fixed num-
ber of consecutive utterances.
In the following two subsections, we use the fol-
lowing notation:
? We consider a text corpus B = {b1, b2, ..., bM}
containing M blocks of text with words from
a vocabulary W = {w1, w2, ..., wN}. M is
a constant scalar representing the number of
blocks of text. N is a constant scalar represent-
ing the number of terms in vocabulary W .
? We pre-process the data by eliminating con-
tent free words such as articles, prepositions
and auxiliary verbs. Then, we proceed by lem-
matizing the remaining words and by adopt-
ing a bag-of-words representation. Next,
we summarize the data in a matrix F =
(f(bi, wi,j))(i,j)?M?N , where f(bi, wi,j) de-
notes the log.entropy weighted frequency of
word wi,j in block bi.
? Each occurrence of a word in a block of
text is considered as representing an ob-
servation (wm,n, bm), i.e. a realization from
an underlying sequence of random variables
(Wm,n, Bm)
1?m?M
1?n?N . wm,n denotes the term
indicator for the n-th word in the m-th block
of text.
? Each pair (wm,n, bm) is associated with a dis-
crete hidden random variable Zm,n over some
finite set Z ={z1, z2, ..., zK}. K is a constant
scalar representing the number of mixture com-
ponents to generate.
? We denote by P (zm,n = zk) or simply by
P (zk) the probability that the k-th topic has
been sampled for the n-th word in the m-th
block of text.
2.1 Aspect Model for Dyadic Data (AMDD)
In this section we describe how we apply latent mod-
eling for dyadic data (Hofmann, 2001) to text repre-
sentation for topic segmentation.
2.1.1 Model Setting
 
 
 
 
 
n,mw  
n,mz  
mb  
M 
block  plate 
n,mw  
n,mz  
mb  
block  plate 
M 
word  plate 
N 
word  plate 
N 
1) Asymmetric PLSA parameterization 2) Symmetric PLSA parameterization 
Figure 1: Graphical model representation of the as-
pect model.
We express the joint or conditional probability
of words and blocks of text, by assuming that the
choice of a word during the generation of a block
of text is independent of the block itself, given some
(unobserved) hidden variable, also called latent vari-
able or aspect.
The graphical representation of the AMDD data
generation process is illustrated in Figure 1 by using
926
the plate notation. That is, the ovals (i.e. the nodes
of the graph) represent probabilistic variables. The
double ovals around the variables wm,n and bm de-
note observed variables. zm,n is the mixture indi-
cator, the hidden variable, that chooses the topic for
the n-th word in the m-th block of text. Arrows in-
dicate conditional dependencies between variables.
For instance, the wm,n variable in the word space
and the bm variable in the block space have no di-
rect dependencies, i.e. it is assumed that the choice
of words in the generation of a block of text is in-
dependent of the block given a hidden variable. The
boxes represent ?plates?, i.e. replicates of sampling
steps with the variable in the lower left corner re-
ferring to the number of samples. For instance, the
?word plate? in Figure 1 illustrates N independently
and identically distributed repeated trials of the ran-
dom variable wm,n.
According to the topology of the asymmetric
AMDD Bayesian network from Figure 1, we can
specify the joint distribution of a word wm,n, a latent
topic zk and a block of text bm: P (wm,n, zk, bm) =
P (bm) ? P (zk|bm) ? P (wm,n|zk). The joint distribu-
tion of a block of text bm and a word wm,n is thus:
P (bm, wm,n) =
K?
k=1
P (wm,n, zk, bm) = P (bm)
?
?K
k=1 P (zk|bm)? ?? ?
mixing proportions
? P (wm,n|zk)
? ?? ?
mixture components
(1)
Equation 1 describes a special case of a finite mix-
ture model, i.e. it uses a convex combination of a set
of component distributions to model the observed
data. That is, each word in a block of text is seen
as a sample from a mixture model, where mixture
components are multinomials P (wm,n|zk) and the
mixing proportions are P (zk|bm).
2.1.2 Inferring and Employing the AMDD
Model
The Expectation-Maximization (EM) algorithm is
the most popular method to estimate the parameters
for mixture models to fit a training corpus. The
EM algorithm for AMDD is based on iteratively
maximizing the log-likelihood function: LPLSA =?M
m=1
?N
n=1f(bm, wm,n) ? logP (wm,n, bm). How-
ever, the EM algorithm for AMDD is prone to over-
fitting since the number of parameters to be esti-
mated grows linearly with the number of blocks of
text. In order to avoid this problem, we employed
the tempered version of the EM algorithm that has
been proposed by Hofmann (2001).
We use the density estimation method in AMDD
to reduce the dimension of the blocks-by-words
space. Thus, instead of using the words as ba-
sic units for each block of text representation, we
employ a ?topic? basis, assuming that a few top-
ics will capture more information than the entire
huge amount of words in the vocabulary. Thus,
the m-th block of text is represented by the vector
(P (z1|bm), P (z2|bm), ..., P (zk|bm)). Then, we use
these posterior probabilities as a threshold to iden-
tify the boundaries of thematic episodes via sup-
port vector classification (Georgescul et al, 2006).
That is, we consider the topic segmentation task as a
binary-classification problem, where each utterance
should be classified as marking the presence or the
absence of a topic shift in the dialogue.
2.2 Latent Dirichlet Allocation (LDA)
Latent Dirichlet Allocation (Blei et al, 2003) can
be seen as an extension of AMDD by defining a
probabilistic mixture model that includes Dirichlet-
distributed priors over the masses of the multinomi-
als P (w|z) and P (z|b).
2.2.1 Model Setting
In order to describe the formal setting of LDA in
our context, we use the following notation in addi-
tion to those given at the beginning of Section 2:
? ~?m is a parameter notation for P (z|b = bm),
the topic mixture proportion for the m-th block
of text;
? ~? is a hyperparameter (a vector of dimension
K) on the mixing proportions ~?m;
? ? =
{
~?m
}M
m=1
is a matrix (of dimension
M ? K), composed by placing the vectors
~?1, ~?2, ..., ~?M as column components;
? ~?k is a parameter notation for P (w|zk), the
mixture component for topic k;
? ~? is a hyperparameter (a vector of dimension
N ) on the mixture components ~?k ;
927
? ? = {~?k}
K
k=1 is a matrix of dimension
K ? N composed by placing the vectors
~?1, ~?2, ..., ~?K as column components;
? Nm denotes the length of the m-th block of text
and is modeled with a Poisson distribution with
constant parameter ?;
 
 
 
 
 
word plate 
?
?  
??  
topic plate 
K 
n,mw  ?k?  
n,mz  
Nm 
m
??  
M 
block  plate 
Figure 2: Graphical model representation of latent
Dirichlet alocation.
LDA generates a stream of observable words
wm,n partitioned into blocks of text ~bm as shown
by the graphical model in Figure 2. The Bayesian
network can be interpreted as follows: the variables
?, ? and z are the three sets of latent variables that
we would like to infer. The plate surrounding ~?k il-
lustrates the repeated sampling of word distributions
for each topic zk until K topics have been generated.
The plate surrounding ~?m illustrates the sampling of
a distribution over topics for each block b for a to-
tal of M blocks of text. The inner plate over zm,n
and wm,n illustrates the repeated sampling of topics
and words until Nm words have been generated for
a block~bm.
Each block of text is first generated by drawing
a topic proportion ~?m, i.e. by picking a distribution
over topics from a Dirichlet distribution. For each
word wm,n from a block of text~bm, a topic indicator
k is sampled for zm,n according to the block-specific
mixture proportion ~?m. That is, ~?m determines
P (zm,n). The topic probabilities ~?k are also sam-
pled from a Dirichlet distribution. The words in each
block of text are then generated by using the corre-
sponding topic-specific term distribution ~?zm,n .
Given the graphical representation of LDA illus-
trated in Figure 2, we can write the joint distribution
of a word wm,n and a topic zk as:
P (wm,n, zk|~?m,?) = P (zk|~?m) ? P (wm,n|~?k).
Summing over k, we obtain the marginal distribu-
tion:
P (wm,n|~?m,?) =
?K
k=1
?
?
? P (zk|~?m)
? ?? ?
mixture proportion
? P (wm,n|~?k)
? ?? ?
mixture component
?
?
?.
Hence, similarly to AMDD (see Equation 1), the
LDA model assumes that a word wm,n is generated
from a random mixture over topics. Topic proba-
bilities are conditioned on the block of text a word
belongs to. Moreover LDA leaves flexibility to
assign a different topic to every observed word and
a different proportion of topics for every block of
text.
The joint distribution of a block of text ~bm
and the latent variables of the model ~zm, ~?m,
?, given the hyperparameters ~?, ~? is further
specified by: P (~bm, ~zm, ~?m,?|~?, ~?) =
topic plate
? ?? ?
P (?|~?) ?
P (~?m|~?) ?
Nm?
n=1
word plate
? ?? ?
P (zm,n|~?m) ? P (wm,n|~?zm,n)
? ?? ?
block plate
.
Therefore, the likelihood of a block~bm is derived
as the marginal distribution obtained by summing
over the zm,n and integrating out the distributions
~?m and ?.
2.2.2 Inferring and Employing the LDA Model
Since the integral involved in computing the like-
lihood of a block ~bm is computationally intractable,
several methods for approximating this posterior
have been proposed, including variational expecta-
tion maximization (Blei et al, 2003) and Markov
chain Monte Carlo methods (Griffiths and Steyvers,
2004).
We follow an approach based on Gibbs sampling
as proposed in (Griffiths and Steyvers, 2004). As
the convergence criteria for the Markov chain, we
928
check how well the parameters cluster semantically
related blocks of text in a training corpus and then
we use these values as estimates for comparable set-
tings.
The LDA model provides a soft clustering of the
blocks of text, by associating them to topics. We
exploit this clustering information, by using the dis-
tribution of topics over blocks of text to further
measure the inter-blocks similarity. As in Section
2.1.2, the last step of our system consists in em-
ploying binary support vector classification to iden-
tify the boundaries of thematic episodes in the text.
That is, we consider as input features for support
vector learning the component values of the vector
(?m,z1 , ?m,z2 , ..., ?m,zk).
3 Experiments
In order to evaluate the performance of AMDD and
LDA for our task of topic segmentation, in our ex-
periments we used the transcripts of ICSI-MR cor-
pus (Janin et al, 2004), which consists of 75 meet-
ing recordings. A subset of 25 meetings, which are
transcribed by humans and annotated with thematic
boundaries (Galley et al, 2003), has been kept for
testing purposes and support vector machine train-
ing. The transcripts of the remaining 50 meetings
have been used for the unsupervised inference of
our latent models. The fitting phase of the mix-
ture models rely on the same data set that have been
pre-processed by tokenization, elimination of stop-
words and lemmatization.
Once the models? parameters are learned, the in-
put data representation is projected into the lower
dimension latent semantic space. The evaluation
phase consists in checking the performance of each
model for predicting thematic boundaries. That is,
we check the performance of the models for predict-
ing thematic boundaries on the same test set. The
size of a block of text during the testing phase has
been set to one, i.e. each utterance has been consid-
ered as a block of text.
Figure 3 compares the performance obtained for
various k values, i.e. various dimensions of the latent
semantic space, or equivalently different numbers of
latent topics. We have chosen k={50, ...400} using
incremental steps of 50.
The performance of each latent model is mea-
0.000
0
0.100
0
0.200
0
0.300
0
0.400
0
0.500
0
0.600
0
0.700
0
0.800
0
0.900
0
50
100
150
200
250
300
350
400
Laten
t spa
ce di
men
sion
Accuracy
PLSA LDA
Figure 3: Results of applying the mixture models for
topic segmentation.
sured by the accuracy Acc = 1 ? Pk, where Pk
denotes the error measure proposed by (Beeferman
et al, 1999). Note that the Pk error allows for a
slight variation in where the hypothesized thematic
boundaries are placed. That is, wrong hypothesized
thematic boundaries occurring in the proximity of
a reference boundary (i.e. in a fixed-size interval of
text) are tolerated. As proposed by (Beeferman et
al., 1999), we set up the size of this interval to half
of the average number of words per segment in the
gold standard segmentation.
As we observe from Figure 3, LDA and AMDD
achieved rather comparable thematic segmenta-
tion accuracy. While LDA steadily outperformed
AMDD, the results do not show a notable advan-
tage of LDA over AMDD. In contrast, AMDD has
better performances for less dimensionality reduc-
tion. That is, the LDA performance curve goes down
when the number of latent topics exceeds over 300.
LDA LCSeg SVMs
Pk error rate 21% 32 % 22%
Table 1: Comparative performance results.
In Table 1, we provide the best results obtained
on ICSI data via LDA modeling. We also reproduce
the results reported on in the literature by (Galley
et al, 2003) and (Georgescul et al, 2006), when
the evaluation of their systems was also done on
ICSI data. The LCSeg system proposed by (Gal-
ley et al, 2003) is based on exploiting merely lex-
ical features. Improved performance results have
929
been obtained by (Galley et al, 2003) when extra
non-lexical features have been adopted in a decision
tree classifier. The system proposed by (Georges-
cul et al, 2006) is based on support vector machines
(SVMs) and is labeled in the table as SVMs. We
observe from the table that our approach based on
combining LDA modeling with SVM classification
outperforms LCSeg and performs comparably to the
system of Georgescul et al (2006). Thus, our exper-
iments show that the LDA word density estimation
approach does capture important information from
the data through 90% less features than a bag-of-
words representation.
4 Conclusions
With the goal of performing linear topic segmen-
tation by exploiting word distributions in the input
text, the focus of this article was on both comparing
theoretical aspects and experimental results of two
probabilistic mixture models. The algorithms are
applied to a meeting transcription data set and are
found to provide an appropriate method for reduc-
ing the size of the data representation, by perform-
ing comparably to previous state-of-the-art methods
for topic segmentation.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34:177?210. Special Issue on Natural
Language Learning.
David M. Blei, Andrew Y. Ng, and Michael Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, pages 993?1022.
Freddy Choi. 2000. Advances in Domain Indepen-
dent Linear Text Segmentation. In Proceedings of the
1st Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL),
Seattle, USA.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multi-Party Conversation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 562?569,
Sapporo, Japan.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. Word Distributions for Thematic Seg-
mentation in a Support Vector Machine Approach. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 101?108,
New York City, USA.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2007. Exploiting Structural Meeting-Specific
Features for Topic Segmentation. In Actes de la
14e`me Confe?rence sur le Traitement Automatique des
Langues Naturelles (TALN), pages 15?24, Toulouse,
France.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing Scientific Topics. In Proceedings of the National
Academy of Sciences, volume 101, pages 5228?5235.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Machine
Learning, 42:177?196.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon,
Jane Edwards, Javier Macias-Guarasa, Nelson Mor-
gan, Barbara Peskin, Elizabeth Shriberg, Andreas
Stolcke, Chuck Wooters, and Britta Wrede. 2004.
The ICSI Meeting Project: Resources and Research.
In Proceedings of the International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
Meeting Recognition Workshop, Montreal, Quebec,
Canada.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics (COL-
ING/ACL), pages 25?32, Sydney, Australia.
Jane Morris and Graeme Hirst. 1991. Lexical Cohe-
sion Computed by Thesaural Relations as an Indicator
of the Structure of Text. Computational Linguistics,
17(1):21?48.
Matthew Purver, Konrad P. Ko?rding, Thomas L. Grif-
fiths, and Joshua B. Tenenbaum. 2006. Unsupervised
Topic Modelling for Multi-Party Spoken Discourse.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING/ACL), pages 17?24, Sydney, Australia.
Jeffrey Reynar. 1994. An Automatic Method of Finding
Topic Boundaries. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 331?333, Las Cruces, New Mexico,
USA.
930
