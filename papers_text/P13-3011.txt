Proceedings of the ACL Student Research Workshop, pages 74?80,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Annotating named entities in clinical text
by combining pre-annotation and active learning
Maria Skeppstedt
Dept. of Computer and Systems Sciences (DSV)
Stockholm University, Forum 100, 164 40 Kista, Sweden
mariask@dsv.su.se
Abstract
For expanding a corpus of clinical text, an-
notated for named entities, a method that
combines pre-tagging with a version of ac-
tive learning is proposed. In order to fa-
cilitate annotation and to avoid bias, two
alternative automatic pre-taggings are pre-
sented to the annotator, without reveal-
ing which of them is given a higher con-
fidence by the pre-tagging system. The
task of the annotator is to select the cor-
rect version among these two alternatives.
To minimise the instances in which none
of the presented pre-taggings is correct,
the texts presented to the annotator are ac-
tively selected from a pool of unlabelled
text, with the selection criterion that one
of the presented pre-taggings should have
a high probability of being correct, while
still being useful for improving the result
of an automatic classifier.
1 Introduction
One of the key challenges for many NLP appli-
cations is to create the annotated corpus needed
for development and evaluation of the application.
Such a corpus is typically created through man-
ual annotation, which is a time-consuming task.
Therefore, there is a need to explore methods for
simplifying the annotation task and for reducing
the amount of data that must be annotated.
Annotation can be simplified by automatic pre-
annotation, in which the task of the annotator is
to improve or correct annotations provided by an
existing system. The amount of data needed to be
annotated can be reduced by active learning, i.e.
by actively selecting data to annotate that is useful
to a machine learning system. When using pre-
tagged data, the annotator might, however, be bi-
ased to choose the annotation provided by the pre-
tagger. Also, if the produced pre-taggings are not
good enough, it is still a time-consuming task to
correct them or select the correct tagging among
many suggestions.
Consequently, there is a need to further explore
how an annotated corpus can be expanded with
less effort and using methods that will not bias the
annotators.
2 Background
The background discusses basic ideas of pre-
annotation and active learning, as well as the parti-
cular challenges associated with annotating clini-
cal text.
2.1 Annotating clinical text
A number of text annotation projects have been
carried out in the clinical domain, some of them
including annotations of clinical named entities,
such as mentions of symptoms, diseases and med-
ication. Such studies have for example been
described by Ogren et al (2008), Chapman et
al. (2008), Roberts et al (2009), Wang (2009),
Uzuner et al (2010), Koeling et al (2011) and Al-
bright et al (2013).
As in many specialised domains, expert annota-
tors are typically required to create a reliable an-
notated clinical corpus. These expert annotators
are often more expensive than annotators without
the required specialised knowledge. It is also diffi-
cult to use crowdsourcing approaches, such as us-
ing e.g. Amazon?s Mechanical Turk to hire on-
line annotators with the required knowledge (Xia
and Yetisgen-Yildiz, 2012). A further challenge
is posed by the content of the clinical data, which
is often sensitive and should therefore only be ac-
cessed by a limited number of people. Research
community annotation is consequently another op-
tion that is not always open to annotation projects
in the clinical domain, even if there are examples
of such community annotations also for clinical
text, e.g. described by Uzuner et al (2010).
74
To simplify the annotation process, and to min-
imise the amount of annotated data is therefore
even more important for annotations in the clini-
cal domain than for annotation in general.
2.2 Pre-annotation
A way to simplify annotation is automatic pre-
annotation (or pre-tagging), in which a text is auto-
matically annotated by an existing system, before
it is given to the annotator. Instead of annotating
unlabelled data, the annotator either corrects mis-
takes made by this existing system (Chou et al,
2006), or chooses between different taggings pro-
vided by the system (Brants and Plaehn, 2000).
The system providing the pre-annotations could
be rule- or terminology based, not requiring an-
notated data (Mykowiecka and Marciniak, 2011),
as well as a machine learning/hybrid system that
uses the annotations provided by the annotator to
constantly improve the pre-annotation (Tomanek
et al, 2012). There exist several annotation tools
that facilitate the use of pre-annotation by allow-
ing the user to import pre-annotations or by pro-
viding pre-annotation included in the tools (Neves
and Leser, 2012).
A condition for pre-annotation to be useful is
that the produced annotations are good enough, or
the effect can be the opposite, slowing the annota-
tors down (Ogren et al, 2008). Another potential
problem with pre-annotation is that it might bias
towards the annotations given by the pre-tagging,
for instance if a good pre-tagger reduces the atten-
tion of the annotators (Fort and Sagot, 2010).
2.3 Active learning
Active learning can be used to reduce the amount
of annotated data needed to successfully train a
machine learning model. Instead of randomly se-
lecting annotation data, instances in the data that
are highly informative, and thereby also highly
useful for the machine learning system, are then
actively selected. (Olsson, 2008, p. 27).
There are several methods for selecting the
most informative instances among the unlabelled
ones in the available pool of data. A frequently
used method is uncertainty sampling, in which in-
stances that the machine learner is least certain
how to classify are selected for annotation. For
a model learning to classify into two classes, in-
stances, for which the classifier has no clear pref-
erence for one of the two alternatives, are chosen
for annotation. If there are more than two classes,
the confidence for the most probable class can be
used as the measure of uncertainty. Only using the
certainty level for the most probable classification
means that not all available information is used,
i.e. the information of the certainty levels for the
less probable classes. (Settles, 2009)
An alternative for a multi-class classifier is
therefore to instead use the difference of the cer-
tainty levels for the two most probable classes. If
cp1 is the most probable class and cp2 is the sec-
ond most probable class for the observation xn,
the margin used for measuring uncertainty for that
instance is:
Mn = P (cp1|xn)? P (cp2|xn) (1)
An instance with a large margin is easy to clas-
sify because the classifier is much more certain of
the most probable classification than on the second
most probable. Instances with a small margin, on
the other hand, are difficult to classify, and there-
fore instances with a small margin are selected for
annotation (Schein and Ungar, 2007). A common
alternative is to use entropy as an uncertainty mea-
sure, which takes the certainty levels of all possi-
ble classes into account (Settles, 2009).
There are also a number of other possible meth-
ods for selecting informative instances for anno-
tation, for instance to use a committee of learners
and select the instances for which the committee
disagrees the most, or to search for annotation in-
stances that would result in the largest expected
change to the current model (Settles, 2009).
There are also methods to ensure that the se-
lected data correctly reflects the distribution in the
pool of unlabelled data, avoiding a selection of
outliers that would not lead to a correct model of
the available data. Such methods for structured
prediction have been described by Symons et al
(2006) and Settles and Craven (2008).
Many different machine learning methods have
been used together with active learning for solving
various NLP tasks. Support vector machines have
been used for text classification (Tong and Koller,
2002), using properties of the support vector ma-
chine algorithm for determining what unlabelled
data to select for classification. For structured out-
put tasks, such as named entity recognition, hid-
den markov models have been used by Scheffer
et al (2001) and conditional random fields (CRF)
by Settles and Craven (2008) and Symons et al
(2006).
75
Olsson (2008) suggests combining active learn-
ing and pre-annotation for a named entity recogni-
tion task, that is providing the annotator with pre-
tagged data from an actively learned named entity
recogniser. It is proposed not to indiscriminately
pre-tagg the data, but to only provide those pre-
annotated labels to the human annotator, for which
the pre-tagger is relatively certain.
3 Method
Previous research on pre-annotation shows two
seemingly incompatible desirable properties in a
pre-annotation system. A pre-annotation that is
not good enough might slow the human annota-
tor down, whereas a good pre-annotation might
make the annotator lose concentration, trusting the
pre-annotation too much, resulting in a biased an-
notation. One possibility suggested in previous
research, is to only provide pre-annotations for
which the pre-annotation system is certain of its
classification. For annotations of named entities in
text, this would mean to only provide pre-tagged
entities for which the pre-annotations system is
certain. Such a high precision pre-tagger might,
however, also bias the human annotator towards
not correcting the pre-annotation.
Even more incompatible seems a combination
between pre-annotation and active learning, that
is to provide the human annotator with pre-tagged
data that has been selected for active learning.
The data selected for annotation when using active
learning, is the data for which the pre-annotator is
most uncertain and therefore the data which would
be least suitable for pre-annotation.
The method proposed here aims at finding a
way of combining pre-annotation and active learn-
ing while reducing the risk of annotation bias.
Thereby decreasing the amount of data that needs
to be annotated as well as facilitating the annota-
tion, without introducing bias. A previous version
of this idea has been outlined by Skeppstedt and
Dalianis (2012).
The method is focused on the annotation of
named entities in clinical text, that is marking of
spans of text as well as classification of the spans
into an entity class.
3.1 Pre-annotation
As in standard pre-annotation, the annotator will
be presented with pre-tagged data, and does not
have to annotate the data from scratch.
To reduce the bias problem that might be asso-
ciated with pre-tagging, the mode of presentation
will, however, be slightly different in the method
proposed here. Instead of presenting the best tag-
ging for the human annotator to correct, or to
present the n best taggings, the two best taggings
produced by a pre-tagger will be presented, with-
out informing the annotator which of them that the
pre-tagger considers most likely.
When being presented with two possible anno-
tations of the same text without knowing which of
them that the pre-annotation system considers as
most likely, the annotator always has to make an
active choice of which annotation to choose. This
reduces the bias to one particular pre-annotation,
thereby eliminating a drawback associated with
standard pre-annotation. Having to consider two
alternatives might add cognitive load to the anno-
tator compared to correcting one alternative, but
ought to be easier than annotating a text that is not
pre-tagged.
The reason for presenting two annotations, as
opposed to three or more, is that it is relatively
easy to compare two texts, letting your eyes wan-
der from one text to the other, when you have one
comparison to make. Having three optional an-
notations would result in three comparisons, and
having four would result in six comparisons, and
so on. Therefore, having two optional annotations
to choose from, reduces the bias problem while at
the same time still offering a method for speeding
up the annotation.
A simple Java program for choosing between
two alternative pre-annotated sentences has been
created (Figure 1). The program randomly
chooses in which of the two text boxes to place
which pre-annotation. The user can either choose
the left or the right annotation, or that none of them
is correct.
The data will be split into sentences, and one
sentence at time will be presented to the annotator
for annotation.
3.2 Active learning
To choose from two presented annotations might
also potentially be faster than making corrections
to one presented annotation. For this to be the
case, however, one of the presented annotations
has to be a correct annotation. In order to achieve
that, the proposed method is to use a version of
active learning.
76
Figure 1: A simple program for choosing between two alternative annotations, showing a constructed
example in English.
The standard use of active learning is to actively
select instances to annotate that are useful to a ma-
chine learner. Instances for which the machine
learning model can make a confident classifica-
tion are not presented to the annotator, as these
instances will be of little benefit for improving the
machine learning system.
The version of active learning proposed here is
retaining this general idea of active learning, but
is also adding an additional constraint to what in-
stances that are actively selected for annotation.
This constraint is to only select text passages for
which it is probable that one of the two best
pre-taggings is correct, i.e. the pre-tagger has to
be confident that one of the two presented pre-
annotations is correct, but it should be uncertain
as to which one of them is correct.
For ensuring that the sentences selected for an-
notation are informative enough, the previously
described difference of the certainty level of the
two most probable classes will be used. The same
standard for expressing margin as used in (1), can
be used here, except that in (1), cp1 and cp2 stand
for classification of one instance, whereas in this
case the output is a sequence of labels, labelling
each token in a sentence. Therefore, cp1 and cp2
stand for the classification of a sequence of labels.
Let cp1 be the most probable labelling sequence,
cp2 the second most probable labelling sequence
and cp3 the third most probable labelling sequence.
Moreover, let xn be the observations in sentence
n, then the following margins can be defined for
that sentence:
MtoSecond n = P (cp1|xn)? P (cp2|xn) (2)
MtoThird n = P (cp1|xn)? P (cp3|xn) (3)
To make the probability high that one of the
two presented pre-annotations is correct, the same
method that is used for determining that an an-
notation instance is informative enough could be
used. However, instead of minimising the margin
between two classification instances, it is ensured
that the margin in high enough. That is, the differ-
ence in certainty level between the two most prob-
able annotations and the third most probable must
be high enough to make it probable that one of the
two best classification candidates is correct. This
can be achieved by forcing MtoThird to be above a
threshold, t.
The criteria for selecting the next candidate sen-
tence to annotate can then be described as:
x? = argmin
x
P (cp1|x)? P (cp2|x) (4)
where
P (cp1|x)? P (cp3|x) > t
As instances with the highest possible P (cp2|x)
in relation to P (cp1|x) are favoured, no threshold
for the margin between P (cp2|x) and P (cp3|x) is
needed.
It might be difficult to automatically determine
an appropriate value of the threshold t. Therefore,
the proposed method for finding a good threshold,
is to adapt it to the behaviour of the annotator. If
the annotator often rejects the two presented pre-
taggings, text passages for which the pre-tagger is
more certain ought to be selected, that is the value
of t ought to be increased. On the other hand,
if one of the presented pre-taggings often is se-
lected by the annotator as the correct annotation,
the value of t can be decreased, possibly allowing
for annotation instances with a smaller MtoSecond.
3.3 Machine learning system
As machine learning system, the conditional ran-
dom fields system CRF++ (Kudo, 2013) will be
77
used. This system uses a combination of forward
Viterbi and backward A* search for finding the
best classification sequence for an input sentence,
given the trained model. It can also produce the
n-best classification sequences for each sentence,
which is necessary for the proposed pre-tagger that
presents the two best pre-taggings to the human
annotator.
CRF++ can also give the conditional probably
for the output, that is for the entire classification
sequence of a sentence, which is needed in the pro-
posed active learning algorithm.
3.4 Materials
There is a corpus of Swedish clinical text, i.e.
the text in the narrative part of the health record,
that contains clinical text from the Stockholm area,
from the years 2006-2008 (Dalianis et al, 2009).
A subset of this corpus, containing texts from an
emergency unit of internal medicine, has been an-
notated for four types of named entities: disorder,
finding, pharmaceutical drug and body structure
(Skeppstedt et al, 2012). For approximately one
third of this annotated corpus, double annotation
has been performed, and the instances, for which
there were a disagreement, have been resolved by
one of the annotators.
The annotated corpus will form the main source
of materials for the study proposed here, and addi-
tional data to annotate will be selected from a pool
of unlabelled data from internal medicine emer-
gency notes.
The larger subset of the annotated data, only
annotated by one annotator, will be referred to
as Single (containing 45 482 tokens), and the
smaller subset, annotated by two annotators, will
be referred to as Double (containing 25 370 to-
kens). The Single subset will be the main source
for developing the pre-annotation/active learning
method, whereas the Double subset will be used
for a final evaluation.
3.5 Step-by-step explanation
The proposed method can be divided into 8 steps:
1. Train a CRF model with a randomly selected
subset of the Single part of the annotated cor-
pus, the seed set. The size of this seed set, as
well as suitable features for the CRF model
will be evaluated using cross validation on
the seed set. The size should be as small as
possible, limiting the amount of initial anno-
tation needed, but large enough to have re-
sults in line with a baseline system using ter-
minology matching for named entity recog-
nition (Skeppstedt et al, 2012).
2. Apply the constructed CRF model on unla-
belled data from the pool of data from in-
ternal medicine emergency notes. Let the
model, which operates on a sentence level,
provide the three most probable label se-
quences for each sentence, together with its
level of certainty.
3. Calculate the difference in certainty be-
tween the most probable and the third most
probable suggestion sequence for each sen-
tence, that is MtoThird. Start with a low
threshold t and place all sentences with
MtoThird above the threshold t in a list of
candidates for presenting to the annotator
(that is the sentences fulfilling the criterion
P (cp1|x)? P (cp3|x) > t).
4. Order the sentences in the list of se-
lected candidates in increasing order of
MtoSecond. Present the sentence with the
lowest MtoSecond to the annotator. This is the
sentence, for which the pre-tagger is most un-
certain of which one of the two most probable
pre-taggings is correct.
Present the most probable pre-annotation
as well as the second most probable pre-
annotation, as shown in Figure 1.
5. If the annotator chooses that none of the pre-
sented pre-annotations is correct, discard the
previous candidate selection and make a new
one from the pool with a higher threshold
value t. Again, order the sentences in increas-
ing order of MtoSecond, and present the sen-
tence with the lowest MtoSecond to the anno-
tator.
Repeat step 3., 4. and 5., gradually increasing
the threshold until the annotator accepts one
of the presented pre-annotations.
6. Continue presenting the annotator with the
two most probable pre-annotations for the
sentences in the list of selected candidate
sentences, and allow the human annotator to
choose one of the pre-annotations.
78
The threshold t could be further adjusted ac-
cording to how often the option ?None? is
chosen.
7. Each selected annotation is added to a set
of annotated data. When a sufficiently large
amount of new sentences have been added to
this set, the model needs to be retrained with
the new data. The retraining of the model can
be carried out as a background process while
the human annotator is annotating. In or-
der to use the annotator time efficiently, there
should not be any waiting time while retrain-
ing.
8. When the model has been retrained, the pro-
cess starts over from step 2.
3.6 Evaluation
The text passages chosen in the selection process
will, as explained above, be used to re-train the
machine learning model, and used when select-
ing new text passages for annotation. The effect
of adding additional annotations will also be con-
stantly measured, using cross validation on the
seed set. The additional data added by the active
learning experiments will, however, not be used
in the validation part of the cross validation, but
only be used as additional training data, in order to
make sure that the results are not improved due to
easily classified examples being added to the cor-
pus.
When an actively selected corpus of the same
size as the entire Single subset of the corpus has
been created, this actively selected corpus will be
used for training a machine learning model. The
performance of this model will then be compared
to a model trained on the single subset. Both mod-
els will be evaluated on the Double subset of the
corpus. The hypothesis is that the machine learn-
ing model trained on the corpus partly created by
pre-tagging and active learning will perform bet-
ter than the model created on the original Single
subset.
4 Conclusion
A method that combines pre-annotation and active
learning, while reducing annotation bias, is pro-
posed. A program for presenting pre-annotated
data to the human annotator for selection has been
constructed, and a corpus of annotated data suit-
able as a seed set and as evaluation data has
been constructed. The active learning part of the
proposed method remains, however, to be imple-
mented.
Applying the proposed methods aims at creat-
ing a corpus suitable for training a machine learn-
ing system to recognise the four entities Disorder,
Finding, Pharmaceutical drug and Body struc-
ture. Moreover, methods for facilitating annotated
corpus construction will be explored, potentially
adding new knowledge to the science of annota-
tion.
Acknowledgements
I am very grateful to the reviewers and the pre-
submission mentor for their many valuable com-
ments. I would also like to thank Hercules Dalia-
nis and Magnus Ahltorp as well as the participants
of the ?Southern California Workshop on Medical
Text Analysis and Visualization? for fruitful dis-
cussions on the proposed method.
References
Daniel Albright, Arrick Lanfranchi, Anwen Fredrik-
sen, William F 4th Styler, Colin Warner, Jena D
Hwang, Jinho D Choi, Dmitriy Dligach, Rod-
ney D Nielsen, James Martin, Wayne Ward, Martha
Palmer, and Guergana K Savova. 2013. Towards
comprehensive syntactic and semantic annotations
of the clinical narrative. J Am Med Inform Assoc,
Jan.
Thorsten Brants and Oliver Plaehn. 2000. Interactive
corpus annotation. In LREC. European Language
Resources Association.
Wendy W Chapman, John N Dowling, and George
Hripcsak. 2008. Evaluation of training with an an-
notation schema for manual annotation of clinical
conditions from emergency department reports. Int
J Med Inform, Epub 2007 Feb 20, 77(2):107?113,
February.
Wen-chi Chou, Richard Tzong-han Tsai, and Ying-
shan Su. 2006. A semi-automatic method for anno-
tating a biomedical proposition bank. In FLAC?06.
ACL.
Hercules Dalianis, Martin Hassel, and Sumithra
Velupillai. 2009. The Stockholm EPR Corpus -
Characteristics and Some Initial Findings. In Pro-
ceedings of ISHIMR 2009, Evaluation and imple-
mentation of e-health and health information initia-
tives: international perspectives. 14th International
Symposium for Health Information Management Re-
search, Kalmar, Sweden, pages 243?249.
Kare?n Fort and Beno??t Sagot. 2010. Influence of
pre-annotation on pos-tagged corpus development.
79
In Proceedings of the Fourth Linguistic Annotation
Workshop, LAW IV ?10, pages 56?63, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Rob Koeling, John Carroll, Rosemary Tate, and
Amanda Nicholson. 2011. Annotating a corpus of
clinical text records for learning to recognize symp-
toms automatically. In Proceedings of the LOUHI
2011, Third International Workshop on Health Doc-
ument Text Mining and Information Analysis.
Taku Kudo. 2013. CRF++: Yet Another CRF toolkit.
http://crfpp.sourceforge.net/. Accessed 2013-05-21.
Agnieszka Mykowiecka and Ma?gorzata Marciniak.
2011. Some remarks on automatic semantic an-
notation of a medical corpus. In Proceedings of
the LOUHI 2011, Third International Workshop
on Health Document Text Mining and Information
Analysis.
Mariana Neves and Ulf Leser. 2012. A survey on an-
notation tools for the biomedical literature. Brief-
ings in Bioinformatics.
Philip Ogren, Guergana Savova, and Christopher
Chute. 2008. Constructing evaluation corpora for
automated clinical named entity recognition. In Pro-
ceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), pages 3143?
3149, Marrakech, Morocco, May. European Lan-
guage Resources Association (ELRA).
Fredrik Olsson. 2008. Bootstrapping Named Entity
Annotation by Means of Active Machine Learning.
Ph.D. thesis, University of Gothenburg. Faculty of
Arts.
Angus Roberts, Robert Gaizauskas, Mark Hepple,
George Demetriou, Yikun Guo, Ian Roberts, and
Andrea Setzer. 2009. Building a semantically an-
notated corpus of clinical texts. J. of Biomedical In-
formatics, 42:950?966, October.
Tobias Scheffer, Christian Decomain, and Stefan Wro-
bel. 2001. Active hidden markov models for in-
formation extraction. In Proceedings of the 4th In-
ternational Conference on Advances in Intelligent
Data Analysis, IDA ?01, pages 309?318, London,
UK, UK. Springer-Verlag.
Andrew I. Schein and Lyle H. Ungar. 2007. Ac-
tive learning for logistic regression: an evaluation.
Mach. Learn., 68(3):235?265, October.
Burr Settles and Mark Craven. 2008. An analysis
of active learning strategies for sequence labeling
tasks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?08, pages 1070?1079, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Burr Settles. 2009. Active learning literature survey.
Computer Sciences Technical Report 1648, Univer-
sity of Wisconsin?Madison.
Maria Skeppstedt and Hercules Dalianis. 2012. Using
active learning and pre-tagging for annotating clin-
ical findings in health record text. In Proceedings
of SMBM 2012 - The 5th International Symposium
on Semantic Mining in Biomedicine, pages 98?99,
Zurich, Switzerland, September 3-4.
Maria Skeppstedt, Maria Kvist, and Hercules Dalianis.
2012. Rule-based entity recognition and coverage of
SNOMED CT in Swedish clinical text. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), pages
1250?1257, Istanbul, Turkey, may. European Lan-
guage Resources Association (ELRA).
Christopher T. Symons, Nagiza F. Samatova, Ramya
Krishnamurthy, Byung H. Park, Tarik Umar, David
Buttler, Terence Critchlow, and David Hysom.
2006. Multi-criterion active learning in conditional
random fields. In Proceedings of the 18th IEEE In-
ternational Conference on Tools with Artificial In-
telligence, ICTAI ?06, pages 323?331, Washington,
DC, USA. IEEE Computer Society.
Katrin Tomanek, Philipp Daumke, Frank Enders, Jens
Huber, Katharina Theres, and Marcel Mu?ller. 2012.
An interactive de-identification-system. In Proceed-
ings of SMBM 2012 - The 5th International Sympo-
sium on Semantic Mining in Biomedicine, pages 82?
86, Zurich, Switzerland, September 3-4.
Simon Tong and Daphne Koller. 2002. Support
vector machine active learning with applications to
text classification. J. Mach. Learn. Res., 2:45?66,
March.
O?zlem Uzuner, Imre Solti, Fei Xia, and Eithon
Cadag. 2010. Community annotation experiment
for ground truth generation for the i2b2 medication
challenge. J Am Med Inform Assoc, 17(5):519?523.
Yefeng Wang. 2009. Annotating and recognising
named entities in clinical notes. In Proceedings of
the ACL-IJCNLP Student Research Workshop, pages
18?26, Singapore.
Fei Xia and Meliha Yetisgen-Yildiz. 2012. Clinical
corpus annotation: Challenges and strategies. In
The Third Workshop on Building and Evaluating Re-
sources for Biomedical Text Mining (BioTxtM), an
LREC Workshop. Turkey.
80
