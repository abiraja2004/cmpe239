2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 347?351,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Insertion and Deletion Models for Statistical Machine Translation
Matthias Huck and Hermann Ney
Human Language Technology and Pattern Recognition Group
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{huck,ney}@cs.rwth-aachen.de
Abstract
We investigate insertion and deletion models
for hierarchical phrase-based statistical ma-
chine translation. Insertion and deletion mod-
els are designed as a means to avoid the omis-
sion of content words in the hypotheses. In
our case, they are implemented as phrase-level
feature functions which count the number of
inserted or deleted words. An English word is
considered inserted or deleted based on lex-
ical probabilities with the words on the for-
eign language side of the phrase. Related tech-
niques have been employed before by Och et
al. (2003) in an n-best reranking framework
and by Mauser et al. (2006) and Zens (2008)
in a standard phrase-based translation system.
We propose novel thresholding methods in
this work and study insertion and deletion fea-
tures which are based on two different types of
lexicon models. We give an extensive exper-
imental evaluation of all these variants on the
NIST Chinese?English translation task.
1 Insertion and Deletion Models
In hierarchical phrase-based translation (Chiang,
2005), we deal with rules X ? ??, ?,? ? where
??, ?? is a bilingual phrase pair that may contain
symbols from a non-terminal set, i.e. ? ? (N ?
VF )+ and ? ? (N ?VE)+, where VF and VE are the
source and target vocabulary, respectively, and N is
a non-terminal set which is shared by source and tar-
get. The left-hand side of the rule is a non-terminal
symbol X ? N , and the ? relation denotes a one-
to-one correspondence between the non-terminals in
? and in ?. Let J? denote the number of terminal
symbols in ? and I? the number of terminal sym-
bols in ?. Indexing ? with j, i.e. the symbol ?j ,
1 ? j ? J?, denotes the j-th terminal symbol on
the source side of the phrase pair ??, ??, and analo-
gous with ?i, 1 ? i ? I? , on the target side.
With these notational conventions, we now de-
fine our insertion and deletion models, each in both
source-to-target and target-to-source direction. We
give phrase-level scoring functions for the four fea-
tures. In our implementation, the feature values are
precomputed and written to the phrase table. The
features are then incorporated directly into the log-
linear model combination of the decoder.
Our insertion model in source-to-target direction
ts2tIns(?) counts the number of inserted words on the
target side ? of a hierarchical rule with respect to the
source side ? of the rule:
ts2tIns(?, ?) =
I??
i=1
J??
j=1
[
p(?i|?j) < ??j
]
(1)
Here, [?] denotes a true or false statement: The result
is 1 if the condition is true and 0 if the condition is
false. The model considers an occurrence of a tar-
get word e an insertion iff no source word f exists
within the phrase where the lexical translation prob-
ability p(e|f) is greater than a corresponding thresh-
old ?f . We employ lexical translation probabilities
from two different types of lexicon models, a model
which is extracted from word-aligned training data
and?given the word alignment matrix?relies on
pure relative frequencies, and the IBM model 1 lex-
icon (cf. Section 2). For ?f , previous authors have
used a fixed heuristic value which was equal for all
347
f ? Vf . In Section 3, we describe how such a global
threshold can be computed and set in a reasonable
way based on the characteristics of the model. We
also propose several novel thresholding techniques
with distinct thresholds ?f for each source word f .
In an analogous manner to the source-to-target di-
rection, the insertion model in target-to-source di-
rection tt2sIns(?) counts the number of inserted words
on the source side ? of a hierarchical rule with re-
spect to the target side ? of the rule:
tt2sIns(?, ?) =
J??
j=1
I??
i=1
[p(?j |?i) < ??i ] (2)
Target-to-source lexical translation probabilities
p(f |e) are thresholded with values ?e which may be
distinct for each target word e. The model consid-
ers an occurrence of a source word f an insertion iff
no target word e exists within the phrase with p(f |e)
greater than or equal to ?e.
Our deletion model, compared to the insertion
model, interchanges the connection of the direction
of the lexical probabilities and the order of source
and target in the sum and product of the term. The
source-to-target deletion model thus differs from the
target-to-source insertion model in that it employs a
source-to-target word-based lexicon model.
The deletion model in source-to-target direction
ts2tDel(?) counts the number of deleted words on the
source side ? of a hierarchical rule with respect to
the target side ? of the rule:
ts2tDel(?, ?) =
J??
j=1
I??
i=1
[
p(?i|?j) < ??j
]
(3)
It considers an occurrence of a source word f a dele-
tion iff no target word e exists within the phrase with
p(e|f) greater than or equal to ?f .
The target-to-source deletion model tt2sDel(?) cor-
respondingly considers an occurrence of a target
word e a deletion iff no source word f exists within
the phrase with p(f |e) greater than or equal to ?e:
tt2sDel(?, ?) =
I??
i=1
J??
j=1
[p(?j |?i) < ??i ] (4)
2 Lexicon Models
We restrict ourselves to the description of the
source-to-target direction of the models.
2.1 Word Lexicon from Word-Aligned Data
Given a word-aligned parallel training corpus, we
are able to estimate single-word based translation
probabilities pRF(e|f) by relative frequency (Koehn
et al., 2003). With N(e, f) denoting counts of
aligned cooccurrences of target word e and source
word f , we can compute
pRF(e|f) =
N(e, f)
?
e? N(e
?, f)
. (5)
If an occurrence of e has multiple aligned source
words, each of the alignment links contributes with
a fractional count.
We denote this model as relative frequency (RF)
word lexicon.
2.2 IBM Model 1
The IBM model 1 lexicon (IBM-1) is the first and
most basic one in a sequence of probabilistic genera-
tive models (Brown et al., 1993). For IBM-1, several
simplifying assumptions are made, so that the proba-
bility of a target sentence eI1 given a source sentence
fJ0 (with f0 = NULL) can be modeled as
Pr(eI1|f
J
1 ) =
1
(J + 1)I
I?
i=1
J?
j=0
pibm1(ei|fj) . (6)
The parameters of IBM-1 are estimated iteratively
by means of the Expectation-Maximization algo-
rithm with maximum likelihood as training criterion.
3 Thresholding Methods
We introduce thresholding methods for insertion and
deletion models which set thresholds based on the
characteristics of the lexicon model that is applied.
For all the following thresholding methods, we dis-
regard entries in the lexicon model with probabilities
that are below a fixed floor value of 10?6. Again, we
restrict ourselves to the description of the source-to-
target direction.
individual ?f is a distinct value for each f , com-
puted as the arithmetic average of all entries
p(e|f) of any e with the given f in the lexicon
model.
348
MT06 (Dev) MT08 (Test)
NIST Chinese?English BLEU [%] TER [%] BLEU [%] TER [%]
Baseline (with s2t+t2s RF word lexicons) 32.6 61.2 25.2 66.6
+ s2t+t2s insertion model (RF, individual) 32.9 61.4 25.7 66.2
+ s2t+t2s insertion model (RF, global) 32.8 61.8 25.7 66.7
+ s2t+t2s insertion model (RF, histogram 10) 32.9 61.7 25.5 66.5
+ s2t+t2s insertion model (RF, all) 32.8 62.0 26.1 66.7
+ s2t+t2s insertion model (RF, median) 32.9 62.1 25.7 67.1
+ s2t+t2s deletion model (RF, individual) 32.7 61.4 25.6 66.5
+ s2t+t2s deletion model (RF, global) 33.0 61.3 25.8 66.1
+ s2t+t2s deletion model (RF, histogram 10) 32.9 61.4 26.0 66.1
+ s2t+t2s deletion model (RF, all) 33.0 61.4 25.9 66.4
+ s2t+t2s deletion model (RF, median) 32.9 61.5 25.8 66.7
+ s2t+t2s insertion model (IBM-1, individual) 33.0 61.4 26.1 66.4
+ s2t+t2s insertion model (IBM-1, global) 33.0 61.6 25.9 66.5
+ s2t+t2s insertion model (IBM-1, histogram 10) 33.7 61.3 26.2 66.5
+ s2t+t2s insertion model (IBM-1, median) 33.0 61.3 26.0 66.4
+ s2t+t2s deletion model (IBM-1, individual) 32.8 61.5 26.0 66.2
+ s2t+t2s deletion model (IBM-1, global) 32.9 61.3 25.9 66.1
+ s2t+t2s deletion model (IBM-1, histogram 10) 32.8 61.2 25.7 66.0
+ s2t+t2s deletion model (IBM-1, median) 32.8 61.6 25.6 66.7
+ s2t insertion + s2t deletion model (IBM-1, individual) 32.7 62.3 25.7 67.1
+ s2t insertion + t2s deletion model (IBM-1, individual) 32.7 62.2 25.9 66.8
+ t2s insertion + s2t deletion model (IBM-1, individual) 33.1 61.3 25.9 66.2
+ t2s insertion + t2s deletion model (IBM-1, individual) 33.0 61.3 26.1 66.0
+ source+target unaligned word count 32.3 61.8 25.6 66.7
+ phrase-level s2t+t2s IBM-1 word lexicons 33.8 60.5 26.9 65.4
+ source+target unaligned word count 34.0 60.4 26.7 65.8
+ s2t+t2s insertion model (IBM-1, histogram 10) 34.0 60.3 26.8 65.2
+ phrase-level s2t+t2s DWL + triplets + discrim. RO 34.8 59.8 27.7 64.7
+ s2t+t2s insertion model (RF, individual) 35.0 59.5 27.8 64.4
Table 1: Experimental results for the NIST Chinese?English translation task (truecase). s2t denotes source-to-target
scoring, t2s target-to-source scoring. Bold font indicates results that are significantly better than the baseline (p < .1).
global The same value ?f = ? is used for all f .
We compute this global threshold by averaging
over the individual thresholds.1
histogram n ?f is a distinct value for each f . ?f is
set to the value of the n+1-th largest probabil-
ity p(e|f) of any e with the given f .
1Concrete values from our experiments are: 0.395847 for
the source-to-target RF lexicon, 0.48127 for the target-to-source
RF lexicon. 0.0512856 for the source-to-target IBM-1, and
0.0453709 for the target-to-source IBM-1. Mauser et al. (2006)
mention that they chose their heuristic thresholds for use with
IBM-1 between 10?1 and 10?4.
all All entries with probabilities larger than the floor
value are not thresholded. This variant may be
considered as histogram ?. We only apply it
with RF lexicons.
median ?f is a median-based distinct value for each
f , i.e. it is set to the value that separates the
higher half of the entries from the lower half of
the entries p(e|f) for the given f .
4 Experimental Evaluation
We present empirical results obtained with the dif-
ferent insertion and deletion model variants on the
349
Chinese?English 2008 NIST task.2
4.1 Experimental Setup
To set up our systems, we employ the open source
statistical machine translation toolkit Jane (Vilar et
al., 2010; Vilar et al., 2012), which is freely avail-
able for non-commercial use. Jane provides efficient
C++ implementations for hierarchical phrase extrac-
tion, optimization of log-linear feature weights, and
parsing-based decoding algorithms. In our experi-
ments, we use the cube pruning algorithm (Huang
and Chiang, 2007) to carry out the search.
We work with a parallel training corpus of 3.0M
Chinese-English sentence pairs (77.5M Chinese /
81.0M English running words). The counts for
the RF lexicon models are computed from a sym-
metrized word alignment (Och and Ney, 2003), the
IBM-1 models are produced with GIZA++. When
extracting phrases, we apply several restrictions, in
particular a maximum length of 10 on source and
target side for lexical phrases, a length limit of five
(including non-terminal symbols) for hierarchical
phrases, and no more than two gaps per phrase.
The models integrated into the baseline are: phrase
translation probabilities and RF lexical translation
probabilities on phrase level, each for both transla-
tion directions, length penalties on word and phrase
level, binary features marking hierarchical phrases,
glue rule, and rules with non-terminals at the bound-
aries, source-to-target and target-to-source phrase
length ratios, four binary features marking phrases
that have been seen more than one, two, three or
five times, respectively, and an n-gram language
model. The language model is a 4-gram with modi-
fied Kneser-Ney smoothing which was trained with
the SRILM toolkit (Stolcke, 2002) on a large collec-
tion of English data including the target side of the
parallel corpus and the LDC Gigaword v3.
Model weights are optimized against BLEU (Pa-
pineni et al., 2002) with standard Minimum Error
Rate Training (Och, 2003), performance is measured
with BLEU and TER (Snover et al., 2006). We em-
ploy MT06 as development set, MT08 is used as un-
seen test set. The empirical evaluation of all our se-
tups is presented in Table 1.
2http://www.itl.nist.gov/iad/mig/tests/
mt/2008/
4.2 Experimental Results
With the best model variant, we obtain a significant
improvement (90% confidence) of +1.0 points BLEU
over the baseline on MT08. A consistent trend to-
wards one of the variants cannot be observed. The
results on the test set with RF lexicons or IBM-1, in-
sertion or deletion models, and (in most of the cases)
with all of the thresholding methods are roughly at
the same level. For comparison we also give a result
with an unaligned word count model (+0.4 BLEU).
Huck et al. (2011) recently reported substantial
improvements over typical hierarchical baseline se-
tups by just including phrase-level IBM-1 scores.
When we add the IBM-1 models directly, our base-
line is outperformed by +1.7 BLEU. We tried to
get improvements with insertion and deletion mod-
els over this setup again, but the positive effect was
largely diminished. In one of our strongest setups,
which includes discriminative word lexicon models
(DWL), triplet lexicon models and a discriminative
reordering model (discrim. RO) (Huck et al., 2012),
insertion models still yield a minimal gain, though.
5 Conclusion
Our results with insertion and deletion models for
Chinese?English hierarchical machine translation
are twofold. On the one hand, we achieved sig-
nificant improvements over a standard hierarchical
baseline. We were also able to report a slight gain
by adding the models to a very strong setup with
discriminative word lexicons, triplet lexicon mod-
els and a discriminative reordering model. On the
other hand, the positive impact of the models was
mainly noticeable when we exclusively applied lex-
ical smoothing with word lexicons which are simply
extracted from word-aligned training data, which
is however the standard technique in most state-of-
the-art systems. If we included phrase-level lexical
scores with IBM model 1 as well, the systems barely
benefited from our insertion and deletion models.
Compared to an unaligned word count model, inser-
tion and deletion models perform well.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
350
References
