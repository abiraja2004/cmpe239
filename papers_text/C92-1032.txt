LEFT-CORNER PARSING AND PSYCHOLOGICAL PLAUSIBILITY 
Ph i l ip  l~esnik 
Depar tment  of  Co l Imuter  and  in format ion  Sc ience 
Un ivers i ty  of  Pennsy lvan ia ,  Ph i lade lph ia ,  PA  19104, USA 
resnik?linc.cis.upenn.edu 
Abst ract  
It is well known that even extremely limited center- 
embedding causes people to have difficulty ill comprehen- 
sion, but that left- and right-branching constractions pro- 
duce no such effect. If the difficulty in comprehension is 
taken to be a result of processing load, as is widely as- 
sumed, then measuring the processing load induced by a 
parsing strategy on these constructions may help determine 
its plausibility as a psychological model. On this basis, it 
has been ~rgued \[A J91, JL83\] that by identifying processing 
load with space utilization, we can rule out both top-down 
and bottom-up arsing as viable candidates for the human 
sentence processing mechanism, attd that left-corner pars- 
ing represents a plausible Mternative. 
Examining their arguments in detail, we find difficulties 
with each presentation. In this paper we revise the argu- 
ment and validate its central claim. In so doing, we discover 
that the key distinction between the parsing methods i  not 
the form of prediction (top-down vs. bottom-up vs. left- 
corner), but rather the ability to iastantiate the operation 
of composition. 
1 Introduction 
One of our most robust observations about language - -  
dating back at least to the seminal work of Miller and 
Chomsky \[MC63\] - -  is that right- and left-branching 
constructions such as ( la) and (lb) seem to cause no 
particular difficulty in processing, but that multiply 
center-embedded constructions such as (lc) are difficult 
to understand. 
a. \[\[\[John's\] brother's\] eat\] despises rats. 
b. This is \[the dog that chased \[the cat that bit 
\[the rat that ate tbe cheese\]\]\]. 
c. #\[The rat that \[the cat that lille dog\] chased\] 
bit\] ate the cheese. 
The standard explanation for this distinction is a 
tight bound on space in the human sentence process- 
ing mechanism: center-embedded constructions require 
that the head noun phrase of each subject be stored un- 
til the processing of the embedded clause is complete 
and the corresponding verb is finally encountered) 
Alternative accounts have been proposed, most shar- 
ing the premise that the parser's capacity for recur- 
sion is limited by bounds on storage. (See, for exmn- 
pie, \[Kim73\] and \[MI64\]; for opposing views and other 
pointers to the literature see \[DJK+82\].) 
The distinction between center-embedding and 
left/right-branching has important implications for 
those who wish to construct psychologically plausible 
models of parsing. Johnson-Laird \[JL83\] observes that 
neither the top-down nor the bottom-up methods of 
constructing a parse tree fit the facts of (1), arid pro- 
poses instead the lesS-well-known alternative of left- 
corner parsing. Abney mid Johnson \[AJgl\] discuss 
a somewhat more general version of Johnson-Laird's 
argument, introducing the abstract notion of a pars- 
ing sf~ntegy in order to characterize what is meant by 
bottom-up, top-down, and left-corner parsing. 
In this paper, we examine the argument as pre- 
sented by Abney and Johnson and by Johnson-Laird, 
and point out a central problem with each variation. 
We then present he argument in a form that remedies 
those difficulties, and, in so doing, we identify a pre- 
viously underrated aspect of the discussion that turns 
out to be of central importance. In particular, we show 
that  the psychological plausibility argument hinges on 
the operation of composition and not left-corner pre- 
diction per se. 
2 Comparing Strategies 
2.1  Summary  o f  the  Argument  
For expository purposes, we begin with tile discussion 
in \[AJ91\]. Abney and Johnson sesame, as we shall, 
that the hunmn sentence processing mechanism con- 
struets a parse tree, consisting of labelled nodes and 
arcs, incrementally over the course of interpreting an 
utterance, though tile global parse tree need never "ex- 
ist ill its entirety at any point." They define a parsing 
IThis oh~rvatlon is by t to  rne~n~ lanttnaage specific, though 
in SOV langttages it is embedding on objects, not subjectl, that 
causes ditllctdty. 
ACRES DE COLING-92, NANTES, 23-28 Aour 1992 1 9 1 PROC. oF COLING-92, NANTES, AUG. 23-28, 1992 
A 
Figure 1: A parse tree 
strategy to be "a way of enumerating the nodes and 
arcs of parse trees." This is, in fact, a generalization of
the concept of a traversal \[ASU86\]. 
A top-down strategy is one in which each node is 
enumerated before any of its descendants are; a bottom- 
up strategy is one in which all the descendants of a node 
are enumerated before it is. So, for example, a top- 
down strategy would enumerate the nodes of the tree in 
Figure 1 in the order ABCDEFGHI, and a bottom-up 
strategy would enumerate them in the order CEFDB- 
HIGA. In a left-corner strategy, for each node ~1, the 
leftmost child of T/is enumerated before r/, and the sib- 
lings of the leftmcet child are enumerated after r/. The 
strategy takes its name from the fact that the first item 
on the r ight-hand side of a context-free rule (its left cor- 
ner) is used to predict the parent node. For example, 
having recognized constituent C in Figure 1, the parser 
predicts an invocation of rule B --4 C D and introduces 
node B. The complete left-corner enumeration of the 
tree is CBEDFAHGI. 
Thus far, we have discussed only the order of enu- 
meration of nodes, and not ares. Abney and John- 
son define as arc-eager any strategy that enumerates 
the arc between two nodes as soon as both nodes are 
present. An are-standard strategy is one that enu- 
merates the connecting arc once either none or all 
of the subtree dominated by the child has been enu- 
merated. For example, the arc-eager left-corner enu- 
meration of the tree in Figure 1 would introduce arc 
(B,D)  just  after node D was enumerated, while the 
arc-standard version of the left-corner strategy would 
first completely enumerate the subtree containing E, 
D, and F,  and then enumerate arc (B, D). 
In order to characterize the space requirements of
a parsing strategy, two more definitions are required. 
A node is said to be incomplete ither if its parent 
has not yet been enumerated (in which case the parser 
must store it until it can be attached to the parent 
node), or if some child has not yet been enumerated 
(in which case the parser must store tire node until 
its child can be attached). The space requirement of 
a parsing strategy, given a grammar,  is the maximum 
number of incomplete nodes at any point during tbe 
enumeration of any parse tree of the grammar. 
Having established this set of definitions, the goal 
is to decide which parsing strategies are psychologi- 
cally plausible, given the facts about the human pars- 
x / e | c ?/-,,, 
'? z ~ n 
/ \ 
v z 
left-branching center-embedded right-branching 
Figure 2: Branching slr~e~ares 
ing mechanism as exemplified by (1). The central claim 
is summarized in the following table: 
Strategy " - Spaxze required ' \] 
No#, ? A~c, L~/t ce . t~ I m~hLl 
Top-d . . . . .  ither o(,) O(u) I o(1) I 
Bottom-up- either 0(1) O(n) \]JO(n)" 
Left . . . . . . . .  tandard 0(1) O(n) O(n) 
Left . . . . . . . . .  get,  0(1) O(a) \ ]Q(1) J 
What  people do .. O(1) O(.)  I o(1) I 
The table can be explained with reference to Fig- 
ure 2. A top-down enumeration of the left-branching 
tree clearly requires storage proportional to n, tile 
height of the tree: at the point when Z is enumer- 
ated, each of A, B , . . . ,  X remains inemnplete because 
its rightmost child has not yet been encounteredfl The 
same holds true for the center-embedded structure: us- 
ing a top-down enumeration, each of A, C, D, . . . ,  X re- 
mains incomplete until the subtree it dominates has 
been entirely enumerated. In contrast, the top-down 
strategy requires only constant space for tim right- 
branching structure: each of A, C . . . .  , X becomes coru- 
plete as soon as its rightmost child is enumerated, so 
the number of incomplete nodes at any time is at most 
two. We conclude that if the human sentence process- 
ing strategy were top-down, people would find increas- 
ing difficulty with both multiply left-branching and 
multiply center-embedded constructions, but not with 
right-branching constructions. The evidence xempli- 
fied by (1) suggests that this is not the case. 
A similar analysis holds for the bottom-up strategy. 
The left-branching structure requires only constant 
space, since each of X , . . . ,  B ,A  becomes complete as 
soon as both children have been enumerated. In con- 
trust, enumerations of the right-branching and center- 
embedded constructions require linear space, since ev- 
ery leftmest child remains incomplete until the subtree 
dominated by its right sibling has been entirely enu- 
merated. The left-corner strategy with arc-standard 
enumeration behaves imilarly to the bottom-up strat- 
egy, since every parent node remains incomplete un- 
til the subtree dominated by its right sibling has been 
2Abney and Johngoa di*cuss space complexity with r?apect 
to  the length of the input string, not the height of the ptmm 
tree, but if we t~sttme the grammar in finitely ambigltotm this 
distinction is of no hnportaxtce. 
Ac I~ DE COLING-92. NANTEs, 23-28 AOI\]T 1992 1 9 2 PROC. OF COLING-92, NANTES, AUO. 23-28. 1992 
entirely enumerated. If increased memory load is re- 
sponsible for increased processing difficulty, as we have 
been assuming, then both the bottom-up strategy and 
the arc-standard left-corner strategy predict that peo- 
ple have more difficulty with right-branching than with 
left-branching structures. Our conclusion is the same 
as for the top-down strategy: the asymmetry of the 
prediction is not supported by the evidence. 
On the other hand, arc-eager enumeration makes a 
critical difference to the left-corner strategy when ap- 
plied to the right-branching structure. Recall that the 
left-corner enumeration of nodes for this structure is 
BADC .... Notice that after node (7 has been enumer- 
ated, arc (A,C) is introduced immediately, and as a 
result, node A is no longer incomplete. In general, 
the arc-eager left-corner strategy will enumerate the 
right-branching structure with at most three nodes in- 
complete at any point. ~ktrthermore, as was the case 
for the bottom-up strategy, the left-branching structure 
requires constant space. We see that only tile center- 
embedded structure requires increased storage as the 
depth of embedding increases. Thus of the four strate- 
gies, the arc-eager version of the left-corner strategy 
is the only one that makes predictions consistent with 
observed behavior. 
2.2  Two Prob lems 
Under the assumptions made by Abney and Johnson, 
the discussion sketched out above does make a case for 
a left-corner strategy being more psychologically plait- 
sible than top-down or bottom-up strategies. However, 
there are two difficulties with the argument as it is pre- 
sented. 
First, by abstracting away from parsing algorithms 
and placing the focus on parsing strategies, Abney anti 
Johnson make it difficult to fairly compare space re- 
quirements across different methods of parsing. With- 
out a formal characterization f the algorithms them- 
selves, it is not clear that their abstract notion of space 
utilization means the same thing in each case. 3
~br example, consider the augmented transition 
network (ATN) in Figure 3, where the actions on tile 
arcs are as follows: 
II: npl ~ * 
I2: result ~ (S (npl *)) 
13: dell ~ * 
14: result ~ (NP (dell *)) 
I5: result ~ a 
I6: result ~ the 
Uppercase are labels represent PUSH operations, and 
lowercase labels represent erminal symbols. In the 
pseudolanguage used here for are actions, npl, dell, 
3\] am grateful to Stuart Shleber for this observation. 
a(t5) 
tl~ (16) 
Figure 3: Fragment of an ATN 
and resull are registers, the leftward arrow (+--) indi- 
cates an assigmnent statement, he pop arc transmits 
control (aud tile contents of the ~esalt register) to the 
calling subuetwork, and the asterisk (*) represents the 
value so transmitted (cf. \[WooT0\]). So, for instance, 
action I4 constructs an NP dominating the structure 
in the dd l  register on tile left, and, on the right, tile 
noun structure received on retnrn froln a push to tile 
N subnetwork. 
Now~ tile ATN is perhaps one of the mo~t common 
examples of a parser operating in a top-down fashion. 
Yet according to the definitions proposed by Abney 
and Jolmson, the enumeration performed by the ATN 
parser given above would seem to make it, an instance 
of a bottom-up strategy. For example, in parsing the 
noun phrase the man, the ATN above wonld recognize 
tile determiner the, then the nonn man, and finally it 
would build and return the structure \[,vthe man\] from 
the NP subnetwork. The source of difficulty lies in the 
decoupling of the parser's hypotheses from the struc- 
tures that it builds. When the determiner the is en- 
countered, no parse-tree structures have been built, but 
the mechanism controlling the ATN's computation has 
stored the hypotheses that we were parsing an S, that 
we had entered the NP subnetwork, and that we had 
subsequently entered the DET subnetwork. These cor- 
respond precisely to the nodes we expect to see enu- 
merated uring the course of a top-down strategy. 
One could, of course, choose in this case to identify 
the space utilization of this parser with the hypothe- 
ses rather than the structures built. Itowever, that 
leaves the status of the structures themselves in ques- 
tion. More to the point, re-characterizing tile storage 
requirements of a particular algorithm is exactly the 
sort of manipulation that the abstract notion of pars- 
ing strategies should help us avoid. 
Tile second difficulty with Abney and Johnson's dis- 
cussion concerns the distinction between arc-eager and 
arc-standard strategies. As they point out, for both 
top-down and bottom-up strategies, the two forms of 
AcrEs i~ COL1NG-92, NANYES, 23-28 AOflr 1992 1 9 3 PROC. OF COLING-92, NANTES, AUO. 23-28, 1992 
arc enumeration are indistinguishable. In addition, 
left-corner parsing with arc-standard enumeration is, 
at least for the purposes of this discussion, virtually 
identical to bottom-up arsing, having no distinguish- 
able effects either with respect o space utilization or 
even with respect o the hypotheses that are proposed.4 
So it seems omewhat odd to introduce a distinction be- 
tween "eager" versus "standard" when it turns out to 
distinguish only one of six possible combinations (top- 
down/eager, top-down/standard, etc.). The question 
of exactly what "eager enumeration" does would seem 
to merit further attention. We shall give it that atten- 
tion shortly, in Section 4. 
3 Compar ing  Automata  
Abney and Johnson's argument is largely an indepen- 
dent account quite similar to one made earlier in \[JL83\]. 
Here we present a brief summary of the argument as 
presented there. Johnson-Laird's presentation, though 
it encounters a difficulty of its own, turns out to com- 
plement Abney and Johnson's and to make clear how 
to solve the difficulties in both. 
Following the standard escription in the compilers 
literature (see, e.g., \[ASU86\]), Johnson-Laird adopts 
the definition of a top-down parser as one that oper- 
ates by recursive descent: it begins with the start sym- 
bol of the grammar and successively rewrites tile left- 
most nonterminal until it reaches a terminal symbol or 
symbols that can be matched against he input. Pars- 
ing in this fashion, the parse tree is constructed top 
down and from left to right. A bottom-up arser builds 
the tree by working upward from the terminal symbols 
in the input string, constructing each parent node af- 
ter all its children have been recognized. A left.corner 
parser recognizes the left-corner of a context-free rule 
bottom-up, and predicts the remaining symbols on the 
right-hand-side of the rule top-down. 
Johnson-Laird examines the psychological p ausibil- 
ity of parsers, not parsing strategies, but otherwise his 
argument is very much the same as the discussion in 
the previous section. He concludes that the symme- 
try of human performance on left- and right-branching 
structures counts against he top-down mid bottom-up 
parsers, and that the left-corner p~trser is a viable alter- 
native that appears to be consistent with the evidence. 
He then provides a more formal characterizatiml of the 
various parsers by expressing each as a push-down au- 
tomaton (PDA). Such a characterization immediately 
*Although top-down filtering can be added (see, e.g., \[PS87, 
p. 182D, Schabea (personal commttrdcation) points out that left- 
corner parsing with top-down ffltethtg iS e~entially the same a.s 
LR parsing. Top-down filtering restricts the non-determinlstic 
choices made by the parser, bat does not affect the bottom-up 
construction of the parse tree along a single computation path. 
remedies the first difficulty we found in \[AJ91\]: the for- 
mal specification of each parsing algorithm permits us 
to express pace utilization uniformly in terms of the 
automaton's stack. 
The top-down and bottom-up automata behave x- 
actly as we would expect. The stack of the bottom- 
up automaton never grows beyond a constant size 
for left-branching constructions, but is potentially un- 
bounded for center-embedded and right-branching con- 
structions. The top-down automaton displays the op- 
posite behavior, the size of its stack size being bounded 
only for right-branching constructions, s 
Of particular interest is Johnson-Laird's construe- 
tion of a PDA for left-corner parsiug, which we consider 
in more detail. The stack alphabet for the left-corner 
PDA includes not only terminal and non-terminal sym- 
bols from thc grau~nar, but also special symbols of tile 
form \[X Y\], where X mad Y are nonterminals. The first 
symbol in such a pair represents tile top-down predic- 
tion of a node, and tile second a node that has been 
encountered bottom-up. The use of these pairs per- 
mits a straightforward combination of left-corner pre- 
diction, which is bottom-up, and top-down prediction 
and matching against he input in the style of a top- 
down automaton. 
tiere we consider an extremely simple left-corner 
automaton, constructed from a grammar having the 
following productions: 
(1) S ~ NP VP 
(2) NP ~ John \] Mary 
(3) VP ~ V NP 
(4) v -~ nke~ 
The rules of tlle automaton are as follows: 
\[ . I 'Inpn't I Stac.k " I New top of staclf..I 
1 John ... 
2 Mary ... 
3 likes 
4 iynored X John 
5 ignored ... X Mary' 
6 ignored ... X likes 
'7 ignored ... \[X NP\]' 
-~  ic, . . . .  a . . .  \ [x  v \ ]  
9' ignored :.. IX X\] 
. . .  John 
... Mary 
.. .  likes 
,.. \[?.NP 1
... \[x NPJ 
... Ix v3 
... \[x s) vv  
. . . \ [XVB\]  SP 
The top of the stack is at right, and rules 4-9 are ac- 
tually schemata for a set of rules in which X can be 
replaced by each of tile nonterminals (S, NP, VP, and 
V). Tile parser begins with S on top of the stack, and 
a string has been successfully recognized if the stack is 
empty and the input exhausted. 
5The a~mlysis bein~ stralghtforward, we omit the details here; 
for n complete discussion of the construction of PDAs for top- 
down and bottom-up pm~ing, see ~LP81, ?3.6\]. 
ACRES DE COLING-92, NANTES, 23-28 AO~T 1992 1 9 4 PROc. oF COLING-92, NANTES, AUO. 23-28, 1992 
/ s~ i 
I ivel 
V (NP i 
I \ ~/  
likes "Jr ...... 
Figure 4: Distinguishing the top-down view of a node 
b'om the bottom-up view 
Rules 1-3 simply introduce texical items onto the 
stack as they are scanned. Rules 4--6 represent bottom- 
up reductions according to the lexical productions of 
the grammar (productions (2) and (4)); for example, 
rule 4 states that if a constituent X has been predicted 
top-down, and the word John is scanned, we continue 
seeking X top-down with the knowledge that we have 
identified an NP bottom-up. Rules 7 and 8 implement 
left-corner prediction: if the left-corner node of a rule 
has been recognized bottom-up, then we hypothesize 
the parent node in bottom-up fashion and also predict 
the right siblings top-down. For example, rule 8 states 
that if a V has been recognized bottom-ul) , we should 
hypothesize that a VP is being recognized and also pre- 
dict the remainder of the VP, namely an NP, top-down. 
Finally, rule 9 pops a symbol off the top of the stack if 
we have predicted a constituent X top-down and then 
succeeded in finding it bottom-up. 
In examining the behavior of this automaton for 
the sentence John likes Mary, a problem immediately 
becomes apparent. The contents of the stack at each 
step during the parse are as follows: 
! ! ..... =1  Joha  VP  VP {VP V\] vp  VP\] ?, - S ~ IS NP\] IS Sl IS Sl IS S I ( I j  ) (a)  (4) {s|  (~) "r 
! \] ! l i t  . . . .  NP INP NP ? . \ [v t  r' vP \ ]  \ [VP VP\] \ [Vp  VP) Is sl IS sl s s I s s (6) (9) o (~) 
As the sentence --- a right-branching structure --- 
is recognized, we find that the stack is accumulating 
symbols of the form \[X X\]. It is clear that as the depth 
of right-branching increases, the number of stacked-up 
symbols of this form will also increase, without upper 
bmmd. Why is this happening? 
Let us distinguish between the top-down "view" of 
a node and the bottom-up (left-corner) "view" of that 
node. Figure 4 makes this distinction explicit: the VP 
predicted top-down by the rule S --* NP VP is dis- 
tinct from the VP predicted in left-corner fashion using 
VP --~ V NP. These are, in fact, precisely the two VPs 
in the symbol \[VP VP\]. Now, enumerating the arc be- 
tween VP and S in the final parse tree is equivalent to 
identifying these two views (dotted ellipses in the fig- 
ure). As long as we have not identified the two views of 
VP as the same node, the arc is not enumerated --- and 
the parent S remains incomplete in the sense defined 
by Abney and Johnson, It is rule 9 in the automa- 
ton that effects this identification: popping \[VP VP\] 
amounts to recognizing that the top-down view and 
the bottom-up view match. Since the operation of the 
automaton prevents the symbol from being popped un- 
til the bottoIn-up view has been completed, it is clear 
that this automaton implements an arc-standard strat- 
egy rather than an arc-eager one. Itence it is not sur- 
prisiug that the antomaton fails to support Johuson- 
Laird's argument: far from being bounded, the stack of 
such automaton can grow without bound as the depth 
of right-brmlching increases. 
4 Arc -eager  Enumerat ion  as 
Compos i t ion  
4.1  An  Easy  F ix . . .  
To summarize thus far, \[AJ91\] and \[JL83\] present wo 
forms of the same argument, but each presentation suf- 
fers from a central shortcoming. Abney and Johnson, 
discussing parsing strategies rather than parsers, fail 
to characterize top-down, bottom-up, and left-corner 
parsing in a way that permits a fair comparison of 
space utilization. Johnson-Laird, ibrmalizing parsers as 
push-down automata, provides a characterization that 
clearly defines the terms of the comparison, but his left- 
corner automaton lacks the properties needed to make 
the argument succeed. 
Modifying the left-corner automaton so that it per- 
forms arc-eager enumeration is straightforward. As dis- 
cussed toward the end of the previous ection, "attach- 
ment" of a node X to its pareut occurs when the symbol 
IX X\], representing the top-down and bottom-up views 
of that node, is removed from the stack. In order to 
attach the node (i.e., enumerate the arc) eagerly, we 
should pop the symbol as soon as it is introduced. For 
the automaton in the previous ection, this amounts to 
augmenting rule schema 8 with the rule 
I \[ Input I St~k I New top o ts t~k  I 
\ [8 '1  ign?red l . , . \ [vPV\ ]  I , , .NP  \[ 
and, in general, augmenting the rules of left-corner pre- 
diction so that symbols of the form \[X X\] are not in- 
troduced obligatorily. 
It is easy to show that the automaton, modified in 
this fashion, requires only a finite stack for arbitrarily 
ACTES DE COLING-92, NANTES, 23-28 Aotrr 1992 1 9 $ Pgoc. OF COLING-92, NAN'I~S, AUG. 23-28. 1992 
(A'--~B1 ...B~ . ) 
(A~ ? BI...B~: 1 
(Bt--. 3'~)...(B~. rk) 
Figure 5: Inferenre-ru& ,liar,ncter~:alion of bottom- 
up reduction step (left) ~d t,,p.down prediction step 
(,~ght). 
deep left- and right-l,ranehmg constructions, but re- 
quires increasing stack ~t,,trq" fi,r c,'nter-embedded con- 
structions as the depth - f  ,-mb~-dding increases. Thus 
we have succeeded in pr,-~enl|ng a complete version of 
the argument in \[AJ91\] and \[JL83\] in the sense that 
1. top-down, bottom-up, and left-corner parsing are 
characterized in a formally precise way, 
2. the chaxacterizations are abstract, in the sense 
that the logic of the algorithms (in the form of non- 
deterministic push-down automata) is separated 
from their control (namely the control of how the 
automata's nondeterministic choices are made), 
3. the notion of space utilization (namely stack size) 
is the same for each case, permitting us to make a 
fair comparison, and 
4. the conclusion, as expected, is that top-down and 
bottom-up parsing both make incorrect predic- 
tions, but a form of left-corner parsing is consistent 
with the apparent behavior of the human sentence 
processing mechanism. 
4 .2  . . .and  i t s  Imp l i ca t ions  
The import of the "fix" in the previous ection is not 
simply that the automaton can be made to display the 
appropriate behavior. It is that the "arc-eager" enu- 
meration strategy is a different (and perhaps mislead- 
ing) description of a purser's ability to perform compo- 
sition on the structures that it is building. 
If we describe the parsers as sets of inference rules 
rather than automata, s the inference permitting arc- 
eager enumeration i the left-corner parser turns out 
to be a rule of composition: A ~ c~ ? B and B ~/3  . 7 
can be composed to form the dotted item A ~/3  . 3'. 
For instance, the effect of rule 8' is to predict VP 
V ? NP  from V, and then immediately compose this 
new item with S --, NP . VP. Equivalently, the rule 
first predicts the VP structure in Figure 4 from the V 
(giving us \[VP VP\], corresponding to the two VP nodes 
the figure), and then immediately identifies the lower 
VP node with the upper one (which removes \[VP VP\]), 
leaving just an S structure that lacks an NP. 
STwo descriptior~ that are formally equivalent. 
In contrast, even if one were to add a rule of com- 
position to the inferential description of top-down and 
bottom-up arsers, it would have no effect. Neither the 
top-down nor the bottom-up arser ever introduces a 
configuration in which the A constituent and B con- 
stituent are both only partially completed (and thus 
can be composed). Instead, these parsers rewrite the 
entire right-hand side of a rule at once (see Figure 5). 
In order for a rule of composition to be relevant, it is 
necessary that the parser introduce both the top-down 
view of a constituent (e.g. B in A ---* ~ ? B) and the 
bottom-up view of that constituent (e.g. B in B ~ ft.3`) 
so that they may later be identified. Unlike top-down 
and bottom-up arsers, a left-corner parser meets this 
criterion. 
By presenting a complete version of the argument 
in \[AJ91\] and \[JL83\], we have essentially re-discovered 
proposals made by Puhnan \[Pu185, Pul86\] and Thomp- 
son et al \[TDL91\]. Both propose parsers with left- 
corner prediction and a composition operation added. 
Pulman motivates his purser's design on grounds of 
psychological plausibility, though he does not present 
a complete version of the argument discussed here. 
Thompson et al are motivated by issues in parallel 
parsing. In addition, we should note that Johnson- 
Laird introduces a parser with a composition-like oper- 
ation later in his discussion, though outside the context 
of a formal comparison among parsing methods. 
Abney (personal communication) points out that, 
though psychologically plausible in terms of the space 
utilization argument we have discussed, the automa- 
ton presented here may nonetheless fail to be plausible 
because of its behavior with regard to local ambigu- 
ity. If we opt to compose whenever possible (e.g., al- 
ways preferring rule 8' to rule 8 when X = VP), which 
seems natural, then left-recursive structures will lead 
to counterintuitive r sults - -  for example, in process- 
ing (2), the automaton will prefer to attach the NP the 
cat as the object of the verb, rather than waiting for 
the full NP the cat's dinner. 
2 John prepared \[\[tlm eat\]'s dinner\]. 
More generally, as Abney and Johnson discuss, there 
is a tradeoff between storage, which is conserved by 
strategies that perform attachment "eagerly," and am- 
biguity, which is avoided by deferring attachment until 
more information is present o resolve it. On the basis 
of the observations we have made here, it appears that 
this tradeoff is expressed most naturally not in terms 
of a comparison between different parsing strategies, 
but rather in terms of the criteria for when to invoke a 
composition operation that is available to the parser. 
ACTES DE COLING-92, NANTES, 23-28 Ao~Yr 1992 1 9 6 PROC. OF COLING -92, NANTES, AUO. 23 -28, 1992 
5 Conc lus ions  
In this paper, we have considered a space-utilization 
argument concerning the psychological plausibility of 
different parsing methods. Both \[AJ91\] and \[JL83\] 
make the same basic claim, namely that top-down 
and bottom-up parsing lead to incorrect predictions 
of asymmetry in human processing - -  predictions that 
can be avoided by utilizing a left-corner strategy. We 
have demonstrated difficulties with both of their for- 
mulations and presented a more precise account. In 
so doing, we have found that composition, rather than 
left-corner prediction per se, plays the central role in 
distinguishing parsing methods. 
In making the argument, we were forced to aban- 
don the abstract characterization f parsing methods 
in terms of strategies, and return to defining parsers 
in terms of their realizations as automata. This has 
the unfortunate consequence of tying the argument to 
context-free gramnrars, losing tire attractive fornralism- 
independent quality evoked in \[AJ91\]. 
Since context:free grammars are no longer generally 
considered likely models for natural language in the 
general case \[Shi85\], one wonders how the discussion 
here might be extended to parsing within more power- 
ful grannnatical frameworks. It is interesting to note 
the relationship between the style of left-corner parsing 
described here and one such framework, combinatory 
categorial grammar (CCG) \[Ste90\]. Composition is an 
integral part of CCG, as is the notion of type-raising, 
which resembles left-corner prediction. 7 The operation 
of a left-corner parser with composition can fairly be 
described as being in the style of CCG, but retain- 
ing the context-free base. Since one attractive feature 
of CCG is its inherent left-to-right, word-by-word in- 
crementality, it is perhaps not surprising to find that 
parsers of CCG tend naturally to meet the criteria for 
psychological p ausibility discussed bere. 
CCG is one instance of a general class known 
as the mildly context-sensitive grammar formalisms 
\[JVSW88\]. We are currently investigating a generaliza- 
tion of the argument presented here to other formalisms 
within that class. 
Acknowledgements  
This research was supported by tile following grants: 
ARO DAAL 03-89-C-0031, DARPA N00014-9O-J-1863, 
NSF IRl 90-16592, and Ben Franklin 91S.3078C-1. I would 
like to thank Steve Abney, Mark Johnson, Aravind Joshi, 
Yves Schabes, Stuart Shieber, and members of tile CLIFF 
group at Penn for their helpfifl discussion and criticism, 
rFor example, NP can be type-ralsed to S/(S\NP), whid~ 
roughly corresponds to S ~ NP . VP. 
References  
\[AJgl\] Steven Abney and Mark Johnson. Memory re- 
quirements and local ambiguities for parttiag 
strategies. Journal of Psycholinguistic Research, 
20(3):233--250, 1991. 
\[ASU86\] Alfred Aho, Ravi Sethi, and Jeffrey Ullmu. 
Compilers: Principles, Techniques, and Toe,t. 
Addison Wesley, 1986. 
\[DJK+82\] A. DeRoeck, R. Johnson, M. King, M. 
net, G. Sampson, and N. Varile. A myth about 
ceutre-embedding. Lingua, 58:327-340, 1981. 
\[JL83\] Philip N. Johnson-Laird. Mental Models. Har- 
vard University Press, 1983. 
\[JVSW88\] A. K. Joshi, K. Vijay-Shanker, and D. J, 
Weir. The convergence of mildly context- 
sensitive grammatical formalisms. In P. Selht 
and T. Wasow, editors, Processing of Lingutsl,c 
Structure. MIT Press, Cambridge, MA, 1988. 
J. Kimball. Seven principles of surface-structure 
parsing in natural language. Cognition, 2:15-47, 
1973. 
Itarry Lewis and Christos Papadimitrion. Ele. 
ments of the Theory of Computation. Prentice- 
Itall, 1981. 
George Miller and Noam Chomsky. Finitary 
models of language users. In R. Luce, R. Bush, 
and E. Galanter, editors, Handbook of Math. 
ematical Psycholcfy , Volume 2. John Wiley, 
1963. 
G. A. Miller and S. Isard. Free recall of self- 
embedded English sentences. Information and 
Control, 7:292--303, 1964. 
l"ernando C. N. Pereira and Stuart M. Shieber. 
Pralog and Natural Language Analysis. Cen-. 
ter for the Study of Language and Information, 
1987. 
Stephen Pulman. A parser that doesn't. In Pro- 
ceedings of the 2nd European ACL, pages 128- 
135, 1985. 
Stephen Pulman. Grammars, parsers, and mem- 
ory limitations. Language and Cognitive Pro. 
cesses, 1(3):197-225, 1986. 
S. M. Shieber. Evidence against he context- 
freeness of naturM language. Linguistics and 
Philosophy, 8:333-343, 1985. 
Mark Steedman. Gapping as constituent coordi- 
nation. Linguistics and Philosophy, 13:207-263, 
April 1990. 
H. Thompson, M. Dixon, and J. Lumping. 
Compose-reduce parsing. In Proceedings o.f the 
29th Atmual Meetit,y of the ACL, pages 87-97, 
June 1991. 
William A. Woods. Transition network gram- 
mars for natural anguage analysis. Commu. 
nications of the ACM, 13(10):591-606, October 
1970. 
\[I(im73\] 
\[LP81\] 
\[MC63\] 
\[MI84\] 
\[ps871 
\[Pu185\] 
\[Pul86\] 
\[Shi85\] 
\[Ste9O\] 
\[TI)L911 
\[WooT0\] 
ACTES DE COLING-92, NAN'NS, 23-28 Aotrr 1992 1 9 7 PROC. OF COLING-92, NAI~t'ES, AUO. 23-28, 1992 
