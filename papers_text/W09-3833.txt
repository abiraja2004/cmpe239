Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 214?217,
Paris, October 2009. c?2009 Association for Computational Linguistics
Smoothing fine-grained PCFG lexicons
Tejaswini Deoskar
ILLC
University of Amsterdam
t.deoskar@uva.nl
Mats Rooth
Dept. of Linguistics and CIS
Cornell University
mr249@cornell.edu
Khalil Sima?an
ILLC
University of Amsterdam
k.simaan@uva.nl
Abstract
We present an approach for smoothing
treebank-PCFG lexicons by interpolating
treebank lexical parameter estimates with
estimates obtained from unannotated data
via the Inside-outside algorithm. The
PCFG has complex lexical categories,
making relative-frequency estimates from
a treebank very sparse. This kind of
smoothing for complex lexical categories
results in improved parsing performance,
with a particular advantage in identify-
ing obligatory arguments subcategorized
by verbs unseen in the treebank.
1 Introduction
Lexical scarcity is a problem faced by all sta-
tistical NLP applications that depend on anno-
tated training data, including parsing. One way
of alleviating this problem is to supplement super-
vised models with lexical information from unla-
beled data. In this paper, we present an approach
for smoothing the lexicon of a treebank PCFG
with frequencies estimated from unannotated data
with Inside-outside estimation (Lari and Young,
1990). The PCFG is an unlexicalised PCFG, but
contains complex lexical categories (akin to su-
pertags in LTAG (Bangalore and Joshi, 1999) or
CCG (Clark and Curran, 2004)) encoding struc-
tural preferences of words, like subcategorization.
The idea behind unlexicalised parsing is that the
syntax and lexicon of a language are largely inde-
pendent, being mediated by ?selectional? proper-
ties of open-class words. This is the intuition be-
hind lexicalised formalisms like CCG: here lexical
categories are fine-grained and syntactic in nature.
Once a word is assigned a lexical category, the
word itself is not taken into consideration further
in the syntactic analysis. Fine-grained categories
imply that lexicons estimated from treebanks will
be extremely sparse, even for a language like En-
glish with a large treebank resource like the Penn
Treebank (PTB) (Marcus et al, 1993). Smoothing
a treebank lexicon with an external wide-coverage
lexicon is problematic due to their respective rep-
resentations being incompatible and without an
obvious mapping, assuming that the external lexi-
con is probabilistic to begin with. In this paper, we
start with a treebank PCFG with fine-grained lex-
ical categories and re-estimate its parameters on a
large corpus of unlabeled data. We then use re-
estimates of lexical parameters (i.e. pre-terminal
to terminal rule probabilities) to smooth the orig-
inal treebank lexical parameters by interpolation
between the two. Since the treebank PCFG itself is
used to propose analyses of new data, the mapping
problem is inherently taken care of. The smooth-
ing procedure takes into account the fact that unsu-
pervised estimation has benefits for unseen or low-
frequency lexical items, but the treebank relative-
frequency estimates are more reliable in the case
of high-frequency items.
2 Treebank PCFG
In order to have fine-grained and linguistic lexi-
cal categories (like CCG) within a simple formal-
ism with well-understood estimation methods, we
first build a PCFG containing such categories from
the PTB. The PCFG is unlexicalised (with lim-
ited lexicalization of certain function words, like
in Klein and Manning (2003)). It is created by
first transforming the PTB (Johnson, 1998) in an
appropriate way and then extracting a PCFG from
the transformed trees (Deoskar and Rooth, 2008).
All functional tags in the PTB (such as NP-SBJ,
PP-TMP, etc.) are maintained, as are all empty
categories, making long-distance dependencies re-
coverable. The PCFG is trained on the standard
training sections of the PTB and performs at the
state-of-the-art level for unlexicalised PCFGs, giv-
ing 86.6% f-score on Sec. 23.
214
VP
VB.np
add
NP
four more
Boeings
PP-TMP
by 1994
PP-CLR
to the
two units.
(a) An NP PP subcategorization frame marked on the
verb ?add? as np. Note that the arguments NP and PP-
CLR are part of the subcategorization frame and are
represented locally on the verb but the adjunct PP-
TMP is not.
VP
VBG.s.e.to
seeking
S.e.to
+E-NP+ VP.to
TO
to
VP
avoid..
(b) An S frame on the verb ?seeking?: +E-
NP+ represents the empty subject of the
S. Note that structure internal to S is also
marked on the verb.
VP
Vb.sb
think
SBAR
+C+ S
the consumer
is right
(c) An SBAR frame: +C+ is the
empty complementizer.
Figure 1: Subcategorized structures are marked as features on the verbal POS category.
An important feature of our PCFG is that pre-
terminal categories for open-class items like verbs,
nouns and adverbs are more complex than PTB
POS tags. They encode information about the
structure selected by the lexical item, in effect,
its subcategorization frame. A pre-terminal in our
PCFG consists of the standard PTB POS tag, fol-
lowed by a sequence of features incorporated into
it. Thus, each PTB POS tag can be considered to
be divided into multiple finer-grained ?supertags?
by the incorporated features. These features en-
code the structure selected by the words. We fo-
cus on verbs in this paper, as they are important
structural determiners. A sequence of one or more
features forms the ?subcategorization frame? of a
verb: three examples are shown in Figure 1. The
features are determined by a fully automated pro-
cess based on PTB tree structure and node labels.
There are 81 distinct subcategorization frames for
verbal categories. The process can be repeated for
other languages with a treebank annotated in the
PTB style which marks arguments like the PTB.
3 Unsupervised Re-estimation
Inside-outside (henceforth I-O) (Lari and Young,
1990), an instance of EM, is an iterative estima-
tion method for PCFGs that, given an initial model
and a corpus of unannotated data, produces mod-
els that assign increasingly higher likelihood to
the corpus at each iteration. I-O often leads to
sub-optimal grammars, being subject to the well-
known problem of local maxima, and dependence
on initial conditions (de Marcken, 1995) (although
there have been positive results using I-O as well,
for e.g. Beil et al (1999)). More recently, Deoskar
(2008) re-estimated an unlexicalised PTB PCFG
using unlabeled Wall Street Journal data. They
compared models for which all PCFG parameters
were re-estimated from raw data to models for
which only lexical parameters were re-estimated,
and found that the latter had better parsing results.
While it is common to constrain EM either by
good initial conditions or by heuristic constraints,
their approach used syntactic parameters from a
treebank model to constrain re-estimation of lex-
ical parameters. Syntactic parameters are rela-
tively well-estimated from a treebank, not being as
sparse as lexical parameters. At each iteration, the
re-estimated lexicon was interpolated with a tree-
bank lexicon, ensuring that re-estimated lexicons
did not drift away from the treebank lexicon.
We follow their methodology of constrained
EM re-estimation. Using the PCFG with fine
lexical categories (as described in ?2) as the ini-
tial model, we re-estimate its parameters from an
unannotated corpus. The lexical parameters of
the re-estimated PCFG form its probabilistic ?lex-
icon?, containing the same fine-grained categories
as the original treebank PCFG. We use this re-
estimated ?lexicon? to smooth the lexical proba-
bilities in the treebank PCFG.
4 Smoothing based on a POS tagger : the
initial model.
In order to use the treebank PCFG as an initial
model for unsupervised estimation, new words
from the unannotated training corpus must be in-
cluded in it ? if not, parameter values for new
words will never be induced. Since the treebank
model contains no information regarding correct
feature sequences for unseen words, we assign all
possible sequences that have occurred in the tree-
bank model with the POS tag of the word. We
assign all possible sequences to seen words as
215
well ? although the word is seen, the correct fea-
ture sequence for a structure in a training sentence
might still be unseen with that word. This is done
as follows: a standard POS-tagger (TreeTagger,
(Schmid, 1994)) is used to tag the unlabeled cor-
pus. A frequency table cpos(w, ?) consisting of
words and POS-tags is extracted from the result-
ing corpus, where w is the word and ? its POS
tag. The frequency cpos(w, ?) is split amongst all
possible feature sequences ? for that POS tag in
proportion to treebank marginals t(?, ?) and t(?)
cpos(w, ?, ?) = t(?, ?)t(?) cpos(w, ?) (1)
Then the treebank frequency t(w, ?, ?) and the
scaled corpus frequency are interpolated to get a
smoothed model tpos. We use ?=0.001, giving a
small weight initially to the unlabeled corpus.
tpos(w, ?, ?) = (1? ?)t(w, ?, ?) + ?cpos(w, ?, ?)
(2)
The first term will be zero for words unseen in the
treebank: their distribution in the smoothed model
will be the average treebank distribution over all
possible feature sequences for a POS tag. For
seen words, the treebank distribution over feature
sequence is largely maintained, but a small fre-
quency is assigned to unseen sequences.
5 Smoothing based on EM re-estimation
After each iteration i of I-O, the expected counts
cemi(w, ?, ?) under the model instance at itera-
tion (i ? 1) are obtained. A smoothed treebank
lexicon temi is obtained by linearly interpolating
the smoothed treebank lexicon tpos(w, ?, ?) and a
scaled re-estimated lexicon c?emi(w, ?, ?).
temi(w, ?, ?) = (1??)tpos(w, ?, ?)+?c?emi (w, ?, ?)
(3)
where 0 < ? < 1. The term c?emi(w, ?, ?) is ob-
tained by scaling the frequencies cemi(w, ?, ?) ob-
tained by I-O, ensuring that the treebank lexicon is
not swamped with the large training corpus1.
c?emi(w, ?, ?) = t(?, ?)?
w cemi(w, ?, ?)
cemi(w, ?, ?)
(4)
? determines the relative weights given to the
treebank and re-estimated model for a word. Since
parameters of high-frequency words are likely
to be more accurate in the treebank model, we
parametrize ? as ?f according to the treebank fre-
quency f = t(w, ?).
1Note that in Eq. 4, the ratio of the two terms involving
cemi is the conditional, lexical probability Pemi(w|?, ?).
6 Experiments
The treebank PCFG is trained on sections 0-22 of
the PTB, with 5000 sentences held-out for evalu-
ation. We conducted unsupervised estimation us-
ing Bitpar (Schmid, 2004) with unannotated Wall
Street Journal data of 4, 8 and 12 million words,
with sentence length <25 words. The treebank
and re-estimated models are interpolated with ? =
0.5 (in Eq. 3). We also parametrize ? for treebank
frequency of words ? optimizing over a develop-
ment set gives us the following values of ?f for
different ranges of treebank word frequencies.
if t(w, ?) <= 5 , ?f = 0.5
if 5 < t(w, ?) <= 15 , ?f = 0.25
if 15 < t(w, ?) <= 50 , ?f = 0.05
if t(w, ?) > 50 , ?f = 0.005
(5)
Evaluations are on held-out data from the PTB
by stripping all PTB annotation and obtaining
Viterbi parses with the parser Bitpar. In addition
to standard PARSEVAL measures, we also eval-
uate parses by another measure specific to sub-
categorization2 : the POS-tag+feature sequence on
verbs in the Viterbi parse is compared against the
corresponding tag+feature sequence on the trans-
formed PTB gold tree, and errors are counted. The
tag-feature sequence correlates to the structure se-
lected by the verb, as exemplified in Fig. 1.
7 Results
There is a statistically significant improvement3
in labeled bracketing f-score on Sec. 23 when
the treebank lexicon is smoothed with an EM-re-
estimated lexicon. In Table 1, tt refers to the base-
line treebank model, smoothed using the POS-
tag smoothing method (from ?4) on the test data
(Sec. 23) in order to incorporate new words from
the test data4. tpos refers to the initial model for
re-estimation, obtained by smoothed the treebank
model with the POS-tag smoothing method with
the large unannotated corpus (4 million words).
This model understandably does not improve over
tt for parsing Sec. 23. tem1,?=0.5 is the model
obtained by smoothing with an EM-re-estimated
model with a constant interpolation factor ? =
0.5. This model gives a statistically significant im-
provement in f-score over both tt and tpos. The
last model tem1,?f is obtained by smoothing with
2PARSEVAL measures are known to be insensitive to sub-
categorization (Carroll et al, 1998).
3A randomized version of a paired-sample t-test is used.
4This is always done before parsing test data.
216
tt tpos tem1,?=0.5 tem1,?f
Recall 86.48 86.48 86.72 87.44
Precision 86.61 86.63 86.95 87.15
f-score 86.55 86.56 *86.83 *87.29
Table 1: Labeled bracketing F-score on section 23.
an interpolation factor as in Eq. 5 : this is the best
model with a statistically significant improvement
in f-score over tt, tpos and tem1,?=0.5.
Since we expect that smoothing will be advanta-
geous for unseen or low-frequency words, we per-
form an evaluation targeted at identifying struc-
tures subcategorized by unseen verbs. Table 2
shows the error reduction in identifying subcat.
frames in Viterbi parses, of unseen verbs and also
of all verbs (seen and unseen) in the testset. A
breakup of error by frame type for unseen verbs is
also shown (here, only frames with >10 token oc-
currences in the test data are shown). In all cases
(unseen verbs and all verbs) we see a substantial
error reduction. The error reduction improves with
larger amounts of unannotated training data.
8 Discussion and Conclusions
We have shown that lexicons re-estimated with I-
O can be used to smooth unlexicalised treebank
PCFGs, with a significant increase in f-score even
in the case of English with a large treebank re-
source. We expect this method to have more
impact for languages with a smaller treebank or
richer tag-set. An interesting aspect is the substan-
tial reduction in subcategorization error for un-
seen verbs for which no word-specific information
about subcategorization exists in the unsmoothed
or POS-tag-smoothed lexicon. The error reduction
in identifying subcat. frames implies that some
constituents (such as PPs) are not only attached
correctly but also identified correctly as arguments
(such as PP-CLR) rather than as adjuncts.
There have been previous attempts to use POS-
tagging technologies (such as HMM or maximum-
entropy based taggers) to enhance treebank-
trained grammars (Goldberg et al (2009) for He-
brew, (Clark and Curran, 2004) for CCG). The re-
estimation method we use builds full parse-trees,
rather than use local features like taggers do, and
hence might have a benefit over such methods. An
interesting option would be to train a ?supertag-
ger? on fine-grained tags from the PTB and to su-
pertag a large corpus to harvest lexical frequen-
Frame # tokens %Error %Error %Error
(test) tpos tem1 Reduc.
All unseen (4M words) 1258 33.47 22.81 31.84
All unseen (8M words) 1258 33.47 22.26 33.49
All unseen (12M words) 1258 33.47 21.86 34.68
transitive 662 23.87 18.73 21.52
intransitive 115 38.26 33.91 11.36
NP PP-CLR 121 34.71 32.23 7.14
PP-CLR 73 27.4 20.55 25
SBAR 124 12.1 12.1 0
S 12 83.33 58.33 30
NP NP 10 90 80 11.11
PRT NP 21 38.1 33.33 12.5
s.e.to (see Fig.1b) 50 16 12 25
NP PP-DIR 11 63.64 54.55 14.28
All verbs (4M) 11710 18.5 16.84 8.97
Table 2: Subcat. error for verbs in Viterbi parses.
cies. This would form another (possibly higher)
baseline for the I-O re-estimation approach pre-
sented here and is the focus of our future work.
References
S. Bangalore and A. K. Joshi. 1999. Supertagging: An Ap-
proach to Almost Parsing. Computational Linguistics,
25:237?265.
F. Beil, G. Carroll, D. Prescher, S. Riezler, and M. Rooth.
1999. Inside-outside estimation of a lexicalized PCFG for
German. In ACL 37.
J. Carroll, G. Minnen, and E. Briscoe. 1998. Can subcate-
gorization probabilities help parsing. In 6th ACL/SIGDAT
Workshop on Very Large Corpora.
S. Clark and J. R. Curran. 2004. The Importance of Supertag-
ging for Wide-Coverage CCG Parsing. In 22nd COLING.
Carl de Marcken. 1995. On the unsupervised induction of
Phrase Structure grammars. In Proceedings of the 3rd
Workshop on Very Large Corpora.
T. Deoskar. 2008. Re-estimation of Lexical Parameters for
Treebank PCFGs. In 22nd COLING.
Tejaswini Deoskar and Mats Rooth. 2008. Induction of
Treebank-Aligned Lexical Resources. In 6th LREC.
Y. Goldberg, R. Tsarfaty, M. Adler, and M. Elhadad. 2009.
Enhancing Unlexicalized Parsing Performance using a
Wide Coverage Lexicon, Fuzzy Tag-set Mapping, and
EM-HMM-based Lexical Probabilities. In EACL-09.
M. Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24(4).
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In ACL 41.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the Inside-Outside algo-
rithm. Computer Speech and Language, 4:35?56.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313?330.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In International Conference on New
Methods in Language Processing.
H. Schmid. 2004. Efficient Parsing of Highly Ambiguous
CFGs with Bit Vectors. In 20th COLING.
217
