Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 134?140,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The TALP-UPC Phrase-based Translation Systems for WMT13:
System Combination with Morphology Generation,
Domain Adaptation and Corpus Filtering
Llu??s Formiga?, Marta R. Costa-jussa`?, Jose? B. Marin?o?
Jose? A. R. Fonollosa?, Alberto Barro?n-Ceden?o??, Llu??s Ma`rquez?
?TALP Research Centre ?Facultad de Informa?tica
Universitat Polite`cnica de Catalunya Universidad Polite?cnica de Madrid
Barcelona, Spain Madrid, Spain
{lluis.formiga,marta.ruiz,jose.marino,jose.fonollosa}@upc.edu
{albarron, lluism}@lsi.upc.edu
Abstract
This paper describes the TALP participa-
tion in the WMT13 evaluation campaign.
Our participation is based on the combi-
nation of several statistical machine trans-
lation systems: based on standard phrase-
based Moses systems. Variations include
techniques such as morphology genera-
tion, training sentence filtering, and do-
main adaptation through unit derivation.
The results show a coherent improvement
on TER, METEOR, NIST, and BLEU
scores when compared to our baseline sys-
tem.
1 Introduction
The TALP-UPC center (Center for Language and
Speech Technologies and Applications at Univer-
sitat Polite`cnica de Catalunya) focused on the En-
glish to Spanish translation of the WMT13 shared
task.
Our primary (contrastive) run is an internal
system selection comprised of different train-
ing approaches (without CommonCrawl, unless
stated): (a) Moses Baseline (Koehn et al,
2007b), (b) Moses Baseline + Morphology Gener-
ation (Formiga et al, 2012b), (c) Moses Baseline
+ News Adaptation (Henr??quez Q. et al, 2011),
(d) Moses Baseline + News Adaptation + Mor-
phology Generation , and (e) Moses Baseline +
News Adaptation + Filtered CommonCrawl Adap-
tation (Barro?n-Ceden?o et al, 2013). Our sec-
ondary run includes is the full training strategy
marked as (e) in the previous description.
The main differences with respect to our last
year?s participation (Formiga et al, 2012a) are: i)
the inclusion of the CommonCrawl corpus, using
a sentence filtering technique and the system com-
bination itself, and ii) a system selection scheme
to select the best translation among the different
configurations.
The paper is organized as follows. Section 2
presents the phrase-based system and the main
pipeline of our baseline system. Section 3 de-
scribes the our approaches to improve the baseline
system on the English-to-Spanish task (special at-
tention is given to the approaches that differ from
last year). Section 4 presents the system combi-
nation approach once the best candidate phrase of
the different subsystems are selected. Section 5
discusses the obtained results considering both in-
ternal and official test sets. Section 6 includes con-
clusions and further work.
2 Baseline system: Phrase-Based SMT
Our contribution is a follow up of our last year par-
ticipation (Formiga et al, 2012a), based on a fac-
tored Moses from English to Spanish words plus
their Part-of-Speech (POS). Factored corpora aug-
ments words with additional information, such as
POS tags or lemmas. In that case, factors other
than surface (e.g. POS) are usually less sparse, al-
lowing the construction of factor-specific language
models with higher-order n-grams. Such language
models can help to obtain syntactically more cor-
rect outputs.
We used the standard models available in Moses
as feature functions: relative frequencies, lexi-
cal weights, word and phrase penalties, wbe-msd-
bidirectional-fe reordering models, and two lan-
guage models (one for surface and one for POS
tags). Phrase scoring was computed using Good-
Turing discounting (Foster et al, 2006).
As aforementioned, we developed five factored
Moses-based independent systems with different
134
approaches. We explain them in Section 3. As
a final decision, we applied a system selection
scheme (Formiga et al, 2013; Specia et al, 2010)
to consider the best candidate for each sentence,
according to human trained quality estimation
(QE) models. We set monotone reordering of
the punctuation signs for the decoding using the
Moses wall feature.
We tuned the systems using the Moses
MERT (Och, 2003) implementation. Our focus
was on minimizing the BLEU score (Papineni et
al., 2002) of the development set. Still, for ex-
ploratory purposes, we tuned configuration (c) us-
ing PRO (Hopkins and May, 2011) to set the ini-
tial weights at every iteration of the MERT algo-
rithm. However, it showed no significant differ-
ences compared to the original MERT implemen-
tation.
We trained the baseline system using all
the available parallel corpora, except for
common-crawl. That is, European Parlia-
ment (EPPS) (Koehn, 2005), News Commentary,
and United Nations. Regarding the monolingual
data, there were more News corpora organized
by years for Spanish. The data is available at
the Translation Task?s website1. We used all
the News corpora to busld the language model
(LM). Firstly, a LM was built for every corpus
independently. Afterwards, they were combined
to produce de final LM.
For internal testing we used the News 2011 and
News 2012 data and concatenated the remaining
three years of News data as a single parallel corpus
for development.
We processed the corpora as in our participa-
tion to WMT12 (Formiga et al, 2012a). Tok-
enization and POS-tagging in both Spanish and
English was obtained with FreeLing (Padro? et al,
2010). Stemming was carried out with Snow-
ball (Porter, 2001). Words were conditionally case
folded based on their POS: proper nouns and ad-
jectives were separated from other categories to
determine whether a string should be fully folded
(no special property), partially folded (noun or ad-
jective) or not folded at all in (acronym).
Bilingual corpora was filtered with the clean-
corpus-n script of Moses (Koehn et al, 2007a), re-
moving those pairs in which a sentence was longer
than 70. For the CommonCrawl corpus we used a
more complex filtering step (cf. Section 3.3).
1http://www.statmt.org/wmt13/translation-task.html
Postprocessing included two special scripts to
recover contractions and clitics. Detruecasing was
done forcing the capitals after the punctuation
signs. Furthermore we used an additional script in
order to check the casing of output names with re-
spect to the source. We reused our language mod-
els and alignments (with stems) from WMT12.
3 Improvement strategies
We tried three different strategies to improve the
baseline system. Section 3.1 shows a strategy
based on morphology simplification plus genera-
tion. Its aim is dealing with the problems raised
by morphology-rich languages, such as Spanish.
Section 3.2 presents a domain?adaptation strategy
that consists of deriving new units. Section 3.3
presents an advanced strategy to filter the good bi-
sentences from the CommonCrawl corpus, which
might be useful to perform the domain adaptation.
3.1 Morphology generation
Following the success of our WMT12 participa-
tion (Formiga et al, 2012a), our first improve-
ment is based on the morphology generalization
and generation approach (Formiga et al, 2012b).
We focus our strategy on simplifying verb forms
only.
The approach first translates into Spanish sim-
plified forms (de Gispert and Marin?o, 2008). The
final inflected forms are predicted through a mor-
phology generation step, based on the shallow
and deep-projected linguistic information avail-
able from both source and target language sen-
tences.
Lexical sparseness is a crucial aspect to deal
with for an open-domain robust SMT when trans-
lating to morphology-rich languages (e.g. Span-
ish) . We knew beforehand (Formiga et al, 2012b)
that morphology generalization is a good method
to deal with generic translations and it provides
stability to translations of the training domain.
Our morphology prediction (generation) sys-
tems are trained with the WMT13 corpora (Eu-
roparl, News, and UN) together with noisy data
(OpenSubtitles). This combination helps to obtain
better translations without compromising the qual-
ity of the translation models. These kind of mor-
phology generation systems are trained with a rel-
atively short amount of parallel data compared to
standard SMT training corpora.
Our main enhancement to this strategy is the
135
addition of source-projected deep features to the
target sentence in order to perform the morphol-
ogy prediction. These features are Dependency
Features and Semantic Role Labelling, obtained
from the source sentence through Lund Depen-
dency Parser2. These features are then projected
to the target sentence as explained in (Formiga et
al., 2012b).
Projected deep features are important to pre-
dict the correct verb morphology from clean and
fluent text. However, the projection of deep fea-
tures is sentence-fluency sensitive, making it un-
reliable when the baseline MT output is poor. In
other words, the morphology generation strategy
becomes more relevant with high-quality MT de-
coders, as their output is more fluent, making the
shallow and deep features more reliable classifier
guides.
3.2 Domain Adaptation through pivot
derived units
Usually the WMT Translation Task focuses on
adapting a system to a news domain, offering an
in-domain parallel corpus to work with. How-
ever this corpus is relatively small compared to
the other corpora. In our previous participation
we demonstrated the need of performing a more
aggressive domain adaptation strategy. Our strat-
egy was based on using in-domain parallel data to
adapt the translation model, but focusing on the
decoding errors that the out-of-domain baseline
system makes when translating the in-domain cor-
pus.
The idea is to identify the system mistakes and
use the in-domain data to learn how to correct
them. To that effect, we interpolate the transla-
tion models (phrase and lexical reordering tables)
with a new adapted translation model with derived
units. We obtained the units identifying the mis-
matching parts between the non-adapted transla-
tion and the actual reference (Henr??quez Q. et al,
2011). This derivation approach uses the origi-
nal translation as a pivot to find a word-to-word
alignment between the source side and the target
correction (word-to-word alignment provided by
Moses during decoding).
The word-to-word monolingual alignment be-
tween output translation target correction was ob-
tained combining different probabilities such as
i)lexical identity, ii) TER-based alignment links,
2http://nlp.cs.lth.se/software/
Corpus Sent. Words Vocab. avg.len.
Original EN 1.48M 29.44M 465.1k 19.90ES 31.6M 459.9k 21.45
Filtered EN 0.78M 15.3M 278.0k 19.72ES 16.6M 306.8k 21.37
Table 1: Commoncrawl corpora statistics for
WMT13 before and after filtering.
iii) lexical model probabilities, iv) char-based Lev-
enshtein distance between tokens and v) filtering
out those alignments from NULL to a stop word
(p = ??).
We empirically set the linear interpolation
weight as w = 0.60 for the baseline translation
models and w = 0.40 for the derived units trans-
lations models. We applied the pivot derived units
strategy to the News domain and to the filtered
Commoncrawl corpus (cf. Section 5). The proce-
dure to filter out the Commoncrawl corpus is ex-
plained next.
3.3 CommonCrawl Filtering
We used the CommonCrawl corpus, provided for
the first time by the organization, as an impor-
tant source of information for performing aggres-
sive domain adaptation. To decrease the impact
of the noise in the corpus, we performed an auto-
matic pre-selection of the supposedly more correct
(hence useful) sentence pairs: we applied the au-
tomatic quality estimation filters developed in the
context of the FAUST project3. The filters? pur-
pose is to identify cases in which the post-editions
provided by casual users really improve over auto-
matic translations.
The adaptation to the current framework is as
follows. Example selection is modelled as a bi-
nary classification problem. We consider triples
(src, ref , trans), where src and ref stand for the
source-reference sentences in the CommonCrawl
corpus and trans is an automatic translation of the
source, generated by our baseline SMT system. A
triple is assigned a positive label iff ref is a bet-
ter translation from src than trans. That is, if the
translation example provided by CommonCrawl is
better than the output of our baseline SMT system.
We used four feature sets to characterize the
three sentences and their relationships: sur-
face, back-translation, noise-based and similarity-
based. These features try to capture (a) the simi-
larity between the different texts on the basis of
3http://www.faust-fp7.eu
136
diverse measures, (b) the length of the different
sentences (including ratios), and (c) the likelihood
of a source or target text to include noisy text.4
Most of them are simple, fast-calculation and
language-independent features. However, back-
translation features require that trans and ref are
back-translated into the source language. We did
it by using the TALP es-en system from WMT12.
Considering these features, we trained lin-
ear Support Vector Machines using SVMlight
(Joachims, 1999). Our training collection was the
FFF+ corpus, with +500 hundred manually anno-
tated instances (Barro?n-Ceden?o et al, 2013). No
adaptation to CommonCrawl was performed. To
give an idea, classification accuracy over the test
partition of the FFF+ corpus was only moderately
good (?70%). However, ranking by classification
score a fresh set of over 6,000 new examples, and
selecting the top ranked 50% examples to enrich a
state-of-the-art SMT system, allowed us to signifi-
cantly improve translation quality (Barro?n-Ceden?o
et al, 2013).
For WMT13, we applied these classifiers to
rank the CommonCrawl translation pairs and then
selected the top 53% instances to be processed by
the domain adaptation strategy. Table 1 displays
the corpus statistics before and after filtering.
4 System Combination
We approached system combination as a system
selection task. More concretely, we applied Qual-
ity Estimation (QE) models (Specia et al, 2010;
Formiga et al, 2013) to select the highest qual-
ity translation at sentence level among the trans-
lation candidates obtained by our different strate-
gies. The QE models are trained with human
supervision, making use of no system-dependent
features.
In a previous study (Formiga et al, 2013),
we showed the plausibility of building reliable
system-independent QE models from human an-
notations. This type of task should be addressed
with a pairwise ranking strategy, as it yields bet-
ter results than an absolute quality estimation ap-
proach (i.e., regression) for system selection. We
also found that training the quality estimation
models from human assessments, instead of au-
tomatic reference scores, helped to obtain better
4We refer the interested reader to (Barro?n-Ceden?o et al,
2013) for a detailed description of features, process, and eval-
uation.
models for system selection for both i) mimicking
the behavior of automatic metrics and ii) learning
the human behavior when ranking different trans-
lation candidates.
For training the QE models we used the data
from the WMT13 shared task on quality estima-
tion (System Selection Quality Estimation at Sen-
tence Level task5), which contains the test sets
from other WMT campaigns with human assess-
ments. We used five groups of features, namely:
i) QuestQE: 17 QE features provided by the Quest
toolkit6; ii) AsiyaQE: 26 QE features provided by
the Asiya toolkit for MT evaluation (Gime?nez and
Ma`rquez, 2010a); iii) LM (and LM-PoS) perplex-
ities trained with monolingual data; iv) PR: Clas-
sical lexical-based measures -BLEU (Papineni et
al., 2002), NIST (Doddington, 2002), and ME-
TEOR (Denkowski and Lavie, 2011)- computed
with a pseudo-reference approach, that is, using
the other system candidates as references (Sori-
cut and Echihabi, 2010); and v) PROTHER: Ref-
erence based metrics provided by Asiya, including
GTM, ROUGE, PER, TER (Snover et al, 2008),
and syntax-based evaluation measures also with a
pseudo-reference approach.
We trained a Support Vector Machine ranker by
means of pairwise comparison using the SVMlight
toolkit (Joachims, 1999), but with the ?-z p? pa-
rameter, which can provide system rankings for
all the members of different groups. The learner
algorithm was run according to the following pa-
rameters: linear kernel, expanding the working set
by 9 variables at each iteration, for a maximum of
50,000 iterations and with a cache size of 100 for
kernel evaluations. The trade-off parameter was
empirically set to 0.001.
Table 2 shows the contribution of different fea-
ture groups when training the QE models. For
evaluating performance, we used the Asiya nor-
malized linear combination metric ULC (Gime?nez
and Ma`rquez, 2010b), which combines BLEU,
NIST, and METEOR (with exact, paraphrases and
synonym variants). Within this scenario, it can
be observed that the quality estimation features
(QuestQE and AsiyaQE) did not obtain good re-
sults, perhaps because of the high similarity be-
tween the test candidates (Moses with different
configurations) in contrast to the strong differ-
ence between the candidates in training (Moses,
5http://www.quest.dcs.shef.ac.uk/wmt13 qe.html
6http://www.quest.dcs.shef.ac.uk
137
Features Asiya ULCWMT?11 WMT?12 AVG WMT?13
QuestQE 60.46 60.64 60.55 60.06
AsiyaQE 61.04 60.89 60.97 60.29
QuestQE+AsiyaQE 60.86 61.07 60.96 60.42
LM 60.84 60.63 60.74 60.37
QuestQE+AsiyaQE+LM 60.80 60.55 60.67 60.21
QuestQE+AsiyaQE+PR 60.97 61.12 61.05 60.54
QuestQE+AsiyaQE+PR+PROTHER 61.05 61.19 61.12 60.69
PR 61.24 61.08 61.16 61.04
PR+PROTHER 61.19 61.16 61.18 60.98
PR+PROTHER+LM 61.11 61.29 61.20 61.03
QuestQE+AsiyaQE+PR+PROTHER+LM 60.70 60.88 60.79 60.14
Table 2: System selection scores (ULC) obtained using QE models trained with different groups of
features. Results displayed for WMT11, WMT12 internal tests, their average, and the WMT13 test
EN?ES BLEU TER
wmt13 Primary 29.5 0.586
wmt13 Secondary 29.4 0.586
Table 4: Official automatic scores for the WMT13
English?Spanish translations.
RBMT, Jane, etc.). On the contrary, the pseudo-
reference-based features play a crucial role in the
proper performance of the QE model, confirming
the hypothesis that PR features need a clear dom-
inant system to be used as reference. The PR-
based configurations (with and without LM) had
no big differences between them. We choose the
best AVG result for the final system combination:
PR+PROTHER+LM, which it is consistent with
the actual WMT13 evaluated afterwards.
5 Results
Evaluations were performed considering different
quality measures: BLEU, NIST, TER, and ME-
TEOR in addition to an informal manual analy-
sis. This manifold of metrics evaluates distinct as-
pects of the translation. We evaluated both over
the WMT11 and WMT12 test sets as internal in-
dicators of our systems. We also give our perfor-
mance on the WMT13 test dataset.
Table 3 presents the obtained results for the
different strategies: (a) Moses Baseline (w/o
commoncrawl) (b) Moses Baseline+Morphology
Generation (w/o commoncrawl) (c) Moses Base-
line+News Adaptation through pivot based align-
ment (w/o commoncrawl) (d) Moses Baseline +
News Adaptation (b) + Morphology Generation
(c) (e) Moses Baseline + News Adaptation (b) +
Filtered CommonCrawl Adaptation.
The official results are in Table 4. Our primary
(contrastive) run is the system combination strat-
egy whereas our secondary run is the full training
strategy marked as (e) on the system combination.
Our primary system was ranked in the second clus-
ter out of ten constrained systems in the official
manual evaluation.
Independent analyzes of the improvement
strategies show that the highest improvement
comes from the CommonCrawl Filtering + Adap-
tation strategy (system e). The second best strat-
egy is the combination of the morphology pre-
diction system plus the news adaptation system.
However, for the WMT12 test the News Adap-
tation strategy contributes to main improvement
whereas for the WMT13 this major improvement
is achieved with the morphology strategy. Analyz-
ing the distance betweem each test set with respect
to the News and CommonCrawl domain to further
understand the behavior of each strategy seems an
interesting future work. Specifically, for further
contrasting the difference in the morphology ap-
proach, it would be nice to analyze the variation in
the verb inflection forms. Hypothetically, the per-
son or the number of the verb forms used may have
a higher tendency to be different in the WMT13
test set, implying that our morphology approach is
further exploited.
Regarding the system selection step (internal
WMT12 test), the only automatic metric that has
an improvement is TER. However, TER is one of
138
EN?ES BLEU NIST TER METEOR
wmt12 Baseline 32.97 8.27 49.27 49.91
wmt12 + Morphology Generation 33.03 8.29 49.02 50.01
wmt12 + News Adaptation 33.22 8.31 49.00 50.16
wmt12 + News Adaptation + Morphology Generation 33.29 8.32 48.83 50.29
wmt12 + News Adaptation + Filtered CommonCrawl Adaptation 33.61 8.35 48.82 50.52
wmt12 System Combination 33.43 8.34 48.78 50.44
wmt13 Baseline 29.02 7.72 51.92 46.96
wmt13 Morphology Generation 29.35 7.73 52.04 47.04
wmt13 News Adaptation 29.19 7.74 51.91 47.07
wmt13 News Adaptation + Morphology Generation 29.40 7.74 51.96 47.12
wmt13 News Adaptation + Filtered CommonCrawl Adaptation 29.47 7.77 51.82 47.22
wmt13 System Combination 29.54 7.77 51.76 47.34
Table 3: Automatic scores for English?Spanish translations.
the most reliable metrics according to human eval-
uation. Regarding the actual WMT13 test, the sys-
tem selection step is able to overcome all the auto-
matic metrics.
6 Conclusions and further work
This paper described the TALP-UPC participa-
tion for the English-to-Spanish WMT13 transla-
tion task. We applied the same systems as in last
year, but enhanced with new techniques: sentence
filtering and system combination.
Results showed that both approaches performed
better than the baseline system, being the sentence
filtering technique the one that most improvement
reached in terms of all the automatic quality indi-
cators: BLEU, NIST, TER, and METEOR. The
system combination was able to outperform the
independent systems which used morphological
knowledge and/or domain adaptation techniques.
As further work would like to focus on further
advancing on the morphology-based techniques.
Acknowledgments
This work has been supported in part by
Spanish Ministerio de Econom??a y Competitivi-
dad, contract TEC2012-38939-C03-02 as well
as from the European Regional Development
Fund (ERDF/FEDER) and the European Commu-
nity?s FP7 (2007-2013) program under the fol-
lowing grants: 247762 (FAUST, FP7-ICT-2009-
4-247762), 29951 (the International Outgoing
Fellowship Marie Curie Action ? IMTraP-2011-
29951) and 246016 (ERCIM ?Alain Bensoussan?
Fellowship).
References
Alberto Barro?n-Ceden?o, Llu??s Ma`rquez, Carlos A.
Henr??quez Q, Llu??s Formiga, Enrique Romero, and
Jonathan May. 2013. Identifying Useful Hu-
man Correction Feedback from an On-line Machine
Translation Service. In Proceedings of the Twenty-
Third International Joint Conference on Artificial
Intelligence. AAAI Press.
Adria` de de Gispert and Jose? B. Marin?o. 2008. On the
impact of morphology in English to Spanish statis-
tical MT. Speech Communication, 50(11-12):1034?
1046.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Language
Technology Research, HLT ?02, pages 138?145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Lluis Formiga, Carlos A. Henr??quez Q., Adolfo
Herna?ndez, Jose? B. Marin?o, Enric Monte, and Jose?
A. R. Fonollosa. 2012a. The TALP-UPC phrase-
based translation systems for WMT12: Morphol-
ogy simplification and domain adaptation. In Pro-
ceedings of the Seventh Workshop on Statistical
Machine Translation, pages 275?282, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
Llu??s Formiga, Adolfo Herna?ndez, Jose? B. Marin?, and
Enrique Monte. 2012b. Improving english to
spanish out-of-domain translations by morphology
generalization and generation. In Proceedings of
139
the AMTA Monolingual Machine Translation-2012
Workshop.
Llu??s Formiga, Llu??s Ma`rquez, and Jaume Pujantell.
2013. Real-life translation quality estimation for mt
system selection. In Proceedings of 14th Machine
Translation Summit (MT Summit), Nice, France,
September. EAMT.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 53?61, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
measures for automatic machine translation evalu-
ation. Machine Translation, 24(3-4):209?240, De-
cember.
Carlos A. Henr??quez Q., Jose? B. Marin?o, and Rafael E.
Banchs. 2011. Deriving translation units using
small additional corpora. In Proceedings of the 15th
Conference of the European Association for Ma-
chine Translation.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods
? Support Vector Learning, chapter Making large-
Scale SVM Learning Practical. MIT Press.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007a. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007b. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Llu??s Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language processing
tools. In Proceedings of 7th Language Resources
and Evaluation Conference (LREC 2010), La Val-
letta, MALTA, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics (ACL).
M. Porter. 2001. Snowball: A language for stemming
algorithms.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and Translation Model Adaptation
using Comparable Corpora. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010.
Machine Translation Evaluation Versus Quality Es-
timation. Machine Translation, 24:39?50, March.
140
