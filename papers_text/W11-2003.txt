Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 8?17,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Which System Differences Matter?
Using `1/`2 Regularization to Compare Dialogue Systems
Jose? P. Gonza?lez-Brenes and Jack Mostow
Project LISTEN
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{joseg,mostow}@cs.cmu.edu
Abstract
We investigate how to jointly explain the per-
formance and behavioral differences of two
spoken dialogue systems. The Join Evalu-
ation and Differences Identification (JEDI),
finds differences between systems relevant
to performance by formulating the problem
as a multi-task feature selection question.
JEDI provides evidence on the usefulness of
a recent method, `1/`p-regularized regres-
sion (Obozinski et al, 2007). We evaluate
against manually annotated success criteria
from real users interacting with five different
spoken user interfaces that give bus schedule
information.
1 Introduction
This paper addresses the problem of how to deter-
mine which differences between two versions of a
system affect their behavior. Researchers in Spo-
ken Dialogue Systems (SDSs) can be perplexed as to
which of the differences between alternative systems
affect performance metrics (Bacchiani et al, 2008).
For example, when testing on real users at differ-
ent periods of time, the variance of the performance
metrics might be higher than the difference between
systems, causing (i) significantly different scores in
identical systems deployed at different times, and
(ii) the same score on different systems (Gonza?lez-
Brenes et al, 2009).
We approach the problem of finding which system
differences matter by describing dialogues as feature
vectors constructed from the logs of dialogs gener-
ated by the SDSs interacting with real users. Hence,
we aim to identify features that jointly characterize
the system differences and the performance of the
SDS being evaluated. These features should be able
to (i) predict a performance metric and (ii) distin-
guish between the two SDS being evaluated.
The main contribution of this paper is a novel al-
gorithm for detecting differences between two sys-
tems that can explain performance. Additionally, we
provide details on how to implement state-of-the-art
multi-task learning for SDSs.
The rest of this manuscript is organized as fol-
lows. Section 2 reviews multi-task feature selection.
Section 3 describes two algorithms to find which
system differences matter. Section 4 describes the
specific SDS used to illustrate our algorithms. Sec-
tion 5 presents some experimental results. Section 6
reviews related prior work. Section 7 presents some
concluding remarks and future work. Appendix A
provides implementation details of the multi-task
learning approach we used.
2 Feature Selection
In this section we describe how we use regression to
perform feature selection. Feature selection meth-
ods construct and select subsets of features in order
to build a good predictor. We focus our attention on
feature selection methods that use complexity (regu-
larization) penalties, because of their recent theoret-
ical and experimental success (Yuan and Lin, 2006;
Park and Hastie, 2007). We provide a more rigorous
description of how to implement this formulation as
an optimization problem in Appendix A.
We use labels to encode the output we want to
predict. For example, if our performance metric is
binary, we label successful dialogues with a +1, and
unsuccesful dialogues with a ?1. Given a training
set consisting of labeled dialogues, we want to learn
8
a model that assigns a label to unseen dialogues. We
follow an approach called empirical risk minimiza-
tion (Obozinski et al, 2007), that aims to minimize
the error of fitting the training data, while penalizing
the complexity of the model:
Minimize Model loss + ? Complexity (1)
Here the hyper-parameter ? controls the trade-off
between a better fit to the training data (with a higher
risk of over-fitting it), and a simpler model, with
fewer features selected (and less predictive power).
We now review the two components of risk mini-
mization, model loss and complexity penalty.
2.1 Model Loss
We model probabilistically the loss of our model
against the real-life phenomenon studied. Given a
dialogue x, with correct label l, its loss using a
model ? is:
loss?(y?, x) ? P (y = l|x; reality)? P (y? = l|x;?)
(2)
Here y? is the predicted value of the event y. Since l
is the true label, P (y = l|x; reality) = 1. To get the
overall loss of the model, we aggregate over the pre-
diction loss of each of the dialogues in the training
set by summing their individual loss calculated with
Equation 2. Let X = {x(1), x(2), . . . x(n)} be the n
dialogues in the training set. Then the overall loss of
model ? is:
loss?(y
(1), x(1)) + ? ? ?+ loss?(y
(n), x(n))
Since we use discrete labels, we use a logistic
function to model their probability. Let x1, . . . xk
be the k features extracted from dialogue x. Then
the logistic regression model is:
P (y? = +1|x;?) =
1
Z
exp(?1x1 + ? ? ?+ ?kxk)
Here ?1...?k are the parameters of the model, and
Z simply normalizes P to ensure that P is a valid
probability function (the range of P should be 0 to
1):
Z = 1 + exp(?1x1 + ? ? ?+ ?kxk)
Multi-task learning solves related regression
problems at the same time using a shared representa-
tion. We now describe the risk-minimization formu-
lation for multi-task learning. Let ym be the value
of the performance metric. Let ys be the label of the
system that generated the dialogue. The individual
dialogue loss of using models ?m and ?s is:
loss?m(y?
m, x) + loss?s(y?
s, x)
2.2 Complexity Penalties
We consider a feature xi to be selected into the
model if its regression coefficient ?i is non-zero.
Complexity penalties encourage selecting only a few
features. We review several commonly used penal-
ties (Zou and Hastie, 2005):
? `2 Penalty. Under some circumstances `2
penalties perform better than other types of
penalties (Zou and Hastie, 2005). The `2
penalty for a model ? is:
||?||`2 ?
?
(?1)2 + ? ? ?+ (?k)2
? `1 Penalty. An `1 penalty induces sparsity by
setting many parameters of the model ? to ex-
actly zero (Tibshirani, 1996).
||?||`1 ? |?1|+ ? ? ?+ |?k|
? `1/`2 Penalty. Yuan and Lin (2006) proposed
a group penalty for penalizing groups of fea-
tures simultaneously. Previous work has shown
that grouping features between tasks encour-
ages features to be used either by all tasks or
by none (Turlach et al, 2005; Obozinski et
al., 2007; Lounici et al, 2009; Puniyani et al,
2010). Our `1/`2 penalty is:
?
?
?
?
?
(?m1 )
2 + (?s1)
2
?
?
?
?+ ...+
?
?
?
?
?
(?mk )
2 + (?sk)
2
?
?
?
?
3 Finding Features that Predict
Performance and System Differences
We find system differences that are predictive of
SDS performance, relying on:
? Describing dialogues as feature vectors. The
behavior of the systems must be describable
by features extracted from the logs of the sys-
tems. A discussion of feature engineering for
dialogue systems is found in (Gonza?lez-Brenes
and Mostow, 2011).
9
? Finding system differences. The features of a
classifier that distinguishes between SDSs, can
be used to identify their differences (Gonza?lez-
Brenes et al, 2009). When comparing two
SDSs, we label the baseline system with ?1,
and the alternate version with +1.
? Modeling performance. Although our ap-
proach does not depend on a specific perfor-
mance metric, in this paper we use dialogue
success, a binary indicator that triggers that
the user?s query was answered by the SDS.
Task completion is cheaper to compute than di-
alogue success, as it does not require a man-
ual human labeled reference, but we consider
that dialogue success is a more accurate metric.
Task completion is used in commercial applica-
tions (Bacchiani et al, 2008), and has been ex-
tensively studied in the literature (Walker et al,
2001; Walker et al, 2002; Hajdinjak and Mi-
helic, 2006; Levin and Pieraccini, 2006; Mo?ller
et al, 2007; Mo?ller et al, 2008; Schmitt et
al., 2010). We encode success of dialogues by
manually annotating them with a binary vari-
able that distinguishes if the user query is ful-
filled by the SDS.
We now present two algorithms to find what dif-
ferences matter between systems. We introduce Se-
rial EvaluatioN Analysis (SERENA) as a scaffold
for the Join Evaluation and Differences Identifica-
tion (JEDI) algorithm.
3.1 SERENA algorithm
The input to SERENA is a collection of log files
created by two different SDSs and two functions that
represent the correct label for the regression tasks.
In our case these functions should return binary la-
bels (+1,?1): one task distinguishes between suc-
cessful and unsuccessful dialogues, and the other
task distinguishes a baseline from an alternative SDS
version. SERENA?s objective is to select features
from one task, and use them to predict the other task.
For example, SERENA selects features that predict
differences between versions, and uses them to pre-
dict performance.
Algorithm 1 provides the pseudo-code for SER-
ENA. Line 1 builds the training set X from parsing
the logs of the SDSs. Lines 2 and 3 create the output
Algorithm 1 SERENA algorithm
Require: Logs1, Logs2 are the collections of SDS
logs of two systems. task1, task2 are func-
tions that return the value of a performance met-
ric, and which system is being evaluated (?1 if
is the baseline, +1 otherwise).
1: X? extract features(Log1,Log2)
2: yt1 ?
[
task1(Logs1)
task1(Logs2)
]
3: yt2 ?
[
task2(Logs1)
task2(Logs2)
]
4: // Select features that explain both tasks:
5: for ? = {0.1, 0.2, . . . } do
6: ?t1 ? regression`1(X,y
t1 , ?)
7: // Get feature weights:
8: X? ? X; where xk|?xk ? X?, ?
t1
k 6= 0
9: ?? ? regression`2(X
?,yt2 , ?c)
10: end for
11: return ??
variables y for the regression tasks. Line 6 returns
the most predictive features using `1 regularization
as described in Section 2. Line 8 builds a new train-
ing set, removing the features that were not selected
in line 6. Line 9 builds the final coefficients by fitting
a `2-regularized model using a constant ?c. We cal-
culate the coefficients using an `2 penalty, because
it has a better fit to the data (Zou and Hastie, 2005).
Moreover, by using the same penalty, we control for
the idiosyncrasies different penalties have in param-
eter learning. In the experiments described in Sec-
tion 5, all of our experiments are reported fitting a
`2-regularized models.
SERENA is not conmutative with regards to the
order of the tasks: selecting the features that predict
performance and using them to predict system dif-
ferences is not the same as the reverse. More impor-
tantly, SERENA only searches in one of the tasks at
a time. We are interested in finding the features that
explain both tasks simultaneously. In the next sub-
section we describe JEDI which makes use of recent
advances in multi-task feature selection in order to
find the features for both tasks at the same time.
3.2 JEDI algorithm
Algorithm 2 provides the pseudo-code for JEDI.
JEDI uses multi-task regression to find the fea-
tures that affect performance and system differences
10
Algorithm 2 JEDI algorithm
Require: Logs1, Logs2 are the collections of SDS
logs of two systems. task1, task2 are func-
tions that return the value of a performance met-
ric, and which system is being evaluated (?1 if
is the baseline, +1 otherwise).
1: X? extract features(Log1,Log2)
2: yt1 ?
[
task1(Logs1)
task1(Logs2)
]
3: yt2 ?
[
task2(Logs1)
task2(Logs2)
]
4: // Select features that explain both tasks:
5: for ? = {0.1, 0.2, . . . } do
6: ?t1?t2 ? regression`1/`2(X, ~y
t1 , ~yt2 , ?)
7: // Get feature weights:
8: X? ? X; where xk|?xk ? X?, ?
t1
k 6= 0
9: ?? ? regression`2(X
?,yt2 , ?c)
10: end for
11: return ??
jointly. To ensure a sparse feature set across the
tasks, it applies a `1/`2 penalty. In this way, a feature
is either selected to predict performance and system
differences, or it is not selected at all. In the next
section we review a concrete example of a SDS in
which we apply SERENA and JEDI.
4 Case Study
We use data collected from Let?s Go!, an experi-
mentary SDS available for research purposes1 that
provides bus planning information to the Pittsburgh
East End community (Raux et al, 2006). Figure 1
shows a sample dialogue.
We assembled three disjoint datasets to have di-
alogues just before and after what we considered
important architectural differences of Let?s Go?s im-
plementation. By going through the change log, we
realize that the magnitude of the differences varies
widely. Table 1 describes the date the systems were
deployed, the description of the change in the SDS,
and the number of dialogues in each set. From the
86 dialogues of ?Set A?, half are a random sam-
ple of the logs before the acoustic model was re-
trained (Log1), and the other half is a sample af-
ter the acoustic model was retrained (Log2). In the
case of the 84 dialogues of ?Set B?, half of the logs
1http://www.dialrc.org/
System: Welcome to the CMU Let?s Go!
Bus Information System. How
may I help you?
User: I?d like to go to Forbes and Mur-
ray.
System: To Forbes and Murray. Where
are you leaving from?
User: Carnegie Mellon.
System: From Carnegie Mellon. There
are five buses running between
Carnegie Mellon and Forbes and
Murray: the 501, the 59U, the
61A, the 61B, and the 61C.
What else can I do for you?
User: When is the next one?
System: The next 61C going to Forbes
and Murray will leave Carnegie
Mellon at 5:13 PM.
Figure 1: Example of a Let?s Go dialogue, from (Raux et
al., 2006)
use an older version of the Olympus SDS infrastruc-
ture (Bohus et al, 2007), and the other half uses
Olympus II. Since each system was deployed in a
different period of time, we want to corroborate that
we are modeling the differences among systems, and
not seasonal. Hence, for control conditions, we also
chose a data set that contained no major change to
the system or to other conditions (Set C).
Sets were built by randomly sampling from the
collection of logs. They have the same number of di-
alogues from each SDS version (baseline/alternate).
Each dialogue was manually annotated to indicate
whether the user?s query was fulfilled, and we re-
moved from our analysis the two dialogues that were
only partially fulfilled. The number of successful di-
alogues is different from the number of unsuccessful
dialogues.
We created a script to extract features from the log
files of Let?s Go!. The script has an explicit list of
features to extract from the event logs, such as the
words that were identified by the Automatic Speech
Recognizer. Although this script is dependent on our
specific log format, it should be a simple program-
ming task to adapt it to a different dialogue system,
provided its logs are comprehensive enough. The
11
Table 1: Dataset Description
Set Size Description Date
A 86
Baseline 8/05 10/05
New acoustic model 12/05 2/05
B 86
Baseline 8/06 10/06
New SDS architecture 6/07 7/07
C 84
Baseline 10/07 11/07
No change 11/07 12/07
script performs the standard transformation of cen-
tering feature values as z-scores with mean zero and
standard deviation one.
Table 2 summarizes the properties we are inter-
ested to model. Dialogue properties are the features
that summarize the behavior of the whole dialogue,
and turn properties work at a finer-grain. We encode
turn properties into features in the following way:
? Global average. Turn properties are averaged
over the entire dialogue.
? Beginning window. Turn properties are aver-
aged across an initial window. Based on pre-
liminary experiments, we defined the window
as the first 5 turns.
? State. We relied on the fact that SDSs are of-
ten engineered as finite state automata (Bohus
et al, 2007). Properties are averaged across the
states that belong to a specific dialogue state
(for example, asking departure place). Because
we are interested in early identification of dif-
ferences, we restricted state features to be in-
side the beginning window.
5 Evaluation
We assess the performance of our algorithms by
evaluating the classification accuracy using the fea-
tures selected. To facilitate assessment of SDS, we
only consider models that select up to 15 features.
Figure 2 reports mean classification accuracy using
five-fold cross-validation. Its first column describes
how well the features selected perform on detecting
system differences, and the second column describes
how well they predict task success as a performance
metric. We compare JEDI and SERENA against the
following approaches:
Table 2: Features
Dialogue Properties
# of re-prompted turns
# of turns
Mean Dialogue length
is evening?, is weekend?, 0-23 hour
Turn Properties
Occurrences of word w
# of parse errors
# of unrecognized words
# of words
# of repeated words
# of unique words
Turn length
Words per minute
Failed prompts (number and percentage)
Mean Utterance Length
Barge-in (in seconds)
Machine-user pause (in seconds)
User-machine pause (in seconds)
Amplitude (power) statistics
? Majority classifier baseline. A classifier that
always selects the majority class (datasets B
and C are not balanced in the number of suc-
cessful dialogues).
? Same Task Classifier We report the classifica-
tion accuracy of the model trained and tested
on the same task. Features are selected using
an `1 penalty, and the coefficients are estimated
with `2-regularized logistic regression. For ex-
ample, in the column of the left, SERENA uses
the most predictive features of system differ-
ences to predict success, while the same task
classifier uses them to predict system differ-
ences. The same task classifier does not answer
?which system differences matter?, it is just an
interesting benchmark.
We used a one-sample t-test to check for sta-
tistically significant differences against the classifi-
cation accuracy of the majority classifier baseline.
We used a paired-sample t-test to check for sig-
nificant differences in classification accuracy be-
tween classifiers. Paired samples have the same ?
hyper-parameter, which was described in the risk-
12
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1  System Differences
# of features
 
Da
tas
et 
A
Cla
ssi
fica
tion
 Ac
c.
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1 Dialogue Success
# of features
Cla
ssi
fica
tion
 Ac
c.
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1
# of features
 
Da
tas
et 
B
Cla
ssi
fica
tion
 Ac
c.
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1
# of features
Cla
ssi
fica
tion
 Ac
c.
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1
# of features
 
Da
tas
et 
C
Cla
ssi
fica
tion
 Ac
c.
 
 
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1
# of features
Cla
ssi
fica
tion
 Ac
c.
 
 
Majority SERENA JEDI Same Task Classifier
Figure 2: Classification accuracy of different feature se-
lection techniques
minimization formulation explained in Section 2.
This hyper-parameter is related to the number of fea-
tures selected ? as ? increases, the number of fea-
tures selected decreases. We use 5% as the signif-
icance level at which to reject the null hypothesis.
When checking for statistical differences, we tested
on the range of ?s computed2.
First we investigate the performance of the sim-
pler algorithm SERENA. For Dataset A, SERENA
does not yield significant differences over the ma-
jority classifier baseline. For Dataset B, SERENA
is significantly better than the majority classifier in
predicting system differences, but is significantly
worse for predicting success. This means that the or-
der in which we choose the tasks in SERENA affects
its performance. SERENA performs significantly
worse in the Control Set C. We conclude that SER-
2? = {100, 30, 25, 20, 19, 18, . . . , 1, 0.5, 0.25, 0.1}
Table 3: Features selected in Dataset A
Feature Suc. Diff. JEDI
System-user pause 5 5
Weekend night? 3
% of failed prompts 4
?Forbes St.? word 5 3
User?s max. power 5
Table 4: Features selected in Dataset B
Feature Suc. Diff. JEDI
% of failed prompts 5 4
User?s power std.dev. 5
Weekend night? 3
Unrecognized word 5
Words/min. 4
User-system pause 5
Turn length 5 5
ENA is not very reliable in predicting which system
differences matter.
We now discuss how well JEDI is able to fill-in for
the deficiencies of SERENA. As an ?upper-bound?,
we will compare it to a classifier trained and tested
in the same task. This classifier significantly dom-
inates over the majority baseline, even for the the
Control Set C, where there were no changes in the
SDS. This suggests that the classifier might be pick-
ing up on seasonal differences. For Set A, JEDI per-
forms significantly better than the majority classi-
fier and than SERENA. For Set B, there are no sig-
nificant differences between the upper-bound clas-
sifier and JEDI when predicting for changes in the
SDS. Again, JEDI dominates over SERENA and the
majority baseline. For the Control Set C, JEDI is
not statistically different from the majority baseline.
This is the expected behavior, since the difference in
performance cannot be explained by the differences
between the SDS. We hypothesize that the classifi-
cation accuracy of JEDI could be used as a distance
function between SDS: The closer the accuracy of
distinguishing SDS is to 50%, the more similar the
SDSs are. Conversely, when JEDI is able to classify
system differences closer to 100%, it is because the
SDSs are more different.
Tables 3 and 4 describe the features selected for
Sets A and B respectively. The numbers indicate
13
in how many folds the feature was selected by JEDI
and by classifiers trained to predict Success and SDS
differences using five-fold cross validation. The ?
used is selected to contain the closest to five features
(ties are resolved randomly). We only report fea-
tures that appeared in at least three folds. In Dataset
A we see that time of day is selected to predict di-
alogue success. Anecdotally, we have noticed that
many users during weekend nights appear to be in-
toxicated when calling the system. JEDI does not
select ?is weekend night? as a feature, because it
has little predictive power to detect system differ-
ences. In Dataset A, JEDI selects a speech recogni-
tion feature (the token ?Forbes St? was recognized),
and an end-pointing feature. Since in Dataset A, the
difference between systems correspond to a differ-
ent acoustic model, these features make sense intu-
itively. In Dataset B, JEDI detected that the features
most predictive with system differences and success
are percentage of failed prompts and the length of
the turn. The models for both systems make sense
after the fact. However, neither model was known
beforehand, nor did we know which of many fea-
tures considered would turn out to be informative.
Anecdotally, the documentation of the history of
changes of Let?s Go! is maintained manually. Some-
times, because of human error, this history is incom-
plete. The ability of JEDI to identify system differ-
ences has been able to help completing the history
of changes (Gonza?lez-Brenes et al, 2009).
6 Relation to Prior Work
The scientific literature offers several performance
metrics to assess SDS performance (Polifroni et al,
1992; Danieli and Gerbino, 1995; Bacchiani et al,
2008; Suendermann et al, 2010). SDS are eval-
uated using different objective and subjective met-
rics. Examples of objective metrics are the mean
number of turns in the dialogue, and dialogue suc-
cess. Subjective evaluations study measure satisfac-
tion through controlled user studies. Ai et al (2007)
studied the differences in using assessment metrics
with real users and paid users.
PARADISE, a notable example of a SDS subjec-
tive evaluation, finds linear predictors of a satisfac-
tion score using automatic and hand-labeled features
(Hajdinjak and Mihelic, 2006; Walker et al, 2001),
or only automatic features (Hastie et al, 2002). Sat-
isfaction scores are calibrated using surveys in con-
trolled experiments (Mo?ller et al, 2007; Mo?ller et
al., 2008). Alternatively, Eckert et al (1998) pro-
posed simulated users to evaluate SDSs. Their per-
formance metric has to be tuned with a subjective
evaluation as well, in which they refer to the PAR-
ADISE methodology. Our approach does not re-
quire user surveys to be calibrated. Moreover, it
would be feasible to adapt JEDI to regress to PAR-
ADISE, or other performance metrics. Our work ex-
tends previous studies that define performance met-
rics, in proposing an algorithm that finds how system
differences are related to performance.
7 Conclusions and Future Work
We have presented JEDI, a novel algorithm that finds
features describing system differences relevant to a
success metric. This is a novel, automated ?glass
box? assessment in the sense of linking changes in
overall performance to specific behavioral changes.
JEDI is an application of feature selection using reg-
ularized regression.
We have presented empirical evidence suggesting
that JEDI?s use of multi-task feature selection per-
forms better than single-task feature selection. Fu-
ture work could extend JEDI to quantify the vari-
ability in performance explained by the differences
found. Common techniques in econometrics, such
as the Seemingly Unrelated Regressions (SUR) for-
mulation (Zellner, 1962), may prove useful for this.
In our approach we used a single binary evalu-
ation criterion. By using a different loss function,
JEDI can be extended to allow continuous-valued
metrics. Moreover, previous work has argued that
evaluating SDSs should not be based on just a sin-
gle criterion (Paek, 2001). JEDI?s multi-task for-
mulation can be extended to include more than one
performance criterion at the same time, and may
prove helpful to understand trade-offs among differ-
ent evaluation criteria.
A Implementation Details of Feature
Selection
In this appendix we review how to set-up multi-task
feature selection as an optimization problem.
14
A.1 `1-Regularized Regression for Single-Task
Feature Selection
We first review using regression with `1 regulariza-
tion for single-task feature selection. Given a train-
ing set represented by X, denoting a n ? k matrix,
where n is the number of dialogues, and k is the
number of features extracted for each dialogue, we
want to find the coefficients of the parameter vector
~?, that can predict the output variables described in
the vector ~y of length n.
For this, we find the parameter vector that mini-
mizes the loss function J , penalized by a regulariza-
tion term (Tibshirani, 1996):
argmin
~?
J(X, ~?, ~y) + ?||~?||`1 (3)
In the case of binary classification, outputs are bi-
nary (any given y = ?1). A commonly used loss
function J is the Logistic Loss:
Jlog(x, ?, y) ?
1
1 + ey(x??)
(4)
The `p-norm of a vector ~? is defined as:
||~?||`p ?
(
k?
i=1
|?i|
p)1/p
The `?-norm is defined as ||~?||`? ?
max(?1, ?2, . . . , ?k).
The regularization term ||~?||`1 in Equation 3 con-
trols model complexity: The higher the value of the
hyper-parameter ?, the smaller number of features
selected. Conversely, the smaller the value of ?,
the better the fit to the training data, with higher
risk of over-fitting it. Thus, Equation 3 jointly per-
forms feature selection and parameter estimation; it
induces sparsity by setting many coefficients of ~?
to zero (Tibshirani, 1996). Features with non-zero
coefficients are considered the features selected.
A.2 `1-Regularized Regression for Multi-Task
Feature Selection
`1 regularization can be used to learn a classifier for
each of T prediction task independently. In our case
we are interested in only two prediction tasks: ver-
sion and success. We will index tasks with super-
script t, and we define Xt as the n ? k training
data for task t, used to predict the output variable ~yt.
Learning each model separately yields the following
optimization problem (Obozinski et al, 2007):
argmin
~?
t
T?
t=1
J(Xt, ~?
t
, ~yt) + ?||~?
t
||`1 (5)
Solving this problem leads to individual sparsity in
each task (each ~?
t
has many zeros), but the model
does not enforce a common subset of features for
all of the related output variables simultaneously
(Turlach et al, 2005). In the next subsection we
study how to achieve global sparsity across tasks.
A.3 `1/`p-Regularized Regression for
Multi-task Feature Selection
Although `1-regularization is very successful at se-
lecting individual features, it does not perform ad-
equately when a group of features should enter
or leave the model simultaneously (Yuan and Lin,
2006). Group LASSO (Yuan and Lin, 2006), which
relies on `1/`p-regularization to overcome this lim-
itation, by allowing groups of feature entering or
leaving the model simultaneously. `1/`p regular-
ization has been studied for multi-task learning by
grouping each of the k features across the T learning
tasks (Turlach et al, 2005; Obozinski et al, 2007;
Lounici et al, 2009; Puniyani et al, 2010).
Let us define B as a n? T matrix, whose tth col-
umn is the parameter vector for the task t. For ex-
ample, since we have two tasks B = [~?
t=1
, ~?
t=2
].
Let ~?g denote the g
th row of B. In the context of
multi-task learning, the `1/`p-norm of a matrix B is
defined as (Obozinski et al, 2007; Puniyani et al,
2010):
||B||`1/`p ?
k?
g=1
||~?g||`p (6)
Multi-task feature selection with `1/`p regular-
ization is formulated as (Obozinski et al, 2007;
Puniyani et al, 2010):
argmin
B
T?
t=1
J(Xt, ~?
t
, ~yt) + ?||B||`1/`2 (7)
When T = 1, the multi-task problem of Equation 7
reduces to the single-task problem of Equation 5.
15
A.4 Optimization procedure
Puniyani et al (2010) describe that finding the pa-
rameter coefficients B of Equation 7 can be achieved
more easily by transforming the problem into an
equivalent single-task multivariate regression. We
follow their procedure to create ~yg, ~?g and Xg:
1. Concatenate the vectors ~yt?s into a single vec-
tor ~yg of length n ? T . In our case, since we
have only two tasks (T = 2), we get the vector
~yg =
[ ~yt=1
~yt=2
]
.
2. Similarly, we concatenate the ~?
t
?s into a k? T
vector ~?g, in our case ~?g =
[ ~?
t=1
~?
t=2
]
.
3. Build a (n ? T )? (k ? T ) block-diagonal matrix
Xg, where Xt?s are placed along the diagonal,
and the rest of the elements are set to zero. In
our case since we only have two tasks this is
Xg =
[
Xt=1 ?
? Xt=2
]
, where each ? denotes a
n ? k zero-matrix. The expanded notation of
Xg is:
Xg ?
?
?
?
?
?
?
?
?
?
?
?
?
xt=1
(1)
1 ... x
t=1(1)
k 0 ... 0
...
...
...
...
xt=1
(n)
1 ... x
t=1(n)
k 0 ... 0
0 ... 0 xt=2
(1)
1 ... x
t=2(1)
k
...
...
...
...
0 ... 0 xt=2
(n)
1 ... x
t=2(n)
k
?
?
?
?
?
?
?
?
?
?
?
?
Thus, the multi-task learning problem from Equa-
tion 7 is equivalent to (Yuan and Lin, 2006; Puniyani
et al, 2010):
argmin
B
J(Xg, ~?g, ~yg) + ?||B||`1/`2 (8)
In this work we solve this optimization problem us-
ing an existing3 implementation of Block Coordi-
nate Descent (Schmidt et al, 2008) that solves re-
gression problems with a `1/`p penalty.
Acknowledgments
This work was supported by the Institute of Ed-
ucation Sciences, U.S. Department of Education,
3Source code: http://www.cs.ubc.ca/?murphyk/
Software/L1CRF/
through Grant R305A080628 to Carnegie Mellon
University. The opinions expressed are those of the
authors and do not necessarily represent the views of
the Institute or U.S. Department of Education. We
thank the educators, students, and LISTENers who
helped generate, collect, and analyze our data, and
the reviewers for their helpful comments. The first
author was partially supported by the Costa Rican
Ministry of Science and Technology (MICIT).
References
H. Ai, A. Raux, D. Bohus, M. Eskenazi, and D. Lit-
man. 2007. Comparing spoken dialog corpora col-
lected with recruited subjects versus real users. In
Proc. of the 8th SIGdial workshop on Discourse and
Dialogue.
M. Bacchiani, F. Beaufays, J. Schalkwyk, M. Schuster,
and B. Strope. 2008. Deploying GOOG-411: Early
lessons in data, measurement, and testing. In IEEE
International Conference on Acoustics, Speech and
Signal Processing, 2008. ICASSP 2008, pages 5260?
5263.
D. Bohus, A. Raux, T. Harris, M. Eskenazi, and A. Rud-
nicky. 2007. Olympus: an open-source framework
for conversational spoken language interface research.
In HLT-NAACL 2007 Workshop on Bridging the Gap:
Academic and Industrial Research in Dialog Technol-
ogy.
M. Danieli and E. Gerbino. 1995. Metrics for evaluat-
ing dialogue strategies in a spoken language system.
In Proceedings of the 1995 AAAI Spring Symposium
on Empirical Methods in Discourse Interpretation and
Generation, pages 34?39.
W. Eckert, E. Levin, and R. Pieraccini. 1998. Automatic
evaluation of spoken dialogue systems. TWLT13: For-
mal semantics and pragmatics of dialogue, pages 99?
110.
J. P. Gonza?lez-Brenes and J. Mostow. 2011. Classify-
ing dialogue in high-dimensional space. Transactions
of Speech and Language Processing; Special Issue on
Machine Learning for Robust and Adaptive Spoken
Dialogue Systems. In press.
J. P. Gonza?lez-Brenes, A. W. Black, and M. Eskenazi.
2009. Describing Spoken Dialogue Systems Differ-
ences. In International Workshop on Spoken Dialogue
Systems, Irsee, Germany. Springer?Verlat.
M. Hajdinjak and F. Mihelic. 2006. The PARADISE
evaluation framework: Issues and findings. Computa-
tional Linguistics, 32(2):263?272.
H. W. Hastie, R. Prasad, and M. Walker. 2002. Auto-
matic evaluation: Using a date dialogue act tagger for
16
user satisfaction and task completion prediction. In In
LREC 2002, pages 641?648.
E. Levin and R. Pieraccini. 2006. Value-based opti-
mal decision for dialog systems. In Spoken Language
Technology Workshop, 2006. IEEE, pages 198 ?201.
K. Lounici, A.B. Tsybakov, M. Pontil, and van de Geer.
2009. Taking advantage of sparsity in multi-task learn-
ing. In Conference on Learning Theory, volume 1050,
page 9, Montreal, Quebec.
S. Mo?ller, P. Smeele, H. Boland, and J. Krebber. 2007.
Evaluating spoken dialogue systems according to de-
facto standards: A case study. Computer Speech and
Language, 21(1):26 ? 53.
S. Mo?ller, K.P. Engelbrecht, and R. Schleicher. 2008.
Predicting the quality and usability of spoken dialogue
services. Speech Communication, 50(8-9):730?744.
G. Obozinski, B. Taskar, and M.I. Jordan. 2007. Multi-
task feature selection. In The Workshop of Struc-
tural Knowledge Transfer for Machine Learning in the
23rd International Conference on Machine Learning
(ICML), Pittsburgh, PA.
T. Paek. 2001. Empirical methods for evaluating dia-
log systems. In ACL 2001 Workshop on Evaluation
Methodologies for Language and Dialogue systems,
pages 3?10.
M.Y. Park and T. Hastie. 2007. L1-regularization path al-
gorithm for generalized linear models. Journal of the
Royal Statistical Society: Series B (Statistical Method-
ology), 69(19):659?677.
J. Polifroni, L. Hirschman, S. Seneff, and V. Zue. 1992.
Experiments in evaluating interactive spoken language
systems. In Proceedings of the workshop on Speech
and Natural Language, pages 28?33. Association for
Computational Linguistics.
K. Puniyani, S. Kim, and E.P. Xing. 2010. Multi-
population GWA mapping via multi-task regularized
regression. Bioinformatics, 26(12):208.
A. Raux, D. Bohus, B. Langner, A.W. Black, and M. Es-
kenazi. 2006. Doing research on a deployed spoken
dialogue system: one year of Let?s Go! experience. In
Ninth International Conference on Spoken Language
Processing. ISCA.
M. Schmidt, K. Murphy, G. Fung, and R. Rosales. 2008.
Structure learning in random fields for heart motion
abnormality detection. In Computer Vision and Pat-
tern Recognition, 2008. CVPR 2008. IEEE Conference
on, pages 1 ?8.
A. Schmitt, M. Scholz, W. Minker, J. Liscombe, and
D. Suendermann. 2010. Is it possible to predict task
completion in automated troubleshooters? In INTER-
SPEECH, pages 94?97.
D. Suendermann, J. Liscombe, R. Pieraccini, and
K. Evanini. 2010. ?How am I Doing??: A new frame-
work to effectively measure the performance of auto-
mated customer care contact centers. In A. Neustein,
editor, Advances in Speech Recognition: Mobile Envi-
ronments, Call Centers, and Clinics, pages 155?180.
Springer.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society.
Series B (Methodological), 58(1):267?288.
B.A. Turlach, W.N. Venables, and S.J. Wright. 2005.
Simultaneous variable selection. Technometrics,
47(3):349?363.
M. Walker, C. Kamm, and D. Litman. 2001. Towards de-
veloping general models of usability with PARADISE.
Natural Language Engineering, 6(3):363?377.
M. A. Walker, I. Langkilde-Geary, H. W. Hastie,
J. Wright, and A. Gorin. 2002. Automatically train-
ing a problematic dialogue predictor for a spoken di-
alogue system. Journal of Artificial Intelligence Re-
search, 16:293?319.
M. Yuan and Y. Lin. 2006. Model selection and esti-
mation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical
Methodology), 68(1):49?67.
A. Zellner. 1962. An efficient method of estimating
seemingly unrelated regressions and tests for aggrega-
tion bias. Journal of the American Statistical Associa-
tion, 57(298):pp. 348?368.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the Elastic Net. Journal of the Royal
Statistical Society, 67:301?320.
17
