  
Name Origin Recognition Using Maximum Entropy Model  
and Diverse Features 
Min Zhang1, Chengjie Sun2, Haizhou Li1, Aiti Aw1, Chew Lim Tan3, Xiaolong Wang2 
1Institute for Infocomm 
Research, Singapore 
{mzhang,hli,aaiti} 
@i2r.a-star.edu.sg 
2Harbin Institute of 
Technology, China 
{cjsun,wangxl} 
@insun.hit.edu.cn 
 
3National University of 
Singapore, Singapore 
tancl@comp.
nus.edu.sg 
Abstract 
Name origin recognition is to identify the 
source language of a personal or location 
name.  Some early work used either rule-
based or statistical methods with single 
knowledge source. In this paper, we cast the 
name origin recognition as a multi-class 
classification problem and approach the 
problem using Maximum Entropy method. 
In doing so, we investigate the use of differ-
ent features, including phonetic rules, n-
gram statistics and character position infor-
mation for name origin recognition. Ex-
periments on a publicly available personal 
name database show that the proposed ap-
proach achieves an overall accuracy of 
98.44% for names written in English and 
98.10% for names written in Chinese, which 
are significantly and consistently better than 
those in reported work.  
1 Introduction 
Many technical terms and proper names, such as 
personal, location and organization names, are 
translated from one language into another with 
approximate phonetic equivalents. The phonetic 
translation practice is referred to as transliteration; 
conversely, the process of recovering a word in its 
native language from a transliteration is called as 
back-transliteration (Zhang et al 2004; Knight 
and Graehl, 1998).  For example, English name 
?Smith? and ????  (Pinyin 1 : Shi-Mi-Si)? in 
                                                 
1 Hanyu Pinyin, or Pinyin in short, is the standard romaniza-
tion system of Chinese. In this paper, Pinyin is given next to 
Chinese form a pair of transliteration and back-
transliteration. In many natural language process-
ing tasks, such as machine translation and cross-
lingual information retrieval, automatic name 
transliteration has become an indispensable com-
ponent.  
Name origin refers to the source language of a 
name where it originates from. For example, the 
origin of the English name ?Smith? and its Chi-
nese transliteration ???? (Shi-Mi-Si)? is Eng-
lish, while both ?Tokyo? and ??? (Dong-Jing)? 
are of Japanese origin. Following are examples of 
different origins of a collection of English-Chinese 
transliterations. 
 
English: Richard-??? (Li-Cha-De) 
Hackensack-????(Ha-Ken-
Sa-Ke) 
Chinese: Wen JiaBao-???(Wen-Jia-
Bao) 
ShenZhen???(Shen-Zhen) 
Japanese: Matsumoto-?? (Song-Ben) 
Hokkaido-???(Bei-Hai-Dao) 
Korean: Roh MooHyun-???(Lu-Wu-
Xuan) 
Taejon-??(Da-Tian) 
Vietnamese: Phan Van Khai-???(Pan-
Wen-Kai) 
Hanoi-??(He-Nei) 
 
In the case of machine transliteration, the name 
origins dictate the way we re-write a foreign word. 
For example, given a name written in English or 
Chinese for which we do not have a translation in 
                                                                            
Chinese characters in round brackets for ease of reading. 
56
  
a English-Chinese dictionary, we first have to de-
cide whether the name is of Chinese, Japanese, 
Korean or some European/English origins. Then 
we follow the transliteration rules implied by the 
origin of the source name. Although all English 
personal names are rendered in 26 letters, they 
may come from different romanization systems. 
Each romanization system has its own rewriting 
rules. English name ?Smith? could be directly 
transliterated into Chinese as ????(Shi-Mi-Si)? 
since it follows the English phonetic rules, while 
the Chinese translation of Japanese name ?Koi-
zumi? becomes ???(Xiao-Quan)? following the 
Japanese phonetic rules. The name origins are 
equally important in back-transliteration practice. 
Li et al (2007) incorporated name origin recogni-
tion to improve the performance of personal name 
transliteration. Besides multilingual processing, 
the name origin also provides useful semantic in-
formation (regional and language information) for 
common NLP tasks, such as co-reference resolu-
tion and name entity recognition. 
Unfortunately, little attention has been given to 
name origin recognition (NOR) so far in the litera-
ture. In this paper, we are interested in two kinds 
of name origin recognition: the origin of names 
written in English (ENOR) and the origin of 
names written in Chinese (CNOR). For ENOR, 
the origins include English (Eng), Japanese (Jap), 
Chinese Mandarin Pinyin (Man) and Chinese Can-
tonese Jyutping (Can). For CNOR, they include 
three origins: Chinese (Chi, for both Mandarin and 
Cantonese), Japanese and English (refer to Latin-
scripted language). 
Unlike previous work (Qu and Grefenstette, 
2004; Li et al, 2006; Li et al, 2007) where NOR 
was formulated with a generative model, we re-
gard the NOR task as a classification problem. We 
further propose using a discriminative learning 
algorithm (Maximum Entropy model: MaxEnt) to 
solve the problem. To draw direct comparison, we 
conduct experiments on the same personal name 
corpora as that in the previous work by Li et al 
(2006). We show that the MaxEnt method effec-
tively incorporates diverse features and outper-
forms previous methods consistently across all test 
cases. 
The rest of the paper is organized as follows: in 
section 2, we review the previous work. Section 3 
elaborates our proposed approach and the features. 
Section 4 presents our experimental setup and re-
ports our experimental results. Finally, we con-
clude the work in section 5. 
2 Related Work 
Most of previous work focuses mainly on ENOR 
although same methods can be extended to CNOR. 
We notice that there are two informative clues that 
used in previous work in ENOR. One is the lexical 
structure of a romanization system, for example, 
Hanyu Pinyin, Mandarin Wade-Giles, Japanese 
Hepbrun or Korean Yale, each has a finite set of 
syllable inventory (Li et al, 2006). Another is the 
phonetic and phonotactic structure of a language, 
such as phonetic composition, syllable structure. 
For example, English has unique consonant 
clusters such as /str/ and /ks/ which Chinese, 
Japanese and Korean (CJK) do not have. 
Considering the NOR solutions by the use of these 
two clues, we can roughly group them into two 
categories: rule-based methods (for solutions 
based on lexical structures) and statistical methods 
(for solutions based on phonotactic structures). 
Rule-based Method  
Kuo and Yang (2004) proposed using a rule-
based method to recognize different romanization 
system for Chinese only. The left-to-right longest 
match-based lexical segmentation was used to 
parse a test word. The romanization system is con-
firmed if it gives rise to a successful parse of the 
test word. This kind of approach (Qu and Grefen-
stette, 2004) is suitable for romanization systems 
that have a finite set of discriminative syllable in-
ventory, such as Pinyin for Chinese Mandarin. For 
the general tasks of identifying the language origin 
and romanization system, rule based approach 
sounds less attractive because not all languages 
have a finite set of discriminative syllable inven-
tory. 
Statistical Method 
1) N-gram Sum Method (SUM): Qu and Gre-
fenstette (2004) proposed a NOR identifier using a 
trigram language model (Cavnar and Trenkle, 
1994) to distinguish personal names of three lan-
guage origins, namely Chinese, Japanese and Eng-
lish. In their work, the training set includes 11,416 
Chinese name entries, 83,295 Japanese name en-
tries and 88,000 English name entries. However, 
the trigram is defined as the joint probabil-
57
  
ity 1 2( )i i ip c c c? ? for 3-character 1 2i i ic c c? ?  rather than 
the commonly used conditional probabil-
ity 1 2( | )i i ip c c c? ? . Therefore, the so-called trigram 
in Qu and Grefenstette (2004) is basically a sub-
string unigram probability, which we refer to as 
the n-gram (n-character) sum model (SUM) in this 
paper. Suppose that we have the unigram count 
1 2( )i i iC c c c? ? for character substring 1 2i i ic c c? ? , the 
unigram is then computed as: 
1 2
1 2
1 2
1 2,
( )
( )
( )
i i i
i i i
i i i
i i ii c c c
C c c c
p c c c
C c c c
? ?
? ?
? ?
? ?
= ?           (1) 
which is the count of character substring 1 2i i ic c c? ?  
normalized by the sum of all 3-character string 
counts in the name list for the language of interest.  
For origin recognition of Japanese names, this 
method works well with an accuracy of 92%. 
However, for English and Chinese, the results are 
far behind with a reported accuracy of 87% and 
70% respectively. 
2) N-gram Perplexity Method (PP): Li et al 
(2006) proposed using n-gram character perplexity 
cPP  to identify the origin of a Latin-scripted name. 
Using bigram, the cPP is defined as: 
1
1 log ( | )
2
Nc
i i 1ic
p c cN
cPP
?
=
? ?
=   (2) 
where cN is the total number of characters in the 
test name, ic is the i
th character in the test name. 
1( | )i ip c c ? is the bigram probability which is 
learned from each name list respectively. As a 
function of model, cPP  measures how good the 
model matches the test data. Therefore, cPP can be 
used to measure how good a test name matches a 
training set. A test name is identified to belong to 
a language if the language model gives rise to the 
minimum perplexity. Li et al (2006) shown that 
the PP method gives much better performance 
than the SUM method. This may be due to the fact 
that the PP measures the normalized conditional 
probability rather than the sum of joint probability. 
Thus, the PP method has a clearer mathematical 
interpretation than the SUM method. 
The statistical methods attempt to overcome the 
shortcoming of rule-based method, but they suffer 
from data sparseness, especially when dealing 
with a large character set, such as in Chinese (our 
experiments will demonstrate this point empiri-
cally). In this paper, we propose using Maximum 
Entropy (MaxEnt) model as a general framework 
for both ENOR and CNOR. We explore and inte-
grate multiple features into the discriminative clas-
sifier and use a common dataset for benchmarking. 
Experimental results show that the MaxEnt model 
effectively incorporates diverse features to demon-
strate competitive performance.   
3 MaxEnt Model and Features 
3.1 MaxEnt Model for NOR 
The principle of maximum entropy (MaxEnt) 
model is that given a collection of facts, choose a 
model consistent with all the facts, but otherwise 
as uniform as possible (Berger et al, 1996). Max-
Ent model is known to easily combine diverse fea-
tures. For this reason, it has been widely adopted 
in many natural language processing tasks. The 
MaxEnt model is defined as: 
( , )
1
1
( | ) j i
K
f c x
i j
j
p c x
Z
?
=
= ?           (3) 
      ( , )
1 1 1
( | ) j i
KN N
f c x
i j
i i j
Z p c x ?
= = =
= =? ??          (4) 
where ic is the outcome label, x is the given obser-
vation, also referred to as an instance. Z is a nor-
malization factor. N  is the number of outcome 
labels, the number of language origins  in our case. 
1 2, , , Kf f fL are feature functions and 
1 2, , , K? ? ?L are the model parameters. Each pa-
rameter corresponds to exactly one feature and can 
be viewed as a ?weight? for the corresponding fea-
ture.  
In the NOR task, c is the name origin label; x is 
a personal name, if is a feature function. All fea-
tures used in the MaxEnt model in this paper are 
binary. For example: 
 
1,    " "& (" ")
( , )
0,  j
if c Eng x contains str
f c x
otherwise
=?
= ??
 
In our implementation, we used Zhang?s maxi-
mum entropy package2. 
3.2 Features 
Let us use English name ?Smith? to illustrate the 
features that we define. All characters in a name 
                                                 
2 http://homepages.inf.ed.ac.uk/s0450736/maxent.html 
58
  
are first converted into upper case for ENOR be-
fore feature extraction. 
N-gram Features: N-gram features are de-
signed to capture both phonetic and orthographic 
structure information for ENOR and orthographic 
information only for CNOR. This is motivated by 
the facts that: 1) names written in English but from 
non-English origins follow different phonetic rules 
from the English one; they also manifest different 
character usage in orthographic form; 2) names 
written in Chinese follows the same pronunciation 
rules (Pinyin), but the usage of Chinese characters 
is distinguishable between different language ori-
gins as reported in Table 2 of (Li et al, 2007).  
The N-gram related features include: 
1) FUni: character unigram <S, M, I, T, H> 
2) FBi: character bigram <SM, MI, IT, TH> 
3) FTri: character trigram <SMI, MIT, ITH > 
Position Specific n-gram Features: We in-
clude position information into the n-gram fea-
tures. This is mainly to differentiate surname from 
given name in recognizing the origin of CJK per-
sonal names written in Chinese. For example, the 
position specific n-gram features of a Chinese 
name ????(Wen-Jia-Bao)? are as follows: 
1) FPUni: position specific unigram  
<0?(Wen), 1?(Jia), 2?(Bao)> 
2) FPBi: position specific bigram  
<0??(Wen-Jia), 1??(Jia-Bao)> 
3) FPTri: position specific trigram  
<0???(Wen-Jia-Bao)> 
Phonetic Rule-based Features: These features 
are inspired by the rule-based methods (Kuo and 
Yang, 2004; Qu and Grefenstette, 2004) that check 
whether an English name is a sequence of sylla-
bles of CJK languages in ENOR task. We use the 
following two features in ENOR task as well. 
1) FMan: a Boolean feature to indicate 
whether a name is a sequence of Chinese 
Mandarin Pinyin.   
2) FCan: a Boolean feature to indicate whether 
a name is a sequence of Cantonese Jyutping. 
Other Features:  
1) FLen: the number of Chinese characters in a 
given name. This feature is for CNOR only.  
The numbers of Chinese characters in per-
sonal names vary with their origins. For ex-
ample, Chinese and Korean names usually 
consist of 2 to 3 Chinese characters while 
Japanese names can have up to 4 or 5 Chi-
nese characters 
2) FFre: the frequency of n-gram in a given 
name. This feature is for ENOR only. In 
CJK names, some consonants or vowels 
usually repeat in a name as the result of the 
regular syllable structure. For example, in 
the Chinese name ?Zhang Wanxiang?, the 
bigram ?an? appears three times 
Please note that the trigram and position spe-
cific trigram features are not used in CNOR due to 
anticipated data sparseness in CNOR3.  
4 Experiments 
We conduct the experiments to validate the effec-
tiveness of the proposed method for both ENOR 
and CNOR tasks. 
4.1 Experimental Setting 
 
Origin #  entries Romanization System 
Eng4 88,799 English 
Man5 115,879 Pinyin 
Can 115,739 Jyutping 
Jap6 123,239 Hepburn 
 
Table 1: DE: Latin-scripted personal name corpus for 
ENOR 
 
 
Origin #  entries 
Eng7 37,644 
Chi8 29,795 
Jap9 33,897 
 
Table 2: DC: Personal name corpus written in Chinese 
characters for CNOR 
 
                                                 
3 In the test set of CNOR, 1080 out of 2980 names of Chinese 
origin do not consist of any bigrams learnt from training data, 
while 2888 out of 2980 names do not consist of any learnt 
trigrams. This is not surprising as most of Chinese names only 
have two or three Chinese characters and in our open testing, 
the train set is exclusive of all entries in the test set.  
4 http://www.census.gov/genealogy/names/ 
5 http://technology.chtsai.org/namelist/  
6 http://www.csse.monash.edu.au/~jwb/enamdict_doc.html 
7 Xinhua News Agency (1992)  
8 http://www.ldc.upenn.edu LDC2005T34 
9 www.cjk.org 
59
  
Datasets: We prepare two data sets which are col-
lected from publicly accessible sources: DE and DC 
for the ENOR and CNOR experiment respectively. 
DE is the one used in (Li et al, 2006), consisting of 
personal names of Japanese (Jap), Chinese (Man), 
Cantonese (Can) and English (Eng) origins. DC 
consists of personal names of Japanese (Jap), Chi-
nese (Chi, including both Mandarin and Canton-
ese) and English (Eng) origins. Table 1 and Table 
2 list their details. In the experiments, 90% of en-
tries in Table 1 (DE) and Table 2 (DC) are ran-
domly selected for training and the remaining 10% 
are kept for testing for each language origin. Col-
umns 2 and 3 in Tables 7 and 8 list the numbers of 
entries in the training and test sets.  
 
Evaluation Methods: Accuracy is usually used to 
evaluate the recognition performance (Qu and 
Gregory, 2004; Li et al, 2006; Li et al, 2007). 
However, as we know, the individual accuracy 
used before only reflects the performance of recall 
and does not give a whole picture about a multi-
class classification task. Instead, we use precision 
(P), recall (R) and F-measure (F) to evaluate the 
performance of each origin. In addition, an overall 
accuracy (Acc) is also given to describe the whole 
performance. The P, R, F and Acc are calculated 
as following: 
 
#        
#          
correctly recognized entries of the given origin
P
entries recognized as the given origin by the system
=
 
 
#        
#      
correctly recognized entries of the given origin
R
entries of the given origin
=
 
 
2PR
F
P R
=
+
     #     
#   
all correctly recognized entries
Acc
all entries
=
 
4.2 Experimental Results and Analysis 
Table 3 reports the experimental results of ENOR. 
It shows that the MaxEnt approach achieves the 
best result of 98.44% in overall accuracy when 
combining all the diverse features as listed in Sub-
section 3.2. Table 3 also measures the contribu-
tions of different features for ENOR by gradually 
incorporating the feature set. It shows that:  
1) All individual features are useful since the 
performance increases consistently when 
more features are being introduced. 
2) Bigram feature presents the most informa-
tive feature that gives rise to the highest 
performance gain, while the trigram feature  
further boosts performance too. 
3) MaxEnt method can integrate the advan-
tages of previous rule-based and statistical 
methods and easily integrate other features. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
    
R(
%)
 
F 
Ac
c(%
) 
Eng 91.40 80.76 85.75
Man 83.05 81.90 82.47
Can 81.13 82.76 81.94
FUni 
Jap 87.31 94.11 90.58
85.29
Eng 97.54 91.10 94.21
Man 97.51 98.10 97.81
Can 97.68 98.05 97.86
+FBi 
Jap 94.62 98.24 96.39
96.72
Eng 97.71 93.79 95.71
Man 98.94 99.37 99.16
Can 99.12 99.19 99.15
+FTri 
Jap 96.19 98.52 97.34
97.97
Eng 97.53 94.64 96.06
Man 99.21 99.43 99.32
Can 99.41 99.24 99.33
+FPUni 
Jap 96.48 98.49 97.47
98.16
Eng 97.68 94.98 96.31
Man 99.32 99.50 99.41
Can 99.53 99.34 99.44
+FPBi 
Jap 96.59 98.52 97.55
98.28
Eng 97.62 94.97 96.27
Man 99.34 99.58 99.46
Can 99.63 99.37 99.50
+FPTri 
Jap 96.61 98.45 97.52
98.30
Eng 97.74 95.06 96.38
Man 99.37 99.59 99.48
Can 99.61 99.41 99.51
+FFre 
Jap 96.66 98.56 97.60
98.35
Eng 97.82 95.11 96.45
Man 99.52 99.68 99.60
Can 99.71 99.59 99.65
 + FMan 
+ FCan 
Jap 96.69 98.59 97.63
98.44
 
Table 3: Contribution of each feature for ENOR 
 
 
60
  
Features Eng Jap Man Can 
FMan -0.357 0.069 0.072 -0.709 
FCan -0.424 -0.062 -0.775 0.066 
 
Table 4: Features weights in ENOR task. 
 
F
ea
tu
re
 
O
ri
gi
n 
P(
%
) 
R(
%
) 
F 
   A
cc(
%
) 
Eng 97.89 98.43 98.16
Chi 95.80 95.03 95.42FUni 
Jap 96.96 97.05 97.00
96.97 
Eng 96.99 98.27 97.63
Chi 96.86 92.11 94.43+FBi 
Jap 95.04 97.73 96.36
96.28 
Eng 97.35 98.38 97.86
Chi 97.29 95.00 96.13+FLen 
Jap 96.78 97.64 97.21
97.14 
Eng 97.74 98.65 98.19
Chi 97.65 96.34 96.99+FPUni 
Jap 97.91 98.05 97.98
97.77 
Eng 97.50 98.43 97.96
Chi 97.61 96.04 96.82+FPBi 
Jap 97.59 97.94 97.76
97.56 
Eng 98.08 99.04 98.56
Chi 97.57 96.88 97.22
FUni 
+FLen 
+ 
FPUni Jap 98.58 98.11 98.34
98.10 
 
Table 5: Contribution of each feature for CNOR 
 
Table 4 reports the feature weights of two fea-
tures ?FMan? and ?FCan? with regard to different 
origins in ENOR task. It shows that ?FCan? has 
positive weight only for origin ?Can? while 
?FMan? has positive weights for both origins 
?Man? and ?Jap?, although the weight for ?Man? 
is higher. This agrees with our observation that the 
two features favor origins ?Man? or ?Can?. The 
feature weights also reflect the fact that some 
Japanese names can be successfully parsed by the 
Chinese Mandarin Pinyin system due to their simi-
lar syllable structure. For example, the Japanese 
name ?Tanaka Miho? is also a sequence of Chi-
nese Pinyin: ?Ta-na-ka Mi-ho?.  
Table 5 reports the contributions of different 
features in CNOR task by gradually incorporating 
the feature set. It shows that:  
1) Unigram features are the most informative 
2) Bigram features degrade performance. This 
is largely due to the data sparseness prob-
lem as discussed in Section 3.2.   
3) FLen is also useful that confirms our intui-
tion about name length. 
Finally the combination of the above three use-
ful features achieves the best performance of 
98.10% in overall accuracy for CNOR as in the 
last row of Table 5. 
In Tables 3 and 5, the effectiveness of each fea-
ture may be affected by the order in which the fea-
tures are incorporated, i.e., the features that are 
added at a later stage may be underestimated. 
Thus, we conduct another experiment using "all-
but-one" strategy to further examine the effective-
ness of each kind of features. Each time, one type 
of the n-gram (n=1, 2, 3) features (including or-
thographic n-gram, position-specific and n-gram 
frequency features) is removed from the whole 
feature set. The results are shown in Table 6. 
 
F
ea
tu
re
s 
O
ri
gi
n 
P(
%)
 
R(
%)
 
F 
Ac
c(%
) 
Eng 97.81 95.01 96.39
Man 99.41 99.58 99.49
Can 99.53 99.48 99.50
w/o 
Uni-
gram 
Jap 96.63 98.52 97.57
98.34
Eng 97.34 95.17 96.24
Man 99.30 99.48 99.39
Can 99.54 99.33 99.43
w/o Bi-
gram 
Jap 96.73 98.32 97.52
98.26
Eng 97.57 94.10 95.80
Man 98.98 99.23 99.10
Can 99.20 99.08 99.14
w/o 
Tri-
gram 
Jap 96.06 98.42 97.23
97.94
 
Table 6: Effect of n-gram feature for ENOR 
 
Table 6 reveals that removing trigram features 
affects the performance most. This suggests that 
trigram features are much more effective for 
ENOR than other two types of features. It also 
shows that trigram features in ENOR does not suf-
fer from the data sparseness issue. 
As observed in Table 5, in CNOR task, 93.96% 
61
  
accuracy is obtained when removing unigram fea-
tures, which is much lower than 98.10% when bi-
gram features are removed. This suggests that uni-
gram features are very useful in CNOR, which is 
mainly due to the data sparseness problem that 
bigram features may have encountered. 
4.3 Model Complexity and Data Sparseness 
Table 7 (ENOR) and Table 8 (CNOR) compare 
our MaxEnt model with the SUM model (Qu and 
Gregory, 2004) and the PP model (Li et al, 2006). 
All the experiments are conducted on the same 
data sets as described in section 4.1. Tables 7 and 
8 show that the proposed MaxEnt model outper-
forms other models. The results are statistically 
significant ( 2? test with p<0.01) and consistent 
across all tests. 
Model Complexity: 
We look into the complexity of the models and 
their effects. Tables 7 and 8 summarize the overall 
accuracy of three models. Table 9 reports the 
numbers of parameters in each of the models. We 
are especially interested in a comparison between 
the MaxEnt and PP models because their perform-
ance is close.  We observe that, using trigram fea-
tures, the MaxEnt model has many more parame-
ters than the PP model does. Therefore, it is not 
surprising if the MaxEnt model outperforms when 
more training data are available. However, the ex-
periment results also show that the MaxEnt model 
consistently outperforms the PP model even with 
the same size of training data. This is largely at-
tributed to the fact that MaxEnt incorporates more 
robust features than the PP model does, such as 
rule-based, length of names features.  
One also notices that PP clearly outperforms 
SUM by using the same number of parameters in 
ENOR and shows comparable performance in 
CNOR tasks. Note that SUM and PP are different 
in two areas: one is the PP model employs word 
length normalization while SUM doesn?t; another 
that the PP model uses n-gram conditional prob-
ability while SUM uses n-character joint probabil-
ity. We believe that the improved performance of 
PP model can be attributed to the effect of usage 
of conditional probability, rather than length nor-
malization since length normalization does not 
change the order of probabilities. 
Data Sparesness: 
We understand that we can only assess the ef-
fectiveness of a feature when sufficient statistics is 
available. In CNOR (see Table 8), we note that the 
Chinese transliterations of English origin use only 
377 Chinese characters, so data sparseness is not a 
big issue. Therefore, bigram SUM and bigram PP 
methods easily achieve good performance for Eng-
lish origin. However, for Japanese origin (repre-
sented by 1413 Chinese characters) and Chinese 
origin (represented by 2319 Chinese characters), 
the data sparseness becomes acute and causes per-
formance degradation in SUM and PP models. We 
are glad to find that MaxEnt still maintains a good 
performance benefiting from other robust features. 
Table 10 compares the overall accuracy of the 
three methods using unigram and bigram features 
in CNOR task, respectively. It shows that the 
MaxEnt method achieves best performance. An-
other interesting finding is that unigram features 
perform better than bigram features for PP and  
MaxEnt models, which shows that  data sparseness 
remains an issue even for MaxEnt model.  
5 Conclusion 
We propose using MaxEnt model to explore di-
verse features for name origin recognition. Ex-
periment results show that our method is more ef-
fective than previously reported methods. Our 
contributions include: 
1) Cast the name origin recognition problem as 
a multi-class classification task and propose 
a MaxEnt solution to it; 
2) Explore and integrate diverse features for 
name origin recognition and propose the 
most effective feature sets for ENOR and 
for CNOR 
In the future, we hope to integrate our name 
origin recognition method with a machine translit-
eration engine to further improve transliteration 
performance. We also hope to study the issue of 
name origin recognition in context of sentence and 
use contextual words as additional features. 
References 
Adam L. Berger, Stephen A. Della Pietra and Vincent J. 
Della Pietra. 1996. A Maximum Entropy Approach 
to Natural Language Processing. Computational Lin-
guistics. 22(1):39?71. 
William B. Cavnar and John M. Trenkle. 1994. Ngram 
based text categorization. In 3rd Annual Symposium 
62
  
on Document Analysis and Information Retrieval, 
275?282. 
Kevin Knight and Jonathan Graehl. 1998. Machine 
Transliteration. Computational Linguistics. 24(4), 
599-612. 
Jin-Shea Kuo and Ying-Kuei Yan. 2004. Generating 
Paired Transliterated-Cognates Using Multiple Pro-
nunciation Characteristics from Web Corpora. PA-
CLIC 18, December 8th-10th, Waseda University, 
Tokyo, Japan, 275?282. 
Haizhou Li, Shuanhu Bai and Jin-Shea Kuo. 2006. 
Transliteration. Advances in Chinese Spoken Lan-
guage Processing. World Scientific Publishing Com-
pany, USA, 341?364. 
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo and Minghui 
Dong. 2007. Semantic Transliteration of Personal 
Names. ACL-2007. 120?127. 
Xinhua News Agency. 1992. Chinese Transliteration of 
Foreign Personal Names. The Commercial Press  
Yan Qu and Gregory Grefenstette. 2004. Finding ideo-
graphic representations of Japanese names written in 
Latin script via language identification and corpus 
validation. ACL-2004. 183?190. 
Min Zhang, Jian Su and Haizhou Li. 2004. Direct Or-
thographical Mapping for Machine Translation. 
COLING-2004. 716-722. 
 
Trigram SUM Trigram PP MaxEnt Origin # training 
entries 
# test 
entries P (%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 79,920 8,879 94.66 72.50 82.11 95.84 94.72 95.28 97.82 95.11 96.45
Man 104,291 11,588 86.79 94.87 90.65 98.99 98.33 98.66 99.52 99.68 99.60
Can 104,165 11,574 90.03 93.87 91.91 96.17 99.67 97.89 99.71 99.59 99.65
Jap 110,951 12,324 89.17 92.84 90.96 98.20 96.29 97.24 96.69 98.59 97.63
Overall Acc (%) 89.57 97.39 98.44 
Table 7: Benchmarking different methods in ENOR task 
Bigram SUM  Bigram PP  MaxEnt Origin # training 
entries 
# test 
entries P(%) R(%) F P(%) R(%) F P(%) R(%) F 
Eng 37,644 3,765 95.94 98.65 97.28 97.58 97.61 97.60 98.08 99.04 98.56 
Chi 29,795 2,980 96.26 87.35 91.59 95.10 87.35 91.06 97.57 96.88 97.22 
Jap 33,897 3,390 93.01 97.67 95.28 90.94 97.43 94.07 98.58 98.11 98.34 
Overall Acc (%) 95.00 94.53 98.10 
Table 8: Benchmarking different methods in CNOR task 
# of parameters for ENOR # of parameters for CNOR 
Methods 
Trigram Unigram Bigram 
MaxEnt  124,692 13,496  182,116 
PP 16,851 4,045 86,490 
SUM  16,851 4,045 86,490 
 
Table 9: Numbers of parameters used in different methods 
 
 SUM PP MaxEnt 
Unigram Features 90.55 97.09 98.10 
Bigram Features 95.00 94.53 97.56 
 
Table 10: Overall accuracy using unigram and bigram features in CNOR task 
63
