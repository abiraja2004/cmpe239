Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 229?237,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Language Identification: The Long and the Short of the Matter
Timothy Baldwin and Marco Lui
Dept of Computer Science and Software Engineering
University of Melbourne, VIC 3010 Australia
tb@ldwin.net, saffsd@gmail.com
Abstract
Language identification is the task of identify-
ing the language a given document is written
in. This paper describes a detailed examina-
tion of what models perform best under dif-
ferent conditions, based on experiments across
three separate datasets and a range of tokeni-
sation strategies. We demonstrate that the task
becomes increasingly difficult as we increase
the number of languages, reduce the amount
of training data and reduce the length of docu-
ments. We also show that it is possible to per-
form language identification without having to
perform explicit character encoding detection.
1 Introduction
With the growth of the worldwide web, ever-
increasing numbers of documents have become
available, in more and more languages. This growth
has been a double-edged sword, however, in that
content in a given language has become more preva-
lent but increasingly hard to find, due to the web?s
sheer size and diversity of content. While the ma-
jority of (X)HTML documents declare their charac-
ter encoding, only a tiny minority specify what lan-
guage they are written in, despite support for lan-
guage declaration existing in the various (X)HTML
standards.1 Additionally, a single encoding can gen-
erally be used to render a large number of languages
such that the document encoding at best filters out
a subset of languages which are incompatible with
the given encoding, rather than disambiguates the
source language. Given this, the need for automatic
means to determine the source language of web doc-
1http://dev.opera.com/articles/view/
mama-head-structure/
uments is crucial for web aggregators of various
types.
There is widespread misconception of language
identification being a ?solved task?, generally as
a result of isolated experiments over homogeneous
datasets with small numbers of languages (Hughes
et al, 2006; Xia et al, 2009). Part of the motivation
for this paper is to draw attention to the fact that, as
a field, we are still a long way off perfect language
identification of web documents, as evaluated under
realistic conditions.
In this paper we describe experiments on lan-
guage identification of web documents, focusing on
the broad question of what combination of tokenisa-
tion strategy and classification model achieves the
best overall performance. We additionally evalu-
ate the impact of the volume of training data and
the test document length on the accuracy of lan-
guage identification, and investigate the interaction
between character encoding detection and language
identification.
One assumption we make in this research, follow-
ing standard assumptions made in the field, is that all
documents are monolingual. This is clearly an un-
realistic assumption when dealing with general web
documents (Hughes et al, 2006), and we plan to re-
turn to investigate language identification over mul-
tilingual documents in future work.
Our contributions in this paper are: the demon-
stration that language identification is: (a) trivial
over datasets with smaller numbers of languages
and approximately even amounts of training data per
language, but (b) considerably harder over datasets
with larger numbers of languages with more skew
in the amount of training data per language; byte-
based tokenisation without character encoding de-
tection is superior to codepoint-based tokenisation
229
with character encoding detection; and simple co-
sine similarity-based nearest neighbour classifica-
tion is equal to or better than models including sup-
port vector machines and naive Bayes over the lan-
guage identification task. We also develop datasets
to facilitate standardised evaluation of language
identification.
2 Background Research
Language identification was arguably established as
a task by Gold (1967), who construed it as a closed
class problem: given data in each of a predefined set
of possible languages, human subjects were asked
to classify the language of a given test document. It
wasn?t until the 1990s, however, that the task was
popularised as a text categorisation task.
The text categorisation approach to language
identification applies a standard supervised classi-
fication framework to the task. Perhaps the best-
known such model is that of Cavnar and Tren-
kle (1994), as popularised in the textcat tool.2
The method uses a per-language character frequency
model, and classifies documents via their relative
?out of place? distance from each language (see
Section 5.1). Variants on this basic method in-
clude Bayesian models for character sequence pre-
diction (Dunning, 1994), dot products of word fre-
quency vectors (Darnashek, 1995) and information-
theoretic measures of document similarity (Aslam
and Frost, 2003; Martins and Silva, 2005). More
recently, support vector machines (SVMs) and ker-
nel methods have been applied to the task of lan-
guage identification task with success (Teytaud and
Jalam, 2001; Lodhi et al, 2002; Kruengkrai et al,
2005), and Markov logic has been used for joint in-
ferencing in contexts where there are multiple evi-
dence sources (Xia et al, 2009).
Language identification has also been carried out
via linguistically motivated models. Johnson (1993)
used a list of stop words from different languages to
identify the language of a given document, choos-
ing the language with the highest stop word over-
lap with the document. Grefenstette (1995) used
word and part of speech (POS) correlation to de-
termine if two text samples were from the same
or different languages. Giguet (1995) developed a
2http://www.let.rug.nl/vannoord/TextCat/
cross-language tokenisation model and used it to
identify the language of a given document based
on its tokenisation similarity with training data.
Dueire Lins and Gonc?alves (2004) considered the
use of syntactically-derived closed grammatical-
class models, matching syntactic structure rather
than words or character sequences.
The observant reader will have noticed that some
of the above approaches make use of notions such
as ?word?, typically based on the naive assumption
that the language uses white space to delimit words.
These approaches are appropriate in contexts where
there is a guarantee of a document being in one of
a select set of languages where words are space-
delimited, or where manual segmentation has been
performed (e.g. interlinear glossed text). However,
we are interested in language identification of web
documents, which can be in any language, includ-
ing languages that do not overtly mark word bound-
aries, such as Japanese, Chinese and Thai; while
relatively few languages fall into this categories,
they are among the most populous web languages
and therefore an important consideration. There-
fore, approaches that assume a language is space-
delimited are clearly not suitable for our purposes.
Equally, approaches which make assumptions about
the availability of particular resources for each lan-
guage to be identified (e.g. POS taggers, or the ex-
istence of precompiled stop word lists) cannot be
used.
Language identification has been applied in a
number of contexts, the most immediate applica-
tion being in multilingual text retrieval, where re-
trieval results are generally superior if the language
of the query is known, and the search is restricted
to only those documents predicted to be in that lan-
guage (McNamee and Mayfield, 2004). It can also
be used to ?word spot? foreign language terms in
multilingual documents, e.g. to improve parsing per-
formance (Alex et al, 2007), or for linguistic corpus
creation purposes (Baldwin et al, 2006; Xia et al,
2009; Xia and Lewis, 2009).
3 Datasets
In the experiments reported in this paper, we em-
ploy three novel datasets, with differing properties
relevant to language identification research:
230
Corpus Documents Languages Encodings Document Length (bytes)
EUROGOV 1500 10 1 17460.5?39353.4
TCL 3174 60 12 2623.2?3751.9
WIKIPEDIA 4963 67 1 1480.8?4063.9
Table 1: Summary of the three language identification datasets
Figure 1: Distribution of languages in the three datasets
(vector of languages vs. the proportion of documents in
that language)
EUROGOV: longer documents, all in a single en-
coding, spread evenly across a relatively small num-
ber (10) of Western European languages; this dataset
is comparable to the datasets conventionally used in
language identification research. As the name would
suggest, the documents were sourced from the Euro-
GOV document collection, as used in the 2005 Web-
CLEF task.
TCL: a larger number of languages (60) across a
wider range of language families, with shorter docu-
ments and a range of character encodings (12). The
collection was manually sourced by the Thai Com-
putational Linguistics Laboratory (TCL) in 2005
from online news sources.
WIKIPEDIA: a slightly larger number of lan-
guages again (67), a single encoding, and shorter
documents; the distribution of languages is intended
to approximate that of the actual web. This col-
lection was automatically constructed by taking the
dumps of all versions of Wikipedia with 1000 or
more documents in non-constructed languages, and
randomly selecting documents from them in a bias-
preserving manner (i.e. preserving the document
distribution in the full collection); this is intended to
represent the document language bias observed on
the web. All three corpora are available on request.
We outline the characteristics of the three datasets
in Table 1. We further detail the language distri-
bution in Figure 1, using a constant vector of lan-
guages for all three datasets, based on the order of
languages in the WIKIPEDIA dataset (in descending
order of documents per language). Of note are the
contrasting language distributions between the three
datasets, in terms of both the languages represented
and the relative skew of documents per language. In
the following sections, we provide details of the cor-
pus compilation and document sampling method for
each dataset.
4 Document Representation
As we are interested in performing language iden-
tification over arbitrary web documents, we re-
quire a language-neutral document representation
which does not make artificial assumptions about the
source language of the document. Separately, there
is the question of whether it is necessary to deter-
mine the character encoding of the document in or-
der to extract out character sequences, or whether
the raw byte stream is sufficient. To explore this
question, we experiment with two document repre-
sentations: (1) byte n-grams, and (2) codepoint n-
grams. In both cases, a document is represented as a
feature vector of token counts.
Byte n-grams can be extracted directly without
explicit encoding detection. Codepoint n-grams, on
the other hand, require that we know the character
encoding of the document in order to perform to-
kenisation. Additionally, they should be based on a
common encoding to prevent: (a) over-fragmenting
the feature space (e.g. ending up with discrete fea-
ture spaces for euc-jp, s-jis and utf-8 in
the case of Japanese); and (b) spurious matches be-
tween encodings (e.g. Japanese hiragana and Ko-
rean hangul mapping onto the same codepoint in
euc-jp and euc-kr, respectively). We use uni-
231
code as the common encoding for all documents.
In practice, character encoding detection is an is-
sue only for TCL, as the other two datasets are in
a single encoding. Where a character encoding was
provided for a document in TCL and it was possi-
ble to transcode the document to unicode based on
that encoding, we used the encoding information. In
cases where a unique encoding was not provided,
we used an encoding detection library based on the
Mozilla browser.3 Having disambiguated the encod-
ing for each document, we transcoded it into uni-
code.
5 Models
In our experiments we use a number of different
language identification models, as outlined below.
We first describe the nearest-neighbour and nearest-
prototype models, and a selection of distance and
similarity metrics combined with each. We then
present three standalone text categorisation models.
5.1 Nearest-Neighbour and Nearest-Prototype
Models
The 1-nearest-neighbour (1NN) model is a common
classification technique, whereby a test document
D is classified based on the language of the clos-
est training document Di (with language l(Di)), as
determined by a given distance or similarity metric.
In nearest-neighbour models, each training doc-
ument is represented as a single instance, mean-
ing that the computational cost of classifying a test
document is proportional to the number of training
documents. A related model which aims to reduce
this cost is nearest-prototype (AM), where each lan-
guage is represented as a single instance, by merging
all of the training instances for that language into a
single centroid via the arithmetic mean.
For both nearest-neighbour and nearest-prototype
methods, we experimented with three similarity and
distance measures in this research:
Cosine similarity (COS): the cosine of the angle
between two feature vectors, as measured by the dot
product of the two vectors, normalised to unit length.
Skew divergence (SKEW): a variant of Kullback-
Leibler divergence, whereby the second distribution
3http://chardet.feedparser.org/
(y) is smoothed by linear interpolation with the first
(x) using a smoothing factor ? (Lee, 2001):
s?(x, y) = D(x || ?y + (1? ?)x)
where:
D(x || y) =
?
i
xi(log2 xi ? log2 yi)
In all our experiments, we set ? to 0.99.
Out-of-place (OOP): a ranklist-based distance
metric, where the distance between two documents
is calculated as (Cavnar and Trenkle, 1994):
oop(Dx, Dy) =
?
t?Dx?Dy
abs(RDx(t)?RDy(t))
RD(t) is the rank of term t in document D, based
on the descending order of frequency in document
D; terms not occurring in document D are conven-
tionally given the rank 1 + maxi RD(ti).
5.2 Naive Bayes (NB)
Naive Bayes is a popular text classification model,
due to it being lightweight, robust and easy to up-
date. The language of test document D is predicted
by:
l?(D) = arg max
li?L
P (li)
|V |
?
j=1
P (tj |li)ND,tj
ND,tj !
where L is the set of languages in the training data,
ND,tj is the frequency of the jth term in D, V is the
set of all terms, and:
P (t|li) =
1 +
?|D |
k=1 Nk,tP (li|Dk)
|V |+ ?|V |j=1
?|D |
k=1 Nk,tjP (li|Dk)
In this research, we use the rainbow imple-
mentation of multinominal naive Bayes (McCallum,
1996).
5.3 Support Vector Machines (SVM)
Support vector machines (SVMs) are one of the
most popular methods for text classification, largely
because they can automatically weight large num-
bers of features, capturing feature interactions in the
process (Joachims, 1998; Manning et al, 2008). The
basic principle underlying SVMs is to maximize the
232
margin between training instances and the calculated
decision boundary based on structural risk minimi-
sation (Vapnik, 1995).
In this work, we have made use of bsvm,4 an
implementation of SVMs with multiclass classifica-
tion support (Hsu et al, 2008). We only report re-
sults for multi-class bound-constrained support vec-
tor machines with linear kernels, as they were found
to perform best over our data.
6 Experimental Methodology
We carry out experiments over the cross-product of
the following options, as described above:
model (?7): nearest-neighbour (COS1NN,
SKEW1NN, OOP1NN), nearest-prototype
(COSAM, SKEWAM),5 NB, SVM
tokenisation (?2): byte, codepoint
n-gram (?3): 1-gram, 2-gram, 3-gram
for a total of 42 distinct classifiers. Each classi-
fier is run across the 3 datasets (EUROGOV, TCL
and WIKIPEDIA) based on 10-fold stratified cross-
validation.
We evaluate the models using micro-averaged
precision (P?), recall (R?) and F-score (F?), as well
as macro-averaged precision (PM ), recall (RM ) and
F-score (FM ). The micro-averaged scores indicate
the average performance per document; as we al-
ways make a unique prediction per document, the
micro-averaged precision, recall and F-score are al-
ways identical (as is the classification accuracy).
The macro-averaged scores, on the other hand, indi-
cate the average performance per language. In each
case, we average the precision, recall and F-score
across the 10 folds of cross validation.6
As a baseline, we use a majority class, or ZeroR,
classifier (ZEROR), which assigns the language with
highest prior in the training data to each of the test
documents.
4http://www.csie.ntu.edu.tw/?cjlin/bsvm/
5We do not include the results for nearest-prototype classi-
fiers with the OOP distance metric as the results were consid-
erably lower than the other methods.
6Note that this means that the averaged FM is not necessar-
ily the harmonic mean of the averaged PM andRM .
Model Token PM RM FM P?/R?/F?
ZEROR ? .020 .084 .032 .100
COS1NN byte .975 .978 .976 .975
COS1NN codepoint .968 .973 .970 .971
COSAM byte .922 .938 .926 .937
COSAM codepoint .908 .930 .913 .931
SKEW1NN byte .979 .979 .979 .977
SKEW1NN codepoint .978 .978 .978 .976
SKEWAM byte .974 .972 .972 .969
SKEWAM codepoint .974 .972 .973 .970
OOP1NN byte .953 .952 .953 .949
OOP1NN codepoint .961 .960 .960 .957
NB byte .975 .973 .974 .971
NB codepoint .975 .973 .974 .971
SVM byte .989 .985 .987 .987
SVM codepoint .988 .985 .986 .987
Table 2: Results for byte vs. codepoint (bigram) tokeni-
sation over EUROGOV
7 Results
In our experiments, we first compare the different
models for fixed n-gram order, then come back to
vary the n-gram order. Subsequently, we examine
the relative performance of the different models on
test documents of differing lengths, and finally look
into the impact of the amount of training data for
a given language on the performance for that lan-
guage.
7.1 Results for the Different Models and
Tokenisation Strategies
First, we present the results for each of the classifiers
in Tables 2?4, based on byte or codepoint tokenisa-
tion and bigrams. In each case, we present the best
result in each column in bold.
The relative performance over EUROGOV and
TCL is roughly comparable for all methods barring
SKEW1NN, with near-perfect scores over all 6 eval-
uation metrics. SKEW1NN is near-perfect over EU-
ROGOV and TCL, but drops to baseline levels over
WIKIPEDIA; we return to discuss this effect in Sec-
tion 7.2.
In the case of EUROGOV, the near-perfect re-
sults are in line with our expectations for the dataset,
based on its characteristics and results reported for
comparable datasets. The results for WIKIPEDIA,
however, fall off considerably, with the best model
achieving an FM of .671 and F? of .869, due to
233
Model Token PM RM FM P?/R?/F?
ZEROR ? .003 .017 .005 .173
COS1NN byte .981 .975 .975 .982
COS1NN codepoint .931 .930 .925 .961
COSAM byte .967 .975 .965 .965
COSAM codepoint .979 .977 .974 .964
SKEW1NN byte .984 .974 .976 .987
SKEW1NN codepoint .910 .210 .320 .337
SKEWAM byte .962 .959 .950 .972
SKEWAM codepoint .968 .961 .957 .967
OOP1NN byte .964 .945 .951 .974
OOP1NN codepoint .901 .892 .893 .933
NB byte .905 .905 .896 .969
NB codepoint .722 .711 .696 .845
SVM byte .981 .973 .977 .984
SVM codepoint .979 .970 .974 .980
Table 3: Results for byte vs. codepoint (bigram) tokeni-
sation over TCL
the larger number of languages, smaller documents,
and skew in the amounts of training data per lan-
guage. All models are roughly balanced in the rel-
ative scores they attain for PM , RM and FM (i.e.
there are no models that have notably higherPM rel-
ative to RM , for example).
The nearest-neighbour models outperform the
corresponding nearest-prototype models to varying
degrees, with the one exception of SKEW1NN over
WIKIPEDIA. The nearest-prototype classifiers were
certainly faster than the nearest-neighbour classi-
fiers, by roughly an order of 10, but this is more
than outweighed by the drop in classification per-
formance. With the exception of SKEW1NN over
WIKIPEDIA, all methods were well above the base-
lines for all three datasets.
The two methods which perform consistently well
at this point are COS1NN and SVM, with COS1NN
holding up particularly well under micro-averaged
F-score while NB drops away over WIKIPEDIA, the
most skewed dataset; this is due to the biasing effect
of the prior in NB.
Looking to the impact of byte- vs. codepoint-
tokenisation on classifier performance over the three
datasets, we find that overall, bytes outperform
codepoints. This is most notable for TCL and
WIKIPEDIA, and the SKEW1NN and NB models.
Given this result, we present only results for byte-
based tokenisation in the remainder of this paper.
Model Token PM RM FM P?/R?/F?
ZEROR ? .004 .013 .007 .328
COS1NN byte .740 .646 .671 .869
COS1NN codepoint .685 .604 .625 .835
COSAM byte .587 .634 .573 .776
COSAM codepoint .486 .556 .483 .725
SKEW1NN byte .005 .013 .008 .304
SKEW1NN codepoint .006 .013 .007 .241
SKEWAM byte .605 .617 .588 .844
SKEWAM codepoint .552 .575 .532 .807
OOP1NN byte .619 .518 .548 .831
OOP1NN codepoint .598 .486 .520 .807
NB byte .496 .454 .442 .851
NB codepoint .426 .349 .360 .798
SVM byte .667 .545 .577 .845
SVM codepoint .634 .494 .536 .818
Table 4: Results for byte vs. codepoint (bigram) tokeni-
sation over WIKIPEDIA
The results for byte tokenisation of TCL are par-
ticularly noteworthy. The transcoding into unicode
and use of codepoints, if anything, hurts perfor-
mance, suggesting that implicit character encoding
detection based on byte tokenisation is the best ap-
proach: it is both more accurate and simplifies the
system, in removing the need to perform encoding
detection prior to language identification.
7.2 Results for Differing n-gram Sizes
We present results with byte unigrams, bigrams and
trigrams in Table 5 for WIKIPEDIA.7 We omit re-
sults for the other two datasets, as the overall trend is
the same as for WIKIPEDIA, with lessened relative
differences between n-gram orders due to the rela-
tive simplicity of the respective classification tasks.
SKEW1NN is markedly different to the other meth-
ods in achieving the best performance with uni-
grams, moving from the worst-performing method
by far to one of the best-performing methods. This
is the result of the interaction between data sparse-
ness and heavy-handed smoothing with the ? con-
stant. Rather than using a constant ? value for all
n-gram orders, it may be better to parameterise it
using an exponential scale such as ? = 1??n (with
7The results for OOP1NN over byte trigrams are missing
due to the computational cost associated with the method, and
our experiment hence not having run to completion at the time
of writing. Extrapolating from the results for the other two
datasets, we predict similar results to bigrams.
234
Model n-gram PM RM FM P?/R?/F?
ZEROR ? .004 .013 .007 .328
COS1NN 1 .644 .579 .599 .816
COS1NN 2 .740 .646 .671 .869
COS1NN 3 .744 .656 .680 .862
COSAM 1 .526 .543 .487 .654
COSAM 2 .587 .634 .573 .776
COSAM 3 .553 .632 .545 .761
SKEW1NN 1 .691 .598 .625 .848
SKEW1NN 2 .005 .013 .008 .304
SKEW1NN 3 .005 .013 .004 .100
SKEWAM 1 .552 .569 .532 .740
SKEWAM 2 .605 .617 .588 .844
SKEWAM 3 .551 .631 .554 .825
OOP1NN 1 .519 .446 .468 .747
OOP1NN 2 .619 .518 .548 .831
NB 1 .576 .578 .555 .778
NB 2 .496 .454 .442 .851
NB 3 .493 .435 .432 .863
SVM 1 .585 .505 .523 .812
SVM 2 .667 .545 .577 .845
SVM 3 .717 .547 .594 .840
Table 5: Results for different n-gram orders over
WIKIPEDIA
? = 0.01, e.g.), based on the n-gram order. We
leave this for future research.
For most methods, bigrams and trigrams are bet-
ter than unigrams, with the one notable exception
of SKEW1NN. In general, there is little separating
bigrams and trigrams, although the best result for is
achieved slightly more often for bigrams than for tri-
grams.
For direct comparability with Cavnar and Tren-
kle (1994), we additionally carried out a preliminary
experiment with hybrid byte n-grams (all of 1- to 5-
grams), combined with simple frequency-based fea-
ture selection of the top-1000 features for each n-
gram order. The significance of this setting is that it
is the strategy adopted by textcat, based on the
original paper of Cavnar and Trenkle (1994) (with
the one exception that we use 1000 features rather
than 300, as all methods other than OOP1NN bene-
fitted from more features). The results are shown in
Table 6.
Compared to the results in Table 5, SKEW1NN and
SKEWAM both increase markedly to achieve the best
overall results. OOP1NN, on the other hand, rises
slightly, while the remaining three methods actually
Model PM RM FM P?/R?/F?
ZEROR .004 .013 .007 .328
COS1NN .735 .664 .682 .865
COSAM .592 .626 .580 .766
SKEW1NN .789 .708 .729 .902
SKEWAM .681 .718 .680 .870
OOP1NN .697 .595 .626 .864
SVM .669 .500 .544 .832
Table 6: Results for mixed n-grams (1?5) and feature se-
lection over WIKIPEDIA (a la? Cavnar and Trenkle (1994))
drop back slightly. Clearly, there is considerably
more experimentation to be done here with mixed
n-gram models and different feature selection meth-
ods, but the results indicate that some methods cer-
tainly benefit from n-gram hybridisation and feature
selection, and also that we have been able to sur-
pass the results of Cavnar and Trenkle (1994) with
SKEW1NN in an otherwise identical framework.
7.3 Breakdown Across Test Document Length
To better understand the impact of test document
size on classification accuracy, we divided the test
documents into 5 equal-size bins according to their
length, measured by the number of tokens. We then
computed F? individually for each bin across the 10
folds of cross validation. We present the breakdown
of results for WIKIPEDIA in Figure 2.
WIKIPEDIA shows a pseudo-logarithmic growth
in F? (= P? = R?) as the test document size in-
creases. This fits with our intuition, as the model
has progressively more evidence to base the classi-
fication on. It also suggests that performance over
shorter documents appears to be the dominating fac-
tor in the overall ranking of the different methods.
In particular, COS1NN and SVM appear to be able to
classify shorter documents most reliably, leading to
the overall result of them being the best-performing
methods.
While we do not show the graph for reasons of
space, the equivalent graph for EUROGOV displays
a curious effect: F? drops off as the test documents
get longer. Error analysis of the data indicates that
this is due to longer documents being more likely
to be ?contaminated? with either data from a sec-
ond language or extra-linguistic data, such as large
tables of numbers or chemical names. This sug-
gests that all the models are brittle when the assump-
235
Figure 2: Breakdown of F? over WIKIPEDIA for test
documents of increasing length
Figure 3: Per-language FM for COS1NN, relative to the
training data size (in MB) for that language
tion of strict monolingualism is broken, or when
the document is dominated by extra-linguistic data.
Clearly, this underlines our assumption of monolin-
gual documents, and suggests multilingual language
identification is a fertile research area even in terms
of optimising performance over our ?monolingual?
datasets.
7.4 Performance Relative to Training Data Size
As a final data point in our analysis, we calculated
the FM for each language relative to the amount of
training data available for that language, and present
the results in the form of a combined scatter plot for
the three datasets in Figure 3. The differing distri-
butions of the three datasets are self-evident, with
most languages in EUROGOV (the squares) both
having reasonably large amounts of training data and
achieving high FM values, but the majority of lan-
guages in WIKIPEDIA (the crosses) having very lit-
tle data (including a number of languages with no
training data, as there is a singleton document in that
language in the dataset). As an overall trend, we can
observe that the greater the volume of training data,
the higher the FM across all three datasets, but there
is considerable variation between the languages in
terms of their FM for a given training data size (the
column of crosses for WIKIPEDIA to the left of the
graph is particularly striking).
8 Conclusions
We have carried out a thorough (re)examination of
the task of language identification, that is predict-
ing the language that a given document is written
in, focusing on monolingual documents at present.
We experimented with a total of 7 models, and
tested each over two tokenisation strategies (bigrams
vs. codepoints) and three token n-gram orders (un-
igrams, bigrams and trigrams). At the same time
as reproducing results from earlier research on how
easy the task can be over small numbers of lan-
guages with longer documents, we demonstrated
that the task becomes much harder for larger num-
bers of languages, shorter documents and greater
class skew. We also found that explicit character
encoding detection is not necessary in language de-
tection, and that the most consistent model overall
is either a simple 1-NN model with cosine similar-
ity, or an SVM with a linear kernel, using a byte
bigram or trigram document representation. We also
confirmed that longer documents tend to be easier to
classify, but also that multilingual documents cause
problems for the standard model of language identi-
fication.
Acknowledgements
This research was supported by a Google Research
Award.
References
Beatrice Alex, Amit Dubey, and Frank Keller. 2007.
Using foreign inclusion detection to improve parsing
performance. In Proceedings of the Joint Conference
236
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
2007 (EMNLP-CoNLL 2007), pages 151?160, Prague,
Czech Republic.
Javed A. Aslam and Meredith Frost. 2003. An
information-theoretic measure for document similar-
ity. In Proceedings of 26th International ACM-SIGIR
Conference on Research and Development in Informa-
tion Retrieval (SIGIR 2003), pages 449?450, Toronto,
Canada.
Timothy Baldwin, Steven Bird, and Baden Hughes.
2006. Collecting low-density language materials on
the web. In Proceedings of the 12th Australasian Web
Conference (AusWeb06). http://www.ausweb.
scu.edu.au/ausweb06/edited/hughes/.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Proceedings of the
Third Symposium on Document Analysis and Informa-
tion Retrieval, Las Vegas, USA.
Marc Darnashek. 1995. Gauging similarity with n-
grams: Language-independent categorization of text.
Science, 267:843?848.
Rafael Dueire Lins and Paulo Gonc?alves. 2004. Au-
tomatic language identification of written texts. In
Proceedings of the 2004 ACM Symposium on Applied
Computing (SAC 2004), pages 1128?1133, Nicosia,
Cyprus.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS 940-273, Computing
Research Laboratory, New Mexico State University.
Emmanuel Giguet. 1995. Categorization according to
language: A step toward combining linguistic knowl-
edge and statistic learning. In Proceedings of the
4th International Workshop on Parsing Technologies
(IWPT-1995), Prague, Czech Republic.
E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 5:447?474.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of Analisi Sta-
tistica dei Dati Testuali (JADT), pages 263?268.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.
2008. A practical guide to support vector classifica-
tion. Technical report, Department of Computer Sci-
ence National Taiwan University.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proceedings of the 5th International
Conference on Language Resources and Evaluation
(LREC 2006), pages 485?488, Genoa, Italy.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: learning with many relevant fea-
tures. In Proceedings of the 10th European Confer-
ence on Machine Learning, pages 137?142, Chemnitz,
Germany.
Stephen Johnson. 1993. Solving the problem of lan-
guage recognition. Technical report, School of Com-
puter Studies, University of Leeds.
Canasai Kruengkrai, Prapass Srichaivattana, Virach
Sornlertlamvanich, and Hitoshi Isahara. 2005. Lan-
guage identification based on string kernels. In Pro-
ceedings of the 5th International Symposium on Com-
munications and Information Technologies (ISCIT-
2005), pages 896?899, Beijing, China.
Lillian Lee. 2001. On the effectiveness of the skew diver-
gence for statistical language analysis. In Proceedings
of Artificial Intelligence and Statistics 2001 (AISTATS
2001), pages 65?72, Key West, USA.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello
Cristianini, and Chris Watkins. 2002. Text classifica-
tion using string kernels. Journal of Machine Learning
Research, 2:419?444.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, UK.
Bruno Martins and Ma?rio J. Silva. 2005. Language iden-
tification in web pages. In Proceedings of the 2005
ACM symposium on Applied computing, pages 764?
768, Santa Fe, USA.
Andrew Kachites McCallum. 1996. Bow: A toolkit for
statistical language modeling, text retrieval, classifica-
tion and clustering. http://www.cs.cmu.edu/
?mccallum/bow.
Paul McNamee and JamesMayfield. 2004. CharacterN -
gram Tokenization for European Language Text Re-
trieval. Information Retrieval, 7(1?2):73?97.
Olivier Teytaud and Radwan Jalam. 2001. Kernel-
based text categorization. In Proceedings of the
International Joint Conference on Neural Networks
(IJCNN?2001), Washington DC, USA.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag, Berlin, Germany.
Fei Xia and William Lewis. 2009. Applying NLP tech-
nologies to the collection and enrichment of language
data on the web to aid linguistic research. In Pro-
ceedings of the EACL 2009 Workshop on Language
Technology and Resources for Cultural Heritage, So-
cial Sciences, Humanities, and Education (LaTeCH ?
SHELT&R 2009), pages 51?59, Athens, Greece.
Fei Xia, William Lewis, and Hoifung Poon. 2009. Lan-
guage ID in the context of harvesting language data off
the web. In Proceedings of the 12th Conference of the
EACL (EACL 2009), pages 870?878, Athens, Greece.
237
