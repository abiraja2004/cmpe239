Coling 2010: Poster Volume, pages 1399?1407,
Beijing, August 2010
Jointly Identifying Entities and Extracting Relations in Encyclopedia
Text via A Graphical Model Approach?
Xiaofeng YU Wai LAM
Information Systems Laboratory
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
{xfyu,wlam}@se.cuhk.edu.hk
Abstract
In this paper, we investigate the problem of en-
tity identification and relation extraction from en-
cyclopedia articles, and we propose a joint discrim-
inative probabilistic model with arbitrary graphical
structure to optimize all relevant subtasks simulta-
neously. This modeling offers a natural formalism
for exploiting rich dependencies and interactions
between relevant subtasks to capture mutual ben-
efits, as well as a great flexibility to incorporate a
large collection of arbitrary, overlapping and non-
independent features. We show the parameter es-
timation algorithm of this model. Moreover, we
propose a new inference method, namely collec-
tive iterative classification (CIC), to find the most
likely assignments for both entities and relations.
We evaluate our model on real-world data from
Wikipedia for this task, and compare with current
state-of-the-art pipeline and joint models, demon-
strating the effectiveness and feasibility of our ap-
proach.
1 Introduction
We investigate a compound information extrac-
tion (IE) problem from encyclopedia articles,
which consists of two subtasks ? recognizing
structured information about entities and extract-
ing the relationships between entities. The most
common approach to this problem is a pipeline
architecture: attempting to perform different sub-
tasks, namely, named entity recognition and rela-
tion extraction between recognized entities in sev-
eral separate, and independent stages. Such kind
of design is widely adopted in NLP.
?The work described in this paper is substantially sup-
ported by grants from the Research Grant Council of the
Hong Kong Special Administrative Region, China (Project
No: CUHK4128/07) and the Direct Grant of the Fac-
ulty of Engineering, CUHK (Project Codes: 2050442 and
2050476). This work is also affiliated with the Microsoft-
CUHK Joint Laboratory for Human-centric Computing and
Interface Technologies.
The most common and simplest approach to
performing compound NLP tasks is the 1-best
pipeline architecture, which only takes the 1-best
hypothesis of each stage and pass it to the next
one. Although it is comparatively easy to build
and efficient to run, this pipeline approach is
highly ineffective and suffers from serious prob-
lems such as error propagation (Finkel et al,
2006; Yu, 2007; Yu et al, 2008). It is not sur-
prising that, the end-to-end performance will be
restricted and upper-bounded.
Usually, one can pass N-best lists between dif-
ferent stages in pipeline architectures, and this of-
ten gives useful improvements (Hollingshead and
Roark, 2007). However, effectively making use of
N-best lists often requires lots of engineering and
human effort (Toutanova, 2005). On the other
hand, one can record the complete distribution at
each stage in a pipeline, to compute or approxi-
mate the complete distribution at the next stage.
Doing this is generally infeasible, and this solu-
tion is rarely adopted in practice.
One promising way to tackle the problem of er-
ror propagation is to explore joint learning which
integrates evidences from multiple sources and
captures mutual benefits across multiple compo-
nents of a pipeline for all relevant subtasks simul-
taneously (e.g., (Toutanova et al, 2005), (Poon
and Domingos, 2007), (Singh et al, 2009)). Joint
learning aims to handle multiple hypotheses and
uncertainty information and predict many vari-
ables at once such that subtasks can aid each other
to boost the performance, and thus usually leads
to complex model structure. However, it is typ-
ically intractable to run a joint model and they
sometimes can hurt the performance, since they
1399
increase the number of paths to propagate errors.
Due to these difficulties, research on building joint
approaches is still in the beginning stage.
A significant amount of recent work has shown
the power of discriminatively-trained probabilistic
graphical models for NLP tasks (Lafferty et al,
2001; Sutton and McCallum, 2007; Wainwright
and Jordan, 2008). The superiority of graphical
model is its ability to represent a large number of
random variables as a family of probability dis-
tributions that factorize according to an underly-
ing graph, and capture complex dependencies be-
tween variables. And this progress has begun to
make the joint learning approach possible.
In this paper we study and formally define the
joint problem of entity identification and relation
extraction from encyclopedia text, and we propose
a joint paradigm in a single coherent framework
to perform both subtasks simultaneously. This
framework is based on undirected probabilistic
graphical models with arbitrary graphical struc-
ture. We show how the parameters in this model
can be estimated efficiently. More importantly, we
propose a new inference method ? collective it-
erative classification (CIC), to find the maximum
a posteriori (MAP) assignments for both entities
and relations. We perform extensive experiments
on real-world data from Wikipedia for this task,
and substantial gains are obtained over state-of-
the-art probabilistic pipeline and joint models, il-
lustrating the promise of our approach.
2 Problem Formulation
2.1 Problem Description
This problem involves identifying entities and dis-
covering semantic relationships between entity
pairs from English encyclopedic articles. The ba-
sic document is an article, which mainly defines
and describes an entity (known as principal en-
tity). This document mentions some other entities
as secondary entities related to the principal en-
tity. Clearly, our task consists of two subtasks ?
first, for entity identification, we need to recog-
nize the secondary entities (both the boundaries
and types of them) in the document 1. Second,
1Since the topic/title of an article usually defines a princi-
pal entity (e.g., a famous person) and it is easy to identify, in
after all the secondary entities are identified, our
goal for relation extraction is to predict what rela-
tion, if any, each secondary entity has to the prin-
cipal entity. We assume that there is no relation-
ship between any two secondary entities in one
document.
As an illustrative example, Figure 1 shows the
task of entity identification and relationship ex-
traction from encyclopedic documents. Here,
Abraham Lincoln is the principal entity. Our
task consists of assigning a set of pre-defined en-
tity types (e.g., PER, DATE, YEAR, and ORG)
to segmentations in encyclopedic documents and
assigning a set of pre-defined relations (e.g.,
birth day, birth year, and member of) for each
identified secondary entity to the principal entity.
2.2 Problem Formulation
Let x be an observation sequence of tokens in
encyclopedic text and x = {x1, ? ? ? , xN}. Let
sp be the principal entity (we assume that it is
known or can be easily recognized), and let s =
{s1, ? ? ? , sL} be a segmentation assignment of ob-
servation sequence x. Each segment si is a triple
si = {?i, ?i, yi}, where ?i is a start position, ?i
is an end position, and yi is the label assigned to
all tokens of this segment. The segment si satis-
fies 0 ? ?i < ?i ? |x| and ?i+1 = ?i + 1. Let
rpn be the relation assignment between principal
entity sp and secondary entity candidate sn from
the segmentation s, and r be the set of relation as-
signments for sequence x.
Let y = {r, s} be the pair of segmentation s and
segment relations r for an observation sequence
x. A valid assignment y must satisfy the condi-
tion that the assignments of the segments and the
assignments of the relations of segments are max-
imized simultaneously. We now formally define
this joint optimization problem as follows:
Definition 1 (Joint Optimization of Entity Iden-tification and Relation Extraction): Given an ob-servation sequence x, the goal of joint optimiza-tion of entity identification and relation extraction
is to find the assignment y? = {r?, s?} that has the
maximum a posteriori (MAP) probability
y? = argmax
y
P (y|x), (1)
this paper we only focus on secondary entity identification.
1400
	

  
 
    
  
	
  
!"# $# %&
	' 	'
'
Figure 1: An example of entity identification and relation extraction excerpted from our dataset. The
secondary entities are in pink color and labeled. The semantic relation of each secondary entity to
the principal entity Abraham Lincoln (in green color and we assume that it is known or can be easily
recognized) is also shown.
where r? and s? denote the most likely relation
assignment and segmentation assignment, respec-
tively.
Note that this problem is usually very challeng-
ing and offers new opportunities for information
extraction, since complex dependencies between
segmentations and relations should be exploited.
3 Our Proposed Model
3.1 Preliminaries
Conditional random fields (CRFs) (Lafferty et al,
2001) are undirected graphical models trained to
maximize the conditional probability of the de-
sired outputs given the corresponding inputs. Let
G be a factor graph (Kschischang et al, 2001)
defining a probability distribution over a set of
output variables o conditioned on observation se-
quences x. C = {?c(oc, xc)} is a set of factors in
G, then the probability distribution over G can be
written as
P (o|x) = 1Z(x)
?
c?C
?c(oc, xc) (2)
where ?c is a potential function and Z(x) =?
o
?
c?C ?c(oc, xc) is a normalization factor.
We assume the potentials factorize according to
a set of features {fk(oc, xc)} as ?c(oc, xc) =
exp(
?
k ?kfk(oc, xc)) so that the family of dis-
tributions is an exponential family. The model
parameters are a set of real-valued weights ? =
{?k}, one weight for each feature. Practical mod-
els rely extensively on parameter tying to use the
same parameters for several factors.
However, the traditional fashion of CRFs can
only deal with single task, they lack the capabil-
ity to represent more complex interaction between
multiple subtasks. In the following we will de-
scribe our joint model in detail for this problem.
3.2 A Joint Model for Entity Identification
and Relation Extraction
Following the notations in Section 2.2, let L and
M be the number of segments and number of
relations for sequence x, respectively. We de-
fine a joint conditional distribution for segmen-
tation s in observation sequence x and segment
relation r in undirected, probabilistic graphical
models. The nature of our modeling enables us
to partition the factors C of G into three groups
{CS , CR, CO}={{?S}, {?R}, {?O}}, namely the
segmentation potential ?S , the relation potential
?R, and the segmentation-relation joint poten-
tial ?O, and each potential is a clique template
whose parameters are tied. The potential function
?S(i, s, x) models segmentation s in x, the poten-
tial function ?R(rpm, rpn, r) (m 6= n) represent
dependencies (e.g., long-distance dependencies,
relation transitivity, etc) between any two rela-
tions in the relation set r, where rpm is the relation
assignment between the principal entity sp and the
secondary entity candidate sm from s, and simi-
larly for rpn. And the joint potential ?O(sp, sj , r)
captures rich and complex interactions between
segmentation s for secondary entity identification
and relation r between each secondary entity can-
didate sj to the principal entity sp. According
to the celebrated Hammersley-Clifford theorem
(Besag, 1974), the joint conditional distribution
P (y|x) = P ({r, s}|x) is factorized as a product
of potential functions over cliques in the graph G
as the form of an exponential family:
P (y|x) = 1Z(x)
(?
CS
?S(i, s, x))
(?
CR
?R(rpm, rpn, r)
)(?
CO
?O(sp, sj , r)
) (3)
1401
where Z(x) = ?y
?
CS ?
S(i, s, x)?CR ?R(rpm
, rpn, r)?CO ?O(sp, sj , r) is the normalizationfactor of the joint model.
We assume the potential functions ?S , ?R
and ?O factorize according to a set of fea-
tures and a corresponding set of real-valued
weights. More specifically, ?S(i, s, x) =
exp(
?|s|
i=1
?K
k=1 ?kgk(i, s, x)). To effectively
capture properties of segmentation, we relax the
first-order Markov assumption to semi-Markov
such that each segment feature function gk(?)
depends on the current segment si, the previ-
ous segment si?1, and the whole observation se-
quence x, that is, gk(i, s, x) = gk(si?1, si, x) =
gk(yi?1, yi, ?i, ?i, x). And transitions within a
segment can be non-Markovian.
Similarly, the potential ?R(rpm, rpn, r) =
exp(
?M
m,n
?W
w=1 ?wqw(rpm, rpn, r)) and ?O(sp,
sj , r) = exp(?Lj=1
?T
t=1 ?tht(sp, sj , r)), where
W and T are number of feature functions, qw(?)
and ht(?) are feature functions, ?w and ?t are
corresponding weights for them. The potential
?R(rpm, rpn, r) allows long-range dependency
representation between different relations rpm and
rpn. For example, if the same secondary en-
tity is mentioned more than once in an obser-
vation sequence, all mentions probably have the
same relation to the principal entity. Using poten-
tial ?R(rpm, rpn, r), evidences for the same entity
segments to the principal entity are shared among
all their occurrences within the document. The
joint factor ?O(sp, sj , r) exploits tight dependen-
cies between segmentations and relations. For ex-
ample, if a segment is labeled as a location and
the principal entity is person, the semantic rela-
tion between them can be birth place or visited,
but cannot be employment. Such dependencies
are essential and modeling them often leads to im-
proved performance. In summary, the probability
distribution of the joint model can be rewritten as:
P (y|x) = 1Z(x) exp
{ |s|?
i=1
K?
k=1
?kgk(i, s, x) +
M?
m,n
W?
w=1
?wqw(rpm, rpn, r) +
L?
j=1
T?
t=1
?tht(sp, sj , r)
}
(4)
 

 

        




  

 










 
Figure 2: Graphical representation of the proba-
bilistic joint model. The gray nodes represent se-
quence tokens {x1, ? ? ? , xN}. Each ellipse repre-
sents a segment consisting of several consecutive
sequence tokens. The pink nodes represent seg-
mentation assignment {s1, ? ? ? , sL} of sequence.
The yellow nodes represent relation assignment
{rp1, ? ? ? , rpL} between the principal entity sp (in
green color) and secondary entity segments.
As illustrated in Figure 2, our model consists
of three sub-structures: a semi-Markov chain on
the segmentations s conditioned on the observa-
tion sequences x, represented by ?S ; potential ?R
measuring dependencies between different rela-
tions rpm and rpn; and a fully-connected graph
on the principal entity sp and each segment sj for
their relations, represented by ?O.
While several special cases of CRFs are of par-
ticular interest, and we emphasize on the differ-
ences and advantages of our model against oth-
ers. Linear-chain CRFs (Lafferty et al, 2001) can
only perform single sequence labeling, they lack
the ability to capture long-distance dependency
and represent complex interactions between mul-
tiple subtasks. Skip-chain CRFs (Sutton and Mc-
callum, 2004) introduce skip edges to model long-
distance dependencies to handle the label consis-
tency problem in single sequence labeling and ex-
traction. 2D CRFs (Zhu et al, 2005) are two-
dimensional conditional random fields incorporat-
ing the two-dimensional neighborhood dependen-
cies in Web pages, and the graphical representa-
tion of this model is a 2D grid. Hierarchical CRFs
(Liao et al, 2007) are a class of CRFs with hi-
erarchical tree structure. Our probabilistic model
for joint entity identification and relation extrac-
tion has distinct graphical structure from 2D and
hierarchical CRFs. And this modeling has sev-
1402
eral advantages over previous probabilistic graph-
ical models by using semi-Markov chains for effi-
cient segmentation and labeling, by representing
long-range dependencies between relations, and
by capturing rich and complex interactions be-
tween relevant subtasks to exploit mutual benefits.
4 Learning the Parameters
Given independent and identically distributed
(IID) training data D = {xi, yi}Ni=1, where xi is
the i-th sequence instance, yi = {ri, si} is the
corresponding segmentation and relation assign-
ments. The objective of learning is to estimate
? = {?k, ?w, ?t} which is the vector of model?s
parameters. Under the IID assumption, we ig-
nore the summation operator ?Ni=1 in the log-
likelihood during the following derivations. To
reduce over-fitting, we use regularization and a
common choice is a spherical Gaussian prior with
zero mean and covariance ?2I . Then the regular-
ized log-likelihood function L for the data is
L = log
[
?(r, s, x)]? log [Z(x)]
?
K?
k=1
?2k
2?2?
?
W?
w=1
?2w
2?2?
?
T?
t=1
?2t
2?2?
(5)
where ?(r, s, x) = exp{?|s|i=1
?K
k=1 ?kgk(i, s, x)
+
?M
m,n
?W
w=1 ?wqw(rpm, rpn, r)+
?L
j=1
?T
t=1
?tht(sp, sj , r)}, Z(x) = ?y
?
?(r, s, x), and
1/2?2?, 1/2?2?, 1/2?2? are regularization parame-
ters.
Taking derivatives of the function L over the
parameter ?k yields:
?L
??k
=
|s|?
i=1
gk(i, s, x)?
|s|?
i=1
gk(i, s, x)P (y|x)
?
K?
k=1
?k
?2?
(6)
Similarly, the partial derivatives of the log-
likelihood with respect to parameters ?w and nut
are as follows:
?L
??w
=
M?
m,n
qw(rpm, rpn, r)?
M?
m,n
qw(rpm, rpn, r)
? P (y|x)?
W?
w=1
?w
?2?
(7)
?L
??t
=
L?
j=1
ht(sp, sj , r)?
L?
j=1
ht(sp, sj , r)P (y|x)
?
T?
t=1
?t
?2?
(8)
The function L is concave, and can be effi-
ciently maximized by standard techniques such
as stochastic gradient and limited memory quasi-
Newton (L-BFGS) algorithms. The parameters ?k
?w and ?t are optimized iteratively until converge.
5 Finding the Most Likely Assignments
The objective of inference is to find y? =
{r?, s?} = argmax{r,s} P (r, s|x) such that both
s? and r? are optimized simultaneously. Unfortu-
nately, exact inference to this problem is generally
prohibitive, since it requires enumerating all pos-
sible segmentation and corresponding relation as-
signments. Consequently, approximate inference
becomes an alternative.
We propose a new algorithm: collective it-
erative classification (CIC) to perform approxi-
mate inference to find the maximum a posteriori
(MAP) segmentation and relation assignments of
our model in an iterative fashion. The basic idea
of CIC is to decode every target hidden variable
based on the assigning labels of its sampled vari-
ables, where the labels might be dynamically up-
dated throughout the iterative process. Collective
classification refers to the classification of rela-
tional objects described as nodes in a graphical
structure, as in our model.
The CIC algorithm performs inference in two
steps, as shown in Algorithm 1. The first step,
bootstrapping, predicts an initial labeling
assignment for a unlabeled sequence xi, given
the trained model P (y|x). The second step
is the iterative classification process
which re-estimates the labeling assignment of xi
several times, picking them in a sample set S
based on initial assignment for xi. Here we exploit
the sampling technique (Andrieu et al, 2003).
The advantages of sampling are summarized as
follows. Sampling stochastically enables us to
generate a wide range of inference situations, and
the samples are likely to be in high probability ar-
eas, increasing our chances of finding the max-
1403
imum, thus leading to more robust and accurate
performance. The CIC algorithm may converge
if none of the labeling assignments change dur-
ing an iteration or a given number of iterations is
reached.
Noticeably, this inference algorithm is also
used to efficiently compute the marginal probabil-
ity P (y|x) during parameter estimation (the nor-
malization constant Z(x) can also be calculated
via approximation techniques). As can be seen,
this algorithm is simple to design, efficient and
scales well w.r.t. the size of data.
6 Experiments
6.1 Data
Our data comes from Wikipedia2, the world?s
largest free online encyclopedia. This dataset con-
sists of 1127 paragraphs from 441 pages from the
online encyclopedia Wikipedia. We labeled 7740
entities into 8 categories, yielding 1243 person,
1085 location, 875 organization, 641 date, 1495
year, 38 time, 59 number, and 2304miscellaneous
names. This dataset alo contains 4701 relation
instances and 53 labeled relation types. The 10
most frequent relation types are job title, visited,
birth place, associate, birth year, member of,
birth day, opus, death year, and death day. Note
that this compound IE task involving entity iden-
tification and relation extraction is very challeng-
ing, and modeling tight interactions between enti-
ties and their relations is highly attractive.
6.2 Feature Set
Accurate entities enable features that are naturally
expected to be useful to boost relation extraction.
And a wide range of rich, overlapping features
can be exploited in our model. These features
include contextual features, part-of-speech (POS)
tags, morphological features, entity-level dictio-
nary features, clue word features. Feature con-
junctions are also used. In leveraging relation ex-
traction to improve entity identification, we use a
combination of syntactic, entity, keyword, seman-
tic, and Wikipedia characteristic features. More
importantly, our model can incorporate multiple
mention features qw(?), which are used to collect
2http://www.wikipedia.org/
Algorithm 1: Collective Iterative Classifica-
tion Inference
Input: A unlabeled sequence xi and a trained
model P (y|x)
Output: The set of predicted assignment
yi = {ri, si}
// Bootstrapping
foreach yi ? Y do
yi ? argmaxyi P (yi|xi);
end
// Iterative Classification
repeat
Generate a sample set S based on initial
label assignment yi for sequence xi;
foreach si ? S doAssign new label assignment to
sample si;
end
until all labels have stabilized or a threshold
number of iterations have elapsed ;
return yi = {ri, si}
evidences from other occurrences of the same sec-
ondary entities for consistent segmentation and re-
lation labeling to the principal entity. The features
ht(?) capture deep dependencies between segmen-
tations and relations, and they are natural and use-
ful to enhance the performance.
6.3 Methodology
We perform four-fold cross-validation on this
dataset, and take the average performance. For
performance evaluation, we use the standard mea-
sures of Precision (P), Recall (R), and F-measure
(the harmonic mean of P and R: 2PRP+R ) for bothentity identification and relation extraction. We
conduct holdout methodology for parameter tun-
ing and optimization of our model. We compare
our approach with a series of linear-chain CRFs:
CRF+CRF and a joint model DCRF (Sutton et
al., 2007): dynamic probabilistic models com-
bined with factored approach to multiple sequence
labeling. CRF+CRF perform entity identification
and relation extraction separately. Relation ex-
traction is viewed as a sequence labeling problem
in the second CRF. All these models exploit stan-
dard parameter learning and inference algorithms
1404
Table 1: Comparative performance of our model, CRF+CRF, and DCRF models for entity identifica-
tion.
Entities CRF+CRF DCRF Our modelP R F1 P R F1 P R F1
person 75.33 83.22 79.08 75.96 83.82 79.70 82.91 84.26 83.58
location 77.03 69.45 73.04 77.68 70.13 73.71 82.94 80.52 81.71
organization 53.78 47.76 50.59 54.55 46.98 50.48 61.63 62.61 62.12
date 98.54 97.53 98.03 97.98 95.22 96.58 98.90 96.24 97.55
year 97.14 99.10 98.11 98.12 99.09 98.60 97.36 99.55 98.44
time 60.00 20.33 30.37 50.00 25.33 33.63 100.0 25.00 40.00
number 98.88 60.33 74.94 100.0 66.00 79.52 100.0 65.52 79.17
miscellaneous 77.42 80.56 78.96 79.81 83.14 81.44 82.69 85.16 83.91
Overall 89.55 88.70 89.12 90.98 90.37 90.67 93.35 93.37 93.36
in our experiments. To avoid over-fitting, penal-
ization techniques on likelihood are performed.
We also use the same set of features for all these
models.
6.4 Experimental Results
Table 1 shows the performance of entity identifi-
cation and Table 2 shows the overall performance
of relation extraction 3, respectively. Our model
substantially outperforms all baseline models on
the overall F-measure for entity identification, re-
sulting in an relative error reduction of up to
38.97% and 28.83% compared to CRF+CRF and
DCRF, respectively. For relation extraction, the
improvements on the F-measure over CRF+CRF
and DCRF are 4.68% and 3.75%. McNemar?s
paired tests show that all improvements of our
model over baseline models are statistically sig-
nificant. These results demonstrate the merits
of our approach by capturing tight interactions
between entities and relations to explore mutual
benefits. The pipeline model CRF+CRF per-
forms entity identification and relation extraction
independently, and suffers from problems such
as error accumulation. For example, CRF+CRF
cannot extract the member of relation between
the secondary entity Republican and the princi-
pal entity George W. Bush, since the organiza-
tion name Republican is incorrectly labeled as a
miscellaneous. By modeling interactions between
two subtasks, enhanced performance is achieved,
as illustrated by DCRF. Unfortunately, training
a DCRF model with unobserved nodes (hidden
variables) makes this approach difficult to opti-
3Due to space limitation, we only present the overall per-
formance and omit the performance for 53 relation types.
Table 2: Comparative performance of our model,
CRF+CRF, and DCRF models for relation extrac-
tion.
Model Precision Recall F-measure
CRF+CRF 70.40 57.85 63.51
DCRF 69.30 60.22 64.44
Our model 72.57 64.30 68.19
mize, as we will show below.
The efficiency of different models is summa-
rized in Table 3. Compared to the pipeline model
CRF+CRF, the learning time of our model is
only a small constant factor slower. Notably,
our model is over orders of magnitude (approx-
imately 15.7 times) faster than the joint model
DCRF. The DCRF model uses loopy belief prop-
agation (LBP) for approximate learning and infer-
ence. When the graph has large tree-width as in
our case, the LBP algorithm in DCRF is ineffi-
cient, and is slow to converge. Using L-BFGS
and the CIC approximate inference algorithms,
both learning and decoding can be carried out ef-
ficiently.
Table 3: Efficiency comparison of different mod-
els on learning time (sec.) and inference time
(sec.).
Model Learning time Inference time
CRF+CRF 2822.55 6.20
DCRF 105993.00 127.50
Our model 6733.69 62.75
Table 4 compares our CIC inference with two
state-of-the-art inference approaches: Gibbs sam-
pling (GS) (Geman and Geman, 1984) and the
iterative classification algorithm (ICA) (Neville
and Jensen, 2000) for our model. The CIC infer-
ence is shown empirically to help improve classi-
1405
Table 4: Comparative performance of different in-
ference algorithms for our model on entity identi-
fication and relation extraction.
Entity Precision Recall F-measure
GS 92.45 92.15 92.30
ICA 92.19 91.98 92.08
CIC 93.35 93.37 93.36
Relation Precision Recall F-measure
GS 71.22 63.29 67.02
ICA 71.58 63.68 67.40
CIC 72.57 64.30 68.19
fication accuracy and robustness over these two al-
gorithms. When probability distributions are very
complex or even unknown, the GS algorithm can-
not be applied. ICA iteratively infers the states
of variables given the current predicted labeling
assignments of neighboring variables as observed
information. Prediction errors on labels may then
propagate during the iterations and the algorithm
will then have difficulties to generalize correctly.
We mention some recently published results re-
lated to Wikipedia datasets (Note that it is difficult
to compare with them strictly, since these results
can be based on different experimental settings).
Culotta et al (2006) used a data set with a 70/30
split for training/testing and Nguyen et al (2007)
used 5930 articles for training and 45 for testing,
to perform relatione extraction from Wikipedia.
And the obtained F-measures were 67.91 and
37.76, respectively. Yu et al (2009) proposed
an integrated approach incorporating probabilis-
tic graphical models with first-order logic to per-
form relation extraction from encyclopedia arti-
cles, with a F-measure of 65.66. All these sys-
tems assume that the golden-standard entities are
already known and they only perform relation ex-
traction. However, such assumption is not valid
in practice. Notably, our approach deals with a
fairly more challenging problem involving both
entity identification and relation extraction, and it
is more applicable to real-world IE tasks.
7 Related Work
A number of previous researchers have taken
steps toward joint models in NLP and informa-
tion extraction, and we mention some recently
proposed, closely related approaches here. Roth
and Yih (2007) considered multiple constraints
between variables from tasks such as named en-
tities and relations, and developed a integer lin-
ear programming formulation to seek an optimal
global assignment to these variables. Zhang
and Clark (2008) employed the generalized per-
ceptron algorithm to train a statistical model for
joint segmentation and POS tagging, and applied
multiple-beam search algorithm for fast decoding.
Toutanova et al (2008) presented a model captur-
ing the linguistic intuition that a semantic argu-
ment frame is a joint structure, with strong depen-
dencies among the arguments. Finkel and Man-
ning (2009) proposed a discriminative feature-
based constituency parser for joint named entity
recognition and parsing. And Dahlmeier et al
(2009) proposed a joint model for word sense dis-
ambiguation of prepositions and semantic role la-
beling of prepositional phrases. However, most of
the mentioned approaches are task-specific (e.g.,
(Toutanova et al, 2008) for semantic role label-
ing, and (Finkel and Manning, 2009) for parsing
and NER), and they can hardly be applicable to
other NLP tasks. Since we capture rich and com-
plex dependencies between subtasks via potential
functions in probabilistic graphical models, our
approach is general and can be easily applied to
a variety of NLP and IE tasks.
8 Conclusion and Future Work
In this paper, we investigate the compound IE task
of identifying entities and extracting relations be-
tween entities in encyclopedia text. And we pro-
pose a unified framework based on undirected,
conditionally-trained probabilistic graphical mod-
els to perform all relevant subtasks jointly. More
importantly, we propose a new algorithm: CIC,
to enable approximate inference to find the MAP
assignments for both segmentations and relations.
As we shown, our modeling offers several advan-
tages over previous models and provides a natural
formalism for this compound task. Experimental
study exhibits that our model significantly outper-
forms state-of-the-art models while also running
much faster than the joint models. In addition, the
superiority of the CIC algorithm is also discussed
and compared. We plan to improve the scalability
of our approach and apply it to other real-world
problems in the future.
1406
References
Christophe Andrieu, Nando de Freitas, Arnaud Doucet, and
Michael I. Jordan. An introduction toMCMC for machine
learning. Machine Learning, 50(1):5?43, 2003.
Julian Besag. Spatial interaction and the statistical analysis
of lattice systems. Journal of the Royal Statistical Society,
36:192?236, 1974.
Aron Culotta, Andrew McCallum, and Jonathan Betz. Inte-
grating probabilistic extraction models and data mining to
discover relations and patterns in text. In Proceedings of
HLT/NAACL-06, pages 296?303, New York, 2006.
Daniel Dahlmeier, Hwee Tou Ng, and Tanja Schultz. Joint
learning of preposition senses and semantic roles of
prepositional phrases. In Proceedings of EMNLP-09,
pages 450?458, Singapore, 2009.
Jenny Rose Finkel and Christopher D. Manning. Joint
parsing and named entity recognition. In Proceedings
of HLT/NAACL-09, pages 326?334, Boulder, Colorado,
2009.
Jenny Rose Finkel, Christopher D. Manning, and Andrew Y.
Ng. Solving the problem of cascading errors: Ap-
proximate Bayesian inference for linguistic annotation
pipelines. In Proceedings of EMNLP-06, pages 618?626,
Sydney, Australia, 2006.
Stuart Geman and Donald Geman. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of im-
ages. IEEE Transitions on Pattern Analysis and Machine
Intelligence, 6:721?741, 1984.
Kristy Hollingshead and Brian Roark. Pipeline iteration. In
Proceedings of ACL-07, pages 952?959, Prague, Czech
Republic, 2007.
Frank R. Kschischang, Brendan J. Frey, and Hans-Andrea
Loeliger. Factor graphs and the sum-product algorithm.
IEEE Transactions on Information Theory, 47:498?519,
2001.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
ICML-01, pages 282?289, 2001.
Lin Liao, Dieter Fox, and Henry Kautz. Extracting places
and activities from GPS traces using hierarchical condi-
tional random fields. International Journal of Robotics
Research, 26:119?134, 2007.
Jennifer Neville and David Jensen. Iterative classification
in relational data. In Proceedings of the AAAI-2000
Workshop on Learning Statistical Models from Relational
Data, pages 42?49, 2000.
Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru Ishizuka. Re-
lation extraction from Wikipedia using subtree mining. In
Proceedings of AAAI-07, pages 1414?1420, Vancouver,
British Columbia, Canada, 2007.
Hoifung Poon and Pedro Domingos. Joint inference in in-
formation extraction. In Proceedings of AAAI-07, pages
913?918, Vancouver, British Columbia, Canada, 2007.
Dan Roth and Wentau Yih. Global inference for entity and
relation identification via a linear programming formula-
tion. In Lise Getoor and Ben Taskar, editors, Introduction
to Statistical Relational Learning. MIT Press, 2007.
Sameer Singh, Karl Schultz, and Andrew Mccallum. Bi-
directional joint inference for entity resolution and seg-
mentation using imperatively-defined factor graphs. In
Proceedings of ECML/PKDD-09, pages 414?429, Bled,
Slovenia, 2009.
Charles Sutton and Andrew Mccallum. Collective segmen-
tation and labeling of distant entities in information ex-
traction. In Proceedings of ICML Workshop on Statistical
Relational Learning and Its Connections to Other Fields,
2004.
Charles Sutton and Andrew McCallum. An introduction to
conditional random fields for relational learning. In Lise
Getoor and Ben Taskar, editors, Introduction to Statistical
Relational Learning. MIT Press, 2007.
Charles Sutton, Andrew McCallum, and Khashayar Rohan-
imanesh. Dynamic conditional random fields: Factor-
ized probabilistic models for labeling and segmenting se-
quence data. Journal of Machine Learning Research,
8:693?723, 2007.
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. Joint learning improves semantic role labeling. In
Proceedings of ACL-05, pages 589?596, 2005.
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. A global joint model for semantic role labeling.
Computational Linguistics, 34:161?191, 2008.
Kristina Toutanova. Effective statistical models for syntactic
and semantic disambiguation. PhD thesis, Stanford Uni-
versity, 2005.
Martin J. Wainwright and Michael I. Jordan. Graphical mod-
els, exponential families, and variational inference. Foun-
dations and Trends in Machine Learning, 1:1?305, 2008.
Xiaofeng Yu, Wai Lam, and Shing-Kit Chan. A framework
based on graphical models with logic for chinese named
entity recognition. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Processing
(IJCNLP-08), pages 335?342, Hyderabad, India, 2008.
Xiaofeng Yu, Wai Lam, and Bo Chen. An integrated dis-
criminative probabilistic approach to information extrac-
tion. In Proceedings of CIKM-09, pages 325?334, Hong
Kong, China, 2009.
Xiaofeng Yu. Chinese named entity recognition with cas-
caded hybrid model. In Proceedings of HLT/NAACL-07,
pages 197?200, Rochester, New York, 2007.
Yue Zhang and Stephen Clark. Joint word segmentation and
POS tagging using a single perceptron. In Proceedings of
ACL-08, pages 888?896, Ohio, USA, 2008.
Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and Wei-
Ying Ma. 2D conditional random fields for Web informa-
tion extraction. In Proceedings of ICML-05, pages 1044?
1051, Bonn, Germany, 2005.
1407
