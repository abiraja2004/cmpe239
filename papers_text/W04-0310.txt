Incrementality in Syntactic Processing: Computational Models
and Experimental Evidence
Patrick STURT
Human Communication Research Centre
Department of Psychology
University of Glasgow
58 Hillhead Street
Glasgow, SCOTLAND
patrick@psy.gla.ac.uk
Abstract
It is a well-known intuition that human sentence
understanding works in an incremental fashion,
with a seemingly constant update of the inter-
pretation through the left-to-right processing of
a string. Such intuitions are backed up by ex-
perimental evidence dating from at least as far
back as Marslen-Wilson (1973), showing that
under many circumstances, interpretations are
indeed updated very quickly.
From a parsing point of view it is interesting
to consider the structure-building processes that
might underlie incremental interpretation?
what kinds of partial structures are built dur-
ing sentence processing, and with what time-
course?
In this talk I will give an overview of the state-
of-the-art of experimental psycholinguistic re-
search, paying particular attention to the time-
course of structure-building. The discussion will
focus on a new line of research (some as yet un-
published) in which syntactic phenomena such
as binding relations (e.g., Sturt, 2003) and un-
bounded dependencies (e.g., Aoshima, Phillips,
& Weinberg, in press) are exploited to make a
very direct test of the availability of syntactic
structure over time.
The experimental research will be viewed
from the perspective of a space of computa-
tional models, which make different predictions
about time-course of structure building. One
dimension in this space is represented by the
parsing algorithm used: For example, within
the framework of Generalized Left Corner Pars-
ing (Demers, 1977), algorithms can be char-
acterized in terms of the point at which a
context-free rule is recognized, in relation to the
recognition-point of the symbols on its right-
hand side. Another relevant dimension is repre-
sented by the type of grammar formalism that
is assumed. For example, with bottom-up pars-
ing algorithms, the degree to which structure-
building is delayed in right-branching structures
depends heavily on whether we employ a tra-
ditional phrase-structure formalism with rigid
constituency, or a cateogorial formalism with
flexible constituency (e.g., Steedman, 2000).
I will argue that the evidence is incompatible
with models which predict systematic delays in
the construction of syntactic structure. In par-
ticular, I will argue against both head-driven
strategies (e.g., Mulders, 2002), and purely
bottom-up parsing strategies, even when flex-
ible constituency is employed. Instead, I will
argue that to capture the data in the most par-
simonious way, we should turn our attention to
those models in which a fully connected syn-
tactic structure is maintained throughout the
processing of a string.
References
Aoshima, S., Phillips, C., & Weinberg, A. (in
press). Processing filler-gap dependencies
in a head-final language. To appear in
Journal of Memory and Language.
Demers, A. J. (1977). Generalized left cor-
ner parsing. In Proceedings of the 4th
acm sigact-sigplan symposium on princi-
ples of programming languages (pp. 170?
182). ACM Press.
Marslen-Wilson, W. (1973). Linguistic struc-
ture and speech shadowing at very short
latencies. Nature, 244, 522?533.
Mulders, I. (2002). Transparent parsing: Head-
driven processing of verb-final structures.
Utrecht: LOT.
Steedman, M. (2000). The syntactic process.
Cambridge, MA: MIT press.
Sturt, P. (2003). The time-course of the appli-
cation of binding constraints in reference
resolution. Journal of Memory and Lan-
guage, 48 (3), 542?562.
