Japanese Dependency Analysis 
using a Determinist ic Finite State Transducer 
Satosh i  Sek ine  
Computer  Science Depar tment  
New York University 
715 Broadway, 7th floor 
New York, NY 10003, USA 
Abst ract  
A deternfinistic finite state transducer is a fast 
device fbr analyzing strings. It takes O(n) time 
to analyze a string of length n. In this 1)al)er, an 
application of this technique to Japanese depen- 
dency analysis will be described. We achieved 
the speed at; a small cost in accuracy. It takes 
about 0.1.7 nfillisecond to analyze one sentence 
(average length is 10 bunsetsu, 1)ased on Pen- 
t:tahiti 650MHz PC, Linux) and we actually 
observed the analysis time to be proportional 
to the sentence length. Thb accuracy is about; 
81% even though very little lexical information 
is used. This is about 17% and 9% better than 
the default and a simple system, respectively. 
We believe the gap between our pertbrmm:ce 
and the best current 1)erforlnm:ce on the stone 
task, about 7%, can be filled by introducing lex- 
ical or sen:antic infornmtion. 
1 I n t roduct ion  
Syntactic analysis or parsing based on tradi- 
tional methods, like Chm't parsing or the GLR 
parsing algorithm, takes cubic or greater tin:e 
in the sentence length to analyze natural lan- 
guage sentences. For Japmmse, Sekine et al 
(Sekine et al, 2000) proposed a Japanese depen- 
dency analyzer which mmlyzes entences in time 
quadratic in the sentence length using a back- 
ward search algorithm. Recently, a mnnber of 
research efforts using Finite State Transducers 
(FST) have been reported. Roche built an En- 
glish syntactic mmlyzer by finding a fixed point 
in a non-deterministic FST (Roche, 1994). But 
it still can't anMyze a sentence in time linear in 
the sentence length. 
In this paper, we will propose a Japanese 
dependency analyzer using a Deterministic Fi- 
nile State Transducer (DFST). The Japmtese 
dependency structure is usually represented by 
relationships between phrasal units called %un- 
sets::'. A lmnsetsu sually contains one or more 
content words, like a noun, verb or adjective, 
and zero or more time:ion words, lil:e a post- 
position (case marker) or verb/noun suffix. A 
dependency between two bunsetsu has a direc- 
tion fl'om a dependent to its head. Figure 1 
shows examples of lmnsetsu and dependencies. 
Each lmnsetsu is separated by "1". The frst 
seglnent "KANOJO-HA" consists of two words, 
KANOJO (She) and HA (subject case, ,narl:er). 
The nmnbers in the "head" line show the head 
ID of corresponding bunsctsus. Note that the 
last segment does not have a head, and it is the 
head lmnsetsu of the sentence. The task of the 
dependency analysis is to find the head ID for 
each lnmsetsu. 
2 Backward  beam search  a lgor i thm 
l?irst, we wouhl lil:e to describe the backwm'd 
beam search algoril:lm: tbr ,\]at)m:ese d pendency 
analysis proposed by Sekinc et sd. (Sekine et al, 
2000). Their experin:ents suggested the method 
proposed in this paper. 
The following characteristics are known ibr 
Japanese dependency. Sekine et al assumed 
these characteristics in order to design the algo- 
rithm 1 
(1) l)ependencies are directed from left to right 
(2) Dependencies don't cross. 
(3) Each bunsetsu except the rightmost one 
has only one head 
(4) Left context is not necessary to deternfine 
a dependency. 
1We know that there are exceptions (Shirai, 1998), 
but the frequencies of such exceptions are very stuN1. 
Characteristic (4) is not well recognized, but based on 
our experiments with humans, it is true more than 90% 
of the time. 
761 
ID 1 2 3 4 5 6 
KANOJ0-HA I KARE-GA I TSUKUTTA I PAI-W0 I YOROKONDE I UKETOTTA. 
(She-subj) (he-subj) (made) (pie-obj) (with pleasure) (received) 
Head 6 3 4 6 6 - 
Translation: She received the pie made by him with pleasure. 
Figure h Example of a Japanese sentence, bunsetsus and dependencies 
Sekine et al proposed a backward beam search 
algorithm (analyze a sentence froln the tail to 
the head, or right to left). Backward search has 
two merits. Let's assume we have analyzed up 
to the/14 + l-st bunsetsu in a sentence of length 
N (0 < M < N). Now we are deciding on the 
head of the M-th bunsetsu. The first merit is 
that the head of the dependency of the /1J-th 
bunsetsu is one of the bunsetsus between t14 + 1 
and N, which are already analyzed. Becmlse of 
this, we don't have to keel) a huge number of 
possible analyses, i.e. we can avoid something 
like active edges in a chart parser, or making 
parallel stacks in GLR parsing, as we can make 
a decision at this time. Also, wc can use the 
bemn search mechanism, t)y keeping a certain 
nmnber of candidates of analyses at each t)un- 
setsu. The width of the beam search can be 
e~Lsily tuned and the memory size of the process 
is proportional to the product of the input sen- 
tence length and the beam search width. The 
other merit is that the 1)ossible heads of the de- 
pendency can be narrowed down because of the 
assmnption of non-crossing dependencies. For 
example, if the K-th bunsetsu depends on the 
L-th bunsetsn (/1J < K < L), then the 21//- 
th bunsetsu can't depend on any bunsetsus be- 
tween I(  and L. According to our experiment, 
this reduced the nmnber of heads to consider to 
less than 50%. 
Uchimoto et al implemented a Japanese de- 
pendency analyzer based on this algorithm in 
combination with the Maxinmm Entropy learn- 
ing method (Uchimoto et al, 2000). The an- 
alyzer demonstrated a high accuracy. Table 1 
shows the relationship between the beam width 
and the accuracy of the system. Froln the table, 
we can see that the accuracy is not sensitive to 
the beam width, and even when the width is 
Beam width Dependency Sentence 
Accuracy Accuracy 
\] 
2 
5 
10 
20 
87.14 
87.16 
87.15 
87.20 
86.21 
40.60 
40.76 
40.60 
40.60 
40.60 
Table h Bemn width and accuracy 
1, which Ineans that at each stage, the depen- 
dency is deterministieally decided, the accuracy 
is almost the same as the best accuracy. This 
means that the left, context of a dependency is
not necessary to decide the dependency, which 
is closely related to characteristic (4). This re- 
sult gives a strong motivation tbr st~rting the 
research reported in this paper. 
3 Idea  
Untbrtunately, the fact that the beam width can 
be 1 by itself can not lead to the analysis in time 
proportional to the input length. Theoretically, 
it takes time quadratic in the length and this is 
observed experimentally as well (see Figure 3 in 
(Sekine et al, 2000)). This is due to the process 
of finding the head among the candidate bun- 
setsns on its right. The time to find the head is 
proportional to the number of candidates, which 
is roughly proportional to the number of bun- 
setsus on its right. The key idea of getting linear 
speed is to make this time a constant. 
Table 2 shows the number of candidate bun- 
setsus and the relative location of the head 
among the candidates (the number of candi- 
date bunsetsus from the bmlsetsu being ana- 
lyzed). The data is derived fl'om 1st to 8th of 
762 
Nmn.of Location of head 
eand. 1 2 3 4 5 6 7 8 9 10 11 12 13 1.4 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
7918 
12824 4948 
10232 3292 
6774 2244 
3692 1294 
1843 614 
848 278 
34O 110 
137 33 
51 11 
17 5 
5 1 
3 () 
() \] 
3126 
1439 1968 
888 660 1114 
398 331. 291 551 
176 133 i33 125264 
83 47 ,78 57 68 121 
28 17 19 21 19 2/t 46 
10 7 3 10 3 6 8 22 
2 0 2 2 /1 2 /, 210  
1 2 0 0 0 0 22  13  
I. 0 0 1 0 0 0 0 0 0 0  
\] 0 0 0 0 0 0 0 0 0 0 2  
rl'able 2: Nmnber of candidate, and location of head 
January part of the Kyoto COl'pus, wlfich is a 
hand created cort)llS tagged.with POS, tmnsetsu 
and dependency relationships (l{urohashi, Na- 
gao, 1997). It is the same 1)ortion used as tile 
training data of the system described late, r. 'J~he 
nmnbc, r of cmMidates is shown vertically and 
the location of the head is shown horizontally. 
l;br exalnple, 2244 in the fom'th row and the 
se(:Olid (-ohmn~ means that there are 2244 bun- 
setsu which have 4 head Calldidates and the 2nd 
fl'om the left is the head of the bunsetsu. 
We can observe that the. data. is very biased. 
The head location is very lilnited. 98.7% of 
instances are covered 1)y the first 4 candidates 
and the last candidate, combined. In the table, 
the data which are not covered by the criteria 
are shown in italics. From this obserw~tion, we 
come to the key idea. We restrict he head can- 
didate locations to consider, mid we remember 
all patterns of categories at the limited loca- 
tions. For each remembered lmttern, we also 
remember where the. head (answer) is. This 
strategy will give us a constant ime select;ion 
of the head. For example, assmne, at ~ certain 
lmnsetsu in the training data, there are five Call- 
didates. Then we. will remember the categories 
of the current lmnsetsu, st\y "A", and five can- 
didates, "B C 1) E F", as wall as the head loca- 
tion, for example "2nd". At the analysis phase, 
if we encounter the same sittmtion, i.e. the same 
category bmlsetsu "A" to be mmlyzed, and the 
same categories of five candidates in the same 
order "13 C D E F", then we can just return tile 
remenfl)ered head location, "2nd". This process 
can be done in constant tilne and eventually, the 
analysis of a sentence can 1)e done in time 1)ro- 
portional to the sentence length. 
For the iml)lementation, we used a DFST in 
which each state represents the patterll of the 
candidates, the input of an edge is the category 
of the. l)llnsetsll 1)eing analyzed and the output 
of a.n edge. is the location of hea(t. 
4: Imp lementat ion  
We still have several 1)rol)lelns whi('h have to 
be solved in order to iint)lement the idea. As- 
SUlning the number of candidates to be 5, the 
t)roblelns are to 
(l) define the c~tegories of head bunsetsu can- 
didates, 
(2) limit the nunlber of patterns (the lmmber 
of states in DFST) to a mallageable range, 
because the Colnbilmtion of five categories 
could t)e huge 
(3) define the categories of intmt bunsetsus, 
(4) deal with unseen events (sparseness l)rob- 
lem). 
hi this section, we l)resent how we imple- 
mented the, system, wlfich at the stone tinle 
763 
shows the solution to the problems. At the end, 
we implemented the systeln in a very small size; 
1200 lines of C progrmn, 188KB data file and 
less than 1MB processing size. 
Structure of DFST 
For the categories of head bunsetsu candi- 
dates, we used JUMAN's POS categories as the 
basis and optimized them using held-out data. 
JUMAN has 42 parts-of-speech (POS) including 
the minor level POSs, and we used the POS of 
the head word of a candidate bunsetsu. We also 
used the information of whether the bunsetsu 
has a colnma or not. The nulnber of categories 
1)ecomes 18 after tuning it using the held-out 
data. 
The input of all edge is the information about 
the 1)unsetsu currently being analyzed. We 
used the inforination of the tail of the bunsetsu 
(mostly function words), and it becomes 40 cat- 
egories according to the stone tuning process. 
The output of an edge is the information of 
which candidate is the head. It is simply a nun> 
ber from 1 to 5. The training data contains 
examples which represent he same state and 
input, but different output. This is due to the 
rough definition of the categories or inherent im- 
possibility of to disambiguating the dependency 
relationship fi'om the given infbrmation only. In 
such cases, we pick the one which is the most 
frequent as the answer in that situation. We will 
discuss the problems caused by this strategy. 
Data size 
Based on the design described above, the 
number of states (or number of patterns) is 
1,889,568 (18 ~) and the number of edges is 
75,582,720 as each state has 40 outgoing edges. 
If we implement it as is, we may need several 
hundred megabytes of data. In order to keep 
it fast, all the data should be in memory, and 
the current memory requirement is too large to 
implement. 
To solve the problem, two ideas are employed. 
First, the states are represented by the combi- 
nation of the candidate categories, i.e. states 
can be located by the 5 candidate categories. 
So, once it transfers from one state to another, 
the new state can be identified from the previ- 
ous state, input bunsetsu and the output of the 
edge. Using this operation, the new state does 
not need to be remembered for each edge and 
this leads to a large data reduction. Second, we 
introduced the default dependency relationship. 
In the Japanese dependency relationship, 64% 
of bunsetsu debend on the next bunsetsu. So 
if this is the output, we don't record the edge 
as it is the default. In other words, if there is 
no edge in~brmation for a particular input at a 
particular state, the outtmt is the next bunsetsu 
(or 1). This gave unexpectedly a large benefit. 
For unseen events, it is reasonable to guess that 
the bunsetsu depends the next bunsetsu. Be- 
cause of the default, we don't have to keep such 
information. Actually the majority (more than 
99%) of states and edges are unseen events, so 
the default strategy helps a lot to reduce the 
data size. 
By the combination of the two ideas, there 
could be a state whidl has absolutely no in- 
formation. If this kind of state is reached in 
the DFST, the output for any input is the next 
bunsetsu and the next state can be calculated 
froln the information you have. In fact, we 
have a lot of states with absolutely no infor- 
mation. Before implementing the supplemen- 
tation, explained in the next section, the num- 
ber of recorded states is only 1,006 and there 
are 1,554 edges (among 1,889,568 possible states 
and 75,582,720 possible edges). After imple- 
menting the supplementation, westill have only 
10,457 states and 31.,316 edges. The data sizes 
(file sizes) are about 15KB and 188KB, respec- 
tively. 
Sparseness problem 
Tile amount of training data is about 8,000 
sentences. As the average number of bunsetsu 
in a sentence is 10, there are about 72,000 data 
points in the training data. This number is very 
much smaller than the nmnber of possible states 
(1,889,568), and it seems obvious that we will 
have a sparseness problem 2. 
In order to supplement the unseen events, 
we use the system developed by Udfimoto 
et.al (Uchimoto et al, 2000). A large cor- 
pus is parsed by the analyzer, and the results 
2However, we can make the system by using the de- 
fault strategy mM surprisingly the accuracy of the sys- 
tem is not so bad. This will be reported in the Experi- 
ment section 
764 
are added to the training cori)us. In prac- 
tice, we parsed two years of lmwspaper articles 
(2,536,229 sentences ofMainichi Shinbun 94 and 
95, excluding Jalmary 1-10, 95, as these are used 
in the Kyoto corpus). 
5 Exper iment  
In this section, we will report the experilnent. 
~?\r(, used the Kyoto corl)us (ve.rsion 2). The 
training is done using January 1-8, 95 (7,960 
sentences), the test is done using Jmmary 9, 
95 (1,246 sentences) mid the parameter tuning 
is done using Jmmary 10, 95 (1,519 sentences; 
held-out data). The input sentences to the sys- 
rein are morl)hologically analyzed and bunsetsu 
nre detected correctly. 
Dependency Accuracy 
Table 3 shows the accuracy of the systems. 
The 'dependency accuracy' metals the percent- 
age of the tmnsetsus whose head is correctly 
mmlyzed. The bmlsetsus are all but the last 
bunsetsu of the sentence, as the last 1)unsetsu 
has no head. The 'default nlethod' is the sys- 
rein in which the all bunsetsus are supt)osed 
to dei)end on the next bunsetsu. ~t'he 'base- 
line reel;hod' is a quite siml)le rule-based sys- 
toni which was created by hand an(t has a\])out 
30 rules. 'l'he details of the system are rel)orted 
in (Uchimoto et al, 1999). The 'Uchimoto's 
system' is tit(; system rel)orted in (Uchimoto ct 
al., 2000). They used the same training data 
m~d test (lata. The del)endency accuracies of 
System Det)en(tency 
Accuracy 
Our system (with supp.) 81.231% 
Our systenl (without sut)p. ) 77.972 % 
Default method (i4.14: % 
Baseline method 72.57 % 
Uchimoto's ystem 87.93 % 
Table 3: Dependency Accuracy 
our systems are 81% with supl)lenlentation a d 
78% without supt)lementation. The result is 
about 17% and 9% better than tile default mid 
the baseline nlethods respectively. Compared 
with Uchimoto's ystem, we are about 7% be- 
hind. But as Uchimoto's ystem used about 
40,()00 features including lexical features, and 
they also introduced combined features (up to 
5), it is natural that our system, which uses only 
18 categories and only combinations of two cat- 
cgories, has less accuracy a.
Analysis speed 
The main objective of the system is speed. 
Table 4 shows the analysis speed on three di f  
fcrent platforms. On the fastest machine, it an- 
alyzes a sentence in 0.17 millisecond. Tat)le 5 
shows a comlmrisou of the analysis speed of 
three difl'erent systems on SPARC-I. Our system 
runs about 100 times faster than Uchimoto's 
system and KNP(Kurohashi, Nagao, 1994). 
Platform Analysis time 
(millisec./sent.) 
PentiulnIII 650MHz 0.17 
SmlEnterprise, 400MHz 0.29 
Ultra SPARC-I, \]70MHz 1.{)3 
Table 4: Analysis Speed 
System Analysis time 
(millisec./sent.) 
Our system 1.03 
Uchimoto's ystem 170 
KNP 86 
Tal)le 5: Coml)arison of Analysis Speed 
Figure 2 shows tile relationship between tile 
sentence length and tile analysis tinle. We used 
tile slowest nlachine (Ultra SPARC-I, 170MHz) 
in order to minimize obserw~tional errors. We 
cml clearly observe that the anMysis tinle is pro- 
portional to the sentence length, as was pre- 
dicted by the algorithm. 
The speed of tile training is slightly slower 
l:han that of tile analysis. The training on the 
smaller training data (about 8000 sentences) 
t;akes M)out 10 seconds on Ultra SPAR.C-I. 
aHowever, our system uses context information offly( ~, 
bunsctsus, which is not used in Uchimoto's system. 
765 
1.5 
1.0 
0.5 
0.0 
Analysis time 
(millisec.) 
1 I 
0 5 I'0 15 2'0 
Sentence length (Num.of bunsetsu) 
Figure 2: Analysis time and Sentence length 
6 Discuss ion 
6.1 The restriction 
People may strongly argue that the main I)rol> 
lenl of the system is the restriction of the head 
location. We eonlpletely agree. We restrict the 
candidate to five bunsetsus, as we described ear- 
lie:', and we thcoretically ignored 1.3% of accu- 
racy. Obviously, this restriction can be loosened 
by widelfing the range of the candidates. For ex- 
ample, 1.3% will be reduced to 0.5% if we take 
6 instead of 5 candidates, hnplenlentation of 
this change is easy. However, the. problen: lies 
with the sparseness. As shown in Table 2, there 
are fewer examples with a large number of bml- 
setsu candidates. For examI)le, there are only 4 
instances which haw~ 14 candidates. It may be 
impossible to accumulate nough training ex- 
mnples of these kinds, even if we use a lot of 
untagged text fbr the supplementation. In such 
cases, we believe, we should have other kinds 
of remedies. One of them is a generalization of 
a large number candidates to a smaller number 
candidates by deleting nniniportant bunsetsus. 
This remains for fl:ture research. 
We can easily imagine that other systems may 
not analyze accurately the cases where the cor- 
rect head is far fl'om the bunsetsu. For example, 
Uchimoto's ystem achieves an accuracy of 41% 
in the cases shown in italics in Table 2, which is 
much lower than 88%, the system's ow~rall ac- 
curacy. So, relative to the Uchimoto's system, 
the restriction caused a loss of accuracy of only 
0.5% (1.3% x 41%) instead of 1.3%. 
6.2 Accuracy 
There are several things to do in order to achieve 
better accuracy. One of the major things is to 
use the information which has not been used, 
but is known to be useful to decide dependency 
relationships. Because the accuracy of the sys- 
tem against the training data is only 81.653% 
(without supplementation), it, is clear that we 
miss some important information, 
We believe the lexical relationships in verb 
frmne element preference (VFEP) is one of the 
most important types of information. Analyz- 
ing the data, we can find evidence that such 
information is crucial. For exmnplc, there are 
236 exmnples in the training corpus where there 
are 4 head candidates and they are bunsetsus 
whose heads are noun, verb, noun and verb, and 
the current bunsetsu ends with a kaku- j  oshi ,  
a major particle. Out of the 236 exami)les, the 
nulnber of cases where the first, second, third 
and last candidate is the head are 60, 142, 3 and 
31, respectively. The answer is widely spread, 
and as the current system takes the most fl'e- 
quent head as the answer, 94 eases out of 236 
(40%) are ignored. This is due to the level of 
categorization which uses only POS informa- 
tion. Looking at the example sentences in this 
ease, we can observe that the VFEP could solve 
the problenl. 
It is not straightfbrward to add such lexieal 
information to the current franlework. If such 
information is incorporated into the state inibr- 
marion, the nmnber of states will become enor- 
mous. We can alternatively take an approach 
which was taken by augmented CFG. In this ap- 
proach, the lexical infbrmation will be referred 
to only when needed. Such a process m~\y slow 
down the analyzer, but since the nuinber of in- 
vocation of the process needed in a :~entence 
may be proportional to the sentence length, we 
believe the entire process may still operate in 
tiine proportional to the sentence length. 
7 Conc lus ion 
We proposed a Japmiese dependency analy- 
sis using a Deterministic Finite State Trans- 
ducer (DFST). The system analyzes a sentence 
with fairly good accuracy of about 81% in 0.17 
millisecond on average. It can be at)plied to 
a wide range of NLP applications, including 
real time Machine Translation, Information Ex- 
766 
traction and Speech Recognition. There are 
a number of eflbrts to improve the accuracy 
of Japanese dependency analysis (Haruno et.al, 
1997) (Fujio, Matsumoto, 1998) (Kanaymna 
et.al, 2000). In particular, it is interesting to see 
that Kanwmna's method uses a similar idea, 
limiting the head candidates, in order to im- 
prove the accuracy. We are plmming to incor- 
porate the fruit of these works to improve our 
accuracy. We believe our resem'ch is comple- 
mentary to these research. We also believe the 
method proposed in this l)aper ln~\y be applic> 
ble to other languages which share the chaiac- 
teristics explained earlier, for example, Korean, 
~lhrkish and others. 
8 Acknowledgment  
The eXl)eriments in this paper were conducted 
using the Kyoto corpus. We really appreciate 
their effort alld their kindness in making it pub- 
lit: domain. We thank l)r. Kuroluu~hi and other 
people who worked in the project. Mr.Uchimoto 
helped us to prepare the d~l;a and the experi- 
ment, mid we would like to thank him. 
References  
Masakazu Fujio, Yuuji Matsumoto. 1998 : 
"JaI)an(;se Dctmndency Strtlctlll'( ~,Analysis 
base(l on l,cxicalizcd Statistics", P'rocccding.s" 
o.f Th, i'rd Co~:/'('.rcnce onEMNLP 1)t)87-9(i 
I:liroshi Kanayama, Kcntaro '\]brisawa, Yutaka 
Mitsuishi, .Jml'ichi Tsujii. 2000 : '"A hybrid 
,lalmnese Parser with Hand-crafl;ed Grmnmar 
and Statistics", Proceedings o/" COLING-O0, 
this pr'oceedings 
Masahiko Haruno, Satoshi Shirai, Yoshiflmfi 
Ooymna. 1998 : "Using Decision ~l~'ees to 
Construct a Practical Parser", Procccding.s o\[ 
COLING-A CL-08 pp505-511 
Sadao Kurohashi, Makoto Nagao. 1994 : "KN 
Parser : Japanese Dependency/Case Struc- 
ture Analyzer", Pwcccdin.qs of the Wor'k- 
shop on Sharable Natural Language l~.c,sourccs 
pp48-55 
Sadao Kurohashi, Makoto Nagao. 1.997 : "Ky- 
oto University text corpus project", Proceed- 
ings of the ANLP-9Z Japan pp115-118 
Emmmmel R.oche. 1994 : "Two Parsing Algo- 
rithms by Means of Finite State Transduc- 
ers", Proceedings of COLING-9/j pp431-435, 
Satoshi Sekine, Kiyotaka Ucllimoto, Hitoshi Isa- 
hara. 2000 : "Statistical Dependency Anal- 
ysis using Backwm'd Beam Search", Proceed- 
ings of COLING-O0, this proceedings 
Satoshi Shirai. 19!18 : "Heuristics and its lim- 
itation", ,JouTvtal of the ANLP, Japan Vol.5 
No.l, t)1)1-2 
Kiyotaka Uchimoto, Satoshi Sekine, Hitoshi Isa- 
hara. 1999 : "Jal)anese Dependency Struc- 
tllre Analysis Based on Maximum Entropy 
Models", ,\]our'nal of Information Processing 
Society of Japan Vol.40, No.9, pp3397-3407 
Kiyotaka Uchimoto, Masaki Mm'ata, Satoshi 
Sekine, Hitoshi Isahara. 2000 : "Dependency 
Model Using l?osterior Context", Pr'oceedings 
of the IIYPT-O0 
767 
