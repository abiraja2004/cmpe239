Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 232?236,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Automatically Acquiring Fine-Grained
Information Status Distinctions in German
Aoife Cahill
Educational Testing Service,
660 Rosedale Road,
Princeton, NJ 08541, USA
acahill@ets.org
Arndt Riester
Institute for Natural Language Processing (IMS)
Pfaffenwaldring 5b
70569 Stuttgart, Germany
arndt.riester@ims.uni-stuttgart.de
Abstract
We present a model for automatically predict-
ing information status labels for German refer-
ring expressions. We train a CRF on manually
annotated phrases, and predict a fine-grained
set of labels. We achieve an accuracy score of
69.56% on our most detailed label set, 76.62%
when gold standard coreference is available.
1 Introduction
The automatic identification of information status
(Prince, 1981; 1992), i.e. categorizing discourse en-
tities into different classes on the given-new scale,
has recently been identified as an important issue in
natural language processing (Nissim, 2006; Rahman
and Ng, 2011; 2012). It is widely acknowledged that
information status and, more generally, information
structure,1 is reflected in word order, in the form of
referring expressions as well as in prosody. In com-
putational linguistics, the ability to automatically la-
bel text with information status, therefore, could be
of great benefit to many applications, including sur-
face realization, text-to-speech synthesis, anaphora
resolution, summarization, etc.
The task of automatically labeling text with infor-
mation status, however, is a difficult one. Part of
1Information structure is usually taken to describe clause-
internal divisions into focus-background, topic-comment, or
theme-rheme, which are in turn defined in terms of contex-
tual factors such as given-new information, salience, contrast
and alternatives, cf. Steedman and Kruijff-Korbayova? (2003),
Krifka (2007). Information status is the subfield of information
structure which exclusively deals with the given-new distinction
and which is normally confined to referring expressions.
the difficulty arises from the fact that, to a certain
degree, such labeling requires world knowledge and
semantic comprehension of the text, but another ob-
stacle is simply that theoretical notions of informa-
tion status are not used consistently in the literature.
In this paper we outline a system, trained on a
small amount of data, that achieves encouraging
results on the task of automatically labeling tran-
scribed German radio news data with fine-grained
information status labels.
2 Learning information status
A simpler variant of the task is anaphoricity de-
tection (discourse-new detection) (Bean and Riloff,
1999; Ng and Cardie, 2002; Uryupina, 2003; Denis
and Baldridge, 2007; Zhou and Kong, 2011), which
divides discourse entities into anaphoric (given) and
new. Identifying discourse-new expressions in texts
is helpful as a precursor to coreference resolution,
since, by definition, there is no need to identify an-
tecedents for new entities.
In the linguistic literature, referring expressions
have been distinguished in much more detail, and
there is reason to believe that this could also provide
useful information for NLP applications. Nissim
(2006) and Rahman and Ng (2011) developed meth-
ods to automatically identify three different classes:
OLD, MEDIATED and NEW expressions. This classi-
fication, which is described in Nissim et al. (2004),
has been used for annotating the Switchboard dialog
corpus (Calhoun et al., 2010), on which both studies
are based. Most recently, Rahman and Ng (2012)
extend their automatic prediction system to a more
fine-grained set of 16 subtypes.
232
Old. The class of OLD entities in Nissim et al.
(2004) is not limited to full-fledged anaphors like in
Example (1a) but also includes cases of generic and
first/second person pronouns like in (1b), which may
or may not possess a previous mention.
(1) a. Shares in General Electric rose as investors
bet that the US company would take more
lucrative engine orders for the A380.
b. I wonder where this comes from.
Mediated. The group of MEDIATED entities mainly
has two subtypes: (2a) shows an expression which
has not been mentioned before but which is depen-
dent on previous context. Such items have also been
called bridging anaphors (Poesio and Vieira, 1998).
(2b) contains a phrase which is generally known but
does not depend on the discourse context.
(2) a. Tomorrow, the Shenzhou 8 spacecraft will
be in a position to attempt the docking.
b. They hope that he will be given the right to
remain in the Netherlands.
New. The label NEW, following Nissim et al. (2004:
1024), applies ?to entities that have not yet been in-
troduced in the dialog and that the hearer cannot in-
fer from previously mentioned entities.?2 Two kinds
of expressions which fall into this category are unfa-
miliar definites (3a) and (specific) indefinites (3b).
(3) a. The man who shot a policeman yesterday
has not been caught yet.
b. Klose scored a penalty in the 80th minute.
Based on work described in Nissim (2006), Rahman
and Ng (2011) develop a machine learning approach
to information-status determination. They develop a
support vector machine (SVM) model from the an-
notated Switchboard dialogs in order to predict the
three possible classes. In an extension of this work,
Rahman and Ng (2012) compare a rule-based sys-
tem to a classifier with features based on the rules to
predict 16 subtypes of the three basic types. On this
extended label set on the dialog data, they achieve
accuracy of 86.4% with gold standard coreference
and 78.7% with automatically detected coreference.
3 Extending Information Status prediction
The work we present here is most similar to that
of Rahman and Ng (2012), however, our work dif-
2Note that this definition fails to exclude cases like (2b).
fers from theirs in a number of important respects.
We (i) experiment with a different information status
classification, derived from Riester et al. (2010), (ii)
use (morpho-)syntactic and functional features auto-
matically extracted from a deep linguistic parser in
our CRF sequence model, (iii) test our approach on
a different language (German), (iv) show that high
accuracy can be achieved with a limited number of
training examples, and (v) that the approach works
on a different genre (transcribed radio news bulletins
which contain complex embedded phrases like an
offer to the minority Tamil population of Sri Lanka,
not typically found in spoken dialog).
The annotation scheme by Riester et al. (2010)
divides referring items differently to Nissim et al.
(2004). Arguments are provided in the former pa-
per and in Baumann and Riester (to appear). As it
stands, the scheme provides too many labels for our
purpose. As a compromise, we group them in seven
classes: GIVEN, SITUATIVE, BRIDGING, UNUSED,
NEW, GENERIC and EXPLETIVE.
Given. Givenness is a central notion in informa-
tion structure theory. Schwarzschild (1999) de-
fines givenness of individual-type entities in terms
of coreference. If desired, GIVEN items can be sub-
classified, e.g. whether they are pronouns or full
noun phrases, and whether the latter are repetitions
or short forms of earlier material, or whether they
consist of lexically new material (epithets).
Situative. 1st and 2nd person pronouns, locative and
temporal adverbials, usually count as deictic expres-
sions since they refer to elements in the utterance sit-
uation. We therefore count them as a separate class.
SITUATIVE entities may, but need not, corefer.
Bridging. Bridging anaphors, as in (2a) above, have
received much attention, see e.g. Asher and Las-
carides (1998) or Poesio and Vieira (1998). Al-
though they are discourse-new, they share properties
with coreference anaphors since they depend on the
discourse context. They represent a class which can
be easily identified by human annotators but are dif-
ficult to capture by automatic techniques.
Unused. In manual annotation practice, it is very of-
ten impossible to decide whether an entity is hearer-
known, since this depends on who we assume the
hearer to be; and even if we agree on a recipient, we
may still be mistaken about their knowledge. For ex-
ample, Wolfgang Bosbach, deputy chairman of the
233
Countable Boolean Descriptive
# Words in phrase* Phrase contains a compound noun Adverbial type, e.g. locative
# Predicative phrases Phrase contains coordination Determiner type, e.g. definite *
# DPs and NPs in phrase Phrase contains time expression Left/Right-most POS tag of phrase
# top category children Phrase contains < 2, 5 or 10 words Highest syntactic node label
# Labels/titles Phrase does not have a complete parse that dominates the phrase
# Depth of syntactic phrase Phrase is a pronoun Grammatical function, e.g. SUBJ *
# Cardinal numbers Phrase contains more than 1 DP Type of pronoun, e.g. demonstrative
# Depth of syntactic phrase and 1 NP (i.e. phrase contains Syntactic shape, e.g. apposition with
ignoring unary branching an embedded argument) a determiner and attributive modifier
# Apposition phrases Head noun appears (partly or completely) Head noun type, e.g. common *
# Year phrases in previous 10 sentences * Head noun number, e.g. singular
Table 1: Features of the CRF prediction model (* indicates feature used in baseline model)
CDU parliamentary group may be known to parts
of a German audience but not to other people.
We address this by collecting both hearer-known
and hearer-unknown definite expressions into one
class UNUSED. This does not rule out further sub-
classification (known/unknown) or the possibility of
using machine learning techniques to identify this
distinction, see Nenkova et al. (2005). The fact that
Rahman and Ng (2011) report the highest confusion
rate between NEW and MEDIATED entities may have
its roots in this issue.
New. Only (specific) indefinites are labeled NEW.
Generic. An issue which is not dealt with in Nissim
et al. (2004) are GENERIC expressions as in Lions
have manes. Reiter and Frank (2010) discuss the
task of identifying generic items in a manner sim-
ilar to the learning tasks presented above, using a
Bayesian network. We believe it makes sense to in-
tegrate genericity detection into information-status
prediction.3
4 German data
Our work is based on the DIRNDL radio news cor-
pus of Eckart et al. (2012) which has been hand-
annotated with information status labels. We choose
a selection of 6668 annotated phrases (1420 sen-
tences). This is an order of magnitude smaller than
the annotated Switchboard corpus of Calhoun et al.
(2010). We parse each sentence with the German
Lexical Functional Grammar of Rohrer and Forst
(2006) using the XLE parser in order to automati-
3Note that in coreference annotation it is an open question
whether two identical generic terms should count as coreferent.
cally extract (morpho-)syntactic and functional fea-
tures for our model.
5 Prediction Model for Information Status
Cahill and Riester (2009) show that there are asym-
metries between pairs of information status labels
contained in sentences, i.e. certain classes of expres-
sions tend to precede certain other classes. We there-
fore treat the prediction of IS labels as a sequence
labeling task.4 We train a CRF using wapiti
(Lavergne et al., 2010), with the features outlined in
Table 1. We also include a basic ?coreference? fea-
ture, similar to the lexical features of Rahman and
Ng (2011), that fires if there is some lexical overlap
of nouns (or compound nouns) in the preceding 10
sentences. The original label set described in Riester
et al. (2010) contains 21 labels. Here we work with
a subset of maximally 12 labels, but also consider
smaller subsets of labels and carry out a mapping to
the Nissim (2006) label set (Table 2).5 We run a 10-
fold cross-validation experiment and report average
prediction accuracy. The results are given in Table
3a. As an informed baseline, we run the same cross-
validation experiment with a subset of features that
roughly correspond to the features of Nissim (2006).
Our models perform statistically significantly better
than the baseline (p < 0.001, using the approximate
randomization test) for all label sets.
4Preliminary experimental evidence showed that the CRF
performed slightly better than a simple multiclass logistic re-
gression model (e.g. compare 72.19 to 72.43 in Table 3a).
5Unfortunately, due to underlying theoretical differences, it
is impossible to map between the Riester label set and the ex-
tended label set used in Rahman and Ng (2012).
234
Total Riester 1 Riester 2 Riester 3 Nissim ?06
462 GIVEN- GIVEN-
GIVEN OLD
PRONOUN PRONOUN
143 GIVEN- GIVEN-REFLEXIVE REFLEXIVE
427 GIVEN-EPITHET
169 GIVEN- GIVEN-REPEATED NOUN
204 GIVEN-SHORT
265 SITUATIVE SITUATIVE SITUATIVE
449 BRIDGING BRIDGING BRIDGING
MEDIATED1271 UNUSED- UNUSED- UNUSEDKNOWN KNOWN
1227 UNUSED- UNUSED-
NEWUNKNOWN UNKNOWN1282 NEW NEW NEW
632 GENERIC GENERIC GENERIC
96 EXPLETIVE EXPLETIVE EXPLETIVE OTHER
Table 2: Varying the granularity of the label sets
As expected, the less fine-grained a label set, the
easier it is to predict the labels. It remains for fu-
ture work to show the effect of different label set
granularities in practical applications. We approx-
imate gold standard coreference information from
the manually annotated labels (e.g. all GIVEN la-
bel types are by their nature coreferent), and carry
out an experiment with gold-standard approximation
of coreference marking. These results are also re-
ported in Table 3a. Here we see a clear performance
difference in the effect of gold-standard corefer-
ence on the Riester label set (increasing around 6-
10%), compared to the Nissim label set (decreasing
slightly). This is an artifact of the way the mapping
was carried out, deriving the gold standard corefer-
ence information from the Riester label set. There is
not a one-to-one mapping between OLD and GIVEN,
and, in the Riester label set, coreferential entities
that are labeled as SITUATIVE (deictic terms) are not
recognized as such.
The feature set in Table 1 reflects the morpho-
syntactic properties of the phrases to be labeled.
Sometimes world knowledge is required in order
to be able to accurately predict a label; for exam-
ple, to know that the pope can be categorized as
UNUSED-KNOWN, because it can occur discourse-
initially, whereas the priest must usually be cate-
gorized as GIVEN. The BRIDGING relationship is
also difficult to capture without some world knowl-
edge. For example, to infer that the waitress can
be categorized as BRIDGING in the context of the
restaurant requires information that links the two
concepts. Rahman and Ng (2012) also note this and
include features based on FrameNet, WordNet and
the ReVerb corpus for English.
For German, we address this issue by introducing
two further types of features into our model based on
the GermaNet resource (Hamp and Feldweg, 1997).
The first type is based on the GermaNet synset of
the head noun in the phrase and its distance from the
root node (the assumption is that entities closer to
root are more generic than those further away). The
second include the sum and maximum of the Lin
semantic relatedness measures (Lin, 1998) of how
similar the head noun of the phrase is to the other
nouns in current and immediately preceding sen-
tence surrounding the phrase (calculated with Ger-
maNet Pathfinder; Finthammer and Cramer, 2008).
The results are given in Table 3b. Here we see a
consistent increase in performance of around 4% for
each label set over the model that does not include
the GermaNet features. Again, we see the same de-
crease in performance on the Nissim label set when
using gold standard coreference information.
Label Set Accuracy Gold Baseline
coref. feats.
Riester 1 65.49 72.49 57.25
Riester 2 67.21 76.88 58.82
Riester 3 72.43 82.22 64.20
Nissim ?06 76.24 74.06 71.70
(a) Only morpho-syntactic features
Label Set Accuracy Gold coreference
Riester 1 69.56 76.62
Riester 2 71.99 79.86
Riester 3 75.82 84.76
Nissim ?06 79.61 78.46
(b) Morpho-syntactic + GermaNet features
Table 3: Cross validation accuracy results
6 Conclusion
In this paper we presented a model for automatically
labeling German text with fine-grained information
status labels. The results reported here show that we
can achieve high accuracy prediction on a complex
text type (transcribed radio news), even with a lim-
ited amount of data.
235
References
