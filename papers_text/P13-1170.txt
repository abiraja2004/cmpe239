Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1733?1743,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Why-Question Answering
using Intra- and Inter-Sentential Causal Relations
Jong-Hoon Oh? Kentaro Torisawa? Chikara Hashimoto ? Motoki Sano?
Stijn De Saeger? Kiyonori Ohtake?
Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
{?rovellia,? torisawa,? ch,? msano,?stijn,?kiyonori.ohtake}@nict.go.jp
Abstract
In this paper, we explore the utility of
intra- and inter-sentential causal relations
between terms or clauses as evidence for
answering why-questions. To the best of
our knowledge, this is the first work that
uses both intra- and inter-sentential causal
relations for why-QA. We also propose
a method for assessing the appropriate-
ness of causal relations as answers to a
given question using the semantic orienta-
tion of excitation proposed by Hashimoto
et al (2012). By applying these ideas
to Japanese why-QA, we improved preci-
sion by 4.4% against all the questions in
our test set over the current state-of-the-
art system for Japanese why-QA. In addi-
tion, unlike the state-of-the-art system, our
system could achieve very high precision
(83.2%) for 25% of all the questions in the
test set by restricting its output to the con-
fident answers only.
1 Introduction
?Why-question answering? (why-QA) is a task to
retrieve answers from a given text archive for a
why-question, such as ?Why are tsunamis gen-
erated?? The answers are usually text fragments
consisting of one or more sentences. Although
much research exists on this task (Girju, 2003;
Higashinaka and Isozaki, 2008; Verberne et al,
2008; Verberne et al, 2011; Oh et al, 2012), its
performance remains much lower than that of the
state-of-the-art factoid QA systems, such as IBM?s
Watson (Ferrucci et al, 2010).
In this work, we propose a quite straightfor-
ward but novel approach for such difficult why-
QA task. Consider the sentence A1 in Table 1,
which represents the causal relation between the
cause, ?the ocean?s water mass ..., waves are gen-
A1 [Tsunamis that can cause large coastal inundation
are generated]effect because [the ocean?s water
mass is displaced and, much like throwing a stone
into a pond, waves are generated.]cause
A2 [Earthquake causes seismic waves which set up
the water in motion with a large force.]cause
This causes [a tsunami.]effect
A3 [Tsunamis]effect are caused by [the sudden dis-
placement of huge volumes of water.]cause
A4 [Tsunamis weaken as they pass through
forests]effect because [the hydraulic resistance of
the trees diminish their energy.]cause
A5 [Automakers in Japan suspended production for an
array of vehicles]effect because [the magnitude 9
earthquake and tsunami hit their country on Friday,
March 11, 2011.]cause
Table 1: Examples of intra/inter-sentential causal
relations. Cause and effect parts of each causal re-
lation, marked with [..]cause and [..]effect, are con-
nected by the underlined cue phrases for causality,
such as because, this causes, and are caused by.
erated,? and its effect, ?Tsunamis ... are gener-
ated.? This is a good answer to the question, ?Why
are tsunamis generated??, since the effect part is
more or less equivalent to the (propositional) con-
tent of the question. Our method finds text frag-
ments that include such causal relations with an
effect part that resembles a given question and pro-
vides them as answers.
Since this idea looks quite intuitive, many peo-
ple would probably consider it as a solution to
why-QA. However, to our surprise, we could not
find any previous work on why-QA that took this
approach. Some methods utilized the causal re-
lations between terms as evidence for finding an-
swers (i.e., matching a cause term with an answer
text and its effect term with a question) (Girju,
2003; Higashinaka and Isozaki, 2008). Other ap-
proaches utilized such clue terms for causality as
?because? as evidence for finding answers (Mu-
rata et al, 2007). However, these algorithms did
not check whether an answer candidate, i.e., a text
fragment that may be provided as an answer, ex-
plicitly contains a complex causal relation sen-
1733
tence with the effect part that resembles a ques-
tion. For example, A5 in Table 1 is an incorrect an-
swer to ?Why are tsunamis generated??, but these
previous approaches would probably choose it as a
proper answer due to ?because? and ?earthquake?
(i.e., a cause of tsunamis). At least in our exper-
imental setting, our approach outperformed these
simpler causality-based QA systems.
Perhaps this approach was previously deemed
infeasible due to two non-trivial technical chal-
lenges. The first challenge is to accurately iden-
tify a wide range of causal relations like those in
Table 1 in answer candidates. To meet this chal-
lenge, we developed a sequence labeling method
that identifies not only intra-sentential causal re-
lations, i.e., the causal relations between two
terms/phrases/clauses expressed in a single sen-
tence (e.g., A1 in Table 1), but also the inter-
sentential causal relations, which are the causal
relations between two terms/phrases/clauses ex-
pressed in two adjacent sentences (e.g., A2) in a
given text fragment.
The second challenge is assessing the appropri-
ateness of each identified causal relation as an an-
swer to a given question. This is important since
the causal relations identified in the answer candi-
dates may have nothing to do with a given ques-
tion. In this case, we have to reject these causal
relations because they are inappropriate as an an-
swer to the question. When a single answer candi-
date contains many causal relations, we also have
to select the appropriate ones. Consider the causal
relations in A1?A4. Those in A1?A3 are appro-
priate answers to ?Why are tsunamis generated??,
but not the one in A4. To assess the appropri-
ateness, the system must recognize textual entail-
ment, i.e., ?tsunamis (are) generated? in the ques-
tion is entailed by all ?tsunamis are generated? in
A1, ?cause a tsunami? in A2 and ?tsunamis are
caused? in A3 but not by ?tsunamis weaken? in
A4. This quite difficult task is currently being
studied by many researchers in the RTE field (An-
droutsopoulos and Malakasiotis, 2010; Dagan et
al., 2010; Shima et al, 2011; Bentivogli et al,
2011). To meet this challenge, we developed a
relatively simple method that can be seen as a
lightweight approximation for this difficult RTE
task, using excitation polarities (Hashimoto et al,
2012).
Through our experiments on Japanese why-QA,
we show that a combination of the above methods
can improve why-QA accuracy. In addition, our
proposed method can be successfully combined
with other approaches to why-QA and can con-
tribute to higher accuracy. As a final result, we im-
proved the precision by 4.4% against all the ques-
tions in our test set over the current state-of-the-art
system of Japanese why-QA (Oh et al, 2012). The
difference in the performance became much larger
when we only compared the highly confident an-
swers of each system. When we made our sys-
tem provide only its confident answers according
to their confidence score given by our system, the
precision of these confident answers was 83.2%
for 25% of all the questions in our test set. In the
same setting, the precision of the state-of-the-art
system (Oh et al, 2012) was only 62.4%.
2 Related Work
Although there were many previous works on the
acquisition of intra- and inter-sentential causal re-
lations from texts (Khoo et al, 2000; Girju, 2003;
Inui and Okumura, 2005; Chang and Choi, 2006;
Torisawa, 2006; Blanco et al, 2008; De Saeger et
al., 2009; De Saeger et al, 2011; Riaz and Girju,
2010; Do et al, 2011; Radinsky et al, 2012), their
application to why-QA was limited to causal re-
lations between terms (Girju, 2003; Higashinaka
and Isozaki, 2008).
As previous attempts to improve why-QA per-
formance, such semantic knowledge as Word-
Net synsets (Verberne et al, 2011), semantic
word classes (Oh et al, 2012), sentiment analy-
sis (Oh et al, 2012), and causal relations between
terms (Girju, 2003; Higashinaka and Isozaki,
2008) has been used. These previous studies took
basically bag-of-words approaches and used the
semantic knowledge to identify certain seman-
tic associations using terms and n-grams. On
the other hand, our method explicitly identifies
intra- and inter-sentential causal relations between
terms/phrases/clauses that have complex struc-
tures and uses the identified relations to answer
a why-question. In other words, our method
considers more complex linguistic structures than
those used in the previous studies. Note that our
method can complement the previous approaches.
Through our experiments, we showed that it is
possible to achieve a higher precision by combin-
ing our proposed method with bag-of-words ap-
proaches considering semantic word classes and
sentiment analysis in our previous work (Oh et al,
1734
Document	 ?retrieval	 ?from	 ?Japanese	 ?web	 ?texts	 ?
Answer	 ?candidate	 ?extrac?on	
Answer	 ?candidate	 ?extrac?on	 ?	 ?from	 ?the	 ?retrieved	 ?documents	 ?
Answer	 ?re-??ranker	 ?
Answer	 ?re-??ranking	
top-n answer	 ?candidates	 ?by	 ?answer	 ?re-??ranking	 ?
Training	 ?data	 ?for	 ?answer	 ?re-??ranking	 ?
Training	 ?data	 ?for	 ?causal	 ?rela?on	 ?recogni?on	 ?
Causal	 ?rela?on	 ?recogni?on	 ?model	 ?top-n answer	 ?candidates	 ?
training	 ?
training	 ?
Why-??ques?on	 ?
recogni?on	 ?	 ?
recogni?on	 ?	 ?
Figure 1: System architecture
2012).
3 System Architecture
We first describe the system architecture of
our QA system before describing our proposed
method. It is composed of two components: an-
swer candidate extraction and answer re-ranking
(Fig. 1). This architecture is basically the same as
that used in our previous work (Oh et al, 2012).
We extended our previous work by introducing
causal relations recognized from answer candi-
dates to the answer re-ranking. The features used
in our previous work are very different from those
in this work, and we found that combining both
improves accuracy.
Answer candidate extraction: In our previous
work, we implemented the method of Murata et
al. (2007) for our answer candidate extractor. We
retrieved documents from Japanese web texts us-
ing Boolean AND and OR queries generated from
the content words in why-questions. Then we ex-
tracted passages of five sentences from these re-
trieved documents and ranked them with the rank-
ing function proposed by Murata et al (2007).
This method ranks a passage higher when it con-
tains more query terms that are closer to each other
in the passage. We used a set of clue terms, includ-
ing the Japanese counterparts of cause and reason,
as query terms for the ranking. The top ranked
passages are regarded as answer candidates in the
answer re-ranking. See Murata et al (2007) for
more details.
Answer re-ranking: Re-ranking the answer
candidates is done by a supervised classifier
(SVMs) (Vapnik, 1995). In our previous work, we
employed three types of features for training the
re-ranker: morphosyntactic features (n-grams of
morphemes and syntactic dependency chains), se-
mantic word class features (semantic word classes
obtained by automatic word clustering (Kazama
and Torisawa, 2008)) and sentiment polarity fea-
tures (word and phrase polarities). Here, we used
semantic word classes and sentiment polarities for
identifying such semantic associations between a
why-question and its answer as ?if a disease?s
name appears in a question, then answers that in-
clude nutrient names are more likely to be correct?
by semantic word classes and ?if something un-
desirable happens, the reason is often also some-
thing undesirable? by sentiment polarities. In this
work, we propose causal relation features gener-
ated from intra- and inter-sentential causal rela-
tions in answer candidates and use them along
with the features proposed in our previous work
for training our re-ranker.
4 Causal Relations for Why-QA
We describe causal relation recognition in Sec-
tion 4.1 and describe the features (of our re-ranker)
generated from causal relations in Section 4.2.
4.1 Causal Relation Recognition
We restrict causal relations to those expressed by
such cue phrases for causality as (the Japanese
counterparts of) because and as a result like in
the previous work (Khoo et al, 2000; Inui and
Okumura, 2005) and recognize them in the fol-
lowing two steps: extracting causal relation candi-
dates and recognizing causal relations from these
candidates.
4.1.1 Extracting Causal Relation Candidates
We identify cue phrases for causality in answer
candidates using the regular expressions in Ta-
ble 2. Then, for each identified cue phrase, we
extract three sentences as a causal relation candi-
date, where one contains the cue phrase and the
other two are the previous and next sentences in
the answer candidates. When there is more than
one cue phrase in an answer candidate, we use
all of them for extracting the causal relation can-
didates, assuming that each of the cue phrases is
linked to different causal relations. We call a cue
phrase used for extracting a causal relation candi-
date a c-marker (causality marker) of the candi-
date to distinguish it from the other cue phrases in
the same causal relation candidate.
1735
Regular expressions Examples
(D|?)? ?? P? ?? (for),??? (for),????
(as a result),???? (for)
?? ?? (since or because of)
?? (??|?) ???? (from the fact that),??
? (by the fact that)
(??|??) C ??? (because),??? (It is be-
cause)
D? RCT (P|C)+ ??? (the reason is), ???
(is the cause),?????? (from
this reason)
Table 2: Regular expressions for identifying cue
phrases for causality. D, P and C represent
demonstratives (e.g., ?? (this) and ?? (that)),
postpositions (including case markers such as ?
(nominative), ? (genitive)), and copula (e.g., ?
? (is) and ??? (is)) in Japanese, respectively.
RCT, which represents Japanese terms meaning
reason, cause, or thanks to, is defined as fol-
lows: RCT = {?? (reason), ?? (cause), ?
? (cause), ??? (cause), ??? (thanks to),
?? (thanks to),?? (reason) }.
4.1.2 Recognizing Causal Relations
Next, we recognize the spans of the cause and ef-
fect parts of a causal relation linked to a c-marker.
We regard this task as a sequence labeling problem
and use Conditional Random Fields (CRFs) (Laf-
ferty et al, 2001) as a machine learning frame-
work. In our task, CRFs take three sentences
of a causal relation candidate as input and gen-
erate their cause-effect annotations with a set of
possible cause-effect IOB labels, including Begin-
Cause (B-C), Inside-Cause (I-C), Begin-Effect (B-
E), Inside-Effect (I-E), and Outside (O). Fig 2
shows an example of such sequence labeling. Al-
though this example is about sequential labeling
shown on English sentences for ease of explana-
tion, it was actually done on Japanese sentences.
We used the three types of feature sets in Table 3
for training the CRFs, where j is in the range of
i? 4 ? j ? i+4 for current position i in a causal
relation candidate.
Type Features
Morphological feature mj , mj+1j , posj , posj+1j
Syntactic feature sj , sj+1j , bj , bj+1j
C-marker feature (mj , cm), (mj+1j , cm)
(sj , cm), (sj+1j , cm)
Table 3: Features for training CRFs, where
xj+1j = xjxj+1
Morphological features: mj and posj in Ta-
ble 3 represent the jth morpheme and the POS tag.
S1:	 ?Earthquake	 ?causes	 ?seismic	 ?waves	 ?which	 ?set	 ?up	 ?the	 ?water	 ?in	 ?mo?on	 ?with	 ?a	 ?large	 ?force.	 ?EOS	 ?S2:	 ?This	 ?causes	 ?a	 ?tsunami.	 ?EOS	 ?S3:	 ?EOA	 ?
S1	 ? Earthquake	 ? causes	 ? ?	 ? with	 ? a	 ? large	 ? force	 ? .	 ? EOS	 ?IOB	 ? B-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? I-??C	 ? O	 ?S2	 ? This	 ? causes	 ? a	 ? tsunami	 ? .	 ? EOS	 ?IOB	 ? O	 ? O	 ? B-??E	 ? I-??E	 ? I-??E	 ? O	 ?S3	 ? EOA	 ?IOB	 ? O	 ?
CRFs	 ?
A	 ?causal	 ?rela?on	 ?candidate	 ?from	 ?A2	 ?
Figure 2: Recognizing causal relations by se-
quence labeling: Underlined text This causes rep-
resents a c-marker, and EOS and EOA represent
end-of-sentence and end-of-answer candidates.
??	 ? ??	 ? ???	 ? ???	 ? ???????	 ? ???	 ? ??	 ? ????????	 ?
water	 ? ice	 ? if	 ?(it)	 ?becomes	 ? its	 ?volume	 ? because	 ?(it)	 ?	 ?increases	 ? an	 ?iceberg	 ? water	 ? float	 ?on	 ?(water)	 ?
Subtree	 ?informa?on	 ?used	 ?for	 ?syntac?c	 ?features	 ?	 ?
subtree	 ? subtree	 ? child	 ? child	 ? c-??marker	 ? subtree-??of-??parent	 ?
subtree-??of-??parent	 ?
parent	 ?
???????	 ?
???	 ?
[??????????????]cause???[?????????????]effect	 ?(Because	 ?[the	 ?volume	 ?of	 ?the	 ?water	 ?increases	 ?if	 ?it	 ?becomes	 ?ice]cause,	 ?[an	 ?iceberg	 ?floats	 ?on	 ?water]effect.)	 ?
????????	 ?
root	 ?
c-??marker	 ?node	 ?
Figure 3: Example of syntactic information related
to a c-marker used for syntactic features
We use JUMAN1, a Japanese morphological ana-
lyzer, for generating our morphological features.
Syntactic features: The span of the causal rela-
tions in a given causal relation candidate strongly
depends on the c-marker in the candidate. Es-
pecially for intra-sentential causal relations, their
cause and effect parts often appear in the subtrees
of the c-marker?s node or those of the c-marker?s
parent node in a syntactic dependency tree struc-
ture. Fig. 3 shows an example that follows this ob-
servation, where the c-marker node is represented
in a hexagon and the other nodes are in a rectan-
gle. Note that each node in Fig. 3 is a word phrase
(called a bunsetsu), which is the smallest unit of
syntactic analysis in Japanese. A bunsetsu is a
syntactic constituent composed of a content word
and several function words such as postpositions
and case markers. Syntactic dependency is repre-
sented by an arrow in Fig. 3. For example, there
is syntactic dependency from word phrase ??
1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
1736
(water) to??? (if (it) becomes), i.e.,?? dep???
???. We encode this subtree information into
sj , which is the syntactic information of a word
phrase to which the jth morpheme belongs. sj
only has one of six values: 1) the c-marker?s node
(c-marker), 2) the c-marker?s child node (child),
3) the c-marker?s parent node (parent), 4) in the c-
marker?s subtree but not the c-marker?s child node
(subtree), 5) in the subtree of the c-marker?s par-
ent node but not the c-marker?s node (subtree-of-
parent) and 6) the others (others). bj is the word
phrase information of the jth morpheme (mj) that
represents whether mj is in the beginning or in-
side a word phrase. For generating our syntactic
features, we use KNP2, a Japanese syntactic de-
pendency parser.
C-marker features: As our c-marker features,
we use a pair composed of c-marker cm and one
of the following: mj , mj+1j , sj , or sj+1j .
4.2 Causal Relation Features
We use terms, partial trees (in a syntactic depen-
dency tree structure), and the semantic orienta-
tion of excitation (Hashimoto et al, 2012) to as-
sess the appropriateness of each causal relation ob-
tained by our causal relation recognizer as an an-
swer to a given question. Finding answers with
term matching and partial tree matching has been
used in the literature of question answering (Girju,
2003; Narayanan and Harabagiu, 2004; Moschitti
et al, 2007; Higashinaka and Isozaki, 2008; Ver-
berne et al, 2008; Surdeanu et al, 2011; Verberne
et al, 2011; Oh et al, 2012), while that with the
excitation polarity is proposed in this work.
We use three types of features. Each fea-
ture type expresses the causal relations in an an-
swer candidate that are determined to be appro-
priate as answers to a given question by term
matching (tf1?tf4), partial tree matching (pf1?
pf4) and excitation polarity matching (ef1?ef4).
We call these causal relations used for generating
our causal relation features candidates of an ap-
propriate causal relation in this section. Note that
if one answer candidate has more than one candi-
date of an appropriate causal relation found by one
matching method, we generated features for each
appropriate candidate and merged all of them for
the answer candidate.
2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
Type Description
tf1 word n-grams of causal relations
tf2 word class version of tf1
tf3 indicator for the existence of candidates of an
appropriate causal relation identified by term
matching in an answer candidate
tf4 number of matched terms in candidates of an ap-
propriate causal relation
pf1 syntactic dependency n-grams (n dependency
chain) of causal relations
pf2 word class version of pf1
pf3 indicator for the existence of candidates of an ap-
propriate causal relation identified by partial tree
matching in an answer candidate
pf4 number of matched partial trees in candidates of
an appropriate causal relation
ef1 types of noun-polarity pairs shared by causal re-
lations and the question
ef2 ef1 coupled with each noun?s word class
ef3 indicator for the existence of candidates of an ap-
propriate causal relation identified by excitation
polarity matching in an answer candidate
ef4 number of noun-polarity pairs shared by the
question and the candidates of an appropriate
causal relation
Table 4: Causal relation features: n in n-grams
is n = {2, 3} and n-grams in an effect part are
distinguished from those in a cause part.
4.2.1 Term Matching
Our term matching method judges that a causal re-
lation is a candidate of an appropriate causal rela-
tion if its effect part contains at least one content
word (nouns, verbs, and adjectives) in the ques-
tion. For example, all the causal relations of A1?
A4 in Table 1 are candidates of an appropriate
causal relation to the question, ?Why is a tsunami
generated??, by term matching with question term
tsunami.
tf1?tf4 are generated from candidates of an ap-
propriate causal relation identified by term match-
ing. The n-grams of tf1 and tf2 are restricted
to those containing at least one content word in
a question. We distinguish this matched word
from the other words by replacing it with QW, a
special symbol representing a word in the ques-
tion. For example, word 3-gram ?this/cause/QW?
is extracted from This causes tsunamis in A2 for
?Why is a tsunami generated?? Further, we cre-
ate a word class version of word n-grams by con-
verting the words in these word n-grams into their
corresponding word class using the semantic word
classes (500 classes for 5.5 million nouns) from
our previous work (Oh et al, 2012). These word
classes were created by applying the automatic
word clustering method of Kazama and Torisawa
(2008) to 600 million Japanese web pages. For
example, the word class version of word 3-gram
1737
?this/cause/QW? is ?this/cause/QW,WCtsunami?,
where WCtsunami represents the word class of
a tsunami. tf3 is a binary feature that indi-
cates the existence of candidates of an appropri-
ate causal relation identified by term matching in
an answer candidate. tf4 represents the degree
of the relevance of the candidates of an appro-
priate causal relation measured by the number of
matched terms: one, two, and more than two.
4.2.2 Partial Tree Matching
Our partial tree matching method judges a causal
relation as a candidate of an appropriate causal re-
lation if its effect part contains at least one par-
tial tree in a question, where the partial tree covers
more than one content word. For example, only
the causal relation A1 among A1?A4 is a can-
didate of an appropriate causal relation for ques-
tion ?Why are tsunamis generated?? by partial
tree matching because only its effect part contains
partial tree ?tsunamis dep??? (are) generated? of the
question.
pf1?pf4 are generated from candidates of an
appropriate causal relation identified by the par-
tial tree matching. The syntactic dependency n-
grams in pf1 and pf2 are restricted to those that
contain at least one content word in a question. We
distinguish this matched content word from the
other content words in the n-gram by converting
it to QW, which represents a content word in the
question. For example, syntactic dependency 2-
gram ?QW dep??? cause? and its word class version
?QW,WCtsunami dep??? cause? are extracted from
Tsunamis that can cause in A1. pf3 is a binary
feature that indicates whether an answer candidate
contains candidates of an appropriate causal rela-
tion identified by partial tree matching. pf4 rep-
resents the degree of the relevance of the candi-
date of an appropriate causal relation measured by
the number of matched partial trees: one, two, and
more than two.
4.2.3 Excitation Polarity Matching
Hashimoto et al (2012) proposed a semantic ori-
entation called excitation polarities. It classifies
predicates with their argument position (called
templates) into excitatory, inhibitory and neu-
tral. In the following, we denote a template
as ?[argument position,predicate].? According to
Hashimoto?s definition, excitatory templates im-
ply that the function, effect, purpose, or the role of
an entity filling an argument position in the tem-
plates is activated/enhanced. On the contrary, in-
hibitory templates imply that the effect, purpose
or the role of an entity is deactivated/suppressed.
Neutral templates are those that neither activate
nor suppress the function of an argument.
We assume that the meanings of a text can
be roughly captured by checking whether each
noun in the text is activated or suppressed in the
sense of the excitation polarity framework, where
the activation and suppression of each entity (or
noun) can be detected by looking at the excita-
tion polarities of the templates that are filled by
the entity. For instance, effect part ?tsunamis
that can cause large coastal inundation are gen-
erated? of A1 roughly means that ?tsunamis? are
activated and ?inundation? is (or can be) acti-
vated. This activation/suppression configuration
of the nouns is consistent with sentence ?tsunamis
are caused? in which ?tsunamis? are activated.
This consistency suggests that A1 is a good an-
swer to question ?Why are tsunamis caused??, al-
though the ?tsunamis? are modified by different
predicates; ?cause? and ?generate.? On the other
hand, effect part ?tsunamis weaken as they pass
through forests? of A4 implies that ?tsunamis?
are suppressed. This suggests that A4 is not
a good answer to ?Why are tsunamis caused??
Note that the consistency checking between ac-
tivation/suppression configurations of nouns3 in
texts can be seen as a rough but lightweight ap-
proximation of the recognition of textual entail-
ments or paraphrases.
Following the definition of excitation polarity
in Hashimoto et al (2012), we manually classi-
fied templates4 to each polarity type and obtained
8,464 excitatory templates, such as [?, ???]
([subject, increase]) and [?, ????] ([sub-
ject, improve]), 2,262 inhibitory templates, such
as [?, ??] ([object, prevent]) and [?, ??]
([subject, die]), and 7,230 neutral templates such
as [?, ???] ([object, consider]). With these
templates, we obtain activation/suppression con-
figurations (including neutral) for the nouns in the
causal relations in the answer candidates and ques-
3 Because the activation/suppression configurations of
nouns come from an excitation polarity of templates, ?[argu-
ment position,predicate],? the semantics of verbs in the tem-
plates are implicitly considered in this consistency checking.
4 Varga et al (2013) has used the same templates as ours,
except they restricted their excitation/inhibitory templates to
those whose polarity is consistent with that given by the au-
tomatic acquisition method of Hashimoto et al (2012).
1738
tions.
Next, we assume that a causal relation is ap-
propriate as an answer to a question if the effect
part of the causal relation and the question share
at least one common noun with the same polarity.
More detailed information concerning the config-
urations of all the nouns in all the candidates of an
appropriate causal relation (including their cause
parts) and the question are encoded into our fea-
ture set ef1?ef4 in Table 4 and the final judgment
is done by our re-ranker.
For generating ef1 and ef2, we classified all the
nouns coupled with activation/suppression/neutral
polarities in a causal relation into three types:
SAME (the question contains the same noun with
the same polarity), DiffPOL (the question con-
tains the same noun with different polarity), and
OTHER (the others). ef1 indicates whether each
type of noun-polarity pair exists in a causal rela-
tion. Note that the types for the effect and cause
parts are represented in distinct features. ef2 is the
same as ef1 except that the types are augmented
with the word classes of the corresponding nouns.
In other words, ef2 indicates whether each type
of noun-polarity pair exists in the causal relation
for each word class. ef3 indicates the existence of
candidates of an appropriate causal relation iden-
tified by this matching scheme, and ef4 repre-
sents the number of noun-polarity pairs shared by
the question and the candidates of an appropriate
causal relations (one, two, and more than two).
5 Experiments
We experimented with causal relation recognition
and why-QA with our causal relation features.
5.1 Data Set for Why-Question Answering
For our experiments, we used the same why-QA
data set as the one used in our previous work (Oh
et al, 2012). This why-QA data set is composed
of 850 Japanese why-questions and their top-20
answer candidates obtained by answer candidate
extraction from 600 million Japanese web pages.
Three annotators checked the top-20 answer can-
didates of these 850 questions and the final judg-
ment was made by their majority vote. Their inter-
rater agreement by Fleiss? kappa reported in Oh et
al. (2012) was substantial (? = 0.634). Among the
850 questions, 250 why-questions were extracted
from the Japanese version of Yahoo! Answers,
and another 250 were created by annotators. In
our previous work, we evaluated the system with
these 500 questions and their answer candidates as
training and test data in 10-fold cross-validation.
The other 350 why-questions were manually built
from passages describing the causes or reasons of
events/phenomena. These questions and their an-
swer candidates were used as additional training
data for testing subsamples in each fold during the
10-fold cross-validation. In our why-QA experi-
ments, we evaluated our why-QA system with the
same settings.
5.2 Data Set for Causal Relation Recognition
We built a data set composed of manually anno-
tated causal relations for evaluating our causal re-
lation recognition. As source data for this data set,
we used the same 10-fold data that we used for
evaluating our why-QA (500 questions and their
answer candidates). We extracted the causal re-
lation candidates from the answer candidates in
each fold, and then our annotator (not an author)
manually marked the span of the cause and effect
parts of a causal relation for each causal relation
candidate, keeping in mind that the causal rela-
tion must be expressed in terms of a c-marker in
a given causal relation candidate. Finally, we had
a data set made of 16,051 causal relation candi-
dates, 8,117 of which had a true causal relation;
the number of intra- and inter-sentential causal re-
lations were 7,120 and 997, respectively.
Note that this data set can be partitioned into ten
folds by using the 10-fold partition of its source
data. We performed 10-fold cross validation to
evaluate our causal relation recognition with this
10-fold data.
5.3 Causal Relation Recognition
We used CRF++5 for training our causal relation
recognizer. In our evaluation, we judged a sys-
tem?s output as correct if both spans of the cause
and effect parts overlapped those in the gold stan-
dard. Evaluation was done by precision, recall,
and F1.
Precision Recall F1
BASELINE 41.9 61.0 49.7
INTRA-SENT 84.5 75.4 79.7
INTER-SENT 80.2 52.6 63.6
ALL 83.8 71.1 77.0
Table 5: Results of causal relation recognition (%)
Table 5 shows the result. BASELINE represents
5 http://code.google.com/p/crfpp/
1739
the result for our baseline system that recognizes
a causal relation by simply taking the two phrases
adjacent to a c-marker (i.e., before and after) as
cause and effect parts of the causal relation. We
assumed that the system had an oracle for judging
correctly whether each phrase is a cause part or an
effect part. In other words, we judged that a causal
relation recognized by BASELINE is correct if both
cause and effect parts in the gold standard are adja-
cent to a c-marker. INTRA-SENT and INTER-SENT
represent the results for intra- and inter-sentential
causal relations and ALL represents the result for
the both causal relations by our method. From
these results, we confirmed that our method rec-
ognized both intra- and inter-sentential causal rela-
tions with over 80% precision, and it significantly
outperformed our baseline system in both preci-
sion and recall rates.
Precision Recall F1
ALL-?MORPH? 80.8 66.4 72.9
ALL-?SYNTACTIC? 82.9 67.0 74.1
ALL-?C-MARKER? 76.3 51.4 61.4
ALL 83.8 71.1 77.0
Table 6: Ablation test results for causal relation
recognition (%)
We also investigated the contribution of the
three types of features used in our causal rela-
tion recognition to the performance. We evalu-
ated the performance when we removed one of
the three types of features (ALL-?MORPH?, ALL-
?SYNTACTIC? and ALL-?C-MARKER?) and com-
pared the results in these settings with the one
when all the feature sets were used (ALL). Ta-
ble 6 shows the result. We confirmed that all the
feature sets improved the performance, and we got
the best performance when using all of them. We
used the causal relations obtained from the 10-fold
cross validation for our why-QA experiments.
5.4 Why-Question Answering
We performed why-QA experiments to confirm
the effectiveness of intra- and inter-sentential
causal relations in a why-QA task. In
this experiment, we compared five systems:
four baseline systems (MURATA, OURCF, OH
and OH+PREVCF) and our proposed method
(PROPOSED).
MURATA corresponds to our answer candidate
extraction.
OURCF uses a re-ranker trained with only our
causal relation features.
OH, which represents our previous work (Oh et
al., 2012), has a re-ranker trained with mor-
phosyntactic, semantic word class, and senti-
ment polarity features.
OH+PREVCF is a system with a re-ranker
trained with the features used in OH and with
the causal relation feature proposed in Hi-
gashinaka and Isozaki (2008). The causal re-
lation feature includes an indicator that deter-
mines whether the causal relations between
two terms appear in a question-answer pair;
cause in an answer and its effect in a question.
We acquired the causal relation instances (be-
tween terms) from 600 million Japanese web
pages using the method of De Saeger et al
(2009) and exploited the top-100,000 causal
relation instances in this system.
PROPOSED has a re-ranker trained with our
causal relation features as well as the three
types of features proposed in Oh et al (2012).
Comparison between OH and PROPOSED re-
veals the contribution of our causal relation
features to why-QA.
We used TinySVM6 with a linear kernel
for training the re-rankers in OURCF, OH,
OH+PREVCF and PROPOSED. Evaluation was
done by P@1 (Precision of the top-answer) and
Mean Average Precision (MAP); they are the same
measures used in Oh et al (2012). P@1 measures
how many questions have a correct top-answer
candidate. MAP measures the overall quality of
the top-20 answer candidates. As mentioned in
Section 5.1, we used 10-fold cross-validation with
the same setting as the one used in Oh et al (2012)
for our experiments.
P@1 MAP
MURATA 22.2 27.0
OURCF 27.8 31.4
OH 37.4 39.1
OH+PREVCF 37.4 38.9
PROPOSED 41.8 41.0
Table 7: Why-QA results (%)
Table 7 shows the evaluation results. Our pro-
posed method outperformed the other four sys-
tems and improved P@1 by 4.4% over OH, which
is the-state-of-the-art system for Japanese why-
6 http://chasen.org/?taku/software/TinySVM/
1740
QA. OURCF showed the performance improve-
ment over MURATA. Although this suggests the
effectiveness of our causal relation features, the
overall performance of OURCF was lower than
that of OH. OH+PREVCF outperformed neither
OH nor PROPOSED. This suggests that our ap-
proach is more effective than previous causality-
based approaches (Girju, 2003; Higashinaka and
Isozaki, 2008), at least in our setting.
 20
 30
 40
 50
 60
 70
 80
 90
 100
 10  20  30  40  50  60  70  80  90  100
P
re
ci
si
on
 (%
)
% of questions
PROPOSED
OH
OurCF
Figure 4: Effect of causal relation features on the
top-answers
We also compared confident answers of
OURCF, OH, and PROPOSED by making each sys-
tem provide only the k confident top-answers (for
k questions) selected by their SVM scores given
by each system?s re-ranker. This reduces the num-
ber of questions that can be answered by a system,
but the top-answers become more reliable as k de-
creases. Fig. 4 shows this result, where the x axis
represents the percentage of questions (against all
the questions in our test set) whose top-answers
are given by each system, and the y axis repre-
sents the precision of the top-answers at a certain
point on the x axis. When both systems provided
top-answers for 25% of all the questions in our test
set, our method achieved 83.2% precision, which
is much higher than OH?s (62.4%). This exper-
iment confirmed that our causal relation features
were also effective in improving the quality of the
highly confident answers.
However, the high precision by our method was
bound to confident answers for a small number
of questions, and the difference in the precision
between OH and PROPOSED in Fig. 4 became
smaller as we considered more answers with lower
confidence. We think that one of the reasons is the
relatively small coverage of the excitation polarity
lexicon, a core resource in our excitation polarity
matching. We are planning to enlarge the lexicon
to deal with this problem.
Next, we investigated the contribution of the
intra- and inter-sentential causal relations to the
performance of our method. We used only one
of the two types of causal relations for generating
causal relation features (INTRA-SENT and INTER-
SENT) for training our re-ranker and compared the
results in these settings with the one when both
were used (ALL (PROPOSED)). Table 8 shows
the result. Both intra- and inter-sentential causal
relations contributed to the performance improve-
ment.
P@1 MAP
INTER-SENT 39.0 39.7
INTRA-SENT 40.4 40.5
ALL (PROPOSED) 41.8 41.0
Table 8: Results with/without intra- and inter-
sentential causal relations (%)
We also investigated the contributions of the
three types of causal relation features by ablation
tests (Table 9). When we do not use the fea-
tures by excitation polarity matching (ALL-{ef1?
ef4}), the performance is the worst. This implies
that the contribution of excitation polarity match-
ing exceeds the other two.
P@1 MAP
ALL-{tf1?tf4} 40.8 40.7
ALL-{pf1?pf4} 41.0 40.9
ALL-{ef1?ef4} 39.6 40.5
ALL (PROPOSED) 41.8 41.0
Table 9: Ablation test results for why-QA (%)
6 Conclusion
In this paper, we explored the utility of intra- and
inter-sentential causal relations for ranking answer
candidates to why-questions. We also proposed a
method for assessing the appropriateness of causal
relations as answers to a given question using the
semantic orientation of excitation. Through ex-
periments, we confirmed that these ideas are ef-
fective for improving why-QA, and our proposed
method achieved 41.8% P@1, which is 4.4% im-
provement over the current state-of-the-art system
of Japanese why-QA. We also showed that our
system achieved 83.2% precision for its confident
answers, when it only provided its confident an-
swers for 25% of all the questions in our test set.
1741
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence Re-
search (JAIR), 38(1):135?187.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang,
and Danilo Giampiccolo. 2011. The seventh pascal
recognizing textual entailment challenge. In Pro-
ceedings of TAC.
E. Blanco, N. Castell, and Dan I. Moldovan. 2008.
Causal relation extraction. In Proceedings of
LREC?08.
Du-Seong Chang and Key-Sun Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing and Manage-
ment, 42(3):662?678.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 16(1):1?17.
Stijn De Saeger, Kentaro Torisawa, Jun?ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proceedings of ICDM ?09, pages 764?769.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun?ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong Hoon Oh, Istv?n Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of EMNLP ?11,
pages 825?835.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of EMNLP ?11, pages 294?303.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: An overview of the
DeepQA project. AI Magazine, 31(3):59?79.
Roxana Girju. 2003. Automatic detection of causal
relations for question answering. In Proceedings of
the ACL 2003 workshop on Multilingual summariza-
tion and question answering, pages 76?83.
Chikara Hashimoto, Kentaro Torisawa, Stijn De
Saeger, Jong-Hoon Oh, and Jun?ichi Kazama. 2012.
Excitatory or inhibitory: A new semantic orientation
extracts contradiction and causality from the web. In
Proceedings of EMNLP-CoNLL ?12.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-
questions. In Proceedings of IJCNLP ?08, pages
418?425.
Takashi Inui and Manabu Okumura. 2005. Investigat-
ing the characteristics of causal relations in Japanese
text. In In Annual Meeting of the Association
for Computational Linguistics (ACL) Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.
Jun?ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings of ACL-08: HLT, pages 407?415.
Christopher S. G. Khoo, Syin Chan, and Yun Niu.
2000. Extracting causal knowledge from a medical
database using graphical patterns. In Proceedings of
ACL ?00, pages 336?343.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML ?01, pages
282?289.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings of ACL ?07,
pages 776?783.
Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A sys-
tem for answering non-factoid Japanese questions
by using passage retrieval weighted based on type
of answer. In Proceedings of NTCIR-6.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In Pro-
ceedings of COLING ?04, pages 693?701.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Jun?ichi Kazama,
and Yiou Wang. 2012. Why question answering
using sentiment analysis and word classes. In Pro-
ceedings of EMNLP-CoNLL ?12, pages 368?378.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for
news events prediction. In Proceedings of WWW
?12, pages 909?918.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contin-
gency relationships with no supervision. In ICSC
?10, pages 361?368.
Hideki Shima, Hiroshi Kanayama, Cheng wei Lee,
Chuan jie Lin, Teruko Mitamura, Yusuke Miyao,
Shuming Shi, and Koichi Takeda. 2011. Overview
of NTCIR-9 RITE: Recognizing Inference in TExt.
In Proceedings of NTCIR-9.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351?383.
1742
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of HLT-NAACL ?06, pages 57?64.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Istvan Varga, Motoki Sano, Kentaro Torisawa, Chikara
Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-
Hoon Oh, and Stijn De Saeger. 2013. Aid is out
there: Looking for help from tweets during a large
scale disaster. In Proceedings of ACL ?13.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2008. Using syntactic infor-
mation for improving why-question answering. In
Proceedings of COLING ?08, pages 953?960.
Suzan Verberne, Lou Boves, and Wessel Kraaij. 2011.
Bringing why-qa to web search. In Proceedings of
ECIR ?11, pages 491?496.
1743
