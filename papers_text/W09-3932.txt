Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 217?224,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Effects of Conversational Agents on Human Communication
in Thought-Evoking Multi-Party Dialogues
Kohji Dohsaka
NTT Communication Science Laboratories
NTT Corporation
2-4, Hikaridai, Seika-cho,
Kyoto 619-0237, Japan
Ryota Asai
Graduate School of
Information Science and Technology
Osaka University, 1-1 Yamadaoka,
Suita, Osaka 565-0871, Japan
Ryuichiro Higashinaka and Yasuhiro Minami and Eisaku Maeda
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Kyoto 619-0237, Japan
{dohsaka,rh,minami,maeda}@cslab.kecl.ntt.co.jp
Abstract
This paper presents an experimental
study that analyzes how conversational
agents activate human communication in
thought-evoking multi-party dialogues be-
tween multi-users and multi-agents. A
thought-evoking dialogue, which is a kind
of interaction in which agents act on user
willingness to provoke user thinking, has
the potential to stimulate multi-party in-
teraction. In this paper, we focus on
quiz-style multi-party dialogues between
two users and two agents as an example
of a thought-evoking multi-party dialogue.
The experiment results showed that the
presence of a peer agent significantly im-
proved user satisfaction and increased the
number of user utterances. We also found
that agent empathic expressions signifi-
cantly improved user satisfaction, raised
user ratings of a peer agent, and increased
user utterances. Our findings will be use-
ful for stimulating multi-party communi-
cation in various applications such as ed-
ucational agents and community facilita-
tors.
1 Introduction
Conversational interfaces including dialogue sys-
tems and conversational agents have been typi-
cally used as a single interface to a single user (Zue
et al, 1994; Allen et al, 2001; Cassell et al,
2000). On the other hand, a new area of re-
search in conversational interfaces is dealing with
multi-party interaction (Traum and Rickel, 2002;
Liu and Chee, 2004; Zheng et al, 2005). Multi-
party conversational interfaces have been applied
to such tasks as training decision-making in team
activities (Traum and Rickel, 2002), collabora-
tive learning (Liu and Chee, 2004), and coordinat-
ing and facilitating interaction in a casual social
group (Zheng et al, 2005).
The advantage of such multi-party dialogues
over two-party cases is that the multi-party case
encourages group interaction and collaboration
among human users. This advantage can be ex-
ploited to foster such human activities as student
learning in more social settings and to build and
maintain social relationships among people. How-
ever, unless users actively engage in the interac-
tion, these multi-party dialogue qualities cannot
be adequately exploited. Our objective is to stim-
ulate human communication in multi-party dia-
logues between multi-users and multi-agents by
raising user willingness to engage in the interac-
tion and increasing the number of user utterances.
As the first step toward this objective, we ex-
ploit a new style of dialogue called thought-
evoking dialogue and experimentally investigate
the impact of a peer agent?s presence and agent
emotional expressions on communication activa-
tion in thought-evoking multi-party dialogues. A
thought-evoking dialogue, an interaction in which
agents act on the willingness of users to provoke
user thinking and encourage involvement in the
dialogue, has the potential to activate interaction
among participants in multi-party dialogues.
Previous work proposed a quiz-style informa-
tion presentation dialogue system (hereafter quiz-
style dialogue system) (Higashinaka et al, 2007a)
that is regarded as a kind of thought-evoking di-
alogue system. This system conveys contents as
biographical facts of famous people through quiz-
style interaction with users by creating a ?Who
is this?? quiz and individually presenting hints.
217
The hints are automatically created from the bi-
ographical facts of people and ordered based on
the difficulty naming the people experienced by
the users (Higashinaka et al, 2007b). Since the
user has to consider the hints to come up with rea-
sonable answers, the system stimulates user think-
ing. This previous work reported that, for in-
teraction between a single user and a computer,
a quiz-style dialogue improved user understand-
ing and willingness to engage in the interaction.
In this paper, we focus on a quiz-style informa-
tion presentation multi-party dialogue (hereafter
quiz-style multi-party dialogue) as an example of
a thought-evoking multi-party dialogue.
A peer agent acts as a peer of the users and par-
ticipates in the interactions in the same way that
the users do. We are interested in the peer agent?s
role in quiz-style multi-party dialogues since the
positive effects of a peer agent on users have been
shown in the educational domain (Chou et al,
2003; Maldonado et al, 2005), which is a promis-
ing application area for quiz-style dialogues. In
the educational domain, a user could benefit not
only from direct communication with a peer agent
but also from overhearing dialogues between a
peer agent and a tutor. Learning by observing oth-
ers who are learning is called vicarious learning
and positively affects user performance (Craig et
al., 2000; Stenning et al, 1999). To the best of our
knowledge, detailed experimental investigations
on the effect of a peer agent on communication
activation have not been reported in multi-party
dialogues between multi-users and multi-agents,
which are our main concern in this paper.
The topic of emotion has gained widespread
attention in human-computer interaction (Bates,
1994; Picard, 1997; Hudlicka, 2003; Prendinger
and Ishizuka, 2004). The impact of an agent?s
emotional behaviors on users has also recently
been studied (Brave et al, 2005; Maldonado et
al., 2005; Prendinger et al, 2005). However, these
previous studies addressed scenario-based interac-
tion in which a user and an agent acted with prede-
termined timing. In this paper, we investigate the
impact of agent emotional expressions on users in
multi-party dialogues in which multiple users and
agents can make utterances with more flexible tim-
ing.
Resembling work by Brave et al (2005), we
classify agent emotional expressions into em-
pathic and self-oriented ones and investigate their
impact on users in a thought-evoking multi-party
dialogue system. As stated above, Brave et
al. (2005) addressed scenario-based Black-jack in-
teraction, but we deal with multi-party dialogues
that enable more flexible turn-taking. Previous
studies (Bickmore and Picard, 2005; Higashinaka
et al, 2008) showed that agent empathic expres-
sions have a positive psychological impact upon
users, but they only examined two-party cases.
Although Traum et al (2002) and Gebhard et
al. (2004) exploited the role of agent emotion in
multi-party dialogues, they did not adequately ex-
amine the effects of agent emotion on communi-
cation activation by experiment.
In this work, we deal with disembodied agents
and focus on their linguistic behaviors. We believe
that our results are useful for designing embodied
conversational agents using other modalities.
This paper presents an experimental study that
analyzes how agents stimulate human communi-
cation in quiz-style multi-party dialogues between
two users and two agents. We are especially inter-
ested in how the presence of a peer agent and agent
emotional expressions improve user satisfaction,
enhance user opinions about the peer agent, and
increase the number of user utterances. Our find-
ings will be useful for stimulating human com-
munication in various applications such as educa-
tional agents and community facilitators.
In the following, Section 2 shows an overview
of our quiz-style multi-party dialogue system.
Section 3 explains the experiment design, and Sec-
tion 4 describes the results. Section 5 concludes
the paper.
2 Thought-Evoking Multi-Party
Dialogue System
We implemented a quiz-style multi-party dialogue
system between multi-users and multi-agents. The
system is a Japanese keyboard-based dialogue sys-
tem with a chat-like interface. The users can make
utterances any time they want. A user utterance
is completed and displayed on the chat window
when the Enter key is pressed.
Our experiment dealt with cases where two
users and two agents engaged in a dialogue. The
two agents are a quizmaster and a peer. The quiz-
master agent creates a ?Who is this?? quiz about
a famous person and presents hints one by one to
the users and the peer agent who guess the correct
answer.
218
1 Whowho Who is this? First hint: Graduated
from the University of Tokyo.
2 Mary I don?t know.
3 Kinoko Yoshida Shigeru.
4 Whowho No, not even close!
5 Jane That?s very difficult.
6 Kinoko Difficult for me, too.
7 Whowho Second hint: Born in Ushigome,
Edo.
...
8 Whowho Third hint: Novelist and scholar of
British literature.
9 Mary Murakami Haruki.
10 Whowho Close!
11 Kinoko You are close. Excellent.
12 Jane Well then, who is it?
13 Whowho Fourth hint: Familiar with Haiku,
Chinese poetry, and calligraphy.
14 Mary Natsume Soseki.
15 Whowho That?s right. Wonderful.
16 Kinoko Mary, excellent. I?m happy for
you.
17 Jane Mary, that?s the right answer. Good
job.
Figure 1: Sample dialogue
Figure 1 shows a sample dialogue. Mary and
Jane are human users. Whowho is the quizmaster
agent, and Kinoko is the peer agent. Quizmaster
agent Whowho presents hints in lines 1, 7, 8, and
13. Users Mary and Jane and peer agent Kinoko
give answers in lines 3, 9, and 14.
The hints were automatically created using
biographical facts (in Japanese) of people in
Wikipedia 1 based on a previously reported
method (Higashinaka et al, 2007b).
2.1 Dialogue acts
The users and the two agents perform several dia-
logue acts based on the dialogue context.
Present-hint: The quizmaster agent presents
hints one by one (lines 1, 7, 8, and 13) in the
sample dialogue shown in Figure 1.
Give-ans: Users and the peer agent give answers
(lines 3, 9, and 14).
Show-difficulty: Users and the peer agent offer
opinions about the quiz difficulty (lines 2, 5,
6, and 12).
1http://ja.wikipedia.org/
Evaluate-ans: When the answer is wrong, the
quizmaster agent evaluates the answer based
on the person-name similarity score (Hi-
gashinaka et al, 2007a) and utters ?very
close!,? ?close!,? ?a little close!,? ?a little far,?
?far,? or ?not even close!? (lines 4 and 10).
Complete-quiz-with-success: When the right
answer is given, the quizmaster agent in-
forms the dialogue participants that the
current quiz is completed (line 15).
Complete-quiz-with-failure: If all hints have
been generated and no right answer is given,
the quizmaster agent gives the right answer,
and the current quiz is completed.
Feedback-on-wrong-ans: Users and the peer
agent give feedback when their own or the
other?s answers are wrong during the current
quiz (line 11).
Feedback-on-success: Users and the peer agent
give feedback when their own or the other?s
answers are right and the current quiz session
is completed (lines 16 and 17).
Feedback-on-failure: Users and the peer agent
give feedback when the current quiz is com-
pleted without the right answer.
Address-hearer: Users and the two agents spec-
ify an intended addressee by uttering the
other?s name (lines 16 and 17).
When a user utterance is input, the system sep-
arates it into word tokens using a Japanese mor-
phological analyzer and converts it into dialogue
acts using hand-crafted grammar. The system can
recognize 120,000 proper names of persons.
2.2 Utterance generation
Surface realization forms were prepared for each
dialogue act by the agents. Agent utterances are
generated by randomly selecting one of the forms.
Some agent dialogue acts can be generated
with emotional expressions. Agent emotional ex-
pressions are categorized into empathic and self-
oriented ones (Brave et al, 2005). The agent
self-oriented emotional expressions (self-oriented
expressions) are oriented to their own state, and
the agent empathic expressions are oriented to the
other?s state and are congruent with the other?s
219
Dialog act Emotion Expressions
Show-
difficulty
EMP Difficult for me, too.
Show-
difficulty
SELF I don?t remember.
That?s so frustrating.
Show-
difficulty
NONE I don?t know.
Feedback-
on-success
EMP You?re right. I?m
happy for you.
Feedback-
on-success
SELF I?m really glad I got
the correct answer.
Feedback-
on-success
NONE You?re right / I?m
right.
Feedback-
on-failure
EMP Too bad you didn?t
know the right an-
swer.
Feedback-
on-failure
SELF I?m disappointed
that I didn?t know
the right answer.
Feedback-
on-failure
NONE I/You didn?t know
the right answer.
Table 1: Examples of agent expressions. EMP
shows empathic expressions, SELF shows self-
oriented expressions, and NONE shows neutral
expressions when neither emotion is present.
welfare. As explained in 3.1, we prepared differ-
ent experimental conditions to determine the pres-
ence/absence of agent empathic and self-oriented
expressions. Based on the conditions, we con-
trolled the agent emotional expressions. Table 1
shows examples of agent empathic, self-oriented,
and neutral expressions.
2.3 Dialogue management
The system maintains a dialogue state in which
the history of the participant?s dialogue acts is
recorded with the time of each act. We prepared
preconditions of each dialogue act by the agents.
For example, the quizmaster agent?s Evaluate-
ans can be executed after the users or the peer
agent provides a wrong answer. The peer agent?s
Feedback-on-success can be executed after the
quizmaster agent performs Complete-quiz-with-
success. We also used the following turn-taking
rules:
1. Either agent must talk when neither the users
nor the agents make utterances within a given
time (4 sec.).
Condition Peer
agent
Empathic Self-
oriented
(0) Absent Absent Absent
(1) Present Absent Absent
(2) Present Present Absent
(3) Present Absent Present
(4) Present Present Present
Table 2: Experimental conditions based on pres-
ence/absence of peer agent and agent empathic
and self-oriented expressions
2. Agents must not talk for a given time (0.5
sec.) after the others talk.
3. The quizmaster agent must move to the next
hint when neither the users nor the peer agent
give a correct answer within a given time (30
sec.).
Based on the dialogue state, the preconditions
of the dialogue acts and the turn-taking rules, the
system chooses the next speaker and its dialogue
act.
3 Experiment
3.1 Experimental conditions
To evaluate the effects of the presence of the peer
agent and the agent emotional expressions, we
prepared five systems under different experimen-
tal conditions, (0), (1), (2), (3), and (4), based on
the presence/absence of the peer agent and agent
empathic and self-oriented expressions. They are
shown in Table 2. In condition (0), the peer agent
was absent, and only the quizmaster agent was
present. In other conditions, both the quizmas-
ter and peer agents were present. In conditions
(0) and (1), neither empathic nor self-oriented ex-
pressions were exhibited. In condition (2), only
empathic expressions were exhibited. In condition
(3), only self-oriented expressions were exhibited.
In condition (4), both empathic and self-oriented
expressions were exhibited.
We evaluated the effects of the presence of the
peer agent by comparing conditions (0) and (1).
We evaluated the effects of agent empathic and
self-oriented expressions by comparing conditions
(1), (2), (3), and (4).
3.2 Measures
We used three measures: user satisfaction, user
opinions about the peer agent, and the number of
220
Questionnaire items
Q1 Did you want to converse with this sys-
tem again? (Willingness to engage in di-
alogue)
Q2 Was the dialogue enjoyable? (Pleasant-
ness of dialogue)
Q3 Did you feel satisfied using the dialogue
system? (Satisfaction of system usage)
Q4 Was the peer agent friendly? (Agent?s
closeness)
Q5 Did you feel that the peer agent cared
about you? (Agent?s caring)
Q6 Was the peer agent likable? (Agent?s lik-
ability)
Q7 Did the peer agent support you?
(Agent?s support)
Table 3: Questionnaire items to evaluate user sat-
isfaction (Q1, Q2, and Q3) and user opinions
about the peer agent (Q4, Q5, Q6, and Q7)
user utterances. Among these measures, we re-
garded the number of user utterances as an ob-
jective measure to evaluate communication activa-
tion. User satisfaction and opinions about the peer
agent are subjective measures based on the ques-
tionnaires (ten-point Likert scale). Table 3 shows
the questionnaires used in the experiment. We ex-
pected that a high level of user satisfaction and
positive opinions about the peer agent would lead
to a high level of user engagement, which would
promote user utterances.
User satisfaction was evaluated from different
perspectives with three questions: Q1, Q2, and
Q3. Q1 focused on user willingness to engage in
the dialogue; Q2 focused on the user experience
of the dialogue?s pleasantness; Q3 focused on user
satisfaction with the system. We evaluated user
satisfaction with averages of the ratings of Q1, Q2,
and Q3. Using the averaged ratings of Likert ques-
tions allows us to apply such parametric statistical
tests as a multi-factor ANOVA since the summed
or averaged responses to Likert questions tend to
follow a normal distribution.
User opinions about the peer agent were evalu-
ated in terms of how the user perceived the peer
agent?s closeness (Q4), its caring (Q5), its likabil-
ity (Q6), and its support (Q7). We evaluated user
opinions about the peer agent with the averaged
ratings of these items. Previous studies showed
that empathic behaviors exhibited by an agent im-
proved user opinions about the agent in a Black-
jack scenario (Brave et al, 2005) and in a social
dialogue between a single user and an agent (Hi-
gashinaka et al, 2008). We examined these items
in multi-party dialogues with flexible turn-taking.
3.3 Procedure
We recruited and paid 64 Japanese adults (32
males and 32 females) for their participation. The
mean ages of the male and female groups were
32.0 and 36.2, respectively (male group: SD=9.2
, min=22, max=59, female group: SD=9.6,
min=20, max=50). The participants were divided
into 32 pairs of the same gender: 16 pairs of males
and 16 pairs of females. The participants in each
pair were unacquainted.
The experiment had a within-participants de-
sign. Each pair of participants successively en-
gaged in dialogues using the five systems under
different experimental conditions. The order of
using the systems was counter-balanced to prevent
order effect.
Before starting the experiment, the participants
were informed that, after completing a dialogue
with each system, they would fill out question-
naires. The questionnaires on user opinions about
the peer agent were used only when it was present
(conditions (1), (2), (3), and (4)). The participants
were also told that the agents were computer pro-
grams and not human participants. During the ex-
periment, each pair of participants was seated in
separate rooms in front of a computer display, a
keyboard, and a mouse, and they could only com-
municate with each other through the system.
In the dialogue with each system, five ?Who
is this?? quizzes about famous people were pre-
sented. The quiz subjects were chosen so that
the difficulty level of the quizzes was approxi-
mately the same in all the systems. For this pur-
pose, we first sorted people in Wikipedia in de-
scending order by their PageRank TM score based
on Wikipedia?s hyper-link structure. We then ex-
tracted the top-50 people and divided them from
the top into five groups of 10. Next we randomly
selected five people from each group to make
five sets of five people of approximately identical
PageRank scores. Each set of five people was used
for quizzes in each system.
On average, a pair of participants took 18 min-
utes to complete a dialogue with each system. The
number of hints that were actually presented in a
221
Figure 2: User satisfaction
quiz averaged 7.5.
4 Results
4.1 User satisfaction
For questions Q1, Q2, and Q3, Cronbach?s alpha
was 0.83, which justified combining these items
into a single index. Therefore we evaluated user
satisfaction with averages of the ratings of these
items. Figure 2 shows user satisfaction under each
experimental condition.
To evaluate the effect of the peer agent?s pres-
ence on user satisfaction, we compared conditions
(0) and (1). The F-test results showed that vari-
ances were assumed to be equal across groups
(p > 0.2), and the Kolmogorov-Smirnov test re-
sults showed that the assumption of normality was
satisfied (p > 0.6). By applying the paired t-test
to both the male and female groups, we found that
the peer agent?s presence significantly improved
user satisfaction (male group: t(31) = 4.2, p <
0.001, female group: t(31) = 2.8, p < 0.008).
To evaluate the effect of the empathic and self-
oriented expressions exhibited by the agents on
user satisfaction, we compared conditions (1),
(2), (3), and (4). A three-factor ANOVA was
conducted with two within-participant factors of
empathic and self-oriented expressions and one
between-participant factor of gender. The F-test
for the homogeneity of variances (p > 0.1) and
the Kolmogorov-Smirnov normality test (p > 0.1)
showed that the data met the ANOVA assump-
tions. As a result of the ANOVA, a signifi-
cant main effect was found for empathic expres-
sions with respect to user satisfaction, F (1, 62) =
92.7, p < 0.001. No significant main effects were
found for either self-oriented expressions or gen-
der, and there were no significant interactions.
Figure 3: User ratings of peer agent
These results showed that the peer agent?s pres-
ence and the agent empathic expressions signif-
icantly improved user satisfaction in quiz-style
multi-party dialogues.
4.2 User opinions about the peer agent
For questions Q4, Q5, Q6, and Q7, Cronbach?s
alpha was 0.92, which justified combining these
items into a single index. Therefore we evaluated
user opinions about the peer agent with the aver-
aged ratings of these items under each experimen-
tal condition. Figure 3 shows the user ratings of
the peer agent under each condition.
To evaluate the effect of agent empathic and
self-oriented expressions on the user ratings of the
peer agent, we compared conditions (1), (2), (3)
and (4). A three-factor ANOVA was conducted
with two within-participant factors of empathic
and self-oriented expressions and one between-
participant factor of gender. The F-test for the
homogeneity of variances (p > 0.3) and the
Kolmogorov-Smirnov normality test (p > 0.2)
showed that the data met the ANOVA assump-
tions. As a result of the ANOVA, a significant
main effect was found for empathic expressions
with respect to the user ratings of the peer agent,
F (1, 62) = 77.4, p < 0.001. There was a
moderate main effect for self-oriented expressions
with respect to the user ratings of the peer agent,
F (1, 62) = 4.38, p < 0.04. There were no sig-
nificant main effects for gender, and there were no
significant interactions.
These results showed that agent empathic ex-
pressions significantly improved user ratings of
the peer agent in quiz-style multi-party dialogues.
222
Figure 4: User utterances per quiz hint
4.3 Number of user utterances
Figure 4 shows the number of user utterances per
quiz hint under each condition.
To evaluate the effect of the peer agent?s pres-
ence on the number of user utterances per quiz
hint, we compared conditions (0) and (1). Based
on the F-test and the Kolmogorov-Smirnov test,
the assumptions of variance homogeneity (p >
0.6) and normality (p > 0.5) were met. By apply-
ing the paired t-test to both the male and female
groups, we found that the presence of the peer
agent significantly increased the number of user
utterances per hint (male group: t(31) = 3.1, p <
0.004, female group: t(31) = 5.6, p < 0.001).
To evaluate the effect of empathic and self-
oriented expressions by agents on the number
of user utterances, we compared conditions (1),
(2), (3), and (4). A three-factor ANOVA was
conducted with two within-participant factors of
empathic and self-oriented expressions and one
between-participant factor of gender. The F-test
for the homogeneity of variances (p > 0.05) and
the Kolmogorov-Smirnov normality test (p > 0.6)
showed that the data met the ANOVA assump-
tions. As a result of the ANOVA, a significant
main effect was found for empathic expressions
with respect to the number of user utterances,
F (1, 62) = 18.9, p < 0.001. No significant main
effects were found for either self-oriented expres-
sions or gender, and there were no significant in-
teractions.
These results showed that the peer agent?s pres-
ence and agent empathic expressions increased
the number of user utterances and stimulated hu-
man communication in quiz-style multi-party dia-
logues.
5 Conclusion
This paper experimentally analyzed how conver-
sational agents stimulate human communication
in thought-evoking multi-party dialogues between
multi-users and multi-agents. As an example of
such multi-party dialogue, we focused on quiz-
style multi-party dialogues between two users and
two agents. We investigated how a peer agent?s
presence and agent emotional expressions influ-
enced user satisfaction, the user ratings of the peer
agent, and the number of user utterances. The
user ratings of the peer agent included user?s per-
ceived closeness, likability and caring from the
peer agent, and the user?s feeling of being sup-
ported by the peer agent.
The experiment results showed that the peer
agent?s presence significantly improved user sat-
isfaction and increased the number of user utter-
ances. We also found significant effects that agent
empathic expressions improved user satisfaction
and user positive ratings of the peer agent and that
they further increased the number of user utter-
ances. These results indicate that employing a peer
agent and agent empathic behaviors in thought-
evoking multi-party dialogues will stimulate inter-
action among people in computer-mediated com-
munication. Our findings will be useful for a
broader class of applications such as educational
agents and community facilitators.
Many directions for future work remain. First,
we plan to extend our work to deal with various
modalities such as speech, gestures, body posture,
facial expressions, and the direction of eye gazes
to investigate the effects of agent representation
(embodied or disembodied) and other modalities
in thought-evoking multi-party dialogues. Second,
we will analyze how agent behaviors influence
users and dialogues in more detail and develop a
more sophisticated dialogue management method
based on our detailed analysis. Learning optimal
dialogue management strategies in multi-party di-
alogues is a challenging research topic. Third, ex-
amining the relationship between user personality
traits and the impact of agents on users is valuable.
Previous work reported that the effect of embodi-
ment depended on user personalities (Lee et al,
2006). This direction is important to the stimula-
tion of multi-party interaction for therapeutic and
emotional support.
223
References
James Allen, Donna Byron, Myroslava Dzikovska,
George Ferguson, Lucian Galescu, and Amanda
Stent. 2001. Toward conversational human-
computer interaction. AI Magazine, 22(4):27?37.
Joseph Bates. 1994. The role of emotion in believable
agents. Communications of the ACM, 37(7):122?
125.
Timothy W. Bickmore and Rosalind W. Picard. 2005.
Establishing and maintaining long-term human-
computer relationships. ACM Transactions on
Computer-Human Interaction, 12(2):293?327.
Scott Brave, Clifford Nass, and Kevin Hutchinson.
2005. Computers that care: investigating the effects
of orientation of emotion exhibited by an embodied
computer agent. International Journal of Human-
Computer Studies, 62(2):161?178.
Justine Cassell, Joseph Sullivan, Scott Prevost, and
Elizabeth Churchill, editors. 2000. Embodied Con-
versational Agents. MIT Press, Cambridge, MA.
Chih-Yueh Chou, Tak-Wai Chan, and Chi-Jen Lin.
2003. Redefining the learning companion: the past,
present, and future of educational agents. Comput-
ers & Education, 40(3):255?269.
Scotty D. Craig, Barry Gholson, Matthew Ventura,
Arthur C. Graesser, and the Tutoring Research
Group. 2000. Overhearing dialogues and mono-
logues in virtual tutoring sessions: Effects on ques-
tioning and vicarious learning. International Jour-
nal of Artificial Intelligence in Education, 11:242?
253.
Patrick Gebhard, Martin Klesen, and Thomas Rist.
2004. Coloring multi-character conversations
through the expression of emotions. In Lecture
Notes in Computer Science (Tutorial and Research
Workshop on Affective Dialogue Systems), volume
3068, pages 128?141.
Ryuichiro Higashinaka, Kohji Dohsaka, Shigeaki
Amano, and Hideki Isozaki. 2007a. Effects of quiz-
style information presentation on user understand-
ing. In Proceedings of the 8th Annual Conference
of the International Speech Communication Associ-
ation, pages 2725?2728.
Ryuichiro Higashinaka, Kohji Dohsaka, and Hideki
Isozaki. 2007b. Learning to rank definitions to
generate quizzes for interactive information presen-
tation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(Poster Presentation), pages 117?120.
Ryuichiro Higashinaka, Kohji Dohsaka, and Hideki
Isozaki. 2008. Effects of self-disclosure and em-
pathy in human-computer dialogue. In Proceedings
of 2008 IEEE Workshop on Spoken Language Tech-
nology, pages 109?112.
Eva Hudlicka. 2003. To feel or not to feel: The role of
affect in human-computer interaction. International
Journal of Human-Computer Studies, 59(1-2):1?32.
Kwan Min Lee, Younbo Jung, Jaywoo Kim, and
Sang Ryong Kim. 2006. Are physically em-
bodied social agents better than disembodied social
agents?: Effects of embodiment, tactile interaction,
and people?s loneliness in human-robot interaction.
International Journal of Human-Computer Studies,
64(10):962?973.
Yi Liu and Yam San Chee. 2004. Intelligent pedagog-
ical agents with multiparty interaction support. In
Proceedings of International Conference on Intelli-
gent Agent Technology, pages 134?140.
Heidy Maldonado, Jong-Eun Roselyn Lee, Scott
Brave, Cliff Nass, Hiroshi Nakajima, Ryota Ya-
mada, Kimihiko Iwamura, and Yasunori Morishima.
2005. We learn better together: enhancing elearn-
ing with emotional characters. In Proceedings of the
2005 Conference on Computer Support for Collab-
orative Learning, pages 408?417.
Rosalind W. Picard. 1997. Affective Computing. MIT
Press, Cambridge, MA.
Helmut Prendinger and Mitsuru Ishizuka, editors.
2004. Life-Like Characters: Tools, Affective Func-
tions, and Applications. Springer, Berlin.
Helmut Prendinger, Junichiro Mori, and Mitsuru
Ishizuka. 2005. Using human physiology to eval-
uate subtle expressivity of a virtual quizmaster in
a mathematical game. International Journal of
Human-Computer Studies, 62(2):231?245.
Keith Stenning, Jean McKendree, John Lee, Richard
Cox, Finbar Dineen, and Terry Mayes. 1999. Vi-
carious learning from educational dialogue. In Pro-
ceedings of the 1999 Conference on Computer Sup-
port for Collaborative Learning, pages 341?347.
David Traum and Jeff Rickel. 2002. Embodied agents
for multi-party dialogue in immersive virtual worlds.
In Proceedings of the 1st International Joint Confer-
ence on. Autonomous Agents and Multi-Agent Sys-
tems, pages 766?773.
Jun Zheng, Xiang Yuan, and Yam San Chee. 2005.
Designing multiparty interaction support in Elva, an
embodied tour guide. In Proceedings of the 4th In-
ternational Joint Conference on Autonomous Agents
and Multiagent Systems, pages 929?936.
Victor Zue, Stephanie Seneff, Joseph Polifroni,
Michael Phillips, Christine Pao, David Goodine,
David Goddeau, and James Glass. 1994. PEGA-
SUS: a spoken dialogue interface for on-line air
travel planning. Speech Communication, 15:331?
340.
224
