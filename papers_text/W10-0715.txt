Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 93?98,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Enriched MT Grammar for Under $100
Omar F. Zaidan and Juri Ganitkevitch
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,juri}@cs.jhu.edu
Abstract
We propose a framework for improving out-
put quality of machine translation systems, by
operating on the level of grammar rule fea-
tures. Our framework aims to give a boost to
grammar rules that appear in the derivations
of translation candidates that are deemed to
be of good quality, hence making those rules
more preferable by the system. To that end, we
ask human annotators on Amazon Mechanical
Turk to compare translation candidates, and
then interpret their preferences of one candi-
date over another as an implicit preference for
one derivation over another, and therefore as
an implicit preference for one or more gram-
mar rules. Our framework also allows us to
generalize these preferences to grammar rules
corresponding to a previously unseen test set,
namely rules for which no candidates have
been judged.
1 Introduction
When translating between two languages, state-
of-the-art statistical machine translation sys-
tems (Koehn et al, 2007; Li et al, 2009) generate
candidate translations by relying on a set of relevant
grammar (or phrase table) entries. Each of those
entries, or rules, associates a string in the source
language with a string in the target language, with
these associations typically learned by examining
a large parallel bitext. By the very nature of the
translation process, a target side sentence e can
be a candidate translation for a source sentence f
only if e can be constructed using a small subset
of the grammar, namely the subset of rules with
source side sequences relevant to the word sequence
of f . However, even this limited set of candidates
(call it E(f)) is quite large, with |E(f)| growing
exponentially in the length of f . The system is able
to rank the translations within E(f) by assigning a
score s(e) to each candidate translation. This score
is the dot product:
s(e) = ~?(e) ? ~w (1)
where ~?(e) is a feature vector characterizing e, and
~w is a system-specific weight vector characterizing
the system?s belief of how much the different fea-
tures reflect translation quality. The features of a
candidate e are computed by examining the way e is
constructed (or derived), and so if we let d(e) be the
derivation of e, the feature vector can be denoted:1
~?(d(e)) = ??1(d(e)), . . . , ?m(d(e))? (2)
where ?i(d(e)) is the value of ith feature function
of d(e) (with a corresponding weight wi in ~w).
To compute the score for a candidate, we examine
its derivation d(e), enumerating the grammar rules
used to construct e: d(e) = (r1, . . . , rk). Typically,
each of the rules will itself have a vector of m fea-
tures, and we calculate the value of a derivation fea-
ture ?i(d(e)) as the sum of the ith feature over all
rules in the derivation:
?i(d(e)) =
?
r?d(e)
?i(r) (3)
1There are other features computed directly, without ex-
amining the derivation (e.g. candidate length, language model
score), but we omit these features from the motivation discus-
sion for clarity.
93
These features are usually either relative frequen-
cies estimated from the training corpus, relating the
rule?s source and target sides, or features that char-
acterize the structure of the rule itself, independently
from the corpus.
Either way, the weightwi is chosen so as to reflect
some belief regarding the correlation between the ith
feature and translation quality. This is usually done
by choosing weights that maximize performance on
a tuning set separate from the training bitext. Un-
like system weights, the grammar rule feature val-
ues are fixed once extracted, and are not modified
during this tuning phase. In this paper, we propose a
framework to augment the feature set to incorporate
additional intuition about how likely a rule is to pro-
duce a translation preferred by a human annotator.
This knowledge is acquired by directly asking hu-
man judges to compare candidate translations, there-
fore determining which subset of grammar rules an-
notators seem to prefer over others. We also seek to
generalize this intuition to rules for which no can-
didates were judged, hence allowing us to impact a
much larger set of rules than just those used in trans-
lating the tuning set.
The paper is organized as follows. We first give
a general description of our framework. We then
discuss our data collection efforts on Amazon Me-
chanical Turk for an Urdu-English translation task,
and make explicit the type of judgments we col-
lect and how they can be used to augment grammar
rules. Before concluding, we propose a framework
for generalizing judgments to unseen grammar rules,
and analyze the data collection process.
2 The General Framework
As initially mentioned, when tuning a SMT system
on a development set, we typically only perform
high-level optimization of the system weights. In
this section we outline an approach that could allow
for lower-level optimization, on the level of individ-
ual grammar rules.
We kick off the process by soliciting judgments
from human annotators regarding the quality of a
subset of candidates (the following section outlines
how candidates are chosen). The resulting judg-
ments on sentences are interpreted to be judgments
on individual grammar rules used in the derivations
of these candidates. And so, if an annotator declares
a candidate to be of high quality, this is considered
a vote of confidence on the individual rules giving
rise to this candidate, and if an annotator declares a
candidate to be of lowl quality, this is considered a
vote of no confidence on the individual rules.
To make use of the collected judgments, we ex-
tend the set of features used in the decoder by a new
feature ?:
~?? = ??1, . . . , ?m, ?? (4)
This feature is the cornerstone of our framework,
as it will hold the quantified and cumulated judg-
ments for each rule, and will be used by the system
at decoding time, in addition to the existing m fea-
tures, incorporating the annotators? judgments into
the translation process.2
The range of possible values for this feature, and
how the feature is computed, depends on how one
chooses to ask annotators to score candidates, and
what form those judgments assume (i.e. are those
judgments scores on a scale? Are they ?better?
vs. ?worse? judgments, and if so, compared to how
many other possibilities?). At this point, we will
only emphasize that the value of ? should reflect
the annotators? preference for the rule, and that it
should be computed from the collected judgments.
We will propose one such method of computing ? in
Section 4, after describing the type of judgments we
collected.
3 Data Collection
We apply our approach to an Urdu-to-English trans-
lation task. We used a syntactically rich SAMT
grammar (Venugopal and Zollmann, 2006), where
each rule in the grammar is characterized by 12 fea-
tures. The grammar was provided by Chris Callison-
Burch (personal communication), and was extracted
from a parallel corpus of 88k sentence pairs.3 One
system using this grammar produced significantly
improved output over submissions to the NIST 2009
Urdu-English task (Baker et al, 2009).
We use the Joshua system (Li et al, 2009)
as a decoder, with system weights tuned using
2In fact, the collected judgments can only cover a small por-
tion of the grammar. We address this coverage problem in Sec-
tion 4.
3LDC catalog number LDC2009E12.
94
Z-MERT (Zaidan, 2009) on a tuning set of 981 sen-
tences, a subset of the 2008 NIST Urdu-English test
set.4 We choose candidates to be judged from the
300-best candidate lists.5
Asking a worker to make a quantitative judgment
of the quality of a particular candidate translation
(e.g. on a 1?7 scale) is a highly subjective and
annotator-dependent process. Instead, we present
workers with pairs of candidates, and ask them to
judge which candidate is of better quality.
How are candidate pairs chosen? We would like
a judgment to have the maximum potential for be-
ing informative about specific grammar rules. In es-
sense, we prefer a pair of candidates if they have
highly similar derivations, yet differ noticeably in
terms of how the decoder ranks them. In other
words, if a relatively minimal change in derivation
causes a relatively large difference in the score as-
signed by the decoder, we are likely to attribute the
difference to very few rule comparisons (or perhaps
only one), hence focusing the comparison on indi-
vidual rules, all the while shielding the annotators
from having to compare grammar rules directly.
Specifically, each pair of candidates (e, e?) is as-
signed a potential score pi(e, e?), defined as:
pi(e, e?) =
s(e)s(e?)
lev(d(e),d(e?))
, (5)
where s(e) is the score assigned by the decoder,
and lev(d,d?) is a distance measure between two
derivations which we will now descibe in more de-
tail. In Joshua, the derivation of a candidate is cap-
tured fully and exactly by a derivation tree, and so
we define lev(d,d?) as a tree distance metric as fol-
lows. We first represent the trees as strings, using the
familiar nested string representation, then compute
the word-based Levenshtein edit distance between
the two strings. An edit has a cost of 1 in general, but
we assign a cost of zero to edit operations on termi-
nals, since we want to focus on the structure of the
derivation trees, rather than on terminal-level lexi-
cal choices.6 Furthermore, we ignore differences in
4LDC catalog number LDC2009E11.
5We exclude source sentences shorter than 4 words long or
that have fewer than 4 candidate translations. This eliminates
roughly 6% of the development set.
6This is not to say that lexical choices are not important, but
lexical choice is heavily influenced by context, which is not cap-
?pure? pre-terminal rules, that only have terminals
as their right-hand side. These decisions effectively
allow us to focus our efforts on grammar rules with
at least one nonterminal in their right-hand side.
We perform the above potential computation on
all pairs formed by the cross product of the top 10
candidates and the top 300 candidates, and choose
the top five pairs ranked by potential.
Our HIT template is rather simple. Each HIT
screen corresponds to a single source sentence,
which is shown to the worker along with the five
chosen candidate pairs. To aid workers who are not
fluent in Urdu7 better judge translation quality, the
HIT also displays one of the available references
for that source sentence. To eliminate potential bias
associated with the order in which candidates are
presented (an annotator might be biased to choos-
ing the first presented candidate, for example), we
present the two candidates in random or- der. Fur-
thermore, for quality assurance, we embed a sixth
candidate pair to be judged, where we pair up a ran-
domly chosen candidate with another reference for
that sentence.8 Presumably, a faithful worker would
be unlikely to prefer a random candidate over the
reference, and so this functions as an embedded self-
verification test. The order of this test, relative to the
five original pairs, is chosen randomly.
4 Incorporating the Judgements
4.1 Judgement Quantification
The judgments we obtain from the procedure de-
scribed in the previous section relate pairs of can-
didate translations. However, we have defined the
accumulation feature ? as a feature for each rule.
Thus, in order to compute ?, we need to project the
judgments onto the rules that tell the two candidates
apart. A simple way to do this is the following: for
a judged candidate pair (e, e?) let U(e) be the set of
tured well by grammar rules. Furthermore, lexical choice is a
phenomenon already well captured by the score assigned to the
candidate by the language model, a feature typically included
when designing ~?.
7We exclude workers from India and restrict the task to
workers with an existing approval rating of 90% or higher.
8The tuning set contains at least three different human refer-
ences for each source sentence, and so the reference ?candidate?
shown to the worker is not the same as the sentence already
identified as a reference.
95
rules that appear in d(e) but not in d(e?), and vice
versa.9 We will assume that the jugdment obtained
for (e, e?) applies for every rule pair in the cartesian
product of U(e) and U(e?). This expansion yields
a set of judged grammar rule pairs J = {(a, b)}
with associated vote counts va>b and vb>a, captur-
ing how often the annotators preferred a candidate
that was set apart by a over a candidate containing
b, and vice versa.
So, following our prior definiton as an expression
of the judges? preference, we can calculate the value
of ? for a rule r as the relative frequency of favorable
judgements:
?(r) =
?
(r,b)?J vr>b
?
(r,b)?J vb>r + vr>b
(6)
4.2 Generalization to Unseen Rules
This approach has a substantial problem: ?, com-
puted as given above, is undefined for a rule that
was never judged (i.e. a rule that never set apart
a pair of candidates presented to the annotators).
Furthermore, as described, the coverage of the col-
lected judgments will be limited to a small subset
of the entire grammar, meaning that when the sys-
tem is asked to translate a new source sentence, it
is highly unlikely that the relevant grammar rules
would have already been judged by an annotator.
Therefore, it is necessary to generalize the collected
judgments/votes and propagate them to previously
unexamined rules.
In order to do this, we propose the following gen-
eral approach: when observing a judgment for a pair
of rules (a, b) ? J , we view that judgement not as
a vote on one of them specifically, but rather as a
comparison of rules similar to a versus rules similar
to b. When calculating ?(r) for any rule r we use
a distance measure over rules, ?, to estimate how
each judgment in J projects to r. This leads to the
following modified computatio of ?(r):
?(r) =
?
(a,b)?J
?(a, r)v?b>a + ?(b, r)v
?
a>b
?(a, r) + ?(b, r)
(7)
9The way we select candidate pairs ensures that U(e) and
U(e?) are both small and expressive in terms of impact on the
decoder ranking. On our data U(e) contained an average of 4
rules.
where v?a>b (and analogously v
?
b>a) is defined as the
relative frequency of a being preferred over b:
v?a>b =
va>b
va>b + vb>a
4.3 A Vector Space Realization
Having presented a general framework for judgment
generalization, we will now briefly sketch a concrete
realization of this approach.
In order to be able to use the common distance
metrics on rules, we define a rule vector space. The
basis of this space will be a new set of rule features
designed specifically for the purpose of describing
the structure of a rule, ~? = ??1, . . . , ?k?. Provided
the exact features chosen are expressive and well-
distributed over the grammar, we expect any con-
ventional distance metric to correlate with rule sim-
ilarity.
We deem a particular ?i good if it quantifies a
quality of the rule that describes the rule?s nature
rather than the particular lexical choices it makes,
i.e. a statistic (such as the rule length, arity, number
of lexical items in the target or source side or the av-
erage covered span in the training corpus), informa-
tion relevant to the rule?s effect on a derivation (such
as nonterminals occuring in the rule and wheter they
are re-ordered) or features that capture frequent lex-
ical cues that carry syntactic information (such as
the co-occurrence of function words in source and
target language, possibly in conjunction with certain
nonterminal types).
5 Results and Analysis
The judgments were collected over a period of
about 12 days (Figure 1). A total of 16,374 labels
were provided (2,729 embedded test labels + 13,645
?true? labels) by 658 distinct workers over 83.1 hours
(i.e. each worker completed an average of 4.2 HITs
over 7.6 minutes). The reward for each HIT was
$0.02, with an additional $0.005 incurred for Ama-
zon Fees. Since each HIT provides five labels, we
obtain 200 (true) labels on the dollar. Each HIT
took an average of 1.83 minutes to complete, for a
labeling rate of 164 true labels/hour, and an effec-
tive wage of $0.66/hour. The low reward does not
seem to have deterred Turkers from completing our
HITs faithfully, as the success rate on the embedded
96
 0 10 20 30 40 50 60 70 80 90 100
 1  2  3  4  5  6  7  8  9  10  11% Completed Time (Days)
Figure 1: Progress of HIT submission over time. There
was a hiatus of about a month during which we collected
no data, which we are omitting for clairty.
True Questions Validation Questions
Preferred % Preferred %
High-
Ranked
40.0% Reference 83.7%
Low-
Ranked
24.1% Random
Candidate
11.7%
No
Difference
35.9% No
Difference
4.65%
Table 1: Distributions of the collected judgments over
the true questions and over the embedded test questions.
?High-Ranked? (resp. ?Low-Ranked?) refers to whether
the decoder assigned a high (low) score to the candidate.
And so, annotators agreed with the decoder 40.0% of the
time, and disagreed 24.1% of the time.
questions was quite high (Table 1).10 From our set
of comparatively judged candidate translations we
extracted competing rule pairs. To reduce the in-
fluence of lexical choices and improve comparabil-
ity, we excluded pure preterminal rules and limited
the extraction to rules covering the same span in the
Urdu source. Figure 3 shows an interesting example
of one such rule pair. While the decoder demon-
strates a clear preference for rule (a) (including it
into its higher-ranked translation 100% of the time),
the Turkers tend to prefer translations generated us-
ing rule (b), disagreeing with the SMT system 60%
of the time. This indicates that preferring the second
rule in decoding may yield better results in terms of
human judgment, in this case potentially due to the
10It should be mentioned that the human references them-
selves are of relatively low quality.
0
5
10
15
20
25
30
1 2 3 4 5 6 7 8 9 10
Candidate Rank
%
 
T
i
m
e
 
C
h
o
s
e
n
 
f
o
r
 
C
o
m
p
a
r
i
s
o
n
Figure 2: Histogram of the rank of the higher-ranked
candidate chosen in pair comparisons. For instance, in
about 29% of chosen pairs, the higher-ranked candidate
was the top candidate (of 300) by decoder score.
(a)  [NP] ! " [NP] [NN+IN] !" # the [NN+IN] [NP] $
(b)  [NP] ! " [NP] !" [NN] !"  # [NN] of [NP] $
Figure 3: A example pair of rules for which judgements
were obtained. The first rule is preferred by the decoder,
while human annotators favor the second rule.
cleaner separation of noun phrases from the prepo-
sitional phrase.
We also examine the distribution of the chosen
candidates. Recall that each pair consists of a high-
ranked candidate from the top-ten list, and a low-
ranked candidate from the top-300 list. The His-
togram of the higher rank (Figure 2) shows that the
high-ranked candidate is in fact a top-three candi-
date over 50% of the time. We also see (Figure 4)
that the low-ranked candidate tends to be either close
in rank to the top-ten list, or far away. This again
makes sense given our definition of potential for a
pair: potential is high if the derivations are very
close (left mode) or if the decoder scores differ con-
siderably (right mode).
Finally, we examine inter-annotator agreement,
since we collect multiple judgments per query. We
find that there is full agreement among the anno-
tators in 20.6% of queries. That is, in 20.6% of
queries, all three annotators answering that query
gave the same answer (out of the three provided
answers). This complete agreement rate is signif-
icantly higher than a rate caused by pure chance
(11.5%). This is a positive result, especially given
97
05
10
15
20
1
-
2
0
2
1
-
4
0
4
1
-
6
0
6
1
-
8
0
8
1
-
1
0
0
1
0
1
-
1
2
0
1
2
1
-
1
4
0
1
4
1
-
1
6
0
1
6
1
-
1
8
0
1
8
1
-
2
0
0
2
0
1
-
2
2
0
2
2
1
-
2
4
0
2
4
1
-
2
6
0
2
6
1
-
2
8
0
2
8
1
-
3
0
0
Candidate Rank
%
 
T
i
m
e
 
C
h
o
s
e
n
 
f
o
r
 
C
o
m
p
a
r
i
s
o
n
Figure 4: Histogram of the rank of the lower-ranked can-
didate chosen in pair comparisons. For instance, in about
16% of chosen candidate pairs, the lower-ranked candi-
date was ranked in the top 20.
how little diversity usually exists in n-best lists,
a fact (purposely) exacerbated by our strategy of
choosing highly similar pairs of candidates. On the
other hand, we observe complete disagreement in
only 14.9% of queries, which is significantly lower
than a rate caused by pure chance (which is 22.2%).
One thing to note is that these percentages are
calculated after excluding the validation questions,
where the complete agreement rate is an expectedly
even higher 64.9%, and the complete disagreement
rate is an expectedly even lower 3.60%.
6 Conclusions and Outlook
We presented a framework that allows us to ?tune?
MT systems on a finer level than system-level fea-
ture weights, going instead to the grammar rule level
and augmenting the feature set to reflect collected
human judgments. A system relying on this new fea-
ture during decoding is expected to have a slightly
different ranking of translation candidates that takes
human judgment into account. We presented one
particular judgment collection procedure that relies
on comparing candidate pairs (as opposed to eval-
uating a candidate in isolation) and complemented
it with one possible method of propagating human
judgments to cover grammar rules relevant to new
sentences.
While the presented statistics over the collected
data suggest that the proposed candidate selection
procedure yields consistent and potentially informa-
tive data, the quantitative effects on a machine trans-
lation system remain to be seen.
Additionally, introducing ? as a new feature
makes it necessary to find a viable weight for it.
While this can be done trivially in running MERT
on arbitrary development data, it may be of interest
to extend the weight optimization procedure in or-
der to preserve the partial ordering induced by the
judgments as best as possible.
Acknowledgments
This research was supported by the EuroMatrix-
Plus project funded by the European Commission,
by the DARPA GALE program under Contract No.
HR0011-06-2-0001, and the NSF under grant IIS-
0713448.
References
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation
(SIMT). In SCALE 2009 Summer Workshop Final Re-
port, pages 135?139.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL, Demonstration Session,
pages 177?180, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proc. of the Fourth
Workshop on Statistical Machine Translation, pages
135?139.
Ashish Venugopal and Andreas Zollmann. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. of the NAACL 2006 Workshop on Statistical
Machine Translation, pages 138?141. Association for
Computational Linguistics.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
98
