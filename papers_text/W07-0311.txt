Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 76?83,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Enhancing commercial grammar-based applications using robust
approaches to speech understanding
Matthieu He?bert
Network ASR R+D, Nuance Communications
1500, Universite?, Suite 935, Montre?al, Que?bec, H3A 3T2, Canada
hebert@nuance.com
Abstract
This paper presents a series of measure-
ments of the accuracy of speech under-
standing when grammar-based or robust
approaches are used. The robust ap-
proaches considered here are based on sta-
tistical language models (SLMs) with the
interpretation being carried out by phrase-
spotting or robust parsing methods. We
propose a simple process to leverage ex-
isting grammars and logged utterances
to upgrade grammar-based applications to
become more robust to out-of-coverage
inputs. All experiments herein are run
on data collected from deployed directed
dialog applications and show that SLM-
based techniques outperform grammar-
based ones without requiring any change
in the application logic.
1 Introduction
The bulk of the literature on spoken dialog systems
is based on the simple architecture in which the
input speech is processed by a statistical language
model-based recognizer (SLM-based recognizer) to
produce a word string. This word string is further
processed by a robust parser (Ward, 1990) or call
router (Gorin et al 1997) to be converted in a se-
mantic interpretation. However, it is striking to see
that a large portion of deployed commercial appli-
cations do not follow this architecture and approach
the recognition/interpretation problem by relying on
hand-crafted rules (context-free grammars - CFGs).
The apparent reasons for this are the up-front cost
and additional delays of collecting domain-specific
utterances to properly train the SLM (not to men-
tion semantic tagging needed to train the call router)
(Hemphill et al 1990; Knight et al 2001; Gorin et
al, 1997). Choosing to use a grammar-based ap-
proach also makes the application predictable and
relatively easy to design. On the other hand, these
applications are usually very rigid: the users are al-
lowed only a finite set of ways to input their requests
and, by way of consequences, these applications suf-
fer from high out-of-grammar (OOG) rates or out-
of-coverage rates.
A few studies have been published compar-
ing grammar-based and SLM-based approaches to
speech understanding. In (Knight et al 2001),
a comparison of grammar-based and robust ap-
proaches is presented for a user-initiative home au-
tomation application. The authors concluded that
it was relatively easy to use the corpus collected
during the course of the application development to
train a SLM which would perform better on out-
of-coverage utterances, while degrading the accu-
racy on in-coverage utterances. They also reported
that the SLM-based system showed slightly lower
word error rate but higher semantic error rate for
the users who know the application?s coverage. In
(Rayner et al 2005), a rigorous test protocol is pre-
sented to compare grammar-based and robust ap-
proaches in the context of a medical translation sys-
tem. The paper highlights the difficulties to con-
struct a clean experimental set-up. Efforts are spent
to control the training set of both approaches to
76
have them align. The training sets are defined as
the set of data available to build each system: for a
grammar-based system, it might be a series of sam-
ple dialogs. (ten Bosch, 2005) presents experiments
comparing grammar-based and SLM-based systems
for na??ve users and an expert user. They conclude
that the SLM-based system is most effective in re-
ducing the error rate for na??ve users. Recently (see
(Balakrishna et al 2006)), a process was presented
to automatically build SLMs from a wide variety
of sources (in-service data, thesaurus, WordNet and
world-wide web). Results on data from commer-
cial speech applications presented therein echo ear-
lier results (Knight et al 2001) while reducing the
effort to build interpretation rules.
Most of the above studies are not based on data
collected on deployed applications. One of the con-
clusions from previous work, based on the measured
fact that in-coverage accuracy of the grammar-based
systems was far better than the SLM one, was that
as people get more experience with the applications,
they will naturally learn its coverage and gravitate
towards it. While this can be an acceptable option
for some types of applications (when the user pop-
ulation tends to be experienced or captive), it cer-
tainly is not a possibility for large-scale commercial
applications that are targeted at the general public. A
few examples of such applications are public transit
schedules and fares information, self-help applica-
tions for utilities, banks, telecommunications busi-
ness, and etc. Steering application design and re-
search based on in-coverage accuracy is not suitable
for these types of applications because a large frac-
tion of the users are na??ves and tend to use more nat-
ural and unconstrained speech inputs.
This paper exploits techniques known since the
90?s (SLM with robust parsing, (Ward, 1990)) and
applies them to build robust speech understanding
into existing large scale directed dialog grammar-
based applications. This practical application of
(Ward, 1990; Knight et al 2001; Rayner et al 2005;
ten Bosch, 2005) is cast as an upgrade problem
which must obey the following constraints.
1. No change in the application logic and to the
voice user interface (VUI)
2. Roughly similar CPU consumption
3. Leverage existing grammars
4. Leverage existing transcribed utterances
5. Simple process that requires little manual inter-
vention
The first constraint dictates that, for each context,
the interpretation engines (from the current and up-
graded systems) must return the same semantics (i.e.
same set of slots).
The rest of this paper is organized as follows. The
next Section describes the applications from which
the data was collected, the experimental set-up and
the accuracy measures used. Section 3 describes
how the semantic truth is generated. The main re-
sults of the upgrade from grammar-based to SLM-
based recognition are presented in Section 4. The
target audience for this paper is composed of appli-
cation developers and researchers that are interested
in the robust information extraction from directed
dialog speech applications targeted at the general
public.
2 Applications, corpus and experimental
set-up
2.1 Application descriptions
As mentioned earlier, the data for this study was col-
lected on deployed commercial directed dialog ap-
plications. AppA is a self-help application in the in-
ternet service provider domain, while AppB is also
a self-help application in the public transportation
domain. Both applications are grammar-based di-
rected dialogs and receive a daily average of 50k
calls. We will concentrate on a subset of contexts
(dialog states) for each application as described in
Table 1. The mainmenu grammars (each application
has its own mainmenu grammar) contain high-level
targets for the rest of the application and are active
once the initial prompt has been played. The com-
mand grammar contains universal commands like
?help?, ?agent?, etc. The origin and destination
grammars contain a list of ? 2500 cities and states
with the proper prefixes to discriminate origin and
destination. num type passenger accepts up to nine
passengers of types adults, children, seniors, etc.
Finally time is self explanatory. For each applica-
tion, the prompt directs the user to provide a specific
77
Context Description Active grammars Training Testing
sentences utts
AppA MainMenu Main menu mainmenu and 5000 5431
for the application commands (350) (642)
AppB MainMenu Main menu mainmenu and 5000 4039
for the application commands (19) (987)
AppB Origin Origin of travel origin, destination 5000 8818
and commands (20486) (529)
AppB Passenger Number and type num type passenger 1500 2312
of passenger and commands (32332) (66)
AppB Time Time of departure time and commands 1000 1149
(4102) (55)
Table 1: Description of studied contexts for each application. Note that the AppB Origin context contains a
destination grammar: this is due to the fact that the same set of grammars was used in the AppB Destination
context (not studied here). ?Training? contains the number of training sentences drawn from the corpus and
used to train the SLMs. As mentioned in Sec. 2.3, in the case of word SLMs, we also use sentences that are
covered by the grammars in each context as backoffs (see Sec. 2). The number of unique sentences covered
by the grammars is in parenthesis in the ?Training? column. The ?Testing? column contains the number of
utterances in the test set. The number of those utterances that contain no speech (noise) is in parenthesis.
piece of information (directed dialog). Each gram-
mar fills a single slot with that information. The in-
formation contained in the utterance ?two adults and
one child? (AppB Passenger context) would be col-
lapsed to fill the num type passenger slot with the
value ?Adult2 Child1?. From the application point
of view, each context can fill only a very limited set
of slots. To keep results as synthesized as possible,
unless otherwise stated, the results from all studied
contexts will be presented per application: as such
results from all contexts in AppB will be pooled to-
gether.
2.2 Corpus description
Table 1 presents the details of the corpus that we
have used for this study. As mentioned above the en-
tire corpora used for this study is drawn from com-
mercially deployed systems that are used by the gen-
eral public. The user population reflects realistic
usage (expert vs na??ve), noise conditions, handsets,
etc. The training utterances do not contain noise ut-
terances and is used primarily for SLM training (no
acoustic adaptation of the recognition models is per-
formed).
2.3 Experimental set-up description
The baseline system is the grammar-based system;
the recognizer uses, on a per-context basis, the gram-
mars listed in Table 1 in parallel. The SLM systems
studied all used the same interpretation engine: ro-
bust parsing with the grammars listed in Table 1 as
rules to fill slots. Note that this allows the applica-
tion logic to stay unchanged since the set of potential
slots returned within any given context is the same as
for the grammar-based systems (see first constraint
in Sec. 1). Adhering to this experimental set-up also
guarantees that improvements measured in the lab
will have a direct impact on the raw accuracy of the
deployed application.
We have considered two different SLM-based
systems in this study: standard SLM (wordSLM)
and class-based SLM (classSLM) (Jelinek, 1990;
Gillett and Ward, 1998). In the classSLM systems,
the classes are defined as the rules of the interpre-
tation engine (i.e. the grammars active for each
context as defined in Table 1). The SLMs are all
trained on a per-context basis (Xu and Rudnicky,
2000; Goel and Gopinath, 2006) as bi-grams with
Witten-Bell discounting. To insure that the word-
SLM system covered all sentences that the grammar-
based system does, we augmented the training set of
78
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.05  0.1  0.15  0.2
CA
-in
FA-total
CA-in/FA-total
grammar-based - automatic
grammar-based - human
wordSLM - automatic
 wordSLM - human  0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
is
io
n
Recall
Recall/Precision
grammar-based - automatic
grammar-based - human
wordSLM - automatic
 wordSLM - human
Figure 1: ROC curves for AppA MainMenu with the automatic or human-generated truth. In each the
grammar-based and SLM-based systems are compared.
the wordSLM (see Table 1) with the list of sentences
that are covered by the baseline grammar-based sys-
tem. This acts as a backoff in case a word or bi-
gram is not found in the training set (not to be con-
fused with bi-gram to uni-gram backoffs found in
standard SLM training). This is particularly helpful
when a little amount of data is available for training
the wordSLM (see Sec. 4.3).
2.4 Accuracy measures
Throughout this paper, we will use two sets of mea-
sures. This is motivated by the fact that applica-
tion developers are familiar with the concepts of cor-
rect/false acceptance at the utterance level. For in-
formation extraction (slot filling) from utterances,
these concepts are restrictive because an utterance
can be partly correct or wrong. In this case we pre-
fer a more relevant measure from the information re-
trieval field: precision and recall on a per-slot basis.
We use the following definitions.
? CA-in = #utts that had ALL slots correct (slot
name and value) / #utts that are in-coverage
(i.e. truth has at least a slot filled)
? FA-total = #utts that had at least one erroneous
slot (slot name or value) / total #utts
? Precision = #slot correct slots (slot name and
value) / #slots returned by system
? Recall = #slot correct slots (slot name and
value) / #slots potential slots (in truth)
Since applications use confidence extensively to
guide the course of dialogue, it is of limited interest
to study forced-choice accuracy (accuracy with no
rejection). Hence, we will present receiver operat-
ing characteristic (ROC) curves. The slot confidence
measure is based on redundancy of a slot/value pair
across the NBest list. For CA-in and FA-total, the
confidence is the average confidence of all slots
present in the utterance. Note that in the case where
each utterance only fills a single slot, CA-in = Re-
call.
3 Truth
Due to the large amount of data processed (see Table
1), semantic tagging by a human may not be avail-
able for all contexts (orthographic transcriptions are
available however). We need to resort to a more au-
tomatic way of generating the truth files while main-
taining a strong confidence in our measurements. To
this end, we need to ensure that any automatic way
of generating the truth will not bias the results to-
wards any of the systems.
The automatic truth can be generated by simply
using the robust parser (see Sec. 2.3) on the or-
thographic transcriptions which are fairly cheap to
acquire. This will generate a semantic interpreta-
tion for those utterances that contain fragments that
79
parse rules defined by the interpretation engine. The
human-generated truth is the result of semantically
tagging all utterances that didn?t yield a full parse
by one of the rules for the relevant context.
Figure 1 presents the ROC curves of human and
automatic truth generation for the grammar-based
and wordSLM systems. We can see that human se-
mantic tagging increases the accuracy substantially,
but this increase doesn?t seem to favor one system
over the other. We are thus led to believe that in our
case (very few well defined non-overlapping classes)
the automatic truth generation is sufficient. This
would not be the case, for example if for a given con-
text a time grammar and number were active classes.
Then, an utterance like ?seven? might lead to an er-
roneous slot being automatically filled while a hu-
man tagger (who would have access to the entire di-
alog) would tag it correctly.
In our experiments, we will use the hu-
man semantically tagged truth when available
(AppA MainMenu and AppB Origin). We have
checked that the conclusions of this paper are not
altered in any way if the automatic semantically
tagged truth had been used for these two contexts.
4 Results and analysis
4.1 Out-of-coverage analysis
Context (#utts) grammar- SLM-based
based
AppA MainMenu 1252 1086
AppB MainMenu 1287 1169
AppB Origin 1617 1161
AppB Passenger 492 414
AppB Time 327 309
Table 2: Number of utterances out-of-coverage for
each context.
Coverage is a function of the interpretation en-
gine. We can readily analyze the effect of going
from a grammar-based interpretation engine (gram-
mars in Table 1 are in parallel) to the robust ap-
proach (rules from grammars in Table 1 are used
in robust parsing). This is simply done by running
the interpretation engine on the orthographic tran-
scriptions. As expected, the coverage increased. Ta-
ble 2 shows the number of utterances that didn?t
fire any rule for each of the interpretation engines.
These include noise utterances as described in Table
1. If we remove the noise utterances, going from
the grammar-based interpretation to an SLM-based
one reduces the out-of-coverage by 31%. This result
is interesting because the data was collected from
directed-dialog applications which should be heav-
ily guiding the users to the grammar-based system?s
coverage.
4.2 Results with recognizer
The main results of this paper are found in Fig-
ure 2. It presents for grammar-based, wordSLM
and classSLM systems the four measurements men-
tioned in Sec.2.4 for AppA and AppB. We have
managed, with proper Viterbi beam settings, to keep
in the increase in CPU (grammar-based system ?
SLM-based system) between 0% and 24% relative.
We can see that the wordSLM is outperforming the
classSLM. The SLM-based systems outperform the
grammar-based systems substantially (? 30 ? 50%
error rate reduction on most of the confidence do-
main). The only exception to this is the classSLM
in AppA: we will come back to this in Sec. 4.4.
This can be interpreted as a different conclusion than
those of (Knight et al 2001; ten Bosch, 2005). The
discrepancy can be tied to the fact that the data we
are studying comes from a live deployment targeted
to the general public. In this case, we can make
the hypothesis that a large fraction of the popula-
tion is composed of na??ve users. As mentioned in
(ten Bosch, 2005), SLM-based systems perform bet-
ter than grammar-based ones on that cross-section of
the user population.
One might argue that the comparison between the
grammar-based and wordSLM systems is unfair be-
cause the wordSLM intrinsically records the a priori
probability that a user says a specific phrase while
the grammar-based system studied here didn?t ben-
efit from this information. In Sec. 4.4, we will ad-
dress this and show that a priori has a negligible ef-
fect in this context.
Note that these impressive results are surprisingly
easy to achieve. A simple process could be as fol-
lows. An application is developed using grammar-
based paradigm. After a limited deployment or pilot
with real users, a wordSLM is built from transcribed
(orthographic) data from the field. Then the recog-
80
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
CA
-in
FA-total
CA-in/FA-total
grammar-based (73ms)
wordSLM (74ms)
classSLM (108ms)
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
is
io
n
Recall
Recall/Precision
grammar-based (73ms)
wordSLM (74ms)
classSLM (108ms)
AppA
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
CA
-in
FA-total
CA-in/FA-total
grammar-based (94ms)
wordSLM (117ms)
classSLM (113ms)
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
is
io
n
Recall
Recall/Precision
grammar-based (94ms)
wordSLM (117ms)
classSLM (113ms)
AppB
Figure 2: ROC curves for AppA (top) and AppB (bottom). In parenthesis is the average time for the
recognition and interpretation.
nition and interpretation engines are upgraded. The
grammars built in the early stages of development
can largely be re-used as interpretation rules.
4.3 Amount of training data for SLM training
For the remaining Sections, we will use precision
and recall for simplicity. We will discuss an ex-
treme case where only a subset of 250 sentences
from the standard training set is used to train the
SLM. We have run experiments with two contexts:
AppA MainMenu and AppB Origin. These con-
texts are useful because a) we have the human-
generated truth and b) they represent extremes in the
complexity of grammars (see Section 2). On one
hand, the grammars for AppA MainMenu can cover
a total of 350 unique sentences while AppB Origin
can cover over 20k. As the amount of training
data for the SLMs is reduced from 5000 down to
250 sentences, the accuracy for AppA MainMenu
is only perceptibly degraded for the wordSLM and
classSLM systems on the entire confidence domain
(not shown here). On the other hand, in the case
of the more complex grammar (class), it is a dif-
ferent story which highlights a second regime. For
AppB Origin, the precision and recall curve is pre-
sented on Figure 3. In the case of classSLM (left),
81
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
is
io
n
Recall
Recall/Precision
grammar-based
classSLM - 5000
classSLM - 250
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
is
io
n
Recall
Recall/Precision
grammar-based
wordSLM - 5000
wordSLM - 250
wordSLM - 250 - no backoff
Figure 3: Precision and recall for the AppB Origin context as the amount of training data for the SLMs is
reduced. On the left, classSLM systems are presented; on the right it is the wordSLM.
even with very little training data, the accuracy is
far better than the grammar-based system and only
slightly degraded by reducing the size of the training
set. In the case of wordSLM (right), we can still see
that the accuracy is better than the grammar-based
system (refer to ?wordSLM - 250? on the graph),
but the reduction of training data has a much more
visible effect. If we remove the sentences that were
drawn from the grammar-based system?s coverage
(backoff - see Sec. 2.3), we can see that the drop in
accuracy is even more dramatic.
4.4 Coverage of interpretation rules and priors
As seen in Sec. 4.2, the classSLM results for AppA
are disappointing. They, however, shed some light
on two caveats of the robust approach described
here. The first caveat is the coverage of the interpre-
tation rules. As described in Sec. 2, the SLM-based
systems? training sets and interpretation rules (gram-
mars from Table 1) were built in isolation. This can
have a dramatic effect: after error analysis of the
classSLM system?s results, we noticed a large frac-
tion of errors for which the recognized string was a
close (semantically identical) variant of a rule in the
interpretation engine (?cancellations? vs ?cancella-
tion?). In response, we implemented a simple tool
to increase the coverage of the grammars (and hence
the coverage of the interpretation rules) using the list
of words seen in the training set. The criteria for se-
lection is based on common stem with a word in the
grammar.
The second caveat is based on fact that the
classSLM suffers from a lack of prior information
once the decoding process enters a specific class
since the grammars (class) do not contain priors.
The wordSLM benefits from the full prior informa-
tion all along the search. We have solved this by
training a small wordSLM within each grammar
(class): for each grammar, the training set for the
small wordSLM is composed of the set of fragments
from all utterances in the main training set that fire
that specific rule. Note that this represents a way
to have the grammar-based and SLM-based systems
share a common training set (Rayner et al 2005).
In Figure 4, we show the effect of increasing the
coverage and adding priors in the grammars. The
first conclusion comes in comparing the grammar-
based results with and without increased coverage
(enhanced+priors in figure) and priors. We see that
the ROC curves are one on top of the other. The only
differences are: a) at low confidence where the en-
hanced+priors version shows better precision, and
b) the CPU consumption is greatly reduced (73ms
? 52ms). When the enhanced+priors version of
the grammars (for classes and interpretation rules)
is used in the context of the classSLM system, we
can see that there is a huge improvement in the accu-
racy: this shows the importance of keeping the SLM
82
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  0.2  0.4  0.6  0.8  1
Pr
ec
is
io
n
Recall
Recall/Precision
gram.-based (73ms)
gram.-based - enhanced+priors (52ms)
classSLM (108ms)
classSLM - enhanced+priors (79ms)
Figure 4: ROC curves for AppA showing the ef-
fect of increasing the grammar coverage and adding
prior information in the grammars.
and interpretation rules in-sync. The final classSLM
ROC curve (Figure 4) is now comparable with its
wordSLM counter-part (Figure 2 upper right graph).
5 Conclusion
We have demonstrated in this paper that grammar-
based systems for commercially deployed directed
dialog applications targeted at the general public
can be improved substantially by using SLMs with
robust parsing. This conclusion is different than
(Rayner et al 2005) and can be attributed to that fact
that the general public is likely composed of a large
portion of na??ve users. We have sketched a very sim-
ple process to upgrade an application from using a
grammar-based approach to a robust approach when
in-service data and interpretation rules (grammars)
are available. We have also shown that only a very
small amount of data is necessary to train the SLMs
(Knight et al 2001). Class-based SLMs should be
favored in the case where the amount of training
data is low while word-based SLMs should be used
when enough training data is available. In the case
of non-overlapping classes, we have demonstrated
the soundness of automatically generated semantic
truth.
6 Acknowledgements
The author would like to acknowledge the helpful
discussions with M. Fanty, R. Tremblay, R. Lacou-
ture and K. Govindarajan during this project.
References
W. Ward. 1990. The CMU Air Travel Information Ser-
vice: Understanding spontaneous speech . Proc. of the
Speech and Natural Language Workshop, Hidden Val-
ley PA, pp. 127?129.
A.L. Gorin, B.A. Parker, R.M. Sachs and J.G. Wilpon.
1997. How may I help you?. Speech Communica-
tions, 23(1):113?127.
C. Hemphill, J. Godfrey and G. Doddington. 1990. The
ATIS spoken language systems and pilot corpus. Proc.
of the Speech and Natural Language Workshop, Hid-
den Valley PA, pp. 96?101.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. Proc. of EuroSpeech.
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, H. Isahara, K. Kanzaki
and Y. Nakao. 2005. A methodology for comparing
grammar-based and robust approaches to speech un-
derstanding. Proc. of EuroSpeech.
L. ten Bosch. 2005. Improving out-of-coverage lan-
guage modelling in a multimodal dialogue system us-
ing small training sets. Proc. of EuroSpeech.
M. Balakrishna, C. Cerovic, D. Moldovan and E. Cave.
2006. Automatic generation of statistical language
models for interactive voice response applications.
Proc. of ICSLP.
J. Gillett and W. Ward. 1998. A language model com-
bining tri-grams and stochastic context-free grammars.
Proc. of ICSLP.
F. Jelinek. 1990. Readings in speech recognition, Edited
by A. Waibel and K.-F. Lee , pp. 450-506. Morgan
Kaufmann, Los Altos.
W. Xu and A. Rudnicky. 2000. Language modeling for
dialog system. Proc. of ICSLP.
V. Goel and R. Gopinath. 2006. On designing context
sensitive language models for spoken dialog systems.
Proc. of ICSLP.
83
