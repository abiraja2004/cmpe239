COL1NG 82, J. Horecky, {e,L )
North-Holland Publishing Company 
(~ Academia, 1982 
AN IMPROVED LEFT-CORNER PARSING ALGORITHM 
Kenneth M. Ross 
Computer Science Laboratory 
Central Research Laboratories 
Texas Instruments Incorporated 
Dallas, Texas 
U.S.A. 
This paper proposes a series of modif ications to the le f t  
corner parsing algorithm for context-free grammars. I t  
is argued that the resu l t inga lgor i thm is both e f f i c ient  
and f lex ib le  and is ,  therefore, a good choice for the 
parser used in a natural language interface.  
INTRODUCTION 
Gr i f f i ths  and Pet.rick (1965) propose several algorithms for recognizing sentences 
of context- free grammars in the general case One of these algorithms, the NBT 
(Non-selective Bottom to Top) Algorithm, has since been cal led a " le f t - corner"  
algorithm. Of la te ,  in terest  has been rekindled in le f t -corner  parsers. Chester 
(1980) proposes a modif icat ion to the Gr i f f i ths  and Petrick algorithm which "com- 
bines phrases before i t  has found a l l  of the i r  components." Slocum (1981) shows 
that a le f t -corner  parser inspired by Gr i f f i ths  and Petr ick 's  algorithm and by 
Chester's performs quite well when compared with parsers based on a Cocke-Kasami- 
Younger algorithm (see Younger 1967). 
This paper w i l l  propose modif ications to Gr i f f i ths  and Petr ick 's  NBT algorithm 
which resul t  in a more e f f i c ient  parsing algorithm. A se lect ive version of this 
new algorithm has been implemented in Maclisp on a DEC 2060 and in Lisp Machine 
Lisp on an LMI Lisp Machine. I t  is being used as the context- free component of 
a parser being used to bui ld natural language interfaces at Texas Instruments. 
This algorithm is l ike  the NBT algorithm and d i f fe rs  from Chester's in that i t  
does not require the grammar to be in a special format. Any rule of a context- 
free grammar is acceptable. The new algorithm builds on the Gr i f f i ths  and 
Petrick algorithm and is an extension of the algorithm proposed in Ross (1981). 
The algorithm given in Gr i f f i ths  and Petrick (1965) (henceforth G+P) is a rec- 
ognit ion algorithm, not a parsing algorithm. Thus, i t  w i l l  only indicate 
whether or not a str ing can be produced from a grammar. I t  w i l l  not produce a 
parse tree. Although algorithms to recognize or parse context- free grammars can 
be stated in terms of push-down store automata, G+P state the i r  algorithm in terms 
of tur ing machines because the algorithm is easier to understand in these terms. 
A somewhat modified version of the i r  algorithm wi l l  be given in the next section. 
These modif ications transform the algorithm into a parsing algorithm and also 
s impl i fy  i t  a b i t .  
The G+P algorithm employs two push down stacks. The modified algorithm to be 
given below w i l l  use three, cal led alpha, beta and gamma. Turing machine instruc-  
tions are of  the fol lowing form, where A,B,C,D,E and F can be arb i t ra ry  str ings of 
symbols from the terminal and nonterminal alphabet. 
\[A,B,C\] --> \[D,E,F\] i f  "Conditions" 
This is to be interpreted as fol lows: 
333 
334 K,M. ROSS 
I f  A is on top of stack alpha, 
B is on top of stack beta, 
C is on top of stack gamma, 
and "Conditions" are satisfied 
then replace A by D, B by E, and C by F. 
THE NBT ALGORITHM 
The NBT algorithm is a nonselective version of the SBT (Selective Bottom to Top) 
algorithm, also given in G+P. The only difference between the two is that the 
SBT algorithm employs a reachability matrix to selectively eliminate bad paths 
before trying them. For more on this, see G+P and Ross (1981). For the purpose 
of this paper, i t  is not necessary to say anything more than that the addition of 
a reachability matrix modifies the algorithm only slightly and serves only to make 
the algorithm more efficient. 
A version of NBT modified to employ a third stack and to parse rather than recog- 
nize strings follows. This algorithm will be modified further throughout he 
paper. 
(1) \[VI,X,Y\] --> \[~,V2 ... Vn,t X,A Y\] 
i f  A --> Vl V2 ... Vn is a rule of 
the phrase structure grammar 
X is in the set of nonterminals and 
Y is anything 
(2) \[X,t,A\] --> \[A X,~,~\] 
i f  A is in the set of nonterminals 
(3) \[B,B,Y\] --> \[~,~,Y\] 
i f  B is in the set of nonterminals or 
terminals 
To begin, put the terminal s t r ing  to be parsed followed by END on stack alpha. 
Put the nonterminal which is to be the root node of the tree to be constructed 
followed by END on stack beta. Put END on stack gamma. The symbol t is neither 
a terminal nor a nonterminal. I f  END is on top of each stack, the st r ing has 
been recognized. I f  none of the tur ing machine instruct ions apply and END is not 
on the top of each stack, the  path which led to this  s i tuat ion  was a bad path and 
does not y ie ld  a va l id  parse. 
The rules necessary to give a parse tree can be stated informal ly  ( i .e . ,  not in 
terms of tur ing machine instruct ions)  as fo l lows:  
When ( I )  is appl ied,  attach V1 beneath A. 
When (3) is appl ied,  attach ~he B on alpha B as the 
r ight  daughter of the top,symbol on gamma. 
Note that there is a formal statement of the parsing version of NBT in Gr i f f i ths  
(1965). However, i t  is somewhat more complicated and obscures what is  going on 
during the parse. Therefore, the informal procedure given above w i l l  be used 
instead. 
In tu i t i ve ly ,  what NBT does is put the symbols on alpha together in a bottom-up 
manner with the ult imate goal of constructing a tree that has, at i t s  top, what- 
ever nonterminal symbol is on top of beta. So, to parse a sentence of English, 
NBT would begin with the lex ica l  categories of the words to be parsed as a 
sentence on alpha and the nonterminal "S" on beta. An appl icat ion of  tur ing 
machine inst ruct ion ( I )  reduces this  problem to a simpler one. ( I )  f inds some 
phrase structure rule containing the symbol that is on top of alpha immediately 
AN ff~PROVED LEFT-CORNER PARSING ALGORITHM 335 
af ter  the arrow. So, i f  the f i r s t  symbol on alpha was "det" ,  the phrase s t ructure  
ru le  NP --> det AdjP N would qua l i fy .  By th is  app l i ca t ion  of  ( I ) ,  the problem 
is reduced to bu i ld ing an AdjP and #inding an N from the symbols on alpha. Once 
th is  is done, the trees fo r  the "det" ,  the "AdjP" and the "N" would be combined 
in to  an NP. By app l i ca t ion  o f  (2) ,  the NP would be put on alpha. Then a ru le  
with NP immediately fo l low ing  the arrow would be looked fo r  so that  ( I )  could 
apply again. 
NBT is a nondeterminist ic  a lgor i thm. The nondeterminism comes from two places. 
F i r s t ly ,  ru le  ( I )  can apply in more than one way. For th is  to happen, there 
would need to be two phrase s t ructure  rules with the same nonterminal symbol 
immediately a f te r  the arrow. The fo l lowing two rules are an example o f  th is .  
X - -> Y Z1 Z2 Z3 
R - -> Y Rl X2 
Secondly, ru le  (3) and ru le ( I )  could apply in the same s i tuat ion .  In tu i t i ve ly ,  
an app l i ca t ion  of  ru le (3) indicates that  a t ree topped by node B was being 
searched fo r  and a tree topped by node B has been found~ so use the tree jus t  
found as the tree that  was sought. Rule ( I )  could apply as well i f  a phrase 
s t ructure  ru le  of  the form X --> B Y1 Y2 . . .  Yn ex isted.  Applying ( I )  indicates 
that  the B being sought is not the B that  was jus t  bu i l t .  Rather, the B that  was 
jus t  bu i l t  is an in i t ia l  subtree of  the B being sought. 
RULZS WITH ABBREVIATIONS 
~ important aspect o f  the modif ied algor ithm being proposed is that  i t  can deal 
d i rec t ly  with rules which employ abbrev iatory  conventions which are u t i l i zed  by 
l ingu is ts .  Thus, parentheses (expressing opt iona lnodes)  and cur ly  brackets 
(expressing the fac t  that  one of  the set o f  nodes in brackets should be chosen) 
can appear in the rules that  the parser accesses when parsing a s t r ing .  
Assume ~hat ie f t  and r ight  parentheses are put o~ stack beta as separate elements. 
Also assume chat le f t  and r ight  cur ly  brackets are put on stack beta as separate 
items. Given these assumptions~ to modify NBT to handle rules with parenthesized 
elements, the fo l lowing tur ing machine ins t ruct ions  must be added. 
(4) IX, ( Cl C2 , . .  Cn ),Y\]  --> IX,C\] C2 . . .  C,~,Y\] 
(5) \ [X , (  C! C2 . . .  Cn ),Y\] --~ \[X,~,V\] 
For a l l  i ,  Ci = ( Cj Cj+l . . .  Cp ) or 
C1 Cl+i . . .  Cm ~ or 
X 
i f  X is i.i the sec of  terminals .  
The f i r s t  ru le  w i l l  apply when the parenthesized node is present. The second ru\]e 
w i l l  apply when the node is not present. The Ci var iab le  handle cases o f  nested 
parentheses or  cur ly  brackets. In formal ly ,  a Ci is a var iab le  that  stands fo r  a 
nonterminal,  a terminal ,  a le f t  parenthesis fol lowed by some number o f  expressions 
which are Ci 's  fol lowed by a r ight  parenthesis ,  or a le f t  cur ly  bracket fo l lowed 
by some number o f  expressions which are Ci 's  fol lowed by a r ight  cur ly  bracket.  
The fo l lowing rules are necessary to d i rec t ly  parse with rules containing cur ly  
brackeLs. 
(6) \[X,{ C1 X,Y\] - -~ \ [X ,{ ,Y \ ]  
(7) Ix,{ ~I x , , \ ]  --> \[X,Cl : ,v\] 
I f  X not = } 
(8) Ix , :  }ov\] .. . .  \ [x,o,Y\]  
(~  \[x~ < Cl } ,? \ ]  - -> \[x,c~,Y\]  
336 K.M. ROSS 
(I0) \[X,:  CI,Y\] --> \ [X , : ,? \ ]  
where : is a special symbol which is 
neither a terminal or a nonterminal symbol, 
C1 is a Ci type var iable as defined ear l ie r .  
Once these modif ications are incorporated, the resul t ing algorithm wi l l  be more 
e f f i c ient  than i f  the NBT algorithm were used with abbreviated rules completely 
expanded into many d i s t inc t  rules. To see why this  is so, consider a s i tuat ion  
in which there was a rule of the form X --> ~I A2 . . .  An (Z). I f  th is  was 
replaced by two rules,  X --> A1 A2 . . .  An Z and X --> A1 A2 . . .  An, the parse 
would have to be sp l i t  immediately upon encounteging X. However, i f  the a l terna-  
t ive  solut ion being proposed were used, rather than parsing for  AI, A2 . . . . .  to 
An twice, they would only be parsed for  once. The parse path would not sp l i t  
unt i l  i t  came time to decide whether we wanted to look for  Z or not. In general, 
every rule which has, fo l lowing the arrow, some number of ob l igatory  elements 
fol lowed by a parenthesized element w i l l  resu l t  in a savings. Thus, any such rule 
can be parsed with more e f f i c ient ly  than the two rules ~t would be turned into i f  
parentheses were el iminated. Note that the addit ional  cost here is quite small. 
For each parenthesized element, (4) and (5) w i l l  each apply once. In the a l terna-  
t ive  so lut ion,  many rules might apply unnecessarily to the parse for  the nodes 
which came before the parenthesized node. 
There is a class of grammars for  which the solut ion proposed here w i l l  require a 
b i t  more work than the solut ion where parentheses are simply el iminated from the 
grammar. These are grammars that only have rules inwhich parenthesized items 
come f i r s t  and have no rules in which ob l igatory  items precede optional ones. In 
a grammar with both kinds of rules,  the savings made far  outweigh t~e amount of 
extra work needed. Since the classes of grammars used in parsing systems 
general ly have both kinds of rules,  my solut ion w i l l  resul t  in a savings for  
these. Note that a s imi lar  e f f i c iency  argument can be made for  the cur ly bracket 
case. 
The above rules w i l l  handle a l l  occurrences of  parentheses and cur ly brackets 
except for  those in which the item immediately fo l lowing the arrow in a phrase 
structure rule is in parentheses or cur ly brackets. The algorithm could be 
modified to handle these cases d i rec t ly ,  however, th is  w i l l  not increase 
e f f ic iency.  Items in cur ly brackets or parentheses that immediately fo l low the 
arrow in a phrase structure rule must be expanded immediately upon encountering 
them. There is no savings in postponing this  expansion unt i l  run time. Putting 
o f f  the choice of how to expand such a phrase structure rule w i l l  not al low paths 
to be mergedtbgether.  Therefore, the best way to handle these is to expand a l l  
such rules into rules that do not have this  property. 
L ingu is t i ca l l y ,  the above is an interest ing resu l t .  Linguists have claimed that 
use of parentheses s impl i f ies  the grammar. Since simpler grammars are preferred 
to more complex ones, a solut ion which collapses two rules to one by parentheses 
is preferable to .a solut ion that has two d i s t inc t  rules. In parsing, we see that 
in many instanc'~s, the ~se of one rule with parentheses rather than two rules 
without results in the parser operating more e f f i c ient ly .  I t  is able to merge 
parse paths together which would have been d i s t inc t  had several context- f ree rules 
not been collapsed together as one, using the abbreviatory conventions. Thus, a 
notat iona ldev ice  which was or ig ina l ly  proposed to s impl i fy  phrase structure rules 
actua l ly  results in a more e f f i c ient  parse in many cases. Therefore, at least for  
some cases, we have addit ional  evidence for  the use of parentheses in phrase 
structure rules. 
AN IMPROVED LEFT-CORNER PARSING ALGORITHM 337 
DEPTH OR BREADTH FIRST? 
There has of yet been no discussion of the order in which the algorithm proceeds. 
The statement of the algorithm is completely neutral in this respect. However, an 
implementation must impose some control structure. When a parse is started, there 
is one 3-tuple containing the information on stacks alpha, beta, and gamma. In 
general, there are many different rules of the parsing algorithm that can be 
applied after this point. In order to assure that all possible paths are pursued 
to completion, i t  is necessary to proceed in a principled way. 
One strategy is to push one state as far as i t  wi l l  go. That is, apply one of 
the rules that are applicable, get a new state, and then apply one of the appli- 
cable rules to that new state. This can continue until either no rules apply or a 
parse is found. I f  no rules apply, i t  was a bad parse path. I f  a parse is found, 
i t  is one of possibly many parses for the sentence. In either case, the algorithm 
must continue on and pursue all other alternative paths. An easy way to do this 
and assure that al l  alternatives are pursued is to backtrack to the last choice 
point, pick another applicable rule, and continue in the manner described earl ier. 
By doing this until the parser has backed up through all  possible choice points, 
all parses of the sentence wil l  be found. A parser that works in this manner is 
a depth-first backtracking parser. This is probably the easiest control structure 
to use for a left-corner parser. 
Alternative control structures are possible. For instance, rather than pursuing 
one path as far as possible, one could go down a parse path to some desired 
distance, save that state for later, and come back up to the top and start some 
other parse path. The original parse path could be pursued later from the point 
at which i t  was stopped. The problem with such an approach is keeping track of 
all the options. 
In the algorithm being proposed here, the decision of whether the parse proceeds 
in a depth-first or breadth-first manner is governed by a parameter which is 
adjustable. Thus, the parser can proceed to a setable depth down each parse path 
before going off and pursuing others. This mechanism works by saving the state 
of the parser when i t  reaches the desired depth down a particular parse path. 
Once all paths are pursued to this depth, the parser is called again with each of 
the states that were saved. 
To enable the parser to function as described-above, the control structure for a 
depth-first parser described earl ier is used. To introduce the ab i l i ty  to proceed 
in a breadth-first manner, the parser is only given a subset of the input string. 
Then, the item MORE is inserted after the last item that is given to the parser. 
I f  no other instructions apply and MORE is on top of stack beta, the parser must 
begin to backtrack as described earl ier. Additionally, the state must be saved. 
Once all backtracking is completed, more input is put on beta and parsing begins 
again with each of the saved states. 
By changing the amount of input that is given, the degree to which the parser pro- 
ceeds either depth or breadth f i r s t  can be controlled. I f  one word is given at a 
time, the parser is compleZely breadth-first. I f  the entire sentence is given, 
i t  is completely depth-first. Any other amount results in some combination of the 
two. 
. This mechanism enables the algor i thm to eas i ly  incorporate a wel l - formed substr ing 
tab le .  A l l  that  needs to be done is compare the set o f  saved states and merge the 
ones that  have subgoals in common. By set t ing  the parameter to d i f fe rent  values, 
the degree to which the wel l - formed substr ing table is  used can be cont ro l led .  
This is par t i cu la r ly  important in l ight  o f  Slocum's resu l ts  which ind icate  that  
the overhead involved in maintaining such a table can exceed the savings that  i t  
338 K.M. ROSS 
gives. By having the degree to which the table :s used be adjustable, the proper 
sett ing can be determined, based on the grammar and the sorts of queries that are 
asked most often. 
Addi t ional ly ,  the algorithm can be used to process the sentence word by word as i t  
is typed in. When used as the parser in a natural language interface,  this can 
increase the speed of a parse since work can proceed as the user is typing and 
composing his input. 
Bibliography 
\[1\] Chester, D., A parsing algorithm that extends phrases, American Journal of 
Computational L inguist ics,  6-2 (1980) 87-96. 
\[2\] Gr i f f i ths ,  T., On procedures for constructing structural  descriptions for 
three parsing algorithms, Communications of the ACM, 8 (1965) 594. 
\[3\] Gr i f f i ths ,  T. and Petrick, S.R., On the re la t ive 'e f f i c ienc ies  of context-free 
granTnar recognizers, Communications of the ACM, 8 (1965) 289-300. 
\[4\] Ross, K., Parsing English phrase structure, Ph.D. Dissertat ion, Dept. of 
L inguist ics,  Univ. of Mass. (Sept. 1981). 
\[5\] Slocum, J . ,  A pract ical  comparison of parsing strategies,  Proceedings of the 
19th Annual Meeting of the ACL, (1981) 1-6. 
\[6\] Younger, D., Recognition and parsing of context-free language in time n3, 
Information and Control I0 (1967) 189-208. 
