Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 161?167,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Relationship between Utterances and ?Enthusiasm?
in Non-task-oriented Conversational Dialogue
Ryoko TOKUHISA
Toyota Central R&D Labs., INC.
Nagakute Aichi JAPAN
tokuhisa@mosk.tytlabs.co.jp
Ryuta TERASHIMA
Toyota Central R&D Labs., INC.
Nagakute Aichi JAPAN
ryuta@mosk.tytlabs.co.jp
Abstract
The goal of this paper is to show how
to accomplish a more enjoyable and en-
thusiastic dialogue through the analysis
of human-to-human conversational dia-
logues. We first created a conversational
dialogue corpus annotated with two types
of tags: one type indicates the particu-
lar aspects of the utterance itself, while
the other indicates the degree of enthusi-
asm. We then investigated the relationship
between these tags. Our results indicate
that affective and cooperative utterances
are significant to enthusiastic dialogue.
1 Introduction
For a non-task-oriented conversational dialogue
system (e.g. home robots), we should strive for
a dialogue strategy that is both enjoyable and
enthusiastic, as well as efficient. Many studies
have been conducted on efficient dialogue strate-
gies (Walker et al, 1998; Litman et al, 2000; Ko-
matani et al, 2002), but it is not clear how to ac-
complish a more ?human-like enthusiasm? for a
conversational dialogue. The goal of this paper is
to show the types of utterances that contribute to
enthusiasm in conversational dialogues.
2 Corpus Annotation
We created a conversational corpus annotated with
two types of tags: one type indicates particular
aspects of the utterance itself, while the other in-
dicates the degree of enthusiasm in the dialogue.
This section describes our corpus and tagging
scheme in detail.
2.1 Corpus Collection
As a result of previous works, several conversa-
tional dialogue corpora have been collected with
various settings (Graff and Bird, 2000; TSENG,
2001). The largest conversational dialogue cor-
pus is the Switchboard Corpus, which consists of
about 2400 conversational English dialogues be-
tween two unfamiliar speakers over the telephone
on one of 70 topics (e.g. pets, family life, educa-
tion, gun control, etc.).
Our corpus was collected from face-to-face in-
teraction between two unfamiliar speakers. The
reasons were 1) face-to-face interaction increases
the number of enthusiastic utterances, relative to
limited conversational channel interaction such as
over the telephone; 2) the interaction between un-
familiar speakers reduces the enthusiasm resulting
from unobserved reasons during the recording; 3)
the exchange in a twoparty dialogue will be sim-
pler than that of a multiparty dialogue.
We created a corpus containing ten conversa-
tional dialogues that were spoken by an operator
(thirties, female) and one of ten subjects (twenties
to sixties, equal numbers of males and females).
Before beginning the recording session, the sub-
ject chose three cards from fifteen cards on the fol-
lowing topics:
Food, Travel, Sport, Hobbies, Movies, Prizes,
TV Programs, Family, Books, School, Music,
Pets, Shopping, Recent Purchases, Celebrities
Straying from the selected topic was permitted,
because these topic cards were only ever intended
as a prompt to start the dialogue. Thus, we col-
lected ten dialogues, each about 20 minutes long.
For convenience, in this paper, we refer to the op-
erator as speaker1, and the subject as speaker2.
161
2.2 Annotation of DAs and RRs
2.2.1 Definition of tagging scheme
Dialogue Acts (DAs) and Rhetorical Relations
(RRs) are well-known tagging schemes for anno-
tating an utterance or a sentence. DAs are tags that
pertain to the function of an utterance itself, while
RRs indicate the relationship between sentences or
utterances. We adopted both tags to allow us to an-
alyze the aspects of utterances in various ways, but
adapted them slightly for our particular needs.
The DA annotations were based on SWBD-
DAMSL and MRDA (Jurafsky et al, 1997;
Dhillon et al, 2004). The SWBD-DAMSL is
the DA tagset for labeling a conversational dia-
logue. The Switchboard Corpus mentioned above
was annotated with SWBD-DAMSL. On the other
hand, MRDA is the DA tagset for labeling the
dialogue of a meeting between multiple partici-
pants. Table 1 shows the correspondence between
SWBD-DAMSL/MRDA and our DAs1. We de-
scribe some of the major adaptations below.
The tags pertaining to questions: In SWBD-
DAMSL and MRDA, the tags pertaining to ques-
tions were classified by the type of their form
(e.g. Wh-question). We re-categorized them into
request and confirm in terms of the ?act? for
Japanese.
The tags pertaining to responses: We subdivided
Accept and Reject into objective responses (ac-
cept,denial) and subjective responses (agree, dis-
agree).
The emotional tags: We added tags that indicate
the expression of admiration and interest.
The overlap tags with the RRs definition: We
did not use any tags (e.g. Summary), that over-
lapped the RR definition.
Consequently, we defined 47 DAs for analyzing a
conversational dialogue.
The RR annotations were based on the rhetor-
ical relation defined in Rhetorical Structure The-
ory (RST) (Mann and Thompson, 1988; Stent and
Allen, 2000). Our RR definition was based only
on informational level relation defined in RST be-
cause we annotated the intentional level with DAs.
Table 2 shows the correspondence between the in-
formational relation of RST and our RRs. We de-
scribe some of the major adaptations below.
Subdivide evaluation: The evaluation reflects the
degree of enthusiasm in the dialogue, so we di-
1The tags listed in italics are based on SWBD-DAMSL
while those in boldface are based on MRDA.
Table 1: Dialogue Act Definition
SWBD-
DAMSL/MRDA
Our DAs Definition
Statement non
opinion
inform objective
fact
inform non opin-
ion
Statement opin-
ion
inform subjective
element
inform opinion
Wh-Question request objective
fact
request non opin-
ion
Yes-No-
question
request agreement request agreement
opinion
Open-
Question
confirm objective
fact
confirm non opin-
ion
Or-Question confirm agreement confirm agreement
opinion
Accept accept accept non opinion
agree accept opinion
Reject denial denial non opinion
disagree denial opinion
not marked express admiration inform admiration
Summary DEL. (mark as RR) ?????
Table 2: Rhetorical Relation Definition
Mann?s RST Our RRs definition
evaluation
(positive)
U2 is a positive evaluation
about U1
Evaluation evaluation
(negative)
U2 is a negative evalua-
tion about U1
evaluation
(neutral)
U2 is neutral evaluation
about U1
Volitional
cause
volitional
cause-effect
U2 is a volitional action,
and U1 cause U2
Volitional re-
sult
No Definition addition U2 consists of a part of U1
vided the Evaluation into three types of evaluation
(positive/negative/neutral).
Integrate the causal relations: We use a di-
rected graph representation for RR annotations, so
that we integrate Non-volitional cause and Non-
volitional result into non-volitional cause-effect,
and Volitional cause and Volitional result into vo-
litional cause-effect.
Add addition relation: The RRs initially repre-
sent the structure of the written text, segmented
into clause-like units. Therefore, they do not cover
those cases in which one clause is uttered by one
speaker, but communicatively completed by an-
other. So, we added an addition to our RRs. The
following is an example of addition.
speaker A: the lunch in our company cafeteria
speaker B: is good value for money
We defined 16 RRs as a result of these adaptations.
162
Context: The father of speaker2 likes watching movies, and so established a home theater system in their living room.
1:speaker2 [addition] that s why my family really loves movies these days <inform objective fact>
you watch them one after another, don t you? <signal understanding>
<confirm objective fact>2:speaker1
[apposition]
[elaboration]
about 2 or 3 movies per week <accept><inform objective fact>
so many? <signal understanding><exclamation><confirm objective fact>
[evaluation3:speaker2
4:speaker1
we sometimes watch many more <accept><inform objective fact>5:speaker2 [elaboration]
I suppose you <signal understanding>6:speaker1
I suppose it s nice to watch them in your home 
without interruptions, right?
<signal understanding><confirm 
agreement><confirm objective fact>
[volitional
elaboration]
(neutral)]
elaboration]
cause-effect]
Figure 1: Example of Dialogue annotated with DAs and RRs (Originally in Japanese)
2.2.2 Annotation of DAs and RRs
DAs and RRs are annotated using the MMAX2
Annotation Tool 2 (Muller and Strube, 2003). Fig-
ure 1 shows an example of our corpus annotated
with DAs and RRs. The ? ? symbol in Figure 1
indicates a DA, while the [ ] symbol indicates an
RR. Below, we describe our annotation process for
DAs and RRs.
Step 1. Utterance Segmentation: All the utter-
ances in the dialogue are segmented into DA seg-
ments, each of which we define as an utterance.
In Figure 1, the utterance is surrounded with a
square. In this step, we also eliminated backchan-
nels from the exchange.
Step 2. Annotation of DAs: DAs are annotated
to all utterances. In those cases in which one DA
alone cannot represent an utterance, two or more
DAs are used (see Figure 1 line 2).
Step 3. Annotation of Adjacency Pairs: Adja-
cency pairs (APs) are labeled. An AP consists of
two utterances where each part is produced by a
different speaker. In Figure 1, the solid and dotted
lines correspond to links between the APs.
Step 4. Annotation of RRs: RRs on APs are la-
beled. A solid line indicates an AP that is labeled
with RRs, while a dotted line indicates an AP that
is not labeled with RRs. If a single RR cannot
represent the type of the relationship, two or more
RRs are used.
2.3 Annotation of Enthusiasm
2.3.1 Related Work on Annotating the degree
of enthusiasm
Wrede et al annotated Involvement to the ICSI
Meeting Recorder Corpus (Wrede and Shriberg,
2This supports multilevel annotation and the creation
of a relationship between utterances. http://www.eml-
research.de/english/research/nlp/down-load/mmax.php
utterance
... backchannel
...
Part Of Dialogue...POD
Dialogue
speaker2
speaker1
Ui-4 Ui-3 Ui-2 Ui-1 Ui Ui+1 Ui+2 Ui+3 Ui+4
PODi-2
PODi-1
PODi
PODi+1
PODi+2
Si-2
Si-1
S i
Si+1
Si+2
Score of enthusiasm...S
Figure 2: Rating the score of the enthusiasm
2003b; Wrede and Shriberg, 2003a). In their
method, a rater judges involvement (agreement,
disagreement, other) or Not especially involved or
Don?t Know, by listening to each utterance with-
out the context of the dialogue. In the exper-
iment, nine raters provided ratings on 45 utter-
ances. Inter-rater agreement between Involved and
Not especially involved yielded a Kappa of ?=.59
(p<.01), but 13 of the 45 utterances (28.9%) were
rated as Don?t Know by at least one of the raters.
For automatic detection, it is certainly effective to
rate Involvement without context. However, the re-
sults indicate that it is quite difficult to recognize
Involvement from a single utterance. Moreover,
the fluctuation of Involvement can not be recog-
nized by this method because Involvement is cate-
gorized into five categories only.
2.3.2 Our Method of Annotating Enthusiasm
In this section, we propose a method for eval-
uating the degree of enthusiasm. We describe the
process for evaluating the degree of enthusiasm.
Step 1. Rating the score of enthusiasm for POD
A rater estimates a score of the enthusiasm
corresponding to the part of dialogue (POD),
which is a series of five utterances. As men-
tioned above, the backchannels are not re-
garded as utterances. In Figure 2, S
i
denotes
163
the score for the enthusiasm of POD
i
. The
value of the score can be from 10 to 90.
90 ... Extreme
70 ... Moderate
50 ... Neutral
30 ... Low
10 ... No
When rating the score, a rater must obey the
following four rules.
1. Listen to each POD more than three
times.
2. Perform estimation based on the entire
POD and not just part of the POD.
3. Be sure that own ratings represented a
consistent continuum.
4. Estimate as participants, not as side-
participants.
We did not give any definitions or examples
to rate the enthusiasm, a rater estimated a
score based on their subjective determination.
Step 2. Calculate the score of enthusiasm for an
utterance
The score of enthusiasm for an utterance U
i
is given by the average of the scores of the
PODs that contain utterance U
i
.
V (U
i
) =
1
5
i+2
?
j=i?2
S
j
(1)
Step 3. Calculate the degree of enthusiasm for an
utterance and an adjacency pair
In this paper, we deal with all the degrees of
enthusiasm as a normalized score, which we
call Enthusiasm, because different raters may
have different absolute levels of enthusiasm.
Then, Enthusiasm for U
i
is given as follows:
E(U
i
) =
V (U
i
) ? V (U)
?
(2)
where
V (U) =
1
n
n
?
i=1
V (U
i
)
? =
?
?
?
?
1
n
n
?
i=1
{V (U
i
) ? V (U)}
2
n denotes the number of utterances in the di-
alogue.
In addition, Enthusiasm for AP
i
is given by
the average of Enthusiasms of the utterances
where are AP
i
.
E(AP
i
) =
1
2
{E(U
j
) + E(U
k
)} (3)
U
j
and U
k
denote the utterances in AP
i
.
3 Estimation of Annotated Corpus
3.1 Reliability of DAs and RRs
We examined the inter-annotator reliability for
two annotators3 for DAs, RRs and APs, using four
dialogues mentioned above. Before the start of the
investigation, one annotator segmented a dialogue
into utterances. The number of segmented utter-
ances was 697. The annotaters annotated them as
described in steps 2 to 4 of Section 2.2.2.
DAs annotation: We can not apply the Kappa
statistics since it cannot be applied to multiple tag
annotations. We then apply formula 4 to examine
the reliability.
ag. = (AgreedDAs) ? 2
Total of DAs annotated by A1andA2
?100 (4)
The result of agreement was 1542 DAs (65.5%)
from a total of 2355 DAs. The major reasons for
the disagreement were as follows.
? Disagreement of subjective/objective ... 124(15.3%)
? Disagreement of request/confirm ... 112(13.8%)
? Disagreement of partial/whole ... 72(8.9%)
Building APs: We examined the agreement of
building APs between utterances. The result of
agreement was 536 APs (85.2%) from the total of
the 629 APs that were built by the annotators. This
result shows that the building of APs is reliable.
RRs annotation: We also examined the agree-
ment of RRs annotation. We applied formula 5
to this examination.
ag. = (AgreedRRs) ? 2Total of RRs annotated by A1andA2?100 (5)
As a result, we found agreement for 576 RRs
(59.6%) out of a total of 967 RRs.
3We refer to these annotators as A1 and A2. A1 is one of
the authors of this paper.
164
Table 3: Correlation between random rating and
sequential rating
correlation coefficient
speaker1 speaker2
twenties,female 0.833 0.881
twenties,male 0.971 0.950
sixties,female 0.972 0.973
sixties,male 0.971 0.958
-3
-2
-1
0
1
2
3
time
En
thu
sia
sm
R3(speaker1) R3(speaker2)
R4(speaker1) R4(speaker2)
Figure 3: Enthusiasm of dialogue of speaker1 and
speaker2(thirties,female)
3.2 Estimation Context Influence on the
rating of Enthusiasm
In order to examine the influence of the context on
the rating of Enthusiasm, one rater noted Enthusi-
asm under two conditions: 1) Listening to PODs
randomly, and 2) Listening to PODs sequentially
as dialogue. Table 3 shows the correlation be-
tween the random rating and the sequential rating.
The correlation coefficient was calculated for the
Enthusiasm of each of the two participants. The
?speaker1? shows the correlation of the Enthusi-
asm rated as speaker1, and ?speaker2? shows the
correlation of the Enthusiasm rated as speaker2.
This was found to be approximately 0.9 in both
cases. These results show that Enthusiasm can be
estimated stably and that the context has little in-
fluence.
4 Relationship between DAs/RRs and
Enthusiasm
We investigated the relationship between
DAs/RRs and Enthusiasm, using four dia-
logues. The DAs/RRs corpus annotated by A1
was used in this analysis because A1 is one
of the authors of this paper and has a better
knowledge of the DAs and RRs tagging scheme
than A2. The Enthusiasm corpus annotated by
R3 was used because we found that R4 rated
Enthusiasm based on non-subjective reasons:
after the examination of the rating, R4 said that
speaker1 spoke enthusiastically but that it seemed
unnatural because speaker1 had to manage the
recording of the dialogue, which appears in the
results as speaker1?s Enthusiasm as annotated by
R4 as a notable difference (see Figure 3).
Figure 4 and 5 show the ratio of the frequency
of DAs and RRs in each of the levels of Enthu-
siasm over a range of 0.5. If DAs and RRs were
evenly annotated for any level of Enthusiasm, the
graph will be completely even. However, the
graph shows the right side as being higher if the
DAs and RRs increase as Enthusiasm increases.
Conversely, the graph shows the left side as being
higher if the DAs and RRs fall as Enthusiasm in-
creases. The number in Figure 4 and 5 indicates
the average Enthusiasm for each DA and RR. If
the average is positive, it means that the frequency
of the DAs and RRs is high in that part in which
Enthusiasm is positive. In contrast, if the average
is negative, it means that the frequency of the DAs
and RRs is high in that part in which Enthusiasm
is negative.
We determined the following two points about
the tendency of the DAs frequency.
Tendency of subjective and objective DAs: The
ratio of the frequency of those DAs related to sub-
jective elements tends to increase as Enthusiasm
increases (see *1 in Figure 4). In contrast, the ra-
tio of the frequency of those DAs pertaining to ob-
jective matters tends to decrease (see *2 in Figure
4) or equilibrate as Enthusiasm increases (see *3
in Figure 4) . We can thus conclude that those ex-
changes related to subjective elements increases in
the enthusiastic dialogue, but those related to ob-
jective elements decrease or equilibrate.
Tendency of affective DAs: The ratio of the fre-
quency of those DAs related to the affective con-
tents tends to increase as Enthusiasm increases
(see *4 in Figure 4). However, express admiration,
which is also related to affective contents, tends to
decrease (see *5 in Figure 4). We then analyzed
several instances of admiration. As a result, we
found that the prosodic characteristic of admira-
tion utterance will cause this tendency.
Furthermore, we noted the following two points
about the tendency of the RRs frequency.
Tendency of additional utterances: The ratio of
the frequency of addition, which completes the
165






sign
al
un
ders
tan
ding acce
pt
infor
m
objecti
ve fa
ct confi
rm
objecti
ve fa
ct i
nfor
m
su
bjecti
ve
elem
en
t
agr
ee
requ
est
 fact
excla
ma
tion
sh
ow int
ere
st
confir
m
agr
eem
en
t
exp
res
s
adm
ira
tion
sign
al p
ar
tia
l no
un
ders
tan
ding
sym
pa
thy
ne
utr
al
requ
est
rep
eti
tion
confir
m othe
r's
sbject
ive den
ial
sh
ow hu
mor
Dialogue Act
Frequ
en
cy(ra
tio)
-2.5 ?-2.0 -2.0?-1.5 -1.5 ?-1.0 -1.0?-0.5 -0.5 ?0.0 0.0?0.5 0.5 ?1.0 1.0?1.5 1.5 ?2.0
0.03 -0.12 -0.03 -0.11
0.30 0.12
-0.07 0.04 0.29 0.11 -0.62 -0.06 -0.04 0.03 0.27 0.70 0.14 1.07
*2
*2
*2
*1 *1 *1*4 *4
*5*4*3
*3
*1
Figure 4: Frequency of DAs per Enthusiasm
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
ela
bora
tion addit
ion
ap
pos
ition
non
 volition
al
cau
se-
effect
int
erp
ret
ati
on
volition
al ca
us
e-
effect
eva
lua
tion
(pos
itive
)
an
tit
he
sis
ins
tan
ce
circu
ms
tan
ce
eva
lua
tion
(ne
utr
al)
su
mm
ar
y
Rhetorical Relation
Frequ
en
cy(ra
tio)
-2.5 ?-2.0 -2.0?-1.5 -1.5 ?-1.0 -1.0?-0.5 -0.5 ?0 0?0.5 0.5 ?1.0 1.0?1.5 1.5 ?2.0

  	
	


 

  

*6 *7
Figure 5: Frequency of RRs per Enthusiasm
Context:Mother of speaker2 does not cook dinner when the 
              father is out.
1 speaker1: but if he s there then she
2 speaker2: cooks a really delicious dinner
3 speaker1: wow
Figure 6: Example of addition
other participant?s utterance, tends to increase as
Enthusiasm increases (see *6 in Figure 5). Figure
6 shows a dialogue example. There are addition
relations between lines 1 and 2. This shows that
the participant makes an utterance cooperatively
by completing the other?s utterances in enthusias-
tic dialogues. Such cooperative utterance is a sig-
nificant component of enthusiastic dialogues.
Tendency of positive evaluation: The ratio of the
frequency of positive evaluation tends to increase
at lower Enthusiasm and higher Enthusiasm (see
*7 in Figure 5). We analyzed some instances of
Context:About a hamster and its exercise instrument.
1 speaker2: two hamsters run together in their exercise wheel
2 speaker2: they run up and down and side by side
3 speaker1: but surely they can t they run together if they aren t
4 speaker2: exactly
5 speaker2: one gets carried along if it stops when the other 
                   continues to run
6 speaker1: is it?  does it lean forward?
getting along very well?
7 speaker2: yes
8 speaker2: sometimes it falls out
9 speaker1: that s so cute
Figure 7: Example of positive evaluation
positive evaluation, we then found that the speaker
tries to arouse the dialogue by an utterance of
positive evaluation at lower Enthusiasm, and the
speaker summarizes the previous discourse with a
positive evaluation at higher Enthusiasm. Figure
7 shows an example of positive evaluation in the
enthusiastic dialogue. In this case, speaker1 ex-
166
presses positive evaluation on line 9 about the el-
ement on line 8. The utterance on line 9 also has
the function of expressing an overall positive eval-
uation of the previous discourse.
5 Conclusion and Future Research
We analyzed the relationship between utterances
and the degree of enthusiasm in human-to-human
conversational dialogue. We first created a conver-
sational dialogue corpus annotated with two types
of tags: DAs/RRs and Enthusiasm. The DA and
RR tagging scheme was adapted from the defini-
tion given in a previous work, and an Enthusiasm
tagging scheme is proposed. Our method of rating
Enthusiasm enables the observation of the fluctu-
ation of Enthusiasm, which enables the detailed
analysis of the relationship between utterances and
Enthusiasm. The result of the analysis shows the
frequency of objective and subjective utterances
related to the level of Enthusiasm. We also found
that affective and cooperative utterances are sig-
nificant in an enthusiastic dialogue.
In this paper, we only analyzed the relationship
between DAs/RRs and Enthusiasm, but we expect
the non-linguistic-feature related with Enthusiasm
so that we would analyze the relationship in future
research. And, we try to achieve more reliable an-
notation by reviewing our tagging scheme. Fur-
thermore, we would apply the results of the analy-
sis to our conversational dialogue system.
References
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey, and
Elizabeth Shriberg. 2004. Meeting Recorder
Project: Dialog Act Labeling Guide. ICSI Techni-
cal Report, (TR-04-002).
David Graff and Steven Bird. 2000. Many Uses, Many
Annotations for Large Speech Corpora: Switch-
board and TDT as Case Studies. LREC2000.
Dan Jurafsky, Liz Shriberg, and Debra Biasca.
1997. Switchboard SWBD-DAMSL Shallow-
Discourse-Function Annotation Coders Manual.
www.dcs.shef.ac.uk/nlp/amities/files/bib/ics-tr-97-
02.pdf.
Kazunori Komatani, Tatsuya Kawahara, Ryosuke Ito,
and Hiroshi Okuno. 2002. Efficient Dialogue Strat-
egy to Find Users? Intended Items from Information
Query Results. In Proceedings of the COLING.
Diane Litman, Satinder Singh, Michael Kearns, and
Marilyn Walker. 2000. NJFun: A Reinforcement
Learning Spoken Dialogue System. In Proceedings
of the ANLP/NAACL.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a Functional
Theory of Text Organization. Text, 8(3):243?281.
Christoph Muller and Michael Strube. 2003. Multi-
Level Annotation in MMAX. In Proceedings of the
4th SIGdial Workshop on Discourse and Dialogue.
Amanda Stent and James Allen. 2000. Annotating Ar-
gumentation Acts in Spoken Dialog. Technical Re-
port 740.
Shu-Chuan TSENG. 2001. Toward a Large Sponta-
neous Mandarin Dialogue Corpus. In Proceedings
of the 2nd SIGdial Workshop on Discourse and Dia-
logue.
Marilyn A. Walker, Jeanne C. Fromer, and Shrikanth
Narayanan. 1998. Learning Optimal Dialogue
Strategies: A Case Study of a Spoken Dialogue
Agent for Email. In Proceedings of COLING/ACL.
Britta Wrede and Elizabeth Shriberg. 2003a. Spotting
?Hot Spots? in Meetings: Human Judgements and
Prosodic Cues. Eurospeech-03, pages 2805?2808.
Britta Wrede and Elizabeth Shriberg. 2003b. The Re-
lationship between Dialogue Acts and Hot Spots in
Meetings. IEEE ASRU Workshop.
167
