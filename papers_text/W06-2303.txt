Robust Parsing of the Proposition Bank
Gabriele Musillo
Depts of Linguistics and Computer Science
University of Geneva
2 Rue de Candolle
1211 Geneva 4
Switzerland
musillo4@etu.unige.ch
Paola Merlo
Department of Linguistics
University of Geneva
2 Rue de Candolle
1211 Geneva 4
Switzerland
merlo@lettres.unige.ch
Abstract
In this paper, we extend an existing statis-
tical parsing model to produce richer out-
put parse trees, annotated with PropBank
semantic role labels. Our results show
that the model can be robustly extended to
produce more complex output parse trees
without any loss in performance and sug-
gest that joint inference of syntactic and
semantic representations is a viable alter-
native to approaches based on a pipeline
of local processing steps.
1 Introduction
Recent successes in statistical syntactic parsing
based on supervised learning techniques trained
on a large corpus of syntactic trees (Collins, 1999;
Charniak, 2000; Henderson, 2003) have brought
forth the hope that the same approaches could be
applied to the more ambitious goal of recover-
ing the propositional content and the frame se-
mantics of a sentence. Moving towards a shal-
low semantic level of representation is a first ini-
tial step towards the distant goal of natural lan-
guage understanding and has immediate applica-
tions in question-answering and information ex-
traction. For example, an automatic flight reserva-
tion system processing the sentence I want to book
a flight from Geneva to Trento will need to know
that from Geneva denotes the origin of the flight
and to Trento denotes its destination. Knowing
that these two phrases are prepositional phrases,
the information provided by a syntactic parser, is
only moderately useful.
The growing interest in learning deeper infor-
mation is to a large extent supported and due to
the recent development of semantically annotated
databases such as FrameNet (Baker et al, 1998)
or the Proposition Bank (Palmer et al, 2005), that
can be used as training resources for a number of
supervised learning paradigms. We focus here on
the Proposition Bank (PropBank). PropBank en-
codes propositional information by adding a layer
of argument structure annotation to the syntactic
structures of the Penn Treebank (Marcus et al,
1993). Verbal predicates in the Penn Treebank
(PTB) receive a label REL and their arguments
are annotated with abstract semantic role labels
A0-A5 or AA for those complements of the pred-
icative verb that are considered arguments while
those complements of the verb labelled with a se-
mantic functional label in the original PTB receive
the composite semantic role label AM-X , where
X stands for labels such as LOC, TMP or ADV,
for locative, temporal and adverbial modifiers re-
spectively. A tree structure with PropBank labels
for a sentence from the PTB (section 00) is shown
in Figure 1 below. PropBank uses two levels of
granularity in its annotation, at least conceptually.
Arguments receiving labels A0-A5 or AA do not
express consistent semantic roles and are specific
to a verb, while arguments receiving an AM-X la-
bel are supposed to be adjuncts and the respective
roles they express are consistent across all verbs.1
Recent approaches to learning semantic role la-
bels are based on two-stage architectures. The first
stage selects the elements to be labelled, while the
second determines the labels to be assigned to the
selected elements. While some of these models
are based on full parse trees (Gildea and Jurafsky,
2002; Gildea and Palmer, 2002), other methods
have been proposed that eschew the need for a full
1There are thirteen semantic role labels for modifiers. See
(Palmer et al, 2005) for a detailed discussion of PropBank
semantic roles labels.
11
S




HH
HH
HH
HH
HH
NP-A1




PP
PP
PP
P
the government?s borrowing authority
VP










HH
HH
HH
HH
XXX
XXX
XXX
XXX
XXX
XX
VBD-REL
dropped
PP-AM-TMP
 HH
IN
at
NP
NN
midnight
NP-AM-TMP
NNP
Tuesday
PP-A4
 HH
TO
to
NP
QP


PP
P
$ 2.80 trillion
PP-A3
 HH
IN
from
NP
QP


PP
P
$ 2.87 trillion
Figure 1: A sample syntactic structure from the PropBank with semantic role annotations.
parse (CoNNL, 2004; CoNLL, 2005). Because of
the way the problem has been formulated ? as a
pipeline of parsing (or chunking) feeding into la-
belling ? specific investigations of integrated ap-
proaches that solve both the parsing and the se-
mantic role labelling problems at the same time
have not been studied.
We present work to test the hypothesis that a
current statistical parser (Henderson, 2003) can
output richer information robustly, that is with-
out any significant degradation of the parser?s ac-
curacy on the original parsing task, by explicitly
modelling semantic role labels as the interface be-
tween syntax and semantics.
We achieve promising results both on the simple
parsing task, where the accuracy of the parser is
measured on the standard Parseval measures, and
also on the parsing task where the more complex
labels of PropBank are taken into account. We will
call the former task Penn Treebank parsing (PTB
parsing) and the latter task PropBank parsing be-
low.
These results have several consequences. On
the one hand, we show that it is possible to build a
single integrated robust system successfully. This
is a meaningful achievement, as a task combining
semantic role labelling and parsing is more com-
plex than simple syntactic parsing. While the shal-
low semantics of a constituent and its structural
position are often correlated, they sometimes di-
verge. For example, some nominal temporal mod-
ifiers occupy an object position without being ob-
jects, like Tuesday in Figure 1 below. On the other
hand, our results indicate that the proposed mod-
els are robust. To model our task accurately, ad-
ditional parameters must be estimated. However,
given the current limited availability of annotated
treebanks, this more complex task will have to be
solved with the same overall amount of data, ag-
gravating the difficulty of estimating the model?s
parameters due to sparse data. The limited avail-
ability of data is increased further by the high vari-
ability of the argumental labels A0-A5 whose se-
mantics is specific to a given verb or a given verb
sense. Solving this more complex problem suc-
cessfully, then, indicates that the models used are
robust.
Finally, we achieve robustness without simpli-
fying the parsing architecture. Specifically, ro-
bustness is achieved without resorting to the stip-
ulation of strong independence assumptions to
compensate for the limited availability and high
variability of data. Consequently, such an achieve-
ment demonstrates not only that the robustness
of the parsing model, but also its scalability and
portability.
2 The Basic Parsing Architecture
To achieve the complex task of assigning seman-
tic role labels while parsing, we use a family of
statistical parsers, the Simple Synchrony Network
(SSN) parsers (Henderson, 2003), which do not
make any explicit independence assumptions, and
are therefore likely to adapt without much modi-
fication to the current problem. This architecture
has shown state-of-the-art performance.
SSN parsers comprise two components, one
which estimates the parameters of a stochastic
model for syntactic trees, and one which searches
for the most probable syntactic tree given the
12
parameter estimates. As with many other sta-
tistical parsers (Collins, 1999; Charniak, 2000),
SSN parsers use a history-based model of parsing.
Events in such a model are derivation moves. The
set of well-formed sequences of derivation moves
in this parser is defined by a Predictive LR push-
down automaton (Nederhof, 1994), which imple-
ments a form of left-corner parsing strategy. The
derivation moves include: projecting a constituent
with a specified label, attaching one constituent
to another, and shifting a tag-word pair onto the
pushdown stack.
Unlike standard history-based models, SSN
parsers do not state any explicit independence as-
sumptions between derivation steps. They use a
neural network architecture, called Simple Syn-
chrony Network (Henderson and Lane, 1998), to
induce a finite history representation of an un-
bounded sequence of moves. The history repre-
sentation of a parse history d1, . . . , di?1, which
we denote h(d1, . . . , di?1), is assigned to the con-
stituent that is on the top of the stack before the ith
move.
The representation h(d1, . . . , di?1) is computed
from a set f of features of the derivation move
di?1 and from a finite set D of recent history rep-
resentations h(d1, . . . , dj), where j < i ? 1. Be-
cause the history representation computed for the
move i ? 1 is included in the inputs to the com-
putation of the representation for the next move
i, virtually any information about the derivation
history could flow from history representation to
history representation and be used to estimate the
probability of a derivation move. However, the re-
cency preference exhibited by recursively defined
neural networks biases learning towards informa-
tion which flows through fewer history represen-
tations. (Henderson, 2003) exploits this bias by
directly inputting information which is considered
relevant at a given step to the history representa-
tion of the constituent on the top of the stack be-
fore that step. In addition to history representa-
tions, the inputs to h(d1, . . . , di?1) include hand-
crafted features of the derivation history that are
meant to be relevant to the move to be chosen
at step i. For each of the experiments reported
here, the set D that is input to the computation of
the history representation of the derivation moves
d1, . . . , di?1 includes the most recent history rep-
resentation of the following nodes: topi, the node
on top of the pushdown stack before the ith move;
the left-corner ancestor of topi (that is, the second
top-most node on the parser?s stack); the leftmost
child of topi; and the most recent child of topi, if
any. The set of features f includes the last move in
the derivation, the label or tag of topi, the tag-word
pair of the most recently shifted word, and the left-
most tag-word pair that topi dominates. Given the
hidden history representation h(d1, ? ? ? , di?1) of a
derivation, a normalized exponential output func-
tion is computed by SSNs to estimate a probabil-
ity distribution over the possible next derivation
moves di.2
The second component of SSN parsers, which
searches for the best derivation given the pa-
rameter estimates, implements a severe pruning
strategy. Such pruning handles the high compu-
tational cost of computing probability estimates
with SSNs, and renders the search tractable. The
space of possible derivations is pruned in two dif-
ferent ways. The first pruning occurs immediately
after a tag-word pair has been pushed onto the
stack: only a fixed beam of the 100 best deriva-
tions ending in that tag-word pair are expanded.
For training, the width of such beam is set to five.
A second reduction of the search space prunes
the space of possible project or attach derivation
moves: a best-first search strategy is applied to the
five best alternative decisions only.
The next section describes our model, extended
to produce richer output parse trees annotated with
semantic role labels.
3 Learning Semantic Role Labels
Previous work on learning function labels during
parsing (Merlo and Musillo, 2005; Musillo and
Merlo, 2005) assumed that function labels repre-
sent the interface between lexical semantics and
syntax. We extend this hypothesis to the seman-
tic role labels assigned in PropBank, as they are
an exhaustive extension of function labels, which
have been reorganised in a coherent inventory of
labels and assigned exhaustively to all sentences in
the PTB. Because PropBank is built on the PTB, it
inherits in part its notion of function labels which
is directly integrated into the AM-X role labels.
A0-A5 or AA labels correspond to many of the
unlabelled elements in the PTB and also to those
elements that PTB annotators had classified as re-
2The on-line version of Backpropagation is used to train
SSN parsing models. It performs a gradient descent with
a maximum likelihood objective function and weight decay
regularization (Bishop, 1995).
13
S




HH
HH
HH
HH
HH
NP-A1




PP
PP
PP
P
the government?s borrowing authority
VP










HH
HH
HH
HH
XXX
XXX
XXX
XXX
XXX
XX
VBD-REL
dropped
PP-AM-TMP
 HH
IN(-AM-TMP)
at
NP
NN
midnight
NP-AM-TMP
NNP(-AM-TMP)
Tuesday
PP-A4
 HH
TO
to
NP
QP


PP
P
$ 2.80 trillion
PP-A3
 HH
IN
from
NP
QP


PP
P
$ 2.87 trillion
Figure 2: A sample syntactic structure with semantic role labels lowered onto the preterminals.
ceiving a syntactic functional label such as SBJ
(subject) or DTV (dative).
Because they are projections of the lexical se-
mantics of the elements in the sentence, semantic
role labels are projected bottom-up, they tend to
appear low in the tree and they are infrequently
found on the higher levels of the parse tree, where
projections of grammatical, as opposed to lexical,
elements usually reside. Because they are the in-
terface level with syntax, semantic labels are also
subject to distributional constraints that govern
syntactic dependencies, such as argument struc-
ture or subcategorization. We attempt to capture
such constraints by modelling the c-command re-
lation. Recall that the c-command relation relates
two nodes in a tree, even if they are not close to
each other, provided that the first node dominat-
ing one node also dominate the other. This notion
of c-command captures both linear and hierarchi-
cal constraints and defines the domain in which
semantic role labelling applies.
While PTB function labels appear to overlap to
a large extent with PropBank semantic rolel labels,
work by (Ye and Baldwin, 2005) on semantic la-
belling prepositional phrases, however, indicates
that the function labels in the Penn Treebank are
assigned more sporadically and heterogeneously
than in PropBank. Apparently only the ?easy?
cases have been tagged functionally, because as-
signing these function tags was not the main goal
of the annotation. PropBank instead was anno-
tated exhaustively, taking all cases into account,
annotating multiple roles, coreferences and dis-
continuous constituents. It is therefore not void
of interest to test our hypothesis that, like function
labels, semantic role labels are the interface be-
tween syntax and semantics, and they need to be
recovered by applying constraints that model both
higher level nodes and lower level ones.
We assume that semantic roles are very often
projected by the lexical semantics of the words in
the sentence. We introduce this bottom-up lexical
information by fine-grained modelling of seman-
tic role labels. Extending a technique presented in
(Klein and Manning, 2003) and adopted in (Merlo
and Musillo, 2005; Musillo and Merlo, 2005) for
function labels, we split some part-of-speech tags
into tags marked with semantic role labels. The
semantic role labels attached to a non-terminal di-
rectly projected by a preterminal and belonging to
a few selected categories (DIR, EXT, LOC, MNR,
PNC, CAUS and TMP) were propagated down to
the pre-terminal part-of-speech tag of its head. To
affect only labels that are projections of lexical se-
mantics properties, the propagation takes into ac-
count the distance of the projection from the lex-
ical head to the label, and distances greater than
two are not included. Figure 2 illustrates the result
of this operation.
In our augmented model, inputs to each history
representation are selected according to a linguis-
tically motivated notion of structural locality over
which dependencies such as argument structure or
subcategorization could be specified.
In SSN parsing models, the set D of nodes that
are structurally local to a given node on top of the
stack defines the structural distance between this
given node and other nodes in the tree. Such a no-
tion of distance determines the number of history
representations through which information passes
14
? ? ? ?
? ? ? ?
?1
?2. . . . . .
. . .. . .
S
VP
C-COMMAND
Figure 3: Flow of information in original SSN parsers (dashed lines), enhanced by biases specific to
semantic role labels to capture the notion of c-command (solid lines).
to flow from the representation of a node i to the
representation of a node j. By adding nodes to
the set D, one can shorten the structural distance
between two nodes and enlarge the locality do-
main over which dependencies can be specified.
To capture a locality domain appropriate for se-
mantic role parsing, we add the most recent child
of topi labelled with a semantic role label to the set
D. These additions yield a model that is sensitive
to regularities in structurally defined sequences
of nodes bearing semantic role labels, within and
across constituents. This modification of the bi-
ases is illustrated in Figure 3.
This figure displays two constituents, S and VP
with some of their respective child nodes. The VP
node is assumed to be on the top of the parser?s
stack, and the S one is supposed to be its left-
corner ancestor. The directed arcs represent the
information that flows from one node to another.
According to the original SSN model in (Hender-
son, 2003), only the information carried over by
the leftmost child and the most recent child of a
constituent directly flows to that constituent. In
the figure above, only the information conveyed
by the nodes ? and ? is directly input to the node
S. Similarly, the only bottom-up information di-
rectly input to the VP node is conveyed by the
child nodes ? and ?. In the original SSN models,
nodes bearing a function label such as ?1 and ?2
are not directly input to their respective parents.
In our extended model, information conveyed by
?1 and ?2 directly flows to their respective par-
ents. So the distance between the nodes ?1 and
?2, which stand in a c-command relation, is short-
ened. For more information on this technique to
capture domains induced by the c-command rela-
tion, see (Musillo and Merlo, 2005).
We report the effects of these augmentations on
parsing results in the experiments described below.
4 Experiments
Our extended semantic role SSN parser was
trained on sections 2-21 and validated on section
24 from the PropBank. Training, validating and
testing data sets consist of the PTB data anno-
tated with PropBank semantic roles labels, as pro-
vided in the CoNLL-2005 shared task (Carreras
and Marquez, 2005).
Our augmented model has a total 613 of non-
terminals to represents both the PTB and Prop-
Bank labels of constituents, instead of the 33 of
the original SSN parser. The 580 newly introduced
labels consist of a standard PTB label followed
by a set of one or more PropBank semantic role
such as PP-AM-TMP or NP-A0-A1. As a result
of lowering the six AM-X semantic role labels,
240 new part-of-speech tags were introduced to
partition the original tag set which consisted of 45
tags. SSN parsers do not tag their input sentences.
To provide the augmented model with tagged in-
put sentences, we trained an SVM tagger whose
features and parameters are described in detail in
(Gimenez andMarquez, 2004). Trained on section
2-21, the tagger reaches a performance of 95.45%
on the test set (section 23) using our new tag set.
As already mentioned, argumental labels A0-A5
are specific to a given verb or a given verb sense,
thus their distribution is highly variable. To re-
duce variability, we add some of the tag-verb pairs
licensing these argumental labels to the vocabu-
15
F R P
PropBank training and PropBank parsing task 82.3 82.1 82.4
PropBank training and PTB parsing task 88.8 88.6 88.9
PTB training and PTB parsing task (Henderson, 2003) 88.6 88.3 88.9
Table 1: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks
and the original SSN parser.
lary of our model. We reach a total of 4970 tag-
word pairs.3 This vocabulary comprises the orig-
inal 512 pairs of the original SSN model, and our
added pairs which must occur at least 10 times in
the training data. Our vocabulary as well as the
new 240 POS tags and the new 580 non-terminal
labels are included in the set f of features input to
the history representations as described in section
2.
We perform two different evaluations on our
model trained on PropBank data. Recall that
we distinguish between two parsing tasks: the
PropBank parsing task and the PTB parsing task.
To evaluate the first parsing task, we compute
the standard Parseval measures of labelled recall
and precision of constituents, taking into account
not only the 33 original labels but also the 580
newly introduced PropBank labels. This evalua-
tion gives us an indication of how accurately and
exhaustively we can recover this richer set of non-
terminal labels. The results, computed on the test-
ing data set from the PropBank, are shown on the
first line of Table 1.
To evaluate the PTB task, we compute the la-
belled recall and precision of constituents, ignor-
ing the set of PropBank semantic role labels that
our model assigns to constituents. This evalua-
tion indicates how well we perform on the stan-
dard PTB parsing task alone, and its results on the
testing data set from the PTB are shown on the
second line of Table 1.
The third line of Table 1 gives the performance
on the simpler PTB parsing task of the original
SSN parser (Henderson, 2003), that was trained
on the PTB data sets contrary to our SSN model
trained on the PropBank data sets.
5 Discussion
These results clearly indicate that our model can
perform the PTB parsing task at levels of per-
3Such pairs consists of a tag and a word token. No attempt
at collecting word types was made.
formance comparable to state-of-the-art statistical
parsing, by extensions that take the nature of the
richer labels to be recovered into account. They
also suggest that the relationship between syntac-
tic PTB parsing and semantic PropBank parsing
is strict enough that an integrated approach to the
problem of semantic role labelling is beneficial.
In particular, recent models of semantic role la-
belling separate input indicators of the correlation
between the structural position in the tree and the
semantic label, such as path, from those indicators
that encode constraints on the sequence, such as
the previously assigned role (Kwon et al, 2004).
In this way, they can never encode directly the con-
straining power of a certain role in a given struc-
tural position onto a following node in its struc-
tural position. In our augmented model, we at-
tempt to capture these constraints by directly mod-
elling syntactic domains defined by the notion of
c-command.
Our results also confirm the findings in (Palmer
et al, 2005). They take a critical look at some
commonly used features in the semantic role la-
belling task, such as the path feature. They sug-
gest that the path feature is not very effective be-
cause it is sparse. Its sparseness is due to the oc-
currence of intermediate nodes that are not rele-
vant for the syntactic relations between an argu-
ment and its predicate. Our model of domains is
less noisy, and consequently more robust, because
it can focus only on c-commanding nodes bearing
semantic role labels, thus abstracting away from
those nodes that smear the pertinent relations.
(Yi and Palmer, 2005) share the motivation of
our work. Like the current work, they observe
that the distributions of semantic labels could po-
tentially interact with the distributions of syntactic
labels and redefine the boundaries of constituents,
thus yielding trees that reflect generalisations over
both these sources of information.
To our knowledge, no results have yet been pub-
lished on parsing the PropBank. Accordingly, it is
not possible to draw a straigthforward quantitative
16
F R P
(Haghighi et al, 2005) 83.4 83.1 83.7
(Pradhan et al, 2005) 83.3 83.0 83.5
(Punyakanok et al, 2005) 83.1 82.8 83.3
(Marquez et al, 2005) 83.1 82.8 83.3
(Surdeanu and Turmo, 2005) 82.7 82.5 83.0
PropBank SSN 81.6 81.3 81.9
Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and state-
of-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank
validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html).
comparison between our PropBank SSN parser
and other PropBank parsers. However, state-of-
the-art semantic role labelling systems (CoNLL,
2005) use parse trees output by state-of-the-art
parsers (Collins, 1999; Charniak, 2000), both for
training and testing, and return partial trees anno-
tated with semantic role labels. An indirect way
of comparing our parser with semantic role la-
bellers suggests itself. We merge the partial trees
output by a semantic role labeller with the output
of a parser it was trained on, and compute Prop-
Bank parsing performance measures on the result-
ing parse trees. The first five lines of Table 2 re-
port such measures for the five best semantic role
labelling systems (Haghighi et al, 2005; Pradhan
et al, 2005; Punyakanok et al, 2005; Marquez
et al, 2005; Surdeanu and Turmo, 2005) accord-
ing to (CoNLL, 2005). The partial trees output
by these systems were merged with the parse trees
returned by (Charniak, 2000)?s parser. These sys-
tems use (Charniak, 2000)?s parse trees both for
training and testing as well as various other infor-
mation sources including sets of n-best parse trees
(Punyakanok et al, 2005; Haghighi et al, 2005)
or chunks (Marquez et al, 2005; Pradhan et al,
2005) and named entities (Surdeanu and Turmo,
2005). While our preliminary results indicated in
the last line of Table 2 are not state-of-the-art, they
do demonstrate the viability of SSN parsers for
joint inference of syntactic and semantic represen-
tations.
6 Conclusions
In this paper, we have explored extensions to an
existing state-of-the-art parsing model. We have
achieved promising results on parsing the Propo-
sition Bank, showing that our extensions are suf-
ficiently robust to produce parse trees annotated
with shallow semantic information. Future work
will lie in extracting semantic role relations from
such richly annotated trees, for applications such
as information extraction or question answering.
In addition, further research will explore the rele-
vance of semantic role features to parse reranking.
Acknowledgements
We thank the Swiss National Science Foundation
for supporting this research under grant number
101411-105286/1. We also thank James Hender-
son and Ivan Titov for allowing us to use and mod-
ify their SSN software, Xavier Carreras for pro-
viding the CoNLL-2005 shared task data sets and
the anonymous reviewers for their valuable com-
ments.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the Thirty-Sixth Annual Meeting of the As-
sociation for Computational Linguistics and Seven-
teenth International Conference on Computational
Linguistics (ACL-COLING?98), pages 86?90, Mon-
treal, Canada.
Christopher M. Bishop. 1995. Neural Networks for
Pattern Recognition. Oxford University Press, Ox-
ford, UK.
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, Ann Arbor,
MI USA.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meeting
of North American Chapter of Association for Com-
putational Linguistics (NAACL?00), pages 132?139,
Seattle, Washington.
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. the-
17
sis, Department of Computer Science, University of
Pennsylvania.
CoNLL. 2005. Ninth Conference on Computational
Natural Language Learning (CoNLL-2005). Ann
Arbor, MI, USA.
CoNNL. 2004. Eighth Conference on Computa-
tional Natural Language Learning (CoNLL-2004).
Boston, MA, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2002),
pages 239?246, Philadelphia, PA.
Jesus Gimenez and Lluis Marquez. 2004. Svmtool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC?04), Lisbon, Portugal.
Aria Haghighi, Kristina Toutanova, and Christopher
Manning. 2005. A joint model for semantic role
labeling. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI USA.
James Henderson and Peter Lane. 1998. A connection-
ist architecture for learning to parse. In Proceedings
of 17th International Conference on Computational
Linguistics and the 36th Annual Meeting of the As-
sociation for Computational Linguistics (COLING-
ACL?98), pages 531?537, University of Montreal,
Canada.
Jamie Henderson. 2003. Inducing history representa-
tions for broad-coverage statistical parsing. In Pro-
ceedings of the Joint Meeting of the North American
Chapter of the Association for Computational Lin-
guistics and the Human Language Technology Con-
ference (NAACL-HLT?03), pages 103?110, Edmon-
ton, Canada.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the ACL (ACL?03), pages
423?430, Sapporo, Japan.
Namhee Kwon, Michael Fleischman, and Eduard
Hovy. 2004. Senseval automatic labeling of se-
mantic roles using maximum entropy models. In
Senseval-3, pages 129?132, Barcelona, Spain.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: the Penn Treebank.
Computational Linguistics, 19:313?330.
Lluis Marquez, Pere Comas, Jesus Gimenez, and Neus
Catala. 2005. Semantic role labeling as sequential
tagging. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI USA.
Paola Merlo and Gabriele Musillo. 2005. Accurate
function parsing. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 620?627, Vancouver, British Columbia,
Canada, October.
Gabriele Musillo and Paola Merlo. 2005. Lexical and
structural biases for function parsing. In Proceed-
ings of the Ninth International Workshop on Pars-
ing Technology, pages 83?92, Vancouver, British
Columbia, October.
Mark Jan Nederhof. 1994. Linguistic Parsing and Pro-
gram Transformations. Ph.D. thesis, Department of
Computer Science, University of Nijmegen.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31:71?105.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role chunking combining complementary
syntactic views. In Proceedings of CoNLL-2005,
Ann Arbor, MI USA.
Vasin Punyakanok, Peter Koomen, Dan Roth, and Wen
tau Yih. 2005. Generalized inference with multiple
semantic role labeling systems. In Proceedings of
CoNLL-2005, Ann Arbor, MI USA.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic role
labeling using complete syntactic analysis. In Pro-
ceedings of CoNLL-2005, Ann Arbor, MI USA.
Patrick Ye and Timothy Baldwin. 2005. Semantic role
labelling of prepositional phrases. In Proceedings of
the Second International Joint Conference on Nat-
ural Language Processing (IJCNLP-05), pages pp.
779?791, Jeju, South Korea.
Szu-ting Yi and Martha Palmer. 2005. The integration
of semantic parsing and semantic role labelling. In
Proceedings of CoNLL?05, Ann Arbor, Michigan.
18
