Tutorial Abstracts of ACL 2012, page 3,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Topic Models, Latent Space Models, Sparse Coding, and All That: A
systematic understanding of probabilistic semantic extraction in large
corpus
Eric Xing
School of Computer Science
Carnegie Mellon University
Abstract
Probabilistic topic models have recently
gained much popularity in informational re-
trieval and related areas. Via such mod-
els, one can project high-dimensional objects
such as text documents into a low dimen-
sional space where their latent semantics are
captured and modeled; can integrate multiple
sources of information?to ?share statistical
strength? among components of a hierarchical
probabilistic model; and can structurally dis-
play and classify the otherwise unstructured
object collections. However, to many practi-
tioners, how topic models work, what to and
not to expect from a topic model, how is it dif-
ferent from and related to classical matrix al-
gebraic techniques such as LSI, NMF in NLP,
how to empower topic models to deal with
complex scenarios such as multimodal data,
contractual text in social media, evolving cor-
pus, or presence of supervision such as la-
beling and rating, how to make topic mod-
eling computationally tractable even on web-
scale data, etc., in a principled way, remain un-
clear. In this tutorial, I will demystify the con-
ceptual, mathematical, and computational is-
sues behind all such problems surrounding the
topic models and their applications by present-
ing a systematic overview of the mathemati-
cal foundation of topic modeling, and its con-
nections to a number of related methods pop-
ular in other fields such as the LDA, admix-
ture model, mixed membership model, latent
space models, and sparse coding. I will offer
a simple and unifying view of all these tech-
niques under the framework multi-view latent
space embedding, and online the roadmap of
model extension and algorithmic design to-
ward different applications in IR and NLP. A
main theme of this tutorial that tie together a
wide range of issues and problems will build
on the ?probabilistic graphical model? formal-
ism, a formalism that exploits the conjoined
talents of graph theory and probability theory
to build complex models out of simpler pieces.
I will use this formalism as a main aid to dis-
cuss both the mathematical underpinnings for
the models and the related computational is-
sues in a unified, simplistic, transparent, and
actionable fashion.
3
