CONTROLLED TRANSFORMATIONAL SENTENCE GENERATION 
Madeleine Bates 
Bolt Beranek and Newman, Inc. 
Robert Ingria 
Department of Linguistics, MIT 
I. INTRODUCTION 
This paper describes a sentence generator that 
was built primari ly to focus on syntactic form 
and syntactic relationships. Our main goal was 
to produce a tutorial system for the English 
language; the intended users of the system are 
people with language delaying handicaps such as 
deafness, and people learning English as a 
foreign language. For these populations, 
extensive exposure to standard English 
constructions (negatives, questions, 
relatlvization, etc.) and their interactions 
is necessary. ? The purpose of the generator was 
to serve as a powerful resource for tutorial 
programs that need examples of particular 
constructions and/or related sentences to embed 
in exercises or examples for the student. The 
focus of the generator is thus not so much on 
what to express as on how to express it in 
acceptable English. This is quite different 
from the focus of most other language 
generation systems. Nonetheless, our system 
could be interfaced to a more goal-directed 
semantic component. 
The mechanism of transformational grammar was 
chosen because it offered both a way to 
exercise tight control over the surface 
syntactic form of a sentence and a good model 
for the production of groups of sentences that 
are syntactical ly related (e.g. the active and 
passive forms of a transitive sentence). By 
control l ing (at a very high level) the rules 
that are applied and by examining the detailed 
syntactic relationships in the tree structures 
at each end of the derivation, the tutorial 
part of the system accesses a great deal of 
information about the syntax of the sentences 
that are produced by the generator; this 
knowledge is used to give explanations and 
hints to the user in the context of the 
particular exercise that the student is 
attempting. 
The transformational generator is composed of 
three magor parts: a base component that 
produces base trees, a transformer that applies 
transformational rules to the  trees to derive a 
surface tree, and a set of mechanisms to 
control the operation of the first two 
components. We will discuss each of the 
components of this system separately. 
2. THE BASE COMPONENT 
The base component is a set of functions that 
implicitly embody context free rules for 
creating a tree structure (phrase marker) in 
the X-bar framework (as discussed by Chomsky 
(1970), Jackendoff (1974), Bresnan (1975) and 
others.) In this system, the major syntactic 
categories (N(oun), V(erb), A(djective) and 
P(reposltion)) are treated as complex symbols 
which are decomposable into the features \[~N\] 
and \[~V\]. This yields the following cross- 
classif ication of these categories: 
This work was sponsored by BEH grant ~G007904514. 
V 
?I 
Figure i. Features in the X-bar System 
The feature "N" marks a given category as 
"nounlike" (and thus corresponds to the 
traditional grammatical notion of 
"substantive") while "V" marks a category as 
"verblike." Nouns and Adjectives are \[?N\] 
because they share certain properties (e.g. 
Adjectives can be used in nominal contexts; in 
highly inflected languages, Adjectives and 
Nouns typically share the same inflectlonal 
paradigms, etc.) Adjectives and Verbs are \[+V\] 
because they share (among other things) various 
morphological traits (e.g. certain verbal 
forms, such as participles, have adjectival 
properties). Verbs and Preposit ions are I-N\] 
because they display common complement 
selection attributes (e.g. they both regularly 
take Nominal complements that bear Accusative 
Case.) (For further discussion of the issue of 
feature decomposition, and for some alternative 
proposals, see Jackendoff (1978) and George 
(1980a, Section 2; 1980b, Section 2).) 
In addition, each syntactic category contains a 
specif ication of its rank (given in terms of 
number of bars, hence the term "X-bar" system). 
For instance, a Noun (N) is of rank 0 and is 
marked with no bars whereas the Noun Phrase 
which it heads is of the same category but 
different (higher) rank. Intermediate 
structures are also permitted; for instance, V * 
(read "V bar") is that portion of the Verb 
Phrase which consists of a Verb and its 
complements (e.g. direct and indirect objects, 
clausal complements, preposit ional phrases, 
etc.) while V ~ (read "V double bar") includes 
V ~ as well as Auxil iary elements. For our 
purposes, we have adopted a uniform two-level 
structure across categories~ that is, each 
category X is taken to have X ~* as its highest 
rank, so that Noun Phrase (NP) in our system is 
N ~,  Verb Phrase is V ~', etc. Minor categories 
(such as DET(erminer), AUX(ilfary), NEG(ative), 
etc.) stand outside this system, as do 
S(entence) and S ~ (a sort of super sentence, 
which contains S and clause introducing 
elements (or "subordinating conjunctions") such 
as that). These categories are not 
decomposable into the features \[?N\] and \[+V\], 
and, except for S and S" , they -do  not ~ave 
different ranks. (It should be noted that the 
adoption of a uniform two-level hypothesis and 
the placlng of S and S ~ outside of the normal 
X-bar system are not uncontroversial--see e.g. 
Jackendoff (1978) and George (1980a, Section 2; 
1980b, Section 2). However, these assumptions 
are found in many variants of the X-bar 
framework and are adequate for our purposes.) 
153 
An example of the internal structure of the P'" 
corresponding to the phrase "to the sad boys" 
is given below: 
p'" \[ -v -N \] 
P" \[ -V -N \] 
P \[ -V -N  \] 
to 
N ~ \[ ~N -V PER. 3 +DEF WU.PL 
+HUMAN GENDER.MALE \] 
DET \[ +DEF \] 
the 
A ~" \[ +N +V \] 
A ~ \[ +N +V \] 
A \[ +N +V \] 
sad 
N ~ \[ +N -V PER. 3 +DEF NU.PL 
+HUMAN GENDER.MALE \] 
N \[ +N -V PER. 3 +DEF NU.PL 
+HUMAN GENDER.MALE \] 
boy 
Figure 2. Part of A Sample Base Structure 
This system of cross-c lass i f icat ion by features 
and by rank permits the creat ion of 
transformations which can refer to a specif ic 
rank or feature without referring to a specif ic 
major category. (See Bresnan (1975) for 
further discussion of this point.) For 
example, the transformation which fronts WH- 
words to form WH-Quest ions treats any X ~ 
category as its target and, hence, can be used 
to question any of the major categories (e.g. 
A'~--"how big is it?"; N' ' - -"what did they do?" 
"which men left?"; P~'--"to whom did you give 
it?"). Similarly, the transformation which 
marks Accusative Case on pronouns applies only 
to those N~'s which fol low a I-N\] category; 
i.e. only to those N~s  which are the objects 
of Verbs or Preposit ions. This al lows us to 
create extremely versat i le transformations 
which apply in a variety of contexts, and frees 
us from the necessity of creat ing several 
transformations, each of which essential ly 
repl icates the Structural  Descr ipt ion and 
Structural  Change of the others, di f fer ing only 
in the category of the affected term. 
A set of constraints (discussed further below) 
is the input to the base component and 
determines the type of base structure which is 
produced. A base structure has both the usual 
features on the nodes (category features such 
as \[+N\] and \[-v\], and select ional features such 
as \[+PROPER\]) and some addit ional  diacr i t ic  
features (such as \[-C\], for case marking) which 
are used to ,govern the appl icat ion of certain 
transformations. 
Lexical insertion is an integral part of the 
construct ion of the tree by the base component. 
It is not essential  that words be chosen for 
the sentence at this time, but it is convenient 
because addit ional features in the structure 
(such as \[+HUMAN\], \[+MALE\]) are needed to guide 
some transformations (for instance, the 
insertion of the correct form of oronouns.) 
In our current system, the choice of words to 
be inserted in the base structure is control led 
by a dict ionary and a semantic networM which 
embodies a l imited number of semantic class 
relat ionships and case restr ict ions to orohibit  
the production of utterances like "The answer 
saw the angry cookie." The network nodes are 
chosen at random for each sentence that is 
generated, but a more powerful semantic 
component could be used to convey particular 
"messages," provided only that it could find 
lexical items to be inserted in the small 
number of posit ions required by the base 
constraints. 
3. THE TRANSFORMATIONAL COMPONENT 
Each transformational rule has a Structural 
Description, Structural  Change, and (optional) 
Condition; however rules are not marked as 
optional or obligatory, as they were in 
tradit ional transformational theory (e.g. 
Chomsky (1955)). Obl igatory transformations 
whose structural descript ions were met would 
apply necessari ly; optional transformations 
would apply at random. Moreover, various 
versions of transformational grammar have 
employed transformations as "fi lters" on 
possible derivations. In older work (e.g. the 
so-cal led "Standard Theory" (ST) of Chomsk7 
(1965)) derivations in which a transformation 
required in a given syntactic conf igurat ion 
failed to apply would block, causing the result 
to be ruled out as ungrammatical  (op. clt., 
p. 138). 
In more recent theories (e.g. the "Extended 
Standard Theory" (EST) of Chomsky (1977) and 
Chomsky and Lasnik (1977)) all t ransformations 
are optional, freely ordered and may apply at 
random. Those derivations in which a 
transformation misappl ies are ruled out by 
independent condit ions on the structures 
produced by the operat ion of the 
transformational  component (Chomsky (1977, 
p. 76)). These frameworks adopt a "generate 
and test" approach, wherein the misappl icat ion 
of transformations during the course of a 
derivat ion (e.g. the failure of a required 
transformation to apply (ST, EST) or the 
appl icat ion of a transformation in a prohibited 
syntactic conf igurat ion (EST)) wil l  result in a 
rejection of this possible derivation. The 
appl icat ion of di f ferent optional 
transformations results in the production of a 
variety of surface forms. 
There are two reasons why we do not use this 
generate and test approach. The first is that  
it is computat ional ly ineff icient to al low the 
transformations to apply at random and to check 
the result to make sure that it is grammatical.  
More importantly, we view the transformations 
as tools to be used by a process outs ide the 
sentence generator . itself. That is, an 
external process determines what the surface 
syntactic form of a given base structure should 
be; the transformations are not independent 
entit ies which make this decis ion on their own. 
For example, a focus mechanism should be able 
to select or prohibit passive sentences, a 
dialogue mechanism should be able to cause 
agent-deletion, and so on. In OUr application, 
tutorial programs select the character ist ics  of 
the sentences to be produced on the basis of 
the syntactic ru le  or rules being exercised in 
the particular tutorial. 
The Structural Change of each transformation 
consists of one or more functions, analogous to 
the transformational elementar ies of 
tradit ional transformational theory (Chomskv 
(1955, pp. 402-407, Section 93.1)). We have 
154 
not adopted the restriction on the Structural 
Change of transformations proposed by more 
recent work in generative grammar (e.g. Chomsky 
(1980, p. 4)) which prohibits "compounding of 
elementaries"; i.e. which limits the Structural 
Change of a transformation to a single 
operation. This would require breaking up many 
transformations into several transformations, 
each of which would have to apply in the 
derivation of a particular syntactic 
construction rather than having one 
transformation that performs the required 
operations. Inasmuch as we are interested in 
utilizing the generative capacity of 
transformational grammar to produce specific 
constructions, this break up of more general, 
overarching transformations into smaller, more 
specific operations is undeslrable. 
The operations that are performed by the rules 
are a combination of classic transformational 
operations (substitution, adjunction, deletion, 
insertion of non-lexical elements such as 
"there" and "do") and operations that llnguists 
sometimes relegate to the base or post- 
transformational processes (insertion of 
pronouns, morphing of inflected forms). By 
making these operations rule-speclflc, many 
related forms can be produced from the same 
base tree and the control mechanisms outside 
the generator itself can speclfv which forms 
are to be produced. (Figure 3 shows some of 
the transformations currently in the system.) 
SUBJECT-AUX-INVERSION 
SD: (S ~ (FEATS (TRANS.1)) COMP (FEATS (WH.+)) 
1 2 
(S N *~ TNS (OPT NODE (FEATS (M.+))))) 
3 4 5 6 
SC: (DeleteNode 6) 
(DeleteNode 5) 
(LChomsk7 2 6) 
(LChomsky 2 5) 
Condition: \[NOT (EQ (QUOTE +) 
(FeatureValue (QUOTE WR) 
(RootFeats 4\] 
RELATIVE-PRONOUN-SPELL-OUT \[REPEATABLE\] 
SD: (S* XX (N "~ N "~ (S" (COMP X (N ~" 
1 2 3 4 5 6 
(FEATS (WH . +)) WH))))) 
7 
SC: (DeleteSons 6) 
(LSon 6 (if (EQ "+(GetFeat 6 ~ HUMAN)) 
then "who 
else ~whlch)) 
Figure 3. Sample Transformations 
Those transformations which affect the 
syntactic form of sentences are apnlied 
cyclically (see (Chomsky (1965, p. 143) for 
more details). Thus transformations apply from 
the "bottom up" durinq the course of a 
the transformations are strictly iand 
extrinsically) ordered. In addition to the 
cyclic syntactic transformations there exists a 
set of post-cyclic transformations, which apply 
after all the cyclic syntactic transformations 
have applied. These post-cyclic 
transformations, whose domain of operation 
ranges over the entire syntactic tree, supply 
the correct morphological forms of all lexical 
and grammatical items. This includes qlvlna 
the correct plural forms of nouns, the 
inflected forms of verbs, the proper forms of 
pronouns (e.g. "he," "she" and "they" in 
subject position and "him," "her," and "them" 
in object position), etc. While it has been 
relatively rare in recent transformational 
analyses to utilize transformations to effect 
this type of morphological "spell-out," this 
mechanism was first proposed in the earliest 
work in generative grammar (Chomsky (1955)). 
Moreover, recent work by George (1980a; 1980b) 
and Ingria (in preparation) suggests that this 
is indeed the correct way of handling such 
morphological processes. 
The transformations as a whole are divided up 
into "families" of related transformations. 
For example, there is a family of 
transformations which apply in the derivation 
of questions (all beginning with the prefix 
WH-); there is a family of morphlng 
transformations (similarly beginning with the 
flagged mnemonic prefix MORPH-). These 
"families" of transformations provide detailed 
control over the generation process. For 
example, all transformations of the W~- family 
will apply to a single syntactic position that 
may be questioned (e.g. subject, direct object, 
object of preposition, etc.), resulting in 
questions of the form "Who died" and "To whom 
did she speak." This familial characterization 
of transformations is similar to the classical 
transformational approach (Chomsky (1955, 
p. 381, Section 90.1)) wherein families of 
transformations were first postulated, because 
of the condition imposed within that framework 
that each transformation must be a single- 
valued mapping. 
Our current sentence generator produces 
declarative sentences, passive sentences (with 
and without agent deletion), dative movement 
sentences, yes-no questions and wh-queetlons 
(including multlple-wh questions such as "Who 
gave what to whom?'), there-insertlon 
sentences, negated sentences (including both 
contracted and emphatic forms), relative 
clauses, finite and infinitival complements 
(e.g., "The teacher wanted Kathy to hurry.'), 
imperative sentences, various complex 
auxiliaries (progressive, perfective, and 
modals), predicate adjectives, and predicate 
nominals. Although not all of these 
constructions are handled in complete 
generality, the generator produces a very large 
and natural subset of English. It is important 
to note that the interactions among all these 
transformations have been taken into account, 
so that any meaningful co~blnatlon of them will 
produce a meaningful, grammatical sentence. 
(Appendix A lists some of the sentences which 
have been produced by the interaction of 
various transformations.) 
derivation, applying first in the most embedded In our application, there is a need to generate 
clause and then working upwards until the ungrammatical utterances occasionally (for 
matrix clause is reached. Within each cycle example, in a tutorial exercising the student's 
155 
abil ity to judge ? the grammatical i tv  of 
utterances). To this end, we have developed an 
addit ional set of transformations that can he 
used to generate utterances which mimic the 
ungrammatical  forms found in the writ ing of the 
language delayed populat ions for which this 
system is intended. For example, deaf and 
hearing- impaired chi ldren often have dif f iculty 
with negative sentences, and replace the not of 
Standard English negation with no and/or place 
the negative element in posit ions in which it 
does not occur in Standard Engl ish (e.g. "The 
mouse is no a big animal," "The girl no has 
gone," "Dogs not can build trees"). The fact 
that these ungrammatical  forms may be model led 
with transformations is highly signif icant, and 
lends support to the claim (Chapman (1974), 
Fromkin (1973)) that ungrammatical  utterances 
are rule-driven. 
4. HIGHER LEVELS OF CONTROL 
In order to manage the creat ion of the base 
trees and the appl icat ion of the 
transformational  rules, we have developed 
several layers of control  mechanisms. The 
first of these is a set of constraints that 
directs the operation of the base comoonent and 
indicates which transformations to try. A 
transformational  constraint  merely turns a 
part icular transformation on or off. The fact 
that a transformation is turned on does not 
guarantee that it wi l l  apply; it merely 
indicates that the Structural  Descr ipt ion and 
Condit ion of that transformation are to be 
tried. Base constraints can have either atomic 
indicators or a list of constraints as their 
values. For example, the direct object 
constraint (DIROBJ (PER 3) (NU PL) ...) 
specif ies all the base constraints necessary to 
produce the N'" subtree for the direct object 
posit ion in the base structure. 
There are a number of dependencies which exist 
among constraints. For example, if the 
transformational  constraint  for the passive 
transformation is turned on, then the base 
component must be instructed to produce a 
direct object and to choose a main verb that 
may be passivized; if the base constraint  for a 
direct object is turned off, then the base 
constraint for an indirect object must be 
turned off as well. A data base of 
implications controls the appl icat ion of 
constraints so that whenever a constraint is 
set (or turned off), the base and/or 
transformational  constraints that its value 
implies are also set. 
The notion of a part icular syntactic 
construct ion transcends the dist inct ion between 
base and transformational  constraints.  The 
"natural" speci f icat ion of a syntactic 
construct ion such as passive or relative clause 
should be made without requir inq detai led 
knowledge of the constraints or "their 
implications. In addition, one might want to 
request, say, a relative clause on the subject, 
without specifying whether the target of 
relat iv izat ion is to be the subject or object 
of the embedded clause. 
We have developed a data base of structures 
called synspecs (for "syntactic 
specif ications") which embody, at a very high 
level, the notion of a syntactic construction. 
These construct ions cannot be identif ied with a 
single constraint or its implied constraints. 
( Implications specify necessary dependencies; 
synspecs specify possible but not necessary 
choices on the part of the system designers 
about what combinations of constraints should 
be invoked under a general name.) A synspec 
can contain an element of choice. The choice 
can be made by any user-def ined function, 
though in our practice most of the choices are 
made at random. One example of this is a 
synspec called wh-quest ion which decides which 
of the synspecs that actual ly set up the 
constraints for a wh-quest ion (question-on- 
subject, question-on-object,  quest ion-on- 
dative, etc.) should be used. The synspecs 
also provide convenient hooks on which to hang 
other information associated with a syntactic 
construction: sentences exempl i fy ing the 
construction, a descr ipt ion of the construct ion 
for purposes of documentation, etc. Figure 4 
snows how several of the synspecs look when 
printed for the user. 
wh-quest ion 
Compute : (PickOne "(quest ion-on-subject  
quest ion-on-object  
quest ion-on-dative))  
Descr ipt ion : (This SynSpec wil l  create any 
one of the questions with 
WH-words.) 
second-person- imperat ive 
BaseConstraints : (( IMPERATIVE . 2) 
(TNS)) 
TransConstraints : 
( (REQUEST-VOCATIVE-DELETION . +} 
(REQUEST-EXCLAMATION-INSERTION . +) 
(REQUEST-YOU-DELETION . +)) 
Examples : ('Open the door!") 
Figure 4. Sample SynSpecs 
Synspecs are invoked through a simple mechanism 
that is avai lable to the tutorial component of 
the system. Each tutorial specif ies the range 
of construct ions relevant to its topic and 
chooses among them for each sentence that is to 
be generated. To produce related sentences, 
the generator is restarted at the 
transformational  component (using the previous 
base tree) after the synspecs specifying the 
relat ionship have been processed.) 
Just as  constraints have implications, so do 
synspecs. The relat ionships that hold among 
synspecs include exclusion (e.g. transit ive- 
sentence excludes predicate-nominal-sentence),  
requirement (e.g. extraposed-relat ive requires 
relative-clause-on-subject or relat lve-clause- 
on-object),  and permission (e.g. predicate- 
adverb-sentence al lows there-insertion). A 
mechanism similar to the implications for 
constraints refines a set of candidate synspecs 
so that the user (or the tutorlals) can make 
choices which are consistent. Thus the user 
does not have to know, understand, or remember 
which combinations of choices are allowed. 
156 
Once some constraints have been set (either 
directly or through synspecs), a command can be 
given to generate a sentence. The generator 
first assigns values to the constraints that 
the user did not specify7 the values chosen are 
guaranteed to be compatible with the previous 
choices, and the implications of these choices 
ensure that contradictory specifications cannot 
be made. Once all constraints have been set, a 
base tree is generated and saved before the 
transformations are applied. Because the base 
structure has been saved, the transformational 
constraints can be reset and the generator 
called to start at the transformational 
component, producing a different surface 
sentence from the same base tree. As many 
sentences as are wanted can be produced in this 
way. 
5. DEVELOPMENT TOOLS 
As one side effect of the development of the 
generative system, we have built a debugging 
environment called the syntactic playground in 
which a user can develop and test various 
components of the generator. This environment 
has become more important than the tutorials in 
testing syntactic hypotheses and exploring the 
power of the language generator. In it, 
dictionary entries, transformations, 
implications and synspecs can be created, 
edited, and saved using interactive routines 
that ensure the correct format of those data 
types. It is also possible here to give 
commands to activate synspecs; this operation 
uses exactly the same interface as programs 
(e.g. tutorials) that use the generator. 
Commands exist in the playground to set base 
constraints to specific values and to turn 
individual transformations on and off without 
activating the implications of those 
operations. This allows the system programmer 
or linguist to have complete control over all 
aspects of the generation process. 
Because the full power of the Interlisp system 
is available to the playground user, the base 
tree can be edited directly, as can any version 
of the tree during the derivation process. 
Transformations can also be "broken" like 
functions, so that when a transformation is 
about to be tried the generator goes into a 
"break" and conducts an interactive dialogue 
with the user who can control the matching of 
the Structural Description, examine the result 
of the match, allow (or not) the application of 
the Structural Change, edit the transformation 
and try it again, and perform many of the 
operations that are available in the general 
playground. In addition to the 
transformational break package there is a trace 
option which, if used, prints the constraints 
selected by the system, the words, and the 
transformations that are tried as they apply or 
fail. The playground has proved to be a 
powerful tool for exploring the interaction of 
various rules and the efficacy of the whole 
generation package. 
6. CONCLUSION 
This is the most syntactically powerful 
generator that we know of. It produces sets of 
related sentences maintaining detailed 
knowledge of the choices that have been made 
and the structure(s) that have been produced. 
Because the notion of "syntactic construction" 
is embodied in an appropriately high level of 
syntactic specification, the generator can be 
externally controlled. It is fast, efficient, 
and very easy to modify and maintain; it has 
been implemented in both Interlisp on a 
DECSystem-20 and UCSD Pascal on the Cromemco 
and Apple computers. It forms the core of a 
set of tutorial programs for English now being 
used by deaf children in a classroom setting, 
and thus is one of the first applications of 
computational linguistics to be used in an 
actual educational environment. 
References 
Bresnan, Joan (1975) "Transformations and 
Categories in Syntax," in R. Butts and 
J. Hintikka, eds. Proceedings of the Fifth 
International Congress of Lo@ic-~- Me- -~od~ 
and Philosophy of Sc~-ence, University of 
W-~tern Ontario, Lo -ndon,~ io .  
Chapman, Robin S. (1974) The Interpretation of 
Deviant Sentences ~ ~ :  A 
~rmat iona l  Approach~- Janus Linguarum~ 
Series Minor, Volume 189, Mouton, The Hague. 
Chomsky, Noam (1955) The Logical Structure of 
Linguistic Theory, unpublished manuscript", 
microfilmed, MIT Libraries, partially published 
by Plenum Press, New York, 1975. 
Chomsky, Noam (1965) ~ of the Theory of 
S~ntax, MIT Press, Cambrldge, Ma'ssa---6~usetts. -- 
Chomsky, Noam (1970) "Remarks on 
Nominalization", in R .A .  Jacobs and P .S .  
Rosenbaum, eds., Readings in 
Transformational Grammar, G inn- -and  Co., 
Waltham, Mass. 
Chomsky, Noam (1973) "Conditions on 
Transformations", in S .A .  Anderson and 
P. Kiparsky, eds., A Festschrlft for Morris 
Halle, Holt, Rinehart--and Winston, New~Yor-~. 
Chomsky, Noam (1977) "On WR-Movement", in 
P. Culicover, T. Wasow and A'~'AkmaJian, eds. 
Formal S~ntax, Academic Press, Inc., New York. 
Chomsky, Noam (1980) "On Binding," Linguistic 
Inquiry ll. 
Chomsky, Noam and Howard Lasnik (1977) "Filters 
and Control", Linguistic Inquiry 8. 
Fromkin, Victoria A. (1973) Speech Errors as 
Linguistic Evidence, Janua Ln~u~,  ~-eri~ 
major, Volume 77, Mouton, The Hague. 
George, Leland M. (1980a) Analogical 
Generalization in Natural Langua_qe Syntax, 
unpublished Doct6~'al D lsser '~aton ,~.  
George, Leland M. (1980b) Analogical 
Generalizations of Natural Language Syntax, 
unpublished manus6"Fip6"7-~. 
Ingria, Robert (in preparation) Sentential 
Complementation in Modern Greek, Doctoral 
Dissertation, MIT. 
Jackendoff, Ray S. (1974) "Introduction to the 
X" Convention", distributed by Indiana 
University Linguistics Club, Bloomington. 
Jackendoff, Ray S. (1978) X" ~ S  ntax: --A Study_ of 
Phrase Structure, Linguistic Inqulry Monograp-~ 
157 ~ MIT Press, Cambridge, Mass. 
A~end ix  A: Sample Sentences 6. Superlat ive Sentences 
i. Transit ive Sentences 
i. The bull ies chased the girl. 
2. What did the bul l ies do to the 
girl? 
3. They chased her. 
4. Who chased the girl? 
5. The bull ies chased her. 
6. Who did they chase? 
7. Whom did they chase? 
8. They chased the girl. 
9. How many bull ies chased the 
girl? 
10. Eight bull ies chased the girl. 
Ii. How many bull ies chased her? 
12. Eight bull ies chased her. 
13. Who got chased? 
14. The girl got chased. 
15. She was chased by the bullies. 
16. The girl was being chased by 
the bullies. 
2. Intransit ive Sentences 
i. What did the girl  do? 
2. She cried. 
3. Who cried? 
4. The girl cried. 
3. Indirect Discourse 
i. Dan said that the girl  is sad. 
2. Dan said that she is sad. 
3. Who said that the girl  is sad? 
4. Transit ive Sentence with Indirect 
Object 
i. The generous boy gave a doll to 
the girl. 
2. The generous boy gave the g i r l  
a doll. 
3. The girl was given a doll. 
4. A doll was given to the girl. 
5. Who gave the girl  a doll? 
6. Who gave what to whom? 
7. What did the generous boy give 
the girl? 
8. He gave her a doll. 
9. What did the generous boy give 
to the girl? 
i0. He gave a doll to her. 
ii. Who gave a doll to the girl? 
12. Who gave the girl a doll? 
13. Which boy gave the girl  a doll? 
14. The generous boy gave her a 
doll. 
15. Which boy gave a doll to the 
girl? 
16. The generous boy gave it to 
he-. 
17. How many dolls did the generous 
boy give the girl? 
18. He gave her one doll. 
5. Comparative Sentences 
!. The soldier was better. 
2. The gentleman wil l  be more 
unhappy. 
3. Al icia is hungrier than Jake. 
4. The chi ldren were angrier than 
Andy. 
158 
I. A pol iceman caught the nicest 
butterfl ies. 
2. A sheepdog was the sickest pet. 
3. The fire chief looks most 
generous. 
4. The smartest man swore. 
5. The oldest bulldog broke the 
dolls. 
7. Sentences with Inf init ives 
I. The teacher wanted Kathy to 
hurry. 
2. The gentleman promised the lady 
to close the door. 
3. The girls were hard to 
ridicule. 
8. Relative Clauses 
I. Whoever embraced the kids wil l  
embrace the ladies. 
2. The girl who was intel l igent 
cheated the adults. 
3. The woman who greased the 
tr icycle mumbled. 
4. The teacher who lost the 
bul ldogs swears. 
9. Negative Sentences 
i. Kim won't help. 
2. Claire didn't help. 
3. The chi ldren won't shout. 
4. Do not slap the ~oodles. 
5. Do not cry. 
i0. Var iet ies of Quantl f iers 
i. No toy breaks. 
2. Some excited boys kissed the 
women. 
3. Some hungry people eat. 
4. Two men cried. 
5. Every new toy broke. 
6. Not every man slips. 
7. The boy won't give the dogs any 
oranges. 
8. The girl  doesn't see any cats. 
9. The old men didn't tell the 
boys any thing. 
i0. The girl  didn't love any body. 
ii. Var iet ies of Pronouns 
i. Bette is the sad one. 
2. Glor ia is the happy one. 
3. Kevin is the saddest. 
4. Kathy is the most cheerful. 
5. Varda liked the sweet apple. 
6. Varda liked the sweet one. 
12. T~u~RE Sentences 
i. There were some toys in the 
dirt. 
2. There were no toys in the dirt. 
3. There weren't any toys in the 
dirt. 
