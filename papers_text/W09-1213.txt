Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 85?90,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Semantic Role Labelling with Markov Logic
Ivan Meza-Ruiz? Sebastian Riedel??
?School of Informatics, University of Edinburgh, UK
?Department of Computer Science, University of Tokyo, Japan
?Database Center for Life Science, Research Organization of Information and System, Japan
?I.V.Meza-Ruiz@sms.ed.ac.uk ? sebastian.riedel@gmail.com
Abstract
This paper presents our system for the CoNLL
2009 Shared Task on Syntactic and Semantic
Dependencies in Multiple Languages (Hajic?
et al, 2009). In this work we focus only on the
Semantic Role Labelling (SRL) task. We use
Markov Logic to define a joint SRL model and
achieve the third best average performance in
the closed Track for SRLOnly systems and the
sixth including for both SRLOnly and Joint
systems.
1 Markov Logic
Markov Logic (ML, Richardson and Domingos,
2006) is a Statistical Relational Learning language
based on First Order Logic and Markov Networks.
It can be seen as a formalism that extends First Or-
der Logic to allow formulae that can be violated with
some penalty. From an alternative point of view, it is
an expressive template language that uses First Or-
der Logic formulae to instantiate Markov Networks
of repetitive structure.
In the ML framework, we model the SRL task
by first introducing a set of logical predicates1 such
as word(Token,Ortho) or role(Token,Token,Role). In
the case of word/2 the predicate represents a word
of a sentence, the type Token identifies the position
of the word and the type Ortho its orthography. In
the case of role/3, the predicate represents a seman-
tic role. The first token identifies the position of the
predicate, the second the syntactic head of the argu-
ment and finally the type Role signals the semantic
role label. We will refer to predicates such as word/2
1In the cases were is not obvious whether we refer to SRL
or ML predicates we add the prefix SRL or ML, respectively.
as observed because they are known in advance. In
contrast, role/3 is hidden because we need to infer it
at test time.
With the ML predicates we specify a set of
weighted first order formulae that define a distribu-
tion over sets of ground atoms of these predicates (or
so-called possible worlds). A set of weighted formu-
lae is called a Markov Logic Network (MLN). For-
mally speaking, an MLN M is a set of pairs (?,w)
where ? is a first order formula and w a real weight.
M assigns the probability
p (y) = 1Z exp
?
? ?
(?,w)?M
w
?
c?C?
f?c (y)
?
? (1)
to the possible world y. Here C? is the set of all
possible bindings of the free variables in ? with the
constants of our domain. f?c is a feature function
that returns 1 if in the possible world y the ground
formula we get by replacing the free variables in ?
by the constants in c is true and 0 otherwise. Z
is a normalisation constant. Note that this distri-
bution corresponds to a Markov Network (the so-
called Ground Markov Network) where nodes repre-
sent ground atoms and factors represent ground for-
mulae.
In this work we use 1-best MIRA (Crammer and
Singer, 2003) Online Learning in order to train the
weights of an MLN. To find the SRL assignment
with maximal a posteriori probability according to
an MLN and observed sentence, we use Cutting
Plane Inference (CPI, Riedel, 2008) with ILP base
solver. This method is used during both test time
and the MIRA online learning process.
85
2 Model
In order to model the SRL task in the ML frame-
work, we propose four hidden predicates. Consider
the example of the previous section:
argument/1 indicates the phrase for which its head
is a specific position is an SRL argument.
In our example argument(2) signals that the
phrase for which the word in position 2 is its
head is an argument (i.e., Ms. Haag).
hasRole/2 relates a SRL predicate to a SRL argu-
ment. For example, hasRole(3,2) relates the
predicate in position 3 (i.e., play) to the phrase
which head is in position 2 (i.e., Ms. Haag).
role/3 identifies the role for a predicate-argument
pair. For example, role(3,2,ARG0) denotes the
role ARG0 for the SRL predicate in the posi-
tion 2 and the SRL argument in position 3.
sense/2 denotes the sense of a predicate at a specific
position. For example, sense(3,02) signals that
the predicate in position 3 has the sense 02.
We also define three sets of observable predicates.
The first set represents information about each token
as provided in the shared task corpora for the closed
track: word for the word form (e.g. word(3,plays));
plemma/2 for the lemma; ppos/2 for the POS tag;
feat/3 for each feature-value pair; dependency/3 for
the head dependency and relation; predicate/1 for
tokens that are predicates according to the ?FILL-
PRED? column. We will refer to these predicates as
the token predicates.
The second set extends the information provided
in the closed track corpus: cpos/2 is a coarse POS
tag (first letter of actual POS tag); possibleArg/1 is
true if the POS tag the token is a potential SRL argu-
ment POS tag (e.g., PUNC is not); voice/2 denotes
the voice for verbal tokens based on heuristics that
use syntactic information, or based on features in the
FEAT column of the data. We will refer to these
predicates as the extended predicates.
Finally, the third set represents dependency infor-
mation inspired by the features proposed by Xue and
Palmer (2004). There are two types of predicates
in this set: paths and frames. Paths capture the de-
pendency path between two tokens, and frames the
subcategorisation frame for a token or a pair of to-
kens. There are directed and undirected versions of
paths, and labelled (with dependency relations) and
unlabelled versions of paths and frames. Finally, we
have a frame predicate with the distance from the
predicate to its head. We will refer to the paths and
most of the frames predicates as the path predicates,
while we will consider the frame predicates for a
unique token part token predicates.
The ML predicates here presented are used within
the formulae of our MLN. We distinguish between
two types of formula: local and global.
2.1 Local formulae
A formula is local if its groundings relate any num-
ber of observed ground atoms to exactly one hidden
ground atom. For example, a grounding of the local
formula
lemma(p,+l1)?lemma(a,+l2) ? hasRole(p, a)
connects a hidden hasRole/2 ground atom to two ob-
served plemma/2 ground atoms. This formula can be
interpreted as the feature for the predicate and argu-
ment lemmas in the argument identification stage of
a pipeline SRL system. Note that the ?+? prefix indi-
cates that there is a different weight for each possible
pair of lemmas (l1, l2).
We divide our local formulae into four sets, one
for each hidden predicate. For instance, the set for
argument/1 only contains formulae in which the hid-
den predicate is argument/1.
The sets for argument/1 and sense/2 predicates
have similar formulae since each predicate only in-
volves one token at time: the SRL argument or the
SRL predicate token. The formulae in these sets are
defined using only token or extended observed pred-
icates.
There are two differences between the argument/1
and sense/2 formulae. First, the argument/1 for-
mulae use the possibleArg/1 predicate as precondi-
tion, while the sense formulae are conditioned on
the predicate/1 predicate. For instance, consider the
argument/1 formula based on word forms:
word(a,+w) ? possibleArg(a) ? argument(a),
and the equivalent version for the sense/2 predicate:
word(p,+w) ? predicate(p) ? sense(p,+s).
This means we only apply the argument/1 formulae
if the token is a potential SRL argument, and the
sense/2 formulae if the token is a SRL predicate.
86
The second difference is the fact that for the
sense/2 formulae we have different weights for each
possible sense (as indicated by the +s term in the
second formula above), while for the argument/1
formulae this is not the case. This follows naturally
from the fact that argument/1 do not explicitly con-
sider senses.
Table 1 presents templates for the local formuale
of argument/1 and sense/2. Templates allow us to
compactly describe the FOL clauses of a ML. The
template column shows the body of a clause. The
last two columns of the table indicate if there is a
clause with the given body and argument(i) (I) or
sense(i,+s) (S) head, respectively. For example,
consider the first row: since the last two columns
of the row are marked, this template expands into
two formulae: word(i,+w) ? argument(i) and
word(i,+w) ? sense(i,+s). Including the pre-
conditions for each hidden predicate we obtain the
following formulae:
possibleArg(i) ? word(i,+w) ? argument(i)
and
predicate(i) ? word(i,+w) ? sense(i,+s).
In the case of the template marked with a ?*?
sign, the parameters P and I, where P ?
{ppos, plemma} and I ? {?2,?1, 0, 1, 2}, have to
be replaced by any combination of possible values.
Since we generate argument and sense formulae
for this template, the row corresponds to 20 formu-
lae in total.
Table 2 shows the local formuale for hasRole/2
and role/3 predicates, for these formulae we use to-
ken, extended and path predicates. In this case,
these templates have as precondition the formula
predicate(p) ? possibleArg(a). This ensures that
the formulae are only applied for SRL predicates
and potential SRL arguments. In the table we in-
clude the values to replace the template parame-
ters with. Some of these formulae capture a no-
tion of distance between SRL predicate and SRL
argument and are implicitely conjoined with a
distance(p, a,+d) atom. If a formulae exists both
with and without distance atom, we write Both in
the ?Dist? column; if it only exists with the distance
atom, we write Only, otherwise No.
Note that Tables 1 and 2 do not mention
the feature information provided in the cor-
Template I S
word(i,+w) X X
P(i+ I,+v)* X X
cpos(i+ 1,+c1) ? cpos(i? 1,+c2) X X
cpos(i+ 1,+c1) ? cpos(i? 1,+c2) ?
cpos(i+ 2,+c3) ? cpos(i? 2,+c4)
X X
dep(i, ,+d) X X
dep( , i,+d) X X
ppos(i,+o) ? dep(i, j,+d) X X
ppos(i,+o1) ? ppos(j,+o2) ?
dep(i, j,+d)
X X
ppos(j,+o1) ? ppos(k,+o2) ?
dep(j, k, ) ? dep(k, i,+d)
X X
plemma(i,+l) ? dep(j, i,+d) X X
frame(i,+f) X X
(Empty Body) X
Table 1: Templates of the local formulae for argument/1
and sense/2. I: head of clause is argument(i), S: head of
clause is sense(i,+s)
pora because this information was not avail-
able for every language. We therefore group
the formulae which consider the feature/3 pred-
icate into another a set we call feature formu-
lae. This is the summary of these formulae:
feat(p,+f,+v) ? sense(p,+s)
feat(p,+f,+v) ? argument(a)
feat(p,+f,+v1) ? feat(p, f,+v2) ?
hasRole(p, a)
feat(p,+f,+v1) ? feat(p, f,+v2) ?
role(p, a,+r)
Additionally, we define a set of language spe-
cific formulae. They are aimed to capture the re-
lations between argument and its siblings for the
hasRole/2 and role/3 predicates. In practice in
turned out that these formulae were only beneficial
for the Japanese language. This is a summary of
such formulae which we called argument siblings:
dep(a, h, ) ? dep(h, c, ) ? ppos(a,+p1)?
ppos(c,+p2) ? hasRole(p, a)
dep(a, h, ) ? dep(h, c, ) ? ppos(a,+p1)?
ppos(c,+p2) ? role(p, a,+r)
dep(a, h, ) ? dep(h, c, ) ? plemma(a,+p1)?
ppos(c,+p2) ? hasRole(p, a)
dep(a, h, ) ? dep(h, c, ) ? plemma(a,+p1)?
ppos(c,+p2) ? role(p, a,+r)
With these sets of formulae we can build specific
MLNs for each language in the shared task. We
group the formulae into the modules: argument/1,
87
Template Parameters Dist. H R
P(p,+v) P ? S1 Both X X
plemma(p,+l) ? ppos(a,+o) No X
ppos(p,+o) ? plemma(a,+l) No X
plemma(p,+l1) ? plemma(a,+l2) Only X X
ppos(p,+o1) ? ppos(a,+o2) Only X
ppos(p,+o1) ? ppos(a+ I,+o2) I ? {?1, 0, 1} Only X
plemma(p,+l) Only X
voice(p,+e) ? lemma(a,+l) Only X
cpos(p,+c1) ? cpos(p+ I,+c2) ? cpos(a,+c3) ? cpos(a+ J, c4) I,J ? {?1, 1}2 No X X
ppos(p,+v1) ? ppos(a, IN) ? dep(a,m, ) ?P(m,+v2) P ? S1 No X X
plemma(p,+v1) ? ppos(a, IN) ? dep(a,m, ) ? ppos(m,+v2) No X X
P(p, a,+v) P ? S2 No X X
P(p, a,+v) ? plemma(p,+l) P ? S3 No X X
P(p, a,+v) ? plemma(p,+l1) ? plemma(a,+l2) P ? S4 No X X
pathFrame(p, a,+t) ? plemma(p,+l) ? voice(p,+e) No X X
pathFrameDist(p, a,+t) Only X X
pathFrameDist(p, a,+t) ? voice(p,+e) Only X X
pathFrameDist(p, a,+t) ? plemma(p,+l) Only X X
P(p, a,+v) ? plemma(a,+l) P ? S5 Only X X
P(p, a,+v) ? ppos(p,+o) P ? S5 Only X X
pathFrameDist(p, a,+t) ? ppos(p,+o1) ? ppos(a,+o2) Only X X
path(p, a,+t) ? plemma(p,+l) ? cpos(a,+c) Only X X
dep( , a,+d) Only X X
dep( , a,+) ? voice(p,+e) Only X X
dep( , a,+d1) ? dep( , p,+d2) Only X X
(EmptyBody) No X X
Table 2: Templates of the local formulae for hasRole/2 and role/3. H: head of clause is hasRole(p, a), R:
head of clause is role(p, a,+r) and S1 = {ppos, plemma}, S2 = {frame, unlabelFrame, path}, S3 =
{frame, pathFrame}, S4 = {frame, pathFrame, path}, S5 = {pathFrameDist, path}
hasRole/2, role/3, sense/3, feature and argument sib-
lings. Table 3 shows the different configurations of
such modules that we used for the individual lan-
guages. We omit to mention the argument/1, has-
Role/2 and role/3 modules because they are present
for all languages.
A more detailed description of the formulae can
be found in our MLN model files.2 They can be
used both as a reference and as input to our Markov
Logic Engine,3 and thus allow the reader to easily
reproduce our results.
2.2 Global formulae
Global formulae relate several hidden ground atoms.
We use them for two purposes: to ensure consis-
2http://thebeast.googlecode.com/svn/
mlns/conll09
3http://thebeast.googlecode.com
Set Feature sense/2 Argument
siblings
Catalan Yes Yes No
Chinese No Yes No
Czech Yes No No
English No Yes No
German Yes Yes No
Japanese Yes No Yes
Spanish Yes Yes No
Table 3: Different configuration of the modules for the
formulae of the languages.
88
tency between the decisions of all SRL stages and
to capture some of our intuition about the task. We
will refer to formulae that serve the first purpose
as structural constraints. For example, a structural
constraint is given by the (deterministic) formula
role(p, a, r) ? hasRole(p, a)
which ensures that, whenever the argument a is
given a label r with respect to the predicate p, this
argument must be an argument of a as denoted by
hasRole(p,a).
The global formulae that capture our intuition
about the task itself can be further divided into two
classes. The first one uses deterministic or hard con-
straints such as
role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2)
which forbids cases where distinct arguments of a
predicate have the same role unless the role de-
scribes a modifier.
The second class of global formulae is soft or non-
deterministic. For instance, the formula
lemma(p,+l) ? ppos(a,+p)
?hasRole(p, a) ? sense(p,+f)
is a soft global formula. It captures the observation
that the sense of a verb or noun depends on the type
of its arguments. Here the type of an argument token
is represented by its POS tag.
Table 4 presents the global formulae used in this
model.
3 Results
For our experiments we use the corpora provided in
the SRLonly track of the shared task. Our MLN
is tested on the following languages: Catalan and
Spanish (Taule? et al, 2008) , Chinese (Palmer and
Xue, 2009), Czech (Hajic? et al, 2006),4 English
(Surdeanu et al, 2008), German (Burchardt et al,
2006), Japanese (Kawahara et al, 2002).
Table 5 presents the F1-scores and training/test
times for the development and in-domain corpora.
Clearly, our model does better for English. This is
4For training we use only sentences shorter than 40 words in
this corpus.
Structural constraints
hasRole(p, a) ? argument(a)
role(p, a, r) ? hasRole(p, a)
argument(a) ? ?p.hasRole(p, a)
hasRole(p, a) ? ?r.role(p, a, r)
Hard constraints
role(p, a, r1) ? r1 6= r2 ? ?role(p, a, r2)
sense(p, s1) ? s1 6= s2 ? ?sense(p, r2)
role (p, a1, r) ? ?mod (r) ? a1 6= a2 ?
?role (p, a2, r)
Soft constraints
role (p, a1, r) ? ?mod (r) ? a1 6= a2 ?
?role (p, a2, r)
plemma(p,+l)?ppos(a,+p)?hasRole(p, a) ?
sense(p,+f)
plemma(p,+l)? role(p, a,+r) ? sense(p,+f)
Table 4: Global formulae for ML model
Language Devel Test Train Test
time time
Average 77.25% 77.46% 11h 29m 23m
Catalan 78.10% 78.00% 6h 11m 14m
Chinese 77.97% 77.73% 36h 30m 34m
Czech 75.98% 75.75% 14h 21m 1h 7m
English 82.28% 83.34% 12h 26m 16m
German 72.05% 73.52% 2h 28m 7m
Japanese 76.34% 76.00% 2h 17m 4m
Spanish 78.03% 77.91% 6h 9m 16m
Table 5: F-scores for in-domain in corpora for each lan-
guage.
in part because the original model was developed for
English.
To put these results into context: our SRL system
is the third best in the SRLOnly track of the Shared
Task, and it is the sixth best on both Joint and SR-
LOnly tracks. For five of the languages the differ-
ence to the F1 scores of the best system is 3%. How-
ever, for German it is 6.19% and for Czech 10.76%.
One possible explanation for the poor performance
on Czech data will be given below. Note that in com-
parison our system does slightly better in terms of
precision than in terms of recall (we have the fifth
best average precision and the eighth average recall).
Table 6 presents the F1 scores of our system for
the out of domain test corpora. We observe a similar
tendency: our system is the sixth best for both Joint
and SRLOnly tracks. We also observe similar large
differences between our scores and the best scores
for German and Czech (i.e., > 7.5%), while for En-
glish the difference is relatively small (i.e., < 3%).
89
Language Czech English German
F-score 77.34% 71.86% 62.37%
Table 6: F-scores for out-domain in corpora for each lan-
guage.
Finally, we evaluated the effect of the argument
siblings set of formulae introduced for the Japanese
MLN. Without this set the F-score is 69.52% for the
Japanese test set. Hence argument siblings formulae
improve performance by more than 6%.
We found that the MLN for Czech was the one
with the largest difference in performance when
compared to the best system. By inspecting our
results for the development set, we found that for
Czech many of the errors were of a rather techni-
cal nature. Our system would usually extract frame
IDs (such as ?play.02?) by concatenating the lemma
of the token and outcome of the sense/2 prediction
(for the ?02? part). However, in the case of Czech
some frame IDs are not based on the lemma of the
token, but on an abstract ID in a vocabulary (e.g.,
?v-w1757f1?). In these cases our heuristic failed,
leading to poor results for frame ID extraction.
4 Conclusion
We presented a Markov Logic Network that per-
forms joint multi-lingual Semantic Role Labelling.
This network achieves the third best semantic F-
scores in the closed track among the SRLOnly sys-
tems of the CoNLL-09 Shared Task, and sixth best
semantic scores among SRLOnly and Joint systems
for the closed task.
We observed that the inclusion of features which
take into account information about the siblings of
the argument were beneficial for SRL performance
on the Japanese dataset. We also noticed that our
poor performance with Czech are caused by our
frame ID heuristic. Further work has to be done in
order to overcome this problem.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, An-
drea Kowalski, Sebastian Pado?, and Manfred
Pinkal. The SALSA corpus: a German corpus
resource for lexical semantics. In Proceedings of
LREC-2006, Genoa, Italy, 2006.
Koby Crammer and Yoram Singer. Ultraconserva-
tive online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?
991, 2003. ISSN 1533-7928.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, and Zdene?k Z?abokrtsky?. Prague
dependency treebank 2.0, 2006.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??,
Llu??s Ma`rquez, Adam Meyers, Joakim Nivre, Se-
bastian Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mi-
ahi Surdeanu, Nianwen Xue, and Yi Zhang. The
CoNLL-2009 shared task: Syntactic and semantic
dependencies in multiple languages. In Proceed-
ings of CoNLL-2009), Boulder, Colorado, USA,
2009.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti
Hasida. Construction of a Japanese relevance-
tagged corpus. In Proceedings of the LREC-2002,
pages 2008?2013, Las Palmas, Canary Islands,
2002.
Martha Palmer and Nianwen Xue. Adding semantic
roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172, 2009.
Matt Richardson and Pedro Domingos. Markov
logic networks. Machine Learning, 62:107?136,
2006.
Sebastian Riedel. Improving the accuracy and ef-
ficiency of map inference for markov logic. In
UAI ?08: Proceedings of the Annual Conference
on Uncertainty in AI, 2008.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. The CoNLL-
2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of
CoNLL-2008, 2008.
Mariona Taule?, Maria Anto`nia Mart??, and Marta
Recasens. AnCora: Multilevel Annotated Cor-
pora for Catalan and Spanish. In Proceedings of
LREC-2008, Marrakesh, Morroco, 2008.
Nianwen Xue and Martha Palmer. Calibrating fea-
tures for semantic role labeling. In EMNLP ?04:
Proceedings of the Annual Conference on Em-
pirical Methods in Natural Language Processing,
2004.
90
