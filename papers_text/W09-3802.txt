Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 13?24,
Paris, October 2009. c?2009 Association for Computational Linguistics
Weighted parsing of trees
Mark-Jan Nederhof
School of Computer Science, University of St Andrews
North Haugh, St Andrews, KY16 9SX, Scotland
Abstract
We show how parsing of trees can be for-
malized in terms of the intersection of two
tree languages. The focus is on weighted
regular tree grammars and weighted tree
adjoining grammars. Potential applica-
tions are discussed, such as parameter es-
timation across formalisms.
1 Introduction
In parsing theory, strings and trees traditionally
have had a very different status. Whereas strings
in general receive the central focus, the trees in-
volved in derivations of strings are often seen as
auxiliary concepts at best. Theorems tend to be
about the power of grammatical formalisms to
produce strings (weak generative power) rather
than trees (strong generative power).
This can be explained by looking at typical
applications of parsing. In compiler construc-
tion for example, one distinguishes between parse
trees and (abstract) syntax trees, the former being
shaped according to a grammar that is massaged
to make it satisfy relatively artificial constraints,
e.g. that of LALR(1), which is required by many
compiler generators (Aho et al, 2007). The form
of syntax trees is often chosen to simplify phases
of semantic processing that follow parsing. As
the machinery used in such processing is generally
powerful, this offers much flexibility in the choice
of the exact shape and labelling of syntax trees, as
intermediate form between parsing and semantic
analysis.
In the study of natural languages, parse trees
have played a more important role. Whereas lin-
guistic utterances are directly observable and trees
deriving them are not, there are nevertheless tradi-
tions within linguistics that would see one struc-
tural analysis of a sentence as strongly preferred
over another. Furthermore, within computational
linguistics there are empirical arguments to claim
certain parses are correct and others are incorrect.
For example, a question answering systems may
verifiably give the wrong answer if the question
is parsed incorrectly. See (Jurafsky and Martin,
2000) for general discussion on the role of parsing
in NLP.
Despite the relative importance of strong gen-
erative power in computational linguistics, there
is still much freedom in how exactly parse trees
are shaped and how vertices are labelled, due to
the power of semantic analysis that typically fol-
lows parsing. This has affected much of the the-
oretical investigations into the power of linguistic
formalisms, and where strong equivalence is con-
sidered at all, it is often ?modulo relabelling? or
allowing minor structural changes.
With the advent of syntax-based machine trans-
lation, trees have however gained much impor-
tance, and are even considered as the main ob-
jects of study. This is because many MT mod-
ules have trees both as input and output, which
means the computational strength of such mod-
ules can be measured only in terms of the tree lan-
guages they accept and the transductions between
tree languages that they implement. See for exam-
ple (Knight, 2007).
In contrast, trees have always been the central
issue in an important and well-established subfield
of formal language theory that studies tree lan-
guages, tree automata and tree transducers (Gc-
seg and Steinby, 1997). The string languages gen-
erated by the relevant formalisms in this context
are mostly taken to be of secondary importance, if
they are considered at all.
This paper focuses on tree languages, but in-
volves a technique that was devised for string lan-
guages, and shows how the technique carries over
to tree languages. The original technique can be
seen as the most fundamental idea in the field of
context-free parsing, as it captures the essence of
13
finding hierarchical structure in a linear sequence.
The generalization also finds structure in a lin-
ear sequence, but now the sequence corresponds
to paths in trees each leading down from a vertex
to a leaf. This means that the proposed type of
parsing is orthogonal to the conventional parsing
of strings.
The insights this offers have the potential to cre-
ate new avenues of research into the relation be-
tween formalisms that were until now considered
only in isolation. We seek credence to this claim
by investigating how probability distributions can
be carried over from tree adjoining grammars to
regular tree grammars, and vice versa.
The implication that the class of tree languages
of tree adjoining grammars (TAGs) is closed under
intersection with regular tree languages is not sur-
prising, as the linear context-free tree languages
(LCFTLs) are closed under intersection with reg-
ular tree languages (Kepser and Mo?nnich, 2006).
The tree languages of TAGs form a subclass of the
LCFTLs, and the main construction in the proof
of the closure result for the latter can be suitably
restricted to the former.
The structure of this paper is as follows. The
main grammatical formalisms considered in this
paper are summarized in Section 2 and Sec-
tion 3 discusses a number of analyses of these for-
malisms that will be used in later sections. Sec-
tion 4 starts by explaining how parsing of a string
can be seen as the construction of a grammar that
generates the intersection of two languages, and
then moves on to a type of parsing involving in-
tersection of tree languages in place of string lan-
guages.
In order to illustrate the implications of the the-
ory, we consider how it can be used to solve a prac-
tical problem, in Section 5. A number of possible
extensions are outlined in Section 6.
2 Formalisms
In this section, we recall the formalisms of
weighted regular tree grammars and weighted tree
adjoining grammars. We use similar notation and
terminology for both, in order to prepare for Sec-
tion 4, where we investigate the combination of
these formalisms through intersection. As a conse-
quence of the required unified notation, we deviate
to some degree from standard definitions, without
affecting generative power however.
For common definitions of weighted regular
tree grammars, the reader is referred to (Graehl
and Knight, 2004). Weighted tree adjoining gram-
mars are a straightforward generalization of prob-
abilistic (or stochastic) tree adjoining grammars,
as introduced by (Resnik, 1992) and (Schabes,
1992).
For both regular tree grammars (RTGs) and tree
adjoining grammars (TAGs), we will write a la-
beled and ordered tree as A(?). where A is the la-
bel of the root node, and ? is a sequence of expres-
sions of the same form that each represent an im-
mediate subtree. In our presentation, labels do not
have explicit ranks, that is, the number of children
of a node is not determined by its label. This al-
lows an interesting generalization, to be discussed
in Section 6.2.
Where we are interested in the string language
generated by a tree-generating grammar, we may
distinguish between two kinds of labels, the ter-
minal labels, which may occur only at leaves, and
the nonterminal labels, which may occur at any
node. It is customary to write terminal leaves as
a instead of a(). The yield of a tree is the string
of occurrences of terminal labels in it, from left to
right. Note that also nonterminal labels may occur
at the leaves, but they will not be included in the
yield; cf. epsilon rules in context-free grammars.
2.1 Weighted regular tree grammars
A weighted regular tree grammar (WRTG) is a 4-
tuple G = (S,L,R, s`), where S and L are two
finite sets of states and labels, respectively, s` ? S
is the initial state, and R is a finite set of rules.
Each rule has the form:
s0 ? A(s1 ? ? ? sm) ?w?,
where s0, s1, . . . , sm are states (0 ? m), A is a
label and w is a weight.
Rewriting starts with a string containing only
the initial state s`. This string is repeatedly rewrit-
ten by replacing the left-hand side state of a rule by
the right-hand side of the same rule, until no state
remains. It may be convenient to assume a canoni-
cal order of rewriting, for example in terms of left-
most derivations (Hopcroft and Ullman, 1979).
Although alternative semirings can be consid-
ered, here we always assume that the weights
of rules are non-negative real numbers, and the
weight of a derivation of a tree is the product of
the weights of the rule occurrences. If several
(left-most) derivations result in the same tree, then
14
the weight of that tree is given by the sum of the
weights of those derivations. Where we are inter-
ested in the string language, the weights of trees
with the same yield are added to obtain the weight
of that yield.
A (weighted) context-free grammar can be seen
as a special case of a (weighted) regular tree gram-
mar, where the set of states equals the set of labels,
and rules have the form:
A ? A(B1 ? ? ?Bm).
Also the class of (weighted) tree substitution
grammars (Sima?an, 1997) can be seen as a spe-
cial case of (weighted) regular tree grammars, by
letting the set of labels overlap with the set of
states, and imposing two constraints on the allow-
able rules. The first constraint is that for each la-
bel that is also a state, all defining rules are of the
form:
A ? A(s1 ? ? ? sm).
The second constraint is that for each state that is
not a label, there is exactly one rule with that state
in the left-hand side. This means that exactly one
subtree (or elementary tree) can be built top-down
out of such states, down to a level where we again
encounter states that are also labels. If desired, we
can exclude infinite elementary trees by imposing
an additional constraint on allowed sets of rules
(no cycles composed of states that are not labels);
alternatively, we can demand that the grammar
does not contain any useless rules, which automat-
ically excludes such infinite elementary trees.
2.2 Weighted linear indexed grammars
Although we are mainly interested in the tree lan-
guages of tree adjoining grammars, we will use
an equivalent representation in terms of linear in-
dexed grammars, in order to obtain a uniform no-
tation with regard to regular tree grammars.
Thus, a weighted linear indexed grammar
(WLIG) is a 5-tuple G = (S, I, L,R, s`), where
S, I and L are three finite sets of states, indices
and labels, respectively, s` ? S is the initial state,
and R is a finite set of rules. Each rule has one of
the following four forms:
1. s0[??] ? A( s1[ ] ? ? ?
sj?1[ ] sj [??] sj+1[ ] ? ? ?
sm[ ] ) ?w?,
where s0, s1, . . . , sm are states (1 ? j ? m),
A is a label and w is a weight;
2. s[ ] ? A() ?w?;
3. s[??] ? s?[???] ?w?, where ? is an index;
4. s[???] ? s?[??] ?w?.
The expression ?? may be thought of as a vari-
able denoting a string of indices on a stack, and
this variable is to be consistently substituted in
the left-hand and the right-hand sides of rules
upon application during rewriting. In other words,
stacks are copied from the left-hand side of a rule
to at most one member in the right-hand side,
which we will call the head of that rule. The ex-
pression [ ] stands for the empty stack and [???] de-
notes a stack with top element ?. Thereby, rules of
the third type implement a stack push and rules of
the fourth type implement a pop. Rewriting starts
from s`[ ]. The four subsets of R containing rules
of the respective four forms above will be referred
to as R1, R2, R3 and R4.
In terms of tree adjoining grammars, which as-
sume a finite number of elementary trees, the in-
tuition behind the four types of rules is as fol-
lows. Rules of the first type correspond to con-
tinued construction of the same elementary tree.
Rules of the third type correspond to the initiation
of a newly adjoined auxiliary tree and rules of the
fourth type correspond to its completion at a foot
node, returning to an embedding elementary tree
that is encoded in the index that is popped. Rules
of the second type correspond to construction of
leaves, as in the case of regular tree grammars.
See further (Vijay-Shanker and Weir, 1994) for the
equivalence of linear indexed grammars and tree
adjoining grammars.
Note that regular tree grammars can be seen as
special cases of linear indexed grammars, by ex-
cluding rules of the third and fourth types, which
means that stacks of indices always remain empty
(Joshi and Schabes, 1997).
2.3 Probabilistic grammars
A weighted regular tree grammar, or weighted lin-
ear indexed grammar, respectively, is called prob-
abilistic if the weights are probabilities, that is,
values between 0 and 1. A probabilistic regular
tree grammar (PRTG) is proper if for each state
s, the probabilities of all rules that have left-hand
side s sum to one.
Properness for a probabilistic linear indexed
grammar (PLIG) is more difficult to define, due
to the possible overlap of applicability between
15
the four types of rules, listed in the section above.
However, if we encode a given TAG as a LIG in a
reasonable way, then a state s may occur both in
left-hand sides of rules from R1 and in left-hand
sides of rules from R3, but all other such overlap
between the four types is precluded.
Intuitively, a state may represent an internal
node of an elementary tree, in which case rules
from both R1 and R3 may apply, or it may rep-
resent a non-foot leaf node, in which case a rule
from R2 may apply, or it may be a foot node, in
which case a rule from R4 may apply.
With this assumption that the only overlap in ap-
plicability is between R1 and R3, properness can
be defined as follows.
? For each state s, either there are no rules in
R1 or R3 with s in the left-hand side, or the
sum of probabilities of all such rules equals
one.
? For each state s, either there are no rules in
R2 with s in the left-hand side, or the sum of
probabilities of all such rules equals one.
? For each state s and index ?, either there
are no rules in R4 with left-hand side s[???],
or the sum of probabilities of all such rules
equals one.
We say a weighted regular tree grammar, or
weighted linear indexed grammar, respectively, is
consistent if the sum of weights of all (left-most)
derivations is one. This is equivalent to saying that
the sum of weights of all trees is one, and to saying
that the sum of weights of all strings is one.
For each consistent WRTG (WLIG, respec-
tively), there is an equivalent proper and consistent
PRTG (PLIG, respectively). The proof lies in nor-
malization. For WRTGs this is a trivial extension
of normalization of weighted context-free gram-
mars, as described for example by (Nederhof and
Satta, 2003). For WLIGs (and weighted TAGs),
the problem of normalization also becomes very
similar once we consider that the set of derivation
trees of tree adjoining grammars can be described
with context-free grammars, and that this carries
over to weighted derivation trees. See also (Sarkar,
1998).
WLIGs seemingly incur an extra complication,
if a state may occur in combination with an index
on top of the associated stack such that no rules are
applicable. However, for LIGs that encode TAGs,
the problem does not arise as, informally, one may
always resume construction of the embedding el-
ementary tree below the foot node of an adjoined
auxiliary tree.
We say a LIG is in TAG-normal form if (a) at
least one rule is applicable for each combination
of state s and index ? such that s[???] is deriv-
able from s`[ ], and (b) the only overlap in ap-
plicability of the four types of rules is between
R1 and R3. Statements in what follows involv-
ing WLIGs (or PLIGs) in TAG-normal form also
hold for weighted (or probabilistic) TAGs.
3 Analysis of grammars
We call a grammar rule useless if it cannot be part
of any derivation of a tree (or of a string, in the
case of grammars with an emphasis on string lan-
guages). We say a grammar is reduced if it does
not contain useless rules.
Whereas most grammars written by hand or in-
duced by a corpus or treebank are reduced, there
are practical operations that turn reduced gram-
mars into grammars with useless rules; we will
see an example in the next section, where gram-
mars are constructed that generate the intersection
of two given languages. In order to determine
whether the intersection is non-empty, it suffices to
identify useless rules in the intersection grammar.
If and only if all rules are useless, the generated
language is empty.
In the case of context-free grammars (see for ex-
ample (Sippu and Soisalon-Soininen, 1988)), the
analysis to identify useless rules can be split into
two phases:
1. a bottom-up phase to identify the grammar
symbols that generate substrings, which may
include the start symbol if the generated lan-
guage is non-empty; and
2. a top-down phase to identify the grammar
symbols that are reachable from the start
symbol.
The intersection of the generating symbols and the
reachable symbols gives the set of useful symbols.
One can then identify useless rules as those that
contain one or more symbols that are not useful.
The procedure for linear indexed grammars is
similarly split into two phases, of which the first
is given in Figure 1 in the form of a deduction
system. The inference rules simultaneously derive
16
(s, s)
{
s ? S (a)
s
{
s[ ] ? A() (b)
s1 ? ? ? sj?1 (sj , s) sj+1 ? ? ? sm
(s0, s)
{
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) (c)
s1 ? ? ? sm
s0
{
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) (d)
(s1, s2)
(s3, s4)
(s0, s4)
{
s0[??] ? s1[???]
s2[???] ? s3[??] (e)
(s1, s2)
s3
s0
{
s0[??] ? s1[???]
s2[???] ? s3[??] (f)
Figure 1: Simultaneous analysis of two kinds of subderivations in a LIG. Items (s, s?) represent existence
of one or more subderivations s[ ] ?? ?(s?[ ]), where ? is a tree with a gap in the form of an unresolved
state s? associated with an empty stack. Furthermore, s and s? are connected through propagation of a
stack of indices, or in other words, the occurrence of s? is the head of a rule, of which the left-hand side
state is the head of another rule, etc., up to s. In the inference rules, items s represent existence of one or
more subderivations s[ ] ?? ?, where ? is a complete tree (without any unresolved states).
two types of item. The generated language is non-
empty if the item s` can be derived.
We will explain inference rule (f), which is the
most involved of the six rules. The two items
in the antecedent indicate the existence of deriva-
tions s1[ ] ?? ?(s2[ ]) and s3[ ] ?? ?. Note
that s1[ ] ?? ?(s2[ ]) implies s1[?] ?? ?(s2[?]),
because an additional element in the bottom of
a stack would not block an existing derivation.
Hence s0[ ] ? s1[?] ?? ?(s2[?]) ? ?(s3[ ]) ??
?(?), which justifies the item s0 in the consequent
of the rule.
After determining which items can be derived
through the deduction system, it is straightforward
to identify those rules that are useful, by applying
the inference rules in reverse, from consequent to
antecedents, starting with s`.
The running time of the analysis is determined
by how often each of the inference rules can be
applied, which is bounded by the number of ways
each can be instantiated with states and rules from
the grammar. The six inference rules together give
us O(|S| + |R2| + |R1| ? |S| + |R1| + |R3| ? |R4| ?
|S| + |R3| ? |R4|) = O(|S| + |R1| ? |S| + |R2|
+ |R3| ? |R4| ? |S|) = |G|3, where we assume a
reasonable measure for the size |G| of a LIG G, for
example, the total number of occurrences of states,
labels and indices in the rules.
It is not difficult to see that there is exactly one
deduction of s` in the deduction system for each
complete derivation in the grammar. We leave the
full proof to the interested reader, but provide the
hint that items (s, s?) can only play a role in a
complete deduction provided s? is rewritten by a
rule that pops an index from the stack. Because
of this, derivations in the grammar of the form
s[ ] ?? ?(s?[ ]) or of the form s[ ] ?? ? can be
divided in a unique way into subderivations repre-
sentable by our items.
The above deduction system is conceptually
very close to a system of equations that expresses
the sum of weights of all derivations in the gram-
mar, or in(s`), in terms of similar values of the
form in(s), which is the sum of weights of all
subderivations s[ ] ?? ?, and in(s, s?), which is
the sum of weights of all subderivations s[ ] ??
?(s?[ ]). The equations are given in Figure 2.
Although the expressions look unwieldy, they
17
in(s0) =
?
s0[ ] ? A() ?w?
w +
?
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) ?w?
w ? in(s1) ? . . . ? in(sm) +
?
s0[??] ? s1[???] ?w?
s2[???] ? s3[??] ?v?
w ? v ? in(s1, s2) ? in(s3)
in(s0, s) = ?(s0 = s) + ?
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) ?w?
w ? in(s1) ? . . . ? in(sj , s) ? . . . ? in(sm) +
?
s0[??] ? s1[???] ?w?
s2[???] ? s3[??] ?v?
w ? v ? in(s1, s2) ? in(s3, s)
Figure 2: The sum of weights of all derivations in a WLIG, or in(s`), is defined by the smallest non-
negative solution to a system of equations. The function ? with a boolean argument evaluates to 1 if the
condition is true and to 0 otherwise.
express exactly the ?inside? value of the weighted
context-free grammar that we can extract out of
the deduction system from Figure 1, by instanti-
ating the inference rules in all possible ways, and
then taking the consequent as the left-hand side of
a rule, and the antecedent as the right-hand side.
The weight is the product of weights of rules that
appear in the side conditions. It is possible to ef-
fectively solve the system of equations, as shown
by (Wojtczak and Etessami, 2007).
In the same vein we can compute ?outside?
values for weighted linear indexed grammars, as
straightforward analogues of the outside values of
weighted and probabilistic context-free grammars.
The outside value is the sum of weights of partial
derivations that may lie ?outside? a subderivation
s[ ] ?? ? in the case of out(s), or a subderivation
s[ ] ?? ?(s?[ ]) in the case of out(s, s?). The equa-
tions in Figure 3 again follow trivially from the
view of Figure 1 as weighted context-free gram-
mar and the usual definition of outside values.
The functions in and out are particularly useful
for PLIGs in TAG-normal form, as they allow the
expected number of occurrences of state s to be
expressed as:
E(s) = in(s) ? out(s)
Similarly, the expected number of subderivations
of the form s[ ] ?? ?(s?[ ]) is:
E(s, s?) = in(s, s?) ? out(s, s?)
We will return to this issue in Section 5.
4 Weighted intersection
Before we discuss intersection on the level of
trees, we first show how a well-established type of
intersection on the level of strings, with weighted
context-free grammars and weighted finite au-
tomata (WFAs), can be trivially extended to re-
place CFGs with RTGs or LIGs. The intersec-
tion paradigm is originally due to (Bar-Hillel et
al., 1964). Extension to tree adjoining grammars
and linear indexed grammars was proposed before
by (Lang, 1994) and (Vijay-Shanker and Weir,
1993b).
4.1 Intersection of string languages
Let us assume aWLIG G with terminal and nonter-
minal labels. Furthermore, we assume a weighted
finite automaton A, with an input alphabet equal
to the set of terminal labels of G. The transitions
of A are of the form:
q a7? q? ?w?,
where q and q? are states, a is a terminal symbol,
and w is a weight. To simplify the presentation,
18
out(s?) = ?(s? = s`) + ?
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) ?w?
k ? {1, . . . , sj?1, sj+1, . . . , sm} s.t. s? = sk
w ? out(s0, s) ? in(sj , s) ?
?
p /? {j, k}
in(sp) +
?
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) ?w?
k ? {1, . . . , sm} s.t. s? = sk
w ? out(s0) ?
?
p 6= k
in(sp) +
?
s0[??] ? s1[???] ?w?
s2[???] ? s?[??] ?v?
w ? v ? out(s0) ? in(s1, s2)
out(s?, s) = ?
s0[??] ? A(s1[ ] ? ? ? sj [??] ? ? ? sm[ ]) ?w?
s? = sj
w ? out(s0, s) ?
?
p 6= j
in(sp) +
?
s0[??] ? s?[???] ?w?
s[???] ? s3[??] ?v?
w ? v ? out(s0, s4) ? in(s3, s4) +
?
s0[??] ? s1[???] ?w?
s2[???] ? s?[??] ?v?
w ? v ? out(s0, s) ? in(s1, s2) +
?
s0[??] ? s?[???] ?w?
s[???] ? s3[??] ?v?
w ? v ? out(s0) ? in(s3)
Figure 3: The outside values in a WLIG.
we ignore epsilon transitions, and assume there is
a unique initial state q` and a unique final state qa.
We can construct a new WLIG G? whose gen-
erated language is the intersection of the language
generated by G and the language accepted by A.
The rules of G? are:
1. (q0, s0, qm)[??] ?
A( (q0, s1, q1)[ ] ? ? ?
(qj?2, sj?1, qj?1)[ ]
(qj?1, sj , qj)[??]
(qj , sj+1, qj+1)[ ] ? ? ?
(qm?1, sm, qm)[ ] ) ?w?,
for each rule s0[??] ? A(s1[ ] ? ? ? sj?1[ ]
sj [??] sj+1[ ] ? ? ? sm[ ]) ?w? from G and se-
quence q0, . . . , qm of states from A;
2. (q, s, q)[ ] ? A() ?w?, for each rule s[ ] ?
A() ?w? from G and state q from A;
3. (q, s, q?)[ ] ? a ?w ? v?, for each rule s[ ] ?
a ?w? from G and transition q a7? q? ?v? from
A;
4. (q, s, q?)[??] ? (q, s?, q?)[???] ?w?, for each
rule s[??] ? s?[???] ?w? from G and states
q, q? from A;
5. (q, s, q?)[???] ? (q, s?, q?)[??] ?w?, for each
rule s[???] ? s?[??] ?w? from G and states
q, q? from A.
The new states (q, s, q?) give (left-most) deriva-
tions in G? that each simultaneously represent one
(left-most) derivation in G of a certain substring,
starting from state s, and one sequence of transi-
tions taking the automaton A from state q to state
q? while scanning the same substring. The initial
state of G? is naturally (q`, s`, qa), which derives
strings in the intersection of the original two lan-
guages.
Further note that each derivation in G? has a
weight that is the product of the weight of the cor-
19
responding derivation in G and the weight of the
corresponding sequence of transitions in A. This
allows a range of useful applications. For exam-
ple, if A is deterministic (the minimum require-
ment is in fact absence of ambiguity) and if it as-
signs the weight one to all transitions, then G? gen-
erates a set of trees that is exactly the subset of
trees generated by G whose yields are accepted by
A. Furthermore, the weights of those derivations
are preserved. If G is a consistent PLIG in TAG-
normal form, and if A accepts the language of all
strings containing a fixed substring x, then the sum
of probabilities of all derivations in G? gives the
substring probability of x. The effective computa-
tion of this probability was addressed in Section 3.
An even more restricted, but perhaps more fa-
miliar case is if A is a linear structure that accepts
a single input string y of length n. Then G? gen-
erates exactly the set of trees generated by G that
have y as yield. In other words, the string y is
thereby parsed.
If G is binary, i.e. all rules have at most two
states in the right-hand side, then G? has a size
that is cubic in n. This may seem surprising, in
the light of the awareness that practical parsing al-
gorithms for tree adjoining grammars have a time
complexity of no less thanO(n6). However, in or-
der to solve the recognition problem, an analysis
is needed to determine whether G? allows at least
one derivation.
The analysis from Figure 1 requires O(|S?| +
|R?1| ? |S?| + |R?2| + |R?3| ? |R?4| ? |S?|) steps, where
|S?| = O(n2) is the number of states of G?, and
|R?1| = O(n3), |R?2| = |R?3| = |R?4| = O(n2) are
the numbers of rules of G?, divided into the four
main types. This leads to an overall time com-
plexity of O(n6), as expected.
The observation that recognition can be harder
than parsing was made before by (Lang, 1994).
The central new insight this provided was that the
notion of ?parsing? is ill-defined in the literature.
One may choose a form in which to capture all
parses of an input allowed by a grammar, but dif-
ferent such forms may incur different costs of ex-
tracting individual parse trees.
In Section 6.2 we will consider the complexity
of parsing and recognition if G is not binary.
4.2 Intersection of tree languages
We now shift our attention from strings to trees,
and consider the intersection of the tree language
generated by a weighted linear indexed grammar
G1 and the tree language generated by a weighted
regular tree grammar G2. This intersection is gen-
erated by another weighted linear indexed gram-
mar G, which has the following rules:
1. (s0, q0)[??] ? A( (s1, q1)[ ] ? ? ?
(sj?1, qj?1)[ ]
(sj , qj)[??]
(sj+1, qj+1)[ ] ? ? ?
(sm, qm)[ ] ) ?w ? v?,
for each rule s0[??] ? A(s1[ ] ? ? ? sj?1[ ]
sj [??] sj+1[ ] ? ? ? sm[ ]) ?w? from G1 and each
rule q0 ? A(q1 ? ? ? qm) ?v? from G2;
2. (s, q)[ ] ? A() ?w ? v?, for each rule s[ ] ?
A() ?w? from G1 and each rule q ? A() ?v?
from G2;
3. (s, q)[??] ? (s?, q)[???] ?w?, for each rule
s[??] ? s?[???] ?w? from G1 and state q from
G2;
4. (s, q)[???] ? (s?, q)[??] ?w?, for each rule
s[???] ? s?[??] ?w? from G1 and state q from
G2.
Much as in the previous section, each (left-
most) derivation in G corresponds to one (left-
most) derivation in G1 and one in G2. Further-
more, these three derivations derive the same la-
belled tree, and a derivation in G has a weight that
is the product of the weights of the corresponding
derivations in G1 and G2.
It can be instructive to look at special cases.
Suppose that G2 is an unambiguous regular tree
grammar of size O(n) generating a single tree t
with n vertices, assigning weight one to all its
rules. Then the above construction can be seen
as parsing of that tree t. The sum of weights of
derivations in G then gives the weight of the tree
in G1. See Section 3 once more for a general way
to compute this weight, as the inside value of the
initial state of G, which is naturally (s`, q`).
In order to do recognition of t, or in other words,
to determine whether G allows at least one deriva-
tion, the analysis from Figure 1 can be used, which
has time complexity O(|S| + |R1| ? |S| + |R2|
+ |R3| ? |R4| ? |S|), where |S| = O(n) is the
number of states of G, and the numbers of rules
are |R1| = O(n), |R2| = |R3| = |R4| = O(n).
Note that |R1| = O(n) because we have assumed
that G2 allows only one derivation of one tree t,
20
hence q0 uniquely determines q1, . . . , qm. Over-
all, we obtain O(n3) steps, which concurs with a
known result about the complexity of TAG parsing
of trees, as opposed to strings (Poller and Becker,
1998).
Another special case is if WLIG G1 simplifies
to a WRTG (i.e. the stacks of indices remain al-
ways empty), which means we compute the inter-
section of two weighted regular tree grammars G1
and G2. For recognition, or in other words to de-
cide non-emptiness of the intersection, we can still
use Figure 1, although now only inference rules
(b) and (d) are applicable (with a small refinement
to the algorithm we can block spurious application
of (a) where no rules exist that pop indices.) The
complexity is determined by (d), which requires
O(|G1| ? |G2|) steps.
5 Parameter estimation
PLIGs allow finer description of probability distri-
butions than PRTG, both over string languages and
over tree languages. However, the (string) pars-
ing complexity of regular tree grammars is O(n3)
and that of LIGs is O(n6). It may therefore be
preferable for reasons of performance to do pars-
ing with a PRTG even when a PTAGs or PLIG is
available with accurately trained probabilities. Al-
ternatively, one may do both, with a PRTG used
in a first phase to heuristically reduce the search
space.
This section outlines how a suitable PRTG G2
can be extracted out of a PLIG G1, assuming the
underlying RTG G?2 without weights is already
given. The tree language generated by G?2 may be
an approximation of that generated by G1. The ob-
jective is to make G2 as close as possible to G1 in
terms of probability distributions over trees. We
assume that G?2 is unambiguous, that is, for each
tree it generates, there is at most one derivation.
The procedure is a variant of the one described
by (Nederhof, 2005). The idea is that derivations
in G1 are mapped to those in G?2, via the trees in the
intersection of the two tree languages. The proba-
bility distribution of states and rules in G2 is esti-
mated based on the expected frequencies of states
and rules from G?2 in the intersection.
Concretely, we turn the RTG G?2 into a PRTG
G??2 that is obtained simply be assigning weight
one to all rules. We then compute the intersec-
tion grammar G as in Section 4.2. Subsequently,
the inside and outside values are computed for G,
as explained in Section 3. The expected number of
occurrences of a rule in G of the form:
(s0, q0)[??] ? A( (s1, q1)[ ] ? ? ?
(sj?1, qj?1)[ ]
(sj , qj)[??]
(sj+1, qj+1)[ ] ? ? ?
(sm, qm)[ ] ) ?w ? v?,
is given by multiplying the outside and inside
probabilities and the rule probability, as usual.
We get two terms however that we need to sum.
The intuition is that we must count both rule oc-
currences used for building initial TAG trees and
those used for building auxiliary TAG trees. This
gives:
w ? v ? out((s0, q0)) ?
?
k
in((sk, qk)) +
w ? v ?
?
s,q
out((s0, q0), (s, q)) ?
in((sj , qj), (s, q)) ?
?
k 6=j
in((sk, qk))
By summing these expected numbers for different
rules s0[??] ? A(s1[ ] ? ? ? sj?1[ ] sj [??] sj+1[ ]
? ? ? sm[ ]), we obtain the expected number of oc-
currences of q0 ? A(q1 ? ? ? qm), Let us denote
this sum by E(q0 ? A(q1 ? ? ? qm)). By summing
these for fixed q0, we obtain the expected number
of occurrences of q0, which we denote by E(q0).
The probability of q0 ? A(q1 ? ? ? qm) in G2 is then
set to be the ratio of E(q0 ? A(q1 ? ? ? qm)) and
E(q0).
By this procedure, the Kullback-Leibler dis-
tance between G1 and G2 is minimized. Although
the present paper deals with very different for-
malisms, the proof of correctness is identical to
that in (Nederhof, 2005). The reason is that in both
cases the mathematical analysis must focus on the
objects in the intersection (strings or trees) which
may correspond to multiple derivations in the orig-
inal model (here G1) but to a single derivation in
the unambiguous model to be trained (here G2),
and each derivation is composed of rules, whose
probabilities are to be multiplied.
6 Extensions
6.1 Transduction
For various formalisms describing (string or tree)
languages, there are straightforward generaliza-
tions that describe a relation between two or more
21
languages, which is known as a transduction. The
idea is that the underlying control mechanism,
such as the states in regular tree grammars or lin-
ear indexed grammars, is now coupled to two or
more surface forms that are synchronously pro-
duced. For example, a rule in a weighted syn-
chronous regular tree grammar (WSRTG) has the
form:
s0 ? A(s1 ? ? ? sm), B(spi(1) ? ? ? spi(m)) ?w?,
where pi is a permutation of 1, . . . ,m. We can gen-
eralize this to having a third label C and a second
permutation pi?, in order to describe simultaneous
relations between three tree languages, etc. In this
section we will restrict ourselves to binary rela-
tions however, and call the first surface form the
input and the second surface form the output. For
synchronous tree adjoining grammars, see for ex-
ample (Shieber, 1994).
If we apply intersection on the input or on the
output of a synchronous grammar formalism, then
this is best seen as composition. This is well-
known in the case of finite-state transducers and
some forms of context-free transduction (Berstel,
1979), and application to a wider range of for-
malisms is gaining interest in the area of machine
translation (Knight, 2007).
With the intersection from Section 4.2 trivially
extended to composition, we can now implement
composition of the form:
?1 ? . . . ? ?k,
where the different ?j are transducers, of which
k ? 1 are (W)SRTGs and at most one is a
(weighted) synchronous LIG ((W)SLIG). The re-
sult of the composition is another (W)SLIG. It
should be noted that a (W)RTS (or (W)LIG) can
be seen as a (W)SRTG (or (W)SLIG, respectively)
that represents the identity relation on its tree lan-
guage.
6.2 Binarization
In the discussion of complexity in Section 4.1, we
assumed that rules are binary, that is, that they
have at most two states in each right-hand side.
However, whereas any context-free grammar can
be transformed into a binary form (e.g. Chomsky
normal form), the grammars as we have defined
them cannot be. We will show that this is to a large
extent a consequence of our definitions, which
were motivated by presentational ease, rather than
by generality.
The main problem is formed by rules of the
form s0 ? A(s1 ? ? ? sm), where m > 2. Such
long rules cannot be broken up into shorter rules
of the same form, as this would require an addi-
tional labelled vertex, changing the tree language.
An apparent solution lies in allowing branching
rules without any label, for example s1 ? s2 s3.
Regrettably this could create substantial computa-
tional problems for intersection of the described
tree languages. As labels provide the mechanism
through which to intersect tree languages, rules
of the above form are somewhat similar to unit
rules or epsilon rules in context-free grammars, in
that they are not bound to observable elements.
Branching rules furthermore have the potential
to generate context-free languages, and therefore
they are more pernicious to intersection, consider-
ing that emptiness of intersection of context-free
languages is undecidable.
It therefore seems better to restrict branching
rules s1 ? s2 s3 to finite-state power, for exam-
ple by making these rules exclusively left-linear
or right-linear. A more elegant but equivalent way
of looking at this may be to have rules of the form:
s0 ? A(R),
where R is a regular language over states. In the
case of linear indexed grammars, we would have
rules of the form:
s[??] ? A(L s?[??] R)
where L and R are regular languages over expres-
sions of the form s[ ]. Appropriate weighted fi-
nite automata can be used to assign weights to se-
quences of such expressions in L and R. With
these extended types of rules, our construction
from Section 4.2 still works. The key observation
here is that regular languages are closed under in-
tersection.
One of the implications of the above extended
definitions is that labels appear not only with-
out fixed ranks, as we have assumed from the
start in Section 2, but even without a bound on
the rank. Concretely, a vertex may appear with
any number of children in a tree. Whereas this
may be unconventional in certain areas of formal
language theory, it is a well-accepted practice in
the parsing of natural language to make the num-
ber of constituents of syntactic categories flexi-
ble and conceptually unbounded; see for example
22
(Collins, 1997). Also the literature on unranked
tree automata is very relevant; see for example
(Schwentick, 2007). Binarization for LIGs was
considered before by (Vijay-Shanker and Weir,
1993a).
6.3 Beyond TAGs
In the light of results by (Kepser and Mo?nnich,
2006) it is relatively straightforward to consider
larger classes of linear context-free tree grammars
in place of tree-adjoining grammars, in order to
generalize the construction in Section 4.2.
The generalization described in what follows
seems less straightforward. Context-free lan-
guages can be characterized in terms of parse trees
in which path sets (sets of strings of labels on
paths from the root to a leaf) are regular. In the
case of tree adjoining languages, the path sets are
context-free. There is a hierarchy of classes of lan-
guages in which the third step is to consider path
sets that are tree adjoining languages (Weir, 1992).
In this paper, we have considered the parsing-as-
intersection paradigm for the first two members of
the hierarchy. It may be possible that the paradigm
is also applicable to the third and following mem-
bers. This avenue is yet to be pursued.
7 Conclusions
This paper has extended the parsing-as-
intersection paradigm from string languages
to tree languages. Probabilities, or weights in
general, were incorporated in this framework in a
natural way. We have discussed one particular ap-
plication involving a special case of the extended
paradigm.
Acknowledgements
Helpful comments by anonymous reviewers are
gratefully acknowledged. The basic result from
Section 4.1 as it pertains to RTGs as subclass of
LIGs was discussed with Heiko Vogler, who pro-
posed two alternative proofs. Sylvain Schmitz
pointed out to me the relevance of literature on lin-
ear context-free tree languages.
References
A.V. Aho, M.S. Lam, R. Sethi, and J.D. Ullman.
2007. Compilers: Principles, Techniques, & Tools.
Addison-Wesley.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On
formal properties of simple phrase structure gram-
mars. In Y. Bar-Hillel, editor, Language and Infor-
mation: Selected Essays on their Theory and Appli-
cation, chapter 9, pages 116?150. Addison-Wesley,
Reading, Massachusetts.
J. Berstel. 1979. Transductions and Context-Free Lan-
guages. B.G. Teubner, Stuttgart.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In 35th Annual Meeting of
the Association for Computational Linguistics, Pro-
ceedings of the Conference, pages 16?23, Madrid,
Spain, July.
J. Graehl and K. Knight. 2004. Training tree transduc-
ers. In HLT-NAACL 2004, Proceedings of the Main
Conference, Boston, Massachusetts, USA, May.
F. Gcseg and M. Steinby. 1997. Tree languages. In
G. Rozenberg and A. Salomaa, editors, Handbook
of Formal Languages, Vol. 3, chapter 1, pages 1?68.
Springer, Berlin.
J.E. Hopcroft and J.D. Ullman. 1979. Introduction
to Automata Theory, Languages, and Computation.
Addison-Wesley.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, edi-
tors, Handbook of Formal Languages. Vol 3: Beyond
Words, chapter 2, pages 69?123. Springer-Verlag,
Berlin/Heidelberg/New York.
D. Jurafsky and J.H. Martin. 2000. Speech and Lan-
guage Processing. Prentice-Hall.
S. Kepser and U. Mo?nnich. 2006. Closure properties
of linear context-free tree languages with an appli-
cation to optimality theory. Theoretical Computer
Science, 354:82?97.
K. Knight. 2007. Capturing practical natural language
transformations. Machine Translation, 21:121?133.
B. Lang. 1994. Recognition can be harder than pars-
ing. Computational Intelligence, 10(4):486?494.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop
on Parsing Technologies, pages 137?148, LORIA,
Nancy, France, April.
M.-J. Nederhof. 2005. A general technique to train
language models on language models. Computa-
tional Linguistics, 31(2):173?185.
P. Poller and T. Becker. 1998. Two-step TAG pars-
ing revisited. In Fourth International Workshop on
Tree Adjoining Grammars and Related Frameworks,
pages 143?146. Institute for Research in Cognitive
Science, University of Pennsylvania, August.
23
P. Resnik. 1992. Probabilistic tree-adjoining grammar
as a framework for statistical natural language pro-
cessing. In Proc. of the fifteenth International Con-
ference on Computational Linguistics, pages 418?
424. Nantes, August.
A. Sarkar. 1998. Conditions on consistency of prob-
abilistic tree adjoining grammars. In 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics, volume 2, pages 1164?1170,
Montreal, Quebec, Canada, August.
Y. Schabes. 1992. Stochastic lexicalized tree-
adjoining grammars. In Proc. of the fifteenth Inter-
national Conference on Computational Linguistics,
pages 426?432. Nantes, August.
Thomas Schwentick. 2007. Automata for XML?a
survey. Journal of Computer and System Sciences,
73:289?315.
S.M. Shieber. 1994. Restricting the weak-generative
capacity of synchronous tree-adjoining grammars.
Computational Intelligence, 10(4):371?385.
K. Sima?an. 1997. Efficient disambiguation by means
of stochastic tree substitution grammars. In D. Jones
and H. Somers, editors, New Methods in Language
Processing. UCL Press, UK.
S. Sippu and E. Soisalon-Soininen. 1988. Parsing The-
ory, Vol. I: Languages and Parsing, volume 15 of
EATCS Monographs on Theoretical Computer Sci-
ence. Springer-Verlag.
K. Vijay-Shanker and D.J. Weir. 1993a. Parsing some
constrained grammar formalisms. Computational
Linguistics, 19(4):591?636.
K. Vijay-Shanker and D.J. Weir. 1993b. The use of
shared forests in tree adjoining grammar parsing. In
Sixth Conference of the European Chapter of the As-
sociation for Computational Linguistics, Proceed-
ings of the Conference, pages 384?393, Utrecht, The
Netherlands, April.
K. Vijay-Shanker and D.J. Weir. 1994. The equiva-
lence of four extensions of context-free grammars.
Mathematical Systems Theory, 27:511?546.
D.J. Weir. 1992. A geometric hierarchy beyond
context-free languages. Theoretical Computer Sci-
ence, 104:235?261.
D. Wojtczak and K. Etessami. 2007. PReMo: an an-
alyzer for Probabilistic Recursive Models. In Tools
and Algorithms for the Construction and Analysis
of Systems, 13th International Conference, volume
4424 of Lecture Notes in Computer Science, pages
66?71, Braga, Portugal. Springer-Verlag.
24
