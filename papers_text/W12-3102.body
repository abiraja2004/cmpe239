Proceedings of the 7th Workshop on Statistical Machine Translation, pages 10?51,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Findings of the 2012 Workshop on Statistical Machine Translation
Chris Callison-Burch
Johns Hopkins University
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
SDL Language Weaver
Lucia Specia
University of Sheffield
Abstract
This paper presents the results of the WMT12
shared tasks, which included a translation
task, a task for machine translation evaluation
metrics, and a task for run-time estimation of
machine translation quality. We conducted a
large-scale manual evaluation of 103 machine
translation systems submitted by 34 teams.
We used the ranking of these systems to mea-
sure how strongly automatic metrics correlate
with human judgments of translation quality
for 12 evaluation metrics. We introduced a
new quality estimation task this year, and eval-
uated submissions from 11 teams.
1 Introduction
This paper presents the results of the shared tasks
of the Workshop on statistical Machine Translation
(WMT), which was held at NAACL 2012. This
workshop builds on six previous WMT workshops
(Koehn and Monz, 2006; Callison-Burch et al.,
2007; Callison-Burch et al., 2008; Callison-Burch
et al., 2009; Callison-Burch et al., 2010; Callison-
Burch et al., 2011). In the past, the workshops have
featured a number of shared tasks: a translation task
between English and other languages, a task for au-
tomatic evaluation metrics to predict human judg-
ments of translation quality, and a system combina-
tion task to get better translation quality by combin-
ing the outputs of multiple translation systems. This
year we discontinued the system combination task,
and introduced a new task in its place:
? Quality estimation task ? Structured predic-
tion tasks like MT are difficult, but the dif-
ficulty is not uniform across all input types.
It would thus be useful to have some mea-
sure of confidence in the quality of the output,
which has potential usefulness in a range of set-
tings, such as deciding whether output needs
human post-editing or selecting the best trans-
lation from outputs from a number of systems.
This shared task focused on sentence-level es-
timation, and challenged participants to rate
the quality of sentences produced by a stan-
dard Moses translation system on an English-
Spanish news corpus in one of two tasks:
ranking and scoring. Predictions were scored
against a blind test set manually annotated with
relevant quality judgments.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dissem-
inate common test sets and public training data with
published performance numbers, and to refine eval-
uation methodologies for machine translation. As
with previous workshops, all of the data, transla-
tions, and collected human judgments are publicly
available.1 We hope these datasets form a valuable
resource for research into statistical machine transla-
tion, system combination, and automatic evaluation
or automatic prediction of translation quality.
2 Overview of the Shared Translation Task
The recurring task of the workshop examines trans-
lation between English and four other languages:
German, Spanish, French, and Czech. We created a
1http://statmt.org/wmt12/results.html
10
test set for each language pair by translating newspa-
per articles. We additionally provided training data
and two baseline systems.
2.1 Test data
The test data for this year?s task was created by hir-
ing people to translate news articles that were drawn
from a variety of sources from November 15, 2011.
A total of 99 articles were selected, in roughly equal
amounts from a variety of Czech, English, French,
German, and Spanish news sites:2
Czech: Blesk (1), CTK (1), E15 (1), den??k (4),
iDNES.cz (3), iHNed.cz (3), Ukacko (2),
Zheny (1)
French: Canoe (3), Croix (3), Le Devoir (3), Les
Echos (3), Equipe (2), Le Figaro (3), Libera-
tion (3)
Spanish: ABC.es (4), Milenio (4), Noroeste (4),
Nacion (3), El Pais (3), El Periodico (3), Prensa
Libre (3), El Universal (4)
English: CNN (3), Fox News (2), Los Angeles
Times (3), New York Times (3), Newsweek (1),
Time (3), Washington Post (3)
German: Berliner Kurier (1), FAZ (3), Giessener
Allgemeine (2), Morgenpost (3), Spiegel (3),
Welt (3)
The translations were created by the professional
translation agency CEET.3 All of the translations
were done directly, and not via an intermediate lan-
guage.
Although the translations were done profession-
ally, we observed a number of errors. These errors
ranged from minor typographical mistakes (I was
terrible. . . instead of It was terrible. . . ) to more
serious errors of incorrect verb choices and nonsen-
sical constructions. An example of the latter is the
French sentence (translated from German):
Il a gratte? une planche de be?ton, perdit des
pie`ces du ve?hicule.
(He scraped against a concrete crash bar-
rier and lost parts of the car.)
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
3http://www.ceet.eu/
Here, the French verb gratter is incorrect, and the
phrase planche de be?ton does not make any sense.
We did not quantify errors, but collected a number
of examples during the course of the manual evalua-
tion. These errors were present in the data available
to all the systems and therefore did not bias the re-
sults, but we suggest that next year a manual review
of the professionally-collected translations be taken
prior to releasing the data in order to correct mis-
takes and provide feedback to the translation agency.
2.2 Training data
As in past years we provided parallel corpora to train
translation models, monolingual corpora to train lan-
guage models, and development sets to tune system
parameters. Some statistics about the training mate-
rials are given in Figure 1.
2.3 Submitted systems
We received submissions from 34 groups across 18
institutions. The participants are listed in Table 1.
We also included two commercial off-the-shelf MT
systems, three online statistical MT systems, and
three online rule-based MT systems. Not all systems
supported all language pairs. We note that the eight
companies that developed these systems did not sub-
mit entries themselves, but were instead gathered by
translating the test data via their interfaces (web or
PC).4 They are therefore anonymized in this paper.
The data used to construct these systems is not sub-
ject to the same constraints as the shared task partic-
ipants. It is possible that part of the reference trans-
lations that were taken from online news sites could
have been included in the systems? models, for in-
stance. We therefore categorize all commercial sys-
tems as unconstrained when evaluating the results.
3 Human Evaluation
As with past workshops, we placed greater empha-
sis on the human evaluation than on the automatic
evaluation metric scores. It is our contention that
automatic measures are an imperfect substitute for
human assessment of translation quality. Therefore,
we define the manual evaluation to be primary, and
4We would like to thank Ondr?ej Bojar for harvesting the
commercial entries, Christian Federmann for the statistical MT
entries, and Herve? Saint-Amand for the rule-based MT entries.
11
Europarl Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 157,302 137,097 158,840 136,151
Words 4,449,786 3,903,339 3,915,218 3,403,043 3,950,394 3,856,795 2,938,308 3,264,812
Distinct words 78,383 57,711 63,805 53,978 130,026 57,464 136,392 52,488
United Nations Training Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech
Sentence 51,827,706 8,627,438 16,708,622 30,663,107 18,931,106
Words 1,249,883,955 247,722,726 410,581,568 576,833,910 315,167,472
Distinct words 2,265,254 926,999 1,267,582 3,336,078 2,304,933
News Test Set
English Spanish French German Czech
Sentences 3003
Words 73,785 78,965 81,478 73,433 65,501
Distinct words 9,881 12,137 11,441 14,252 17,149
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of
distinct words (case-insensitive) is based on the provided tokenizer.
12
ID Participant
CMU Carnegie Mellon University (Denkowski et al., 2012)
CU-BOJAR Charles University - Bojar (Bojar et al., 2012)
CU-DEPFIX Charles University - DEPFIX (Rosa et al., 2012)
CU-POOR-COMB Charles University - Bojar (Bojar et al., 2012)
CU-TAMCH Charles University - Tamchyna (Tamchyna et al., 2012)
CU-TECTOMT Charles University - TectoMT (Dus?ek et al., 2012)
DFKI-BERLIN German Research Center for Artificial Intelligence (Vilar, 2012)
DFKI-HUNSICKER German Research Center for Artificial Intelligence - Hunsicker (Hunsicker et al., 2012)
GTH-UPM Technical University of Madrid (Lo?pez-Luden?a et al., 2012)
ITS-LATL Language Technology Laboratory @ University of Geneva (Wehrli et al., 2009)
JHU Johns Hopkins University (Ganitkevitch et al., 2012)
KIT Karlsruhe Institute of Technology (Niehues et al., 2012)
LIMSI LIMSI (Le et al., 2012)
LIUM University of Le Mans (Servan et al., 2012)
PROMT ProMT (Molchanov, 2012)
QCRI Qatar Computing Research Institute (Guzman et al., 2012)
QUAERO The QUAERO Project (Markus et al., 2012)
RWTH RWTH Aachen (Huck et al., 2012)
SFU Simon Fraser University (Razmara et al., 2012)
UEDIN-WILLIAMS University of Edinburgh - Williams (Williams and Koehn, 2012)
UEDIN University of Edinburgh (Koehn and Haddow, 2012)
UG University of Toronto (Germann, 2012)
UK Charles University - Zeman (Zeman, 2012)
UPC Technical University of Catalonia (Formiga et al., 2012)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C] Three online statistical machine translation systems
RBMT-[1,3,4] Three rule-based statistical machine translation systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations
from the commercial, online, and rule-based systems were crawled by us, not submitted by the respective companies,
and are therefore anonymized. Anonymized identifiers were chosen so as to correspond with the WMT11 systems.
13
Language Pair Num Label Labels per
Systems Count System
Czech-English 6 6,470 1,078.3
English-Czech 13 11,540 887.6
German-English 16 7,135 445.9
English-German 15 8,760 584.0
Spanish-English 12 5,705 475.4
English-Spanish 11 7,375 670.4
French-English 15 6,975 465.0
English-French 15 7,735 515.6
Overall 103 61,695 598
Table 2: A summary of the WMT12 ranking task, show-
ing the number of systems and number of labels (rank-
ings) collected for each of the language translation tasks.
use the human judgments to validate automatic met-
rics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct on the scale of our
workshop. We distributed the workload across a
number of people, beginning with shared-task par-
ticipants and interested volunteers. This year, we
also opened up the evaluation to non-expert anno-
tators hired on Amazon Mechanical Turk (Callison-
Burch, 2009). To ensure that the Turkers provided
high quality annotations, we used controls con-
structed from the machine translation ranking tasks
from prior years. Control items were selected such
that there was high agreement across the system de-
velopers who completed that item. In all, there were
229 people who participated in the manual evalua-
tion, with 91 workers putting in more than an hour?s
worth of effort, and 21 putting in more than four
hours. After filtering Turker rankings against the
controls to discard Turkers who fell below a thresh-
old level of agreement on the control questions,
there was a collective total of 336 hours of usable
labor. This is similar to the total of 361 hours of
labor collected for WMT11.
We asked annotators to evaluate system outputs
by ranking translated sentences relative to each
other. This was our official determinant of trans-
lation quality. The total number of judgments col-
lected for each of the language pairs is given in Ta-
ble 2.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the instruc-
tions simple:
You are shown a source sentence followed
by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions, and asked to rank them.
We refer to each of these as ranking tasks or some-
times blocks.
Every language task had more than five partici-
pating systems ? up to a maximum of 16 for the
German-English task. Rather than attempting to get
a complete ordering over the systems in each rank-
ing task, we instead relied on random selection and
a reasonably large sample size to make the compar-
isons fair.
We use the collected rank labels to assign each
system a score that reflects how highly that system
was usually ranked by the annotators. The score for
some systemA reflects how frequently it was judged
to be better than other systems. Specifically, each
block in whichA appears includes four implicit pair-
wise comparisons (against the other presented sys-
tems). A is rewarded once for each of the four com-
parisons in which A wins, and its score is the num-
ber of such winning pairwise comparisons, divided
by the total number of non-tying pairwise compar-
isons involving A.
This scoring metric is different from that used in
prior years in two ways. First, the score previously
included ties between system rankings. In that case,
the score for A reflected how often A was rated as
better than or equal to other systems, and was nor-
malized by all comparisons involving A. However,
this approach unfairly rewards systems that are sim-
ilar (and likely to be ranked as tied). This is prob-
lematic since many of the systems use variations of
the same underlying decoder (Bojar et al., 2011).
A second difference is that this year we no longer
include comparisons against reference translations.
In the past, reference translations were included
14
among the systems to be ranked as controls, and
the pairwise comparisons were used in determin-
ing the best system. However, workers have a very
clear preference for reference translations, so includ-
ing them unduly penalized systems that, through
(un)luck of the draw, were pitted against the ref-
erences more often. These changes are part of a
broader discussion of the best way to produce the
system ranking, which we discuss at length in Sec-
tion 4.
The system scores are reported in Section 3.3.
Appendix A provides detailed tables that contain
pairwise head-to-head comparisons between pairs of
systems.
3.2 Inter- and Intra-annotator agreement in
the ranking task
Each year we calculate the inter- and intra-annotator
agreement for the human evaluation, since a reason-
able degree of agreement must exist to support our
process as a valid evaluation setup. To ensure we
had enough data to measure agreement, we occa-
sionally showed annotators items that were repeated
from previously completed items. These repeated
items were drawn from ones completed by the same
annotator and from different annotators.
We measured pairwise agreement among anno-
tators using Cohen?s kappa coefficient (?) (Cohen,
1960), which is defined as
? =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the anno-
tators agree, and P (E) is the proportion of time that
they would agree by chance. Note that ? is basically
a normalized version of P (A), one which takes into
account how meaningful it is for annotators to agree
with each other, by incorporating P (E). Note also
that ? has a value of at most 1 (and could possibly
be negative), with higher rates of agreement result-
ing in higher ?.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate at
which annotators agree, in the context of pairwise
comparisons. P (A) is computed similarly for intra-
annotator agreement (i.e. self-consistency), but over
pairwise comparisons that were annotated more than
once by a single annotator.
As for P (E), it should capture the probability that
two annotators would agree randomly. Therefore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is com-
puted empirically, by observing how often annota-
tors actually rank two systems as being tied. We
note here that this empirical computation is a depar-
ture from previous years? analyses, where we had
assumed that the three categories are equally likely
(yielding P (E) = 19 +
1
9 +
1
9 =
1
3 ). We believe that
this is a more principled approach, which faithfully
reflects the motivation of accounting for P (E) in the
first place.
Table 3 gives ? values for inter-annotator and
intra-annotator agreement. These give an indica-
tion of how often different judges agree, and how
often single judges are consistent for repeated judg-
ments, respectively. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0 ? 0.2 is slight, 0.2 ? 0.4
is fair, 0.4 ? 0.6 is moderate, 0.6 ? 0.8 is sub-
stantial, and 0.8 ? 1.0 is almost perfect. Based on
these interpretations, the agreement for sentence-
level ranking is fair for inter-annotator and moder-
ate for intra-annotator agreement. Consistent with
previous years, intra-annotator agreement is higher
than inter-annotator agreement, except for English?
Czech.
An important difference from last year is that the
evaluations were not constrained only to workshop
participants, but were made available to all Turk-
ers. The workshop participants were trusted to com-
plete the tasks in good faith, and we have multiple
years of data establishing general levels of inter- and
intra-annotator agreement. Their HITs were unpaid,
and access was limited with the use of a qualifica-
tion. The Turkers completed paid tasks, and we used
controls to filter out fraudulent and unconscientious
workers.
15
INTER-ANNOTATOR AGREEMENT INTRA-ANNOTATOR AGREEMENT
LANGUAGE PAIRS P (A) P (E) ? P (A) P (E) ?
Czech-English 0.567 0.405 0.272 0.660 0.405 0.428
English-Czech 0.576 0.383 0.312 0.566 0.383 0.296
German-English 0.595 0.401 0.323 0.733 0.401 0.554
English-German 0.598 0.394 0.336 0.732 0.394 0.557
Spanish-English 0.540 0.408 0.222 0.792 0.408 0.648
English-Spanish 0.504 0.398 0.176 0.566 0.398 0.279
French-English 0.568 0.406 0.272 0.719 0.406 0.526
English-French 0.519 0.388 0.214 0.634 0.388 0.401
WMT12 0.568 0.396 0.284 0.671 0.396 0.455
WMT11 0.601 0.362 0.375 0.722 0.362 0.564
Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11
rows contain the results from the European languages individual systems task (Callison-Burch et al. (2011), Table 7).
Agreement rates vary widely across languages.
For inter-annotator agreements, the range is 0.176 to
0.336, while intra-annotator agreement ranges from
0.279 to 0.648. We note in particular the low agree-
ment rates among judgments in the English-Spanish
task, which is reflected in the relative lack of statis-
tical significance Table 4. The agreement rates for
this year were somewhat lower than last year.
3.3 Results of the Translation Task
We used the results of the manual evaluation to an-
alyze the translation quality of the different systems
that were submitted to the workshop. In our analy-
sis, we aimed to address the following questions:
? Which systems produced the best translation
quality for each language pair?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 4 shows the system ranking for each of the
translation tasks. For each language pair, we define
a system as ?winning? if no other system was found
statistically significantly better (using the Sign Test,
at p ? 0.10). In some cases, multiple systems are
listed as winners, either due to a large number of par-
ticipants or a low number of judgments per system
pair, both of which are factors that make it difficult
to achieve statistical significance.
As in prior years, unconstrained online systems
A and B are among the best for many tasks, with
a few notable exceptions. CU-DEPFIX, which post-
processes the output of ONLINE-B, was judged as
the best system for English-Czech. For the French-
English and English-French tasks, constrained sys-
tems came out on top, with LIMSI appearing both
times. Consistent with prior years, the rule-based
systems performed very well on the English-German
task. A rule-based system also had a good showing
for English-Spanish, but not really anywhere else.
Among the systems competing in all tasks, no sin-
gle system consistently appeared among the top en-
trants. Participants that competed in all tasks tended
to fair worse, with the exception of UEDIN. Addi-
tionally, KIT appeared in four tasks and was a con-
strained winner each time.
4 Methods for Overall Ranking
Last year one of the long papers published at WMT
criticized our method for compiling the overall rank-
ing for systems in the translation task (Bojar et
al., 2011). This year another paper shows some
additional potential inconsistencies in the rankings
(Lopez, 2012). In this section we delve into a de-
tailed analysis of a variety of methods that use the
human evaluation to create an overall ranking of sys-
tems.
In the human evaluation, we collect ranking judg-
ments for output from five systems at a time. We in-
terpret them as 10 ?
(
5?4
2
)
pairwise judgments over
systems and use these to analyze how each system
faired compared against each of the others. Not all
16
Czech-English
3,603?3,718 comparisons/system
System C? >others
ONLINE-B ? N 0.65
UEDIN ? Y 0.60
CU-BOJAR Y 0.53
ONLINE-A N 0.53
UK Y 0.37
JHU Y 0.32
Spanish-English
1,527?1,775 comparisons/system
System C? >others
ONLINE-A ? N 0.62
ONLINE-B ? N 0.61
QCRI ? Y 0.60
UEDIN ?? Y 0.58
UPC Y 0.57
GTH-UPM Y 0.52
RBMT-3 N 0.51
JHU Y 0.48
RBMT-4 N 0.46
RBMT-1 N 0.42
ONLINE-C N 0.42
UK Y 0.19
French-English
1,437?1,701 comparisons/system
System C? >others
LIMSI ?? Y 0.63
KIT ?? Y 0.61
ONLINE-A ? N 0.59
CMU ?? Y 0.57
ONLINE-B ? N 0.57
UEDIN Y 0.55
LIUM Y 0.52
RWTH Y 0.52
RBMT-1 N 0.46
RBMT-3 N 0.46
UK Y 0.44
SFU Y 0.44
RBMT-4 N 0.43
JHU Y 0.41
ONLINE-C N 0.32
English-Czech
2,652?3,146 comparisons/system
System C? >others
CU-DEPFIX ? N 0.66
ONLINE-B N 0.63
UEDIN ? Y 0.56
CU-TAMCH N 0.56
CU-BOJAR ? Y 0.54
CU-TECTOMT ? Y 0.53
ONLINE-A N 0.53
COMMERCIAL-1 N 0.48
COMMERCIAL-2 N 0.46
CU-POOR-COMB Y 0.44
UK Y 0.44
SFU Y 0.36
JHU Y 0.32
English-Spanish
2,013?2,294 comparisons/system
System C? >others
ONLINE-B ? N 0.65
RBMT-3 N 0.58
ONLINE-A ? N 0.56
PROMT N 0.55
UPC ? Y 0.52
UEDIN ? Y 0.52
RBMT-4 N 0.46
RBMT-1 N 0.45
ONLINE-C N 0.43
UK Y 0.41
JHU Y 0.36
English-French
1,410?1,697 comparisons/system
System C? >others
LIMSI ?? Y 0.66
RWTH Y 0.62
ONLINE-B N 0.60
KIT ?? Y 0.59
LIUM Y 0.55
UEDIN Y 0.53
RBMT-3 N 0.52
ONLINE-A N 0.51
PROMT N 0.51
RBMT-1 N 0.48
JHU Y 0.44
UK Y 0.40
RBMT-4 N 0.39
ONLINE-C N 0.39
ITS-LATL N 0.36
German-English
1,386?1,567 comparisons/system
System C? >others
ONLINE-A ? N 0.65
ONLINE-B ? N 0.65
QUAERO Y 0.61
RBMT-3 N 0.60
UEDIN ? Y 0.60
RWTH ? Y 0.56
KIT ? Y 0.55
LIMSI Y 0.54
QCRI Y 0.52
RBMT-1 N 0.51
RBMT-4 N 0.50
ONLINE-C N 0.43
DFKI-BERLIN Y 0.40
UK Y 0.37
JHU Y 0.34
UG Y 0.17
English-German
1,777?2,160 comparisons/system
System C? >others
ONLINE-B ? N 0.64
RBMT-3 N 0.63
RBMT-4 ? N 0.58
RBMT-1 N 0.56
LIMSI ? Y 0.55
ONLINE-A N 0.54
UEDIN-WILLIAMS ? Y 0.51
KIT ? Y 0.50
DFKI-HUNSICKER N 0.48
UEDIN ? Y 0.47
RWTH ? Y 0.47
ONLINE-C N 0.47
UK Y 0.45
JHU Y 0.43
DFKI-BERLIN Y 0.25
C? indicates whether system is constrained (unhighlighted rows): trained only using supplied training data, standard
monolingual linguistic tools, and, optionally, LDC?s English Gigaword.
? indicates a win: no other system is statistically significantly better at p-level ? 0.10 in pairwise comparison.
? indicates a constrained win: no other constrained system is statistically better.
Table 4: Official results for the WMT12 translation task. Systems are ordered by their > others score, reflecting how
often their translations won in pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.
17
pairwise comparisons detect statistical significantly
superior quality of either system, and we note this
accordingly.
It is desirable to additionally produce an overall
ranking. In the past evaluation campaigns, we used
two different methods to obtain such a ranking, and
this year we use yet another one. In this section, we
discuss each of these overall ranking methods and a
few more.
4.1 Rank Ranges
In the first human evaluation, we use fluency and
adequacy judgments on a scale from 1 to 5 (Koehn
and Monz, 2006). We normalized the scores on a
per-sentence basis, thus converting them to a rela-
tive ranking in a 5-system comparison. We listed
systems by the average of these scores over all sen-
tences, in which they were judged.
We did not report ranks, but rank ranges. To
give an example: if a system scored neither sta-
tistically significantly better nor statistically signif-
icantly worse than 3 other systems, we assign it the
rank range 1?4. The given evidence is not sufficient
to rank it exactly, but it does rank somewhere in the
top 4.
In subsequent years, we did not continue the re-
porting of rank ranges (although they can be ob-
tained by examining the pairwise comparison ta-
bles), but we continued to report systems as win-
ners whenever there was not statistically signifi-
cantly outperformed by any other system.
4.2 Ratio of Wins and Ties
In the following years (Callison-Burch et al., 2007;
Callison-Burch et al., 2008; Callison-Burch et al.,
2009; Callison-Burch et al., 2010; Callison-Burch et
al., 2011), we abandoned the idea of using fluency
and adequacy judgments, since they showed to be
less reliable than simple ranking of system transla-
tions. We also started to interpret the 5-system com-
parison as a set of pairwise comparisons.
Systems were then ranked by the ratio of how of-
ten they were ranked better or equal to any of the
other systems.
Given a set J of sentence-level judgments
(s1, s2, c) where s1 ? S and s2 ? S are two sys-
tems and
c =
?
??
??
win if s1 better than s2
tie if s1 equal to s2
loss if s1 worse than s2
(1)
then we can count the total number of wins and ties
of a system s as
win(s) = |{(s1, s2, c) ? J : s = s1, c = win}|+
|{(s1, s2, c) ? J : s = s2, c = loss}|
loss(s) = |{(s1, s2, c) ? J : s = s1, c = loss}|+
|{(s1, s2, c) ? J : s = s2, c = win}|
tie(s) = |{(s1, s2, c) ? J : s = s1, c = tie}|+
|{(s1, s2, c) ? J : s = s2, c = tie}|
(2)
and rank systems by the ratio
score(s) =
win(s) + tie(s)
win(s) + loss(s) + tie(s)
(3)
This ratio was used for the official rankings over
the last five years.
4.3 Ratio of Wins (Ignoring Ties)
Bojar et al. (2011) present a persuasive argument
that our ranking scheme is biased towards systems
that are similar to many other systems. Given that
most of the systems are based on phrase-based mod-
els trained on the same training data, this is indeed a
valid concern.
They suggest ignoring ties, and using as ranking
score instead the following ratio:
score(s) =
win(s)
win(s) + loss(s)
(4)
This ratio is used for the official ranking this year.
4.4 Minimizing Pairwise Ranking Violations
Lopez (2012, in this volume) argues against using
aggregate statistics over a set of very diverse judg-
ments. Instead, a ranking that has the least number
of pairwise ranking violations is said to be preferred.
If we define the number of pairwise wins as
win(s1, s2) = |{(s1, s2, c) ? J : c = win}|+
|{(s2, s1, c) ? J : c = loss}|
(5)
then we define a count function for pairwise order
violations as
18
score(s1, s2) = max(0,win(s2, s1)? win(s1, s2))
(6)
Given a bijective ranking function R(s)? i with
the codomain of consecutive integers starting at 1,
the total number of pairwise ranking violations is de-
fined as
score(R) =
?
R(si)<R(sj)
score(si, sj) (7)
Finding the optimal rankingR that minimizes this
score is not trivial, but given the number of systems
involved in this evaluation campaign, it is quite man-
ageable.
4.5 Most Probable Ranking
We now introduce a variant to Lopez?s ranking
method. We motivate it first.
Consider the following scenario:
win(A,B) = 20 win(B,A) = 0
win(B,C) = 40 win(C,B) = 20
win(C,A) = 60 win(A,C) = 40
Since this constitutes a circle, there are three
rankings with the minimum number of 20 violation
(ABC, BCA, CAB).
However, we may want to take the ratio of wins
and losses for each pairwise ranking into account.
Using maximum likelihood estimation, we can de-
fine the probability that system s1 is better than sys-
tem s2 on a randomly drawn sentence as
p(s1 > s2) =
win(s1, s2)
win(s1, s2) + win(s2, s1)
(8)
We can then go on to define5 the probability of a
5Sketch of derivation:
p(s1 > s2 > s3) = p(s1 first)p(s2 second|s1 first)
(chain rule)
p(s1 first) = p(s1 > s2 and s1 > s3)
= p(s1 > s2)p(s1 > s3)
(independence assumption)
p(s2 sec.|s1 first) = p(s2 second)
(independence assumption)
= p(s2 > s3)
ranking of three systems as:
p(s1 > s2 > s3) = p(s1 > s2)p(s1 > s3)p(s2 > s3)
(9)
This function scores the three rankings in the ex-
ample above as follows:
p(A > B > C) = 2020
40
100
40
60 = 0.27
p(B > C > A) = 4060
0
20
60
100 = 0
p(C > A > B) = 60100
20
60
20
20 = 0.20
One disadvantage of this and the previous rank-
ing method is that they do not take advantage of all
available evidence. Consider the example:
win(A,B) = 100 win(B,A) = 0
win(A,C) = 60 win(C,A) = 40
win(B,C) = 50 win(C,B) = 50
Here, system A is clearly ahead, but how about B
and C? They are tied in their pairwise comparison.
So, both ABC and ACB have no pairwise ranking
violations and their most probable ranking score, as
defined above, is the same.
B is clearly worse than A, but C has a fighting
chance, and this should be reflected in the ranking.
The following two overall ranking methods over-
come this problem.
4.6 Monte Carlo Playoffs
The sports world is accustomed to the problem of
finding a ranking of sports teams, but being only able
to have pairwise competitions (think basketball or
football). One strategy is to stage playoffs.
Let?s say there are 4 systems: A,B, C, andD. As
in well-known play-off fashion, they are first seeded.
In our case, this happens randomly, say, 1:A, 2:B,
3:C, 4:D (for simplicity?s sake).
First round: A plays against D, B plays against
C. How do they play? We randomly select a sen-
tence on which they were compared (no ties). If A
is better according to human judgment than D, then
A wins.
Let?s say, A wins against D, and B loses against
C. This leads us to the final A against C and the
3rd place game D against B, in which, say, A and D
win. The resulting final ranking is ACDB.
We repeat this a million times with a different ran-
dom seeding every time, and compute the average
rank, which is then used for overall ranking.
19
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.641: ONLINE-B RBMT-4 RBMT-4 6.16: ONLINE-B 0.640 (1-2): ONLINE-B
2 0.627: RBMT-3 ONLINE-B ONLINE-B 6.39: RBMT-3 0.622 (1-2): RBMT-3
3 0.577: RBMT-4 RBMT-3 RBMT-3 6.98: RBMT-4 0.578 (3-5): RBMT-4
4 0.557: RBMT-1 RBMT-1 RBMT-1 7.32: RBMT-1 0.553 (3-6): RBMT-1
5 0.547: LIMSI ONLINE-A ONLINE-A 7.46: LIMSI 0.543 (3-7): LIMSI
6 0.537: ONLINE-A UEDIN-WILLIAMS LIMSI 7.57: ONLINE-A 0.534 (4-8): ONLINE-A
7 0.509: UEDIN-WILLIAMS LIMSI UEDIN-WILLIAMS 7.87: UEDIN-WILLIAMS 0.511 (5-9): UEDIN-WILLIAMS
8 0.503: KIT KIT KIT 7.98: KIT 0.503 (6-11): KIT
9 0.476: DFKI-HUNSICKER DFKI-HUNSICKER DFKI-HUNSICKER 8.32: UEDIN 0.477 (7-13): UEDIN
10 0.475: UEDIN ONLINE-C ONLINE-C 8.38: DFKI-HUNSICKER 0.472 (8-13): DFKI-HUNSICKER
11 0.470: RWTH UEDIN UEDIN 8.41: ONLINE-C 0.470 (8-13): ONLINE-C
12 0.470: ONLINE-C UK UK 8.44: RWTH 0.468 (8-13): RWTH
13 0.448: UK RWTH RWTH 8.72: UK 0.447 (10-14): UK
14 0.435: JHU JHU JHU 8.87: JHU 0.434 (12-14): JHU
15 0.249: DFKI-BERLIN DFKI-BERLIN DFKI-BERLIN 11.15: DFKI-BERLIN 0.249 (15): DFKI-BERLIN
Table 5: Overall ranking with different methods (English?German)
4.7 Expected Wins
In European national football competitions, each
team plays against each other team, and at the end
the number of wins decides the rankings.6 We can
simulate this type of tournament as well with Monte
Carlo methods. However, in the limit, each team will
be on average ranked based on its expected number
of wins in the competition. We can compute the ex-
pected number of wins straightforward as
score(si) =
1
|S| ? 1
?
j,j 6=i
p(si > sj) (10)
Note that this is very similar to Bojar?s method of
ranking systems, with one additional and important
twist. We can rewrite Equation 4, the variant that
ignores ties, as:
score(si) =
win(si)
win(si)+loss(si)
(11)
=
?
j,j 6=i win(si,sj)?
j,j 6=i win(si,sj)+loss(si,sj)
(12)
This section?s Equation 10 can be rewritten as:
score(si) =
1
|S|
?
j,j 6=i
win(si, sj)
win(si, sj) + loss(si, sj)
(13)
The difference is that the new overall ranking
method normalizes the win ratios per pairwise rank-
ing. And this makes sense, since it overcomes one
6They actually play twice against each other, to balance out
home field advantage, which is not a concern here.
problem with our traditional and Bojar?s ranking
method.
Previously, some systems were put at an dis-
advantage, if they are compared more frequently
against good systems than against bad systems. This
could happen, if participants were not allowed to
rank their own systems (a constraint we enforced
in the past, but no longer). This was noticed by
judges a few years ago, when we had instant re-
porting of rankings during the evaluation period. If
you have one of the best systems and carry out a lot
of human judgments, then competitors? systems will
creep up higher, since they are not compared against
your own (very good) system anymore, but more fre-
quently against bad systems.
4.8 Comparison
Table 5 shows the different rankings for English?
German, a rather typical example. The table dis-
plays the ranking of the systems according to five
different methods, alongside with system scores ac-
cording to the ranking method: the win ratio (Bo-
jar), the average rank (MC Playoffs), and the ex-
pected win ratio (Expected Wins). For the latter, we
performed bootstrap resampling and computed rank
ranges that lie in a 95% confidence interval. You
can find the tables for the other language pairs in the
annex.
The win-based methods (Bojar, MC Playoffs, Ex-
pected Wins) give very similar rankings ? exhibit-
ing mostly just the occasional pairwise flip or for
20
many language pairs the ranking is identical. The
same is true for the two methods based on pairwise
rankings (Lopez, Most Probable). However, the two
types of ranking lead to significantly different out-
comes.
For instance, the win-based methods are pretty
sure that ONLINE-B and RBMT-3 are the two top
performers. Bootstrap resampling of rankings ac-
cording to Expected Wins ranking draws a clear
line between them and the rest. However, Lopez?s
method ranks RBMT-4 first. Why? In direct com-
parison of the three systems, RBMT-4 beats statis-
tically insignificantly ONLINE-B 45% wins against
42% wins and essentially ties with RBMT-3 41%
wins against 41% wins (ONLINE-B beats RBMT-3
49%?35%, p ? 0.01).
We use Bojar?s method as our official method for
ranking in Table 4 and as the human judgments that
we used when calculating how well automatic eval-
uation metrics correlate with human judgments.
4.9 Number of Judgments Needed
In general, there are not enough judgments to rank
systems unambiguously. How many judgments do
we need?
We may extrapolate this number from the num-
ber of judgments we have. Figure 2 provides some
hints. The outlier is Czech?English, for which only
6 systems were submitted and we can separate them
almost completely even at p-level 0.01. For all the
other language pairs, we can only draw for around
40% of the pairwise comparisons conclusions with
that level of statistical significance.
Since the plots also contains the ratio of signifi-
cant conclusions when sub-sampling the number of
judgments, we obtain curves with a clear upward
slope. For English?Czech, for which we were able
to collect much more judgments, we can draw over
60% significant conclusions. The curve for this lan-
guage pair does not look much different than the
other languages, suggesting that doubling the num-
ber of judgments should allow similar levels for
them as well.
5 Metrics Task
In addition to allowing us to analyze the translation
quality of different systems, the data gathered during
p-level 0.01
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
p-level 0.05
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
p-level 0.10
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
Figure 2: Ratio of statistically significant pairwise com-
parisons at different p-levels, based on number of pair-
wise judgments collected.
21
Metric IDs Participant
AMBER National Research Council Canada (Chen et al., 2012)
METEOR CMU (Denkowski and Lavie, 2011)
SAGAN-STS FaMAF, UNC, Argentina (Castillo and Estrella, 2012)
SEMPOS Charles University (Macha?c?ek and Bojar, 2011)
SIMBLEU University of Sheffield (Song and Cohn, 2011)
SPEDE Stanford University (Wang and Manning, 2012)
TERRORCAT University of Zurich, DFKI, Charles U (Fishel et al., 2012)
BLOCKERRCATS, ENXERRCATS, WORD-
BLOCKERRCATS, XENERRCATS, POSF
DFKI (Popovic, 2012)
Table 6: Participants in the metrics task.
the manual evaluation is useful for validating auto-
matic evaluation metrics. Table 6 lists the partici-
pants in this task, along with their metrics.
A total of 12 metrics and their variants were sub-
mitted to the metrics task by 8 research groups. We
provided BLEU and TER scores as baselines. We
asked metrics developers to score the outputs of
the machine translation systems and system com-
binations at the system-level and at the segment-
level. The system-level metrics scores are given in
the Appendix in Tables 29?36. The main goal of
the metrics shared task is not to score the systems,
but instead to validate the use of automatic metrics
by measuring how strongly they correlate with hu-
man judgments. We used the human judgments col-
lected during the manual evaluation for the transla-
tion task and the system combination task to calcu-
late how well metrics correlate at system-level and
at the segment-level.
5.1 System-Level Metric Analysis
We measured the correlation of the automatic met-
rics with the human judgments of translation qual-
ity at the system-level using Spearman?s rank cor-
relation coefficient ?. We converted the raw scores
assigned to each system into ranks. We assigned a
human ranking to the systems based on the percent
of time that their translations were judged to be bet-
ter than the translations of any other system in the
manual evaluation (Equation 4).
When there are no ties, ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
C
S
-E
N
-
6
S
Y
S
T
E
M
S
D
E
-E
N
-
16
S
Y
S
T
E
M
S
E
S
-E
N
-
12
S
Y
S
T
E
M
S
F
R
-E
N
-
15
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
System-level correlation for translations into English
SEMPOS .94 .92 .94 .80 .90
AMBER .83 .79 .97 .85 .86
METEOR .66 .89 .95 .84 .83
TERRORCAT .71 .76 .97 .88 .83
SIMPBLEU .89 .70 .89 .82 .82
TER -.89 -.62 -.92 -.82 .81
BLEU .89 .67 .87 .81 .81
POSF .66 .66 .87 .83 .75
BLOCKERRCATS -.64 -.75 -.88 -.74 .75
WORDBLOCKEC -.66 -.67 -.85 -.77 .74
XENERRCATS -.66 -.64 -.87 -.77 .74
SAGAN-STS .66 n/a .91 n/a n/a
Table 7: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average absolute
value.
22
E
N
-C
Z
-
10
S
Y
S
T
E
M
S
E
N
-D
E
-
22
S
Y
S
T
E
M
S
E
N
-E
S
-
15
S
Y
S
T
E
M
S
E
N
-F
R
-
17
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
System-level correlation for translations out of English
SIMPBLEU .83 .46 .42 .94 .66
BLOCKERRCATS -.65 -.53 -.47 -.93 .64
ENXERRCATS -.74 -.38 -.47 -.93 .63
POSF .80 .54 .37 .69 .60
WORDBLOCKEC -.71 -.37 -.47 -.81 .59
TERRORCAT .65 .48 .58 .53 .56
AMBER .71 .25 .50 .75 .55
TER -.69 -.41 -.45 -.66 .55
METEOR .73 .18 .45 .82 .54
BLEU .80 .22 .40 .71 .53
SEMPOS .52 n/a n/a n/a n/a
Table 8: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average abso-
lute value.
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher absolute value
for ? is making predictions that are more similar to
the human judgments than an automatic evaluation
metric with a lower absolute ?.
The system-level correlations are shown in Ta-
ble 7 for translations into English, and Table 8 out
of English, sorted by average correlation across the
language pairs. The highest correlation for each
language pair and the highest overall average are
bolded. Once again this year, many of the metrics
had stronger correlation with human judgments than
BLEU. The metrics that had the strongest correlation
this year were SEMPOS for the into English direc-
tion and SIMPBLEU for the out of English direc-
tion.
5.2 Segment-Level Metric Analysis
We measured the metrics? segment-level scores with
the human rankings using Kendall?s tau rank corre-
F
R
-E
N
(1
15
94
PA
IR
S
)
D
E
-E
N
(1
19
34
PA
IR
S
)
E
S
-E
N
(9
79
6
PA
IR
S
)
C
S
-E
N
(1
10
21
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations into English
SPEDE07-PP .26 .28 .26 .21 .25
METEOR .25 .27 .25 .21 .25
AMBER .24 .25 .23 .19 .23
SIMPBLEU .19 .17 .19 .13 .17
TERRORCAT .18 .19 .18 .19 .19
XENERRCATS .17 .18 .18 .13 .17
POSF .16 .18 .15 .12 .15
WORDBLOCKEC .15 .16 .17 .13 .15
BLOCKERRCATS .07 .08 .08 .06 .07
SAGAN-STS n/a n/a .21 .20 n/a
Table 9: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average correla-
tion.
E
N
-F
R
(1
15
62
PA
IR
S
)
E
N
-D
E
(1
45
53
PA
IR
S
)
E
N
-E
S
(1
18
34
PA
IR
S
)
E
N
-C
S
(1
88
05
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations out of English
METEOR .26 .18 .21 .16 .20
AMBER .23 .17 .22 .15 .19
TERRORCAT .18 .19 .18 .18 .18
SIMPBLEU .2 .13 .18 .10 .15
ENXERRCATS .20 .11 .17 .09 .14
POSF .15 .13 .15 .13 .14
WORDBLOCKEC .19 .1 .17 .1 .14
BLOCKERRCATS .13 .04 .12 .01 .08
Table 10: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average corre-
lation.
23
lation coefficient. We calculated Kendall?s tau as:
? =
num concordant pairs - num discordant pairs
total pairs
where a concordant pair is a pair of two translations
of the same segment in which the ranks calculated
from the same human ranking task and from the cor-
responding metric scores agree; in a discordant pair,
they disagree. In order to account for accuracy- vs.
error-based metrics correctly, counts of concordant
vs. discordant pairs were calculated specific to these
two metric types. The possible values of ? range
between 1 (where all pairs are concordant) and ?1
(where all pairs are discordant). Thus an automatic
evaluation metric with a higher value for ? is mak-
ing predictions that are more similar to the human
judgments than an automatic evaluation metric with
a lower ? .
We did not include cases where the human rank-
ing was tied for two systems. As the metrics produce
absolute scores, compared to five relative ranks in
the human assessment, it would be potentially un-
fair to the metric to count a slightly different met-
ric score as discordant with a tie in the relative hu-
man rankings. A tie in automatic metric rank for
two translations was counted as discordant with two
corresponding non-tied human judgments.
The correlations are shown in Table 9 for trans-
lations into English, and Table 10 out of English,
sorted by average correlation across the four lan-
guage pairs. The highest correlation for each lan-
guage pair and the highest overall average are
bolded. For the into English direction SPEDE and
METEOR tied for the highest segment-level correla-
tion. METEOR performed the best for the out of En-
glish direction, with AMBER doing admirably well
in both the into- and the out-of-English directions.
6 Quality Estimation task
Quality estimation aims to provide a quality indica-
tor for machine translated sentences at various gran-
ularity levels. It differs from MT evaluation, because
quality estimation techniques do not rely on refer-
ence translations. Instead, quality estimation is gen-
erally addressed using machine learning techniques
to predict quality scores. Potential applications of
quality estimation include:
? Deciding whether a given translation is good
enough for publishing as is
? Informing readers of the target language only
whether or not they can rely on a translation
? Filtering out sentences that are not good
enough even for post-editing by professional
translators
? Selecting the best translation among options
from multiple systems.
This shared-task provides a first common ground
for development and comparison of quality estima-
tion systems, focusing on sentence-level estimation.
It provides training and test datasets, along with
evaluation metrics and a baseline system. The goals
of this shared task are:
? To identify new and effective quality indicators
(features)
? To identify alternative machine learning tech-
niques for the problem
? To test the suitability of the proposed evalua-
tion metrics for quality estimation systems
? To establish the state of the art performance in
the field
? To contrast the performance of regression and
ranking techniques.
The task provides datasets for a single language
pair, text domain and MT system: English-Spanish
news texts produced by a phrase-based SMT sys-
tem (Moses) trained on Europarl and News Com-
mentaries corpora provided in the WMT10 transla-
tion task. As training data, translations were man-
ually annotated for quality in terms of post-editing
effort (1-5 scores) and were provided together with
their source sentences, reference translations, and
post-edited translations (Section 6.1). The shared-
task consisted on automatically producing quality-
estimations for a blind test-set, where English source
sentences and their MT-translations were used as in-
puts. Hidden (and subsequently publicly-released)
manual effort-annotations of those translations (ob-
tained in the same fashion as for the training data)
24
were used as reference labels to evaluate the per-
formance of the participating systems (Section 6.1).
Participants also had full access to the translation
engine-related resources (Section 6.1) and could use
any additional external resources. We have also pro-
vided a software package to extract baseline quality
estimation features (Section 6.3).
Participants could submit up to two systems for
two variations of the task: ranking, where par-
ticipants submit a ranking of translations (no ties
allowed), without necessarily giving any explicit
scores for translations, and scoring, where partici-
pants submit a score for each sentence (in the [1,5]
range). Each of these subtasks is evaluated using
specific metrics (Section 6.2).
6.1 Datasets and resources
Training data
The training data used was selected from data
available from previous WMT shared-tasks for
machine-translation: a subset of the WMT10
English-Spanish test set, and a subset of the WMT09
English-Spanish test set, for a total of 1832 sen-
tences.
The training data consists of the following re-
sources:
? English source sentences
? Spanish machine-translation outputs, created
using the SMT Moses engine
? Effort scores, created by using three profes-
sional post-editors using guidelines describ-
ing Post-Editing (PE) effort from highest effort
(score 1) to lowest effort (score 5)
? Post-Editing output, created by a pool of pro-
fessional post-editors starting from the source
sentences and the Moses translations; these PE
outputs were created before the effort scores
were elicited, and were shown to the PE-effort
judges to facilitate their effort estimates
? Spanish translation outputs, created as part of
the WMT machine-translation shared-task as
reference translations for the English source
sentences (independent of any MT output).
The guidelines used by the PE-effort judges to as-
sign scores 1-5 for each of the ?source, MT-output,
PE-output? triplets are the following:
[1] The MT output is incomprehensible, with lit-
tle or no information transferred accurately. It
cannot be edited, needs to be translated from
scratch.
[2] About 50-70% of the MT output needs to be
edited. It requires a significant editing effort in
order to reach publishable level.
[3] About 25-50% of the MT output needs to be
edited. It contains different errors and mis-
translations that need to be corrected.
[4] About 10-25% of the MT output needs to be
edited. It is generally clear and intelligible.
[5] The MT output is perfectly clear and intelligi-
ble. It is not necessarily a perfect translation,
but requires little or no editing.
Providing reliable effort estimates turned out to
be a difficult task for the PE-effort judges, even in
the current set-up (with post edited outputs available
for consultation). To eliminate some of the noise
from these judgments, we performed an intermedi-
ate cleaning step, in which we eliminated the sen-
tences for which the difference between the max-
imum score and the minimum score assigned be-
tween the three judges was > 1. We started the
data-creation process from a total of 2000 sentences
for the training set, and the final 1832 sentences we
selected as training data were the ones that passed
through this intermediate cleaning step.
Besides score disagreement, we noticed another
trend on the human judgements of PE-effort. Some
judges tend to give more moderate scores (in the
middle of available range), while others like to com-
mit also to scores that are more in the extremes of
the available range. Since the quality estimation task
would be negatively influenced by having most of
the scores in the middle of the range, we have chosen
to compute the final effort scores as an weighted av-
erage between the three PE-effort scores, with more
weight given to the judges with higher standard de-
viation from their own mean score. We have used
25
weights 3, 2, and 1 for the three PE-effort judges ac-
cording to this criterion. There is an additional ad-
vantage resulting from this weighted average score:
instead of obtaining average numbers only at val-
ues x.0, x.33, and x.66 (for unweighted average)7,
the weighted averages are spread more evenly in the
range [1, 5].
A few variations of the training data were pro-
vided, including version with cases restored and a
version detokenized. In addition, engine-internal
information from Moses such as phrase and word
alignments, detailed model scores, etc. (parameter
-trace), n-best lists and stack information from the
search graph as a word graph (parameter -output-
word-graph) as produced by the Moses engine were
provided.
The rationale behind releasing this engine-
internal data was to make it possible for this shared-
task to address quality estimation using a glass-box
approach, that is, making use of information from
the internal workings of the MT engine.
Test data
The test data was a subset of the WMT12 English-
Spanish test set, consisting of 442 sentences. The
test data consists of the following files:
? English source sentences
? Spanish machine-translation outputs, created
using the same SMT Moses engine used to cre-
ate the training data
? Effort scores, created by using three profes-
sional post-editors8 using guidelines describing
PE effort from highest effort (score 1) to lowest
effort (score 5)
The first two files were the input for the quality-
estimation shared-task participating systems. Since
the Moses engine used to create the MT outputs was
the same as the one used for generating the train-
ing data, the engine-internal resources are the same
7These three values are the only ones possible given the
cleaning step we perform prior to averaging the scores, which
ensures that the difference between the maximum score and the
minimum score is at most 1.
8The same post-editors that were used to create the training
data were used to create the test data.
as the ones we released as part of the training data
package.
The effort scores were released after the partic-
ipants submitted their shared-task submission, and
were solely used to evaluate the submissions accord-
ing to the established metrics. The guidelines used
by the PE-effort judges to assign 1-5 scores were the
same as the ones used for creating the training data.
We have used the same criteria to ensure the con-
sistency of the human judgments. The initial set of
candidates consisted of 604 sentences, of which only
442 met this criteria. The final scores used as gold-
values have been obtained using the same weighted-
average scheme as for the training data.
Resources
In addition to the training and test materials, we
made several additional resources that were used for
the baseline QE system and/or the SMT system that
produced the training and test datasets:
? The SMT training corpus: source and target
sides of the corpus used to train the Moses en-
gine. These are a concatenation of the Eu-
roparl and the news-commentary data sets from
WMT10 that were tokenized, cleaned (remov-
ing sentences longer than 80 tokens) and true-
cased.
? Two Language models: 5-gram LM generated
from the interpolation of the two target cor-
pora after tokenization and truecasing (used
by Moses) and a trigram LM generated from
the two source corpora and filtered to remove
singletons (used by the baseline QE system).
We also provided unigram, bigram and trigram
counts (used in the baseline QE system).
? An IBM Model 1 table that generated by
Giza++ using the SMT training corpora.
? A word-alignment file as produced by the
grow-diag-final heuristic in Moses for the SMT
training set.
? A phrase table with word alignment informa-
tion generated from the parallel corpora.
? The Moses configuration file used for decod-
ing.
26
6.2 Evaluation metrics
Ranking metrics
For the ranking task, we defined a novel met-
ric that provides some advantages over a more tra-
ditional ranking metrics like Spearman correlation.
Our metric, called DeltaAvg, assumes that the refer-
ence test set has a number associated with each en-
try that represents its extrinsic value. For instance,
using the effort scale we described in Section 6.1,
we associate a value between 1 and 5 with each
sentence, representing the quality of that sentence.
Given these values, our metric does not need an ex-
plicit reference ranking, the way the Spearman rank-
ing correlation does.9 The goal of the DeltaAvg met-
ric is to measure how valuable a proposed ranking
(which we call a hypothesis ranking) is according to
the extrinsic values associated with the test entries.
We first define a parameterized version of this
metric, called DeltaAvg[n]. The following notations
are used: for a given entry sentence s, V (s) repre-
sents the function that associates an extrinsic value
to that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.10 We also use the notation
Si,j =
?j
k=i Sk. Using these notations, we define:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1
? V (S) (14)
When the valuation function V is clear from the con-
text, we write DeltaAvg[n] for DeltaAvgV [n]. The
parameter n represents the number of quantiles we
want to split the set S into. For instance, n = 2
gives DeltaAvg[2] = V (S1) ? V (S), hence it mea-
sures the difference between the quality of the top
9A reference ranking can be implicitly induced according to
these values; if, as in our case, higher values mean better sen-
tences, then the reference ranking is defined such that higher-
scored sentences rank higher than lower-scored sentences.
10If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
quantile (top half) S1 and the overall quality (rep-
resented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2 ? V (S)))/2, hence it measures an aver-
age difference across two cases: between the quality
of the top quantile (top third) and the overall qual-
ity, and between the quality of the top two quan-
tiles (S1?S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average differ-
ence in quality across n ? 1 cases, with each case
measuring the impact in quality of adding an addi-
tional quantile, from top to bottom. Finally, we de-
fine:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
(15)
whereN = |S|/2. As before, we write DeltaAvg for
DeltaAvgV when the valuation function V is clear
from the context. The DeltaAvg metric is an aver-
age across all DeltaAvg[n] values, for those n values
for which the resulting quantiles have at least 2 en-
tries (no singleton quantiles). The DeltaAvg metric
has some important properties that are desired for a
ranking metric (see Section 6.4 for the results of the
shared-task that substantiate these claims):
? it is non-parametric (i.e., it does not depend on
setting particular parameters)
? it is automatic and deterministic (and therefore
consistent)
? it measures the quality of a hypothesis rank-
ing from an extrinsic perspective (as offered by
function V )
? its values are interpretable: for a given set of
ranked entries, a value DeltaAvg of 0.5 means
that, on average, the difference in quality be-
tween the top-ranked quantiles and the overall
quality is 0.5
? it has a high correlation with the Spearman rank
correlation coefficient, which makes it as use-
ful as the Spearman correlation, with the added
advantage of its values being extrinsically in-
terpretable.
27
In the rest of this paper, we present results for
DeltaAvg using as valuation function V the Post-
Editing effort scores, as defined in Section 6.1.
We also report the results of the ranking task using
the more-traditional Spearman correlation.
Scoring metrics
For the scoring task, we use two metrics that have
been traditionally used for measuring performance
for regression tasks: Mean Absolute Error (MAE) as
a primary metric, and Root of Mean Squared Error
(RMSE) as a secondary metric. For a given test set
S with entries si, 1 ? i ? |S|, we denote by H(si)
the proposed score for entry si (hypothesis), and by
V (si) the reference value for entry si (gold-standard
value). We formally define our metrics as follows:
MAE =
?N
i=1 |H(si)? V (si)|
N
(16)
RMSE =
?
?N
i=1(H(si)? V (si))
2
N
(17)
where N = |S|. Both these metrics are non-
parametric, automatic and deterministic (and there-
fore consistent), and extrinsically interpretable. For
instance, a MAE value of 0.5 means that, on aver-
age, the absolute difference between the hypothe-
sized score and the reference score value is 0.5. The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalizes larger errors more (via
the square function).
6.3 Participants
Eleven teams (listed in Table 11) submitted one or
more systems to the shared task, with most teams
submitting for both ranking and scoring subtasks.
Each team was allowed up to two submissions (for
each subtask). In the descriptions below participa-
tion in the ranking is denoted (R) and scoring is de-
noted (S).
Baseline system (R, S): the baseline system used
the feature extraction software (also provided
to all participants). It analyzed the source and
translation files and the SMT training corpus
to extract the following 17 system-independent
features that were found to be relevant in previ-
ous work (Specia et al., 2009):
? number of tokens in the source and target
sentences
? average source token length
? average number of occurrences of the tar-
get word within the target sentence
? number of punctuation marks in source
and target sentences
? LM probability of source and target sen-
tences using language models described in
Section 6.1
? average number of translations per source
word in the sentence: as given by IBM 1
model thresholded so that P (t|s) > 0.2,
and so that P (t|s) > 0.01 weighted by
the inverse frequency of each word in the
source side of the SMT training corpus
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower fre-
quency words) and 4 (higher frequency
words) in the source side of the SMT train-
ing corpus
? percentage of unigrams in the source sen-
tence seen in the source side of the SMT
training corpus
These features are used to train a Support Vec-
tor Machine (SVM) regression algorithm using
a radial basis function kernel with the LIBSVM
package (Chang and Lin, 2011). The ?,  and C
parameters were optimized using a grid-search
and 5-fold cross validation on the training set.
We note that although the system is referred to
as a ?baseline?, it is in fact a strong system.
Although it is simple it has proved to be ro-
bust across a range of language pairs, MT sys-
tems, and text domains. It is a simpler variant
of the system used in (Specia, 2011). The ratio-
nale behind having such a strong baseline was
to push systems to exploit alternative sources
of information and combination / learning ap-
proaches.
SDLLW (R, S): Both systems use 3 sets of fea-
tures: the 17 baseline features, 8 system-
dependent features from the decoder logs of
Moses, and 20 features developed internally.
Some of these features made use of additional
data and/or resources, such as a secondary
28
ID Participating team
PRHLT-UPV Universitat Politecnica de Valencia, Spain (Gonza?lez-Rubio et al., 2012)
UU Uppsala University, Sweden (Hardmeier et al., 2012)
SDLLW SDL Language Weaver, USA (Soricut et al., 2012)
Loria LORIA Institute, France (Langlois et al., 2012)
UPC Universitat Politecnica de Catalunya, Spain (Pighin et al., 2012)
DFKI DFKI, Germany (Avramidis, 2012)
WLV-SHEF University of Wolverhampton & University of Sheffield, UK (Felice and Specia, 2012)
SJTU Shanghai Jiao Tong University, China (Wu and Zhao, 2012)
DCU-SYMC Dublin City University, Ireland & Symantec, Ireland (Rubino et al., 2012)
UEdin University of Edinburgh, UK (Buck, 2012)
TCD Trinity College Dublin, Ireland (Moreau and Vogel, 2012)
Table 11: Participants in the WMT12 Quality Evaluation shared task.
MT system that was used as pseudo-reference
for the hypothesis, and POS taggers for both
languages. Feature-selection algorithms were
used to select subsets of features that directly
optimize the metrics used in the task. System
?SDLLW M5PbestAvgDelta? uses a resulting
15-feature set optimized towards the AvgDelta
metric. It employs an M5P model to learn a
decision-tree with only two linear equations.
System ?SDLLW SVM? uses a 20-feature set
and an SVM epsilon regression model with ra-
dial basis function kernel with parameters C,
gamma, and epsilon tuned on a development
set (305 training instances). The model was
trained with 10-fold cross validation and the
tuning process was restarted several times us-
ing different starting points and step sizes to
avoid overfitting. The final model was selected
based on its performance on the development
set and the number of support vectors.
UU (R, S): System ?UU best? uses the 17 base-
line features, plus 82 features from Hardmeier
(2011) (with some redundancy and some over-
lap with baseline features), and constituency
trees over input sentences generated by the
Stanford parser and dependency trees over both
input and output sentences generated by the
MaltParser. System ?UU bltk? uses only the
17 baseline features plus constituency and de-
pendency trees as above. The machine learn-
ing component in both cases is SVM regres-
sion (SVMlight software). For the ranking task,
the ranking induced by the regression output
is used. The system uses polynomial kernels
of degree 2 (UU best) and 3 (UU bltk) as well
as two different types of tree kernels for con-
stituency and dependency trees, respectively.
The SVM margin/error trade-off, the mixture
proportion between tree kernels and polyno-
mial kernels and the degree of the polynomial
kernels were optimised using grid search with
5-fold cross-validation over the training set.
TCD (R, S): ?TCD M5P-resources-only? uses
only the baseline features, while ?TCD M5P-
all? uses the baseline and additional features.
A number of metrics (used as features in
TCD M5P-all) were proposed which work in
the following way: given a sentence to eval-
uate (source sentence for complexity or target
sentence for fluency), it is compared against
some reference data using similarity mea-
sures (various metrics which compare distri-
butions of n-grams). The training data was
used as reference, along with the Google n-
grams dataset. Several learning methods were
tested using Weka on the training data (10-
fold cross-validation). The system submission
uses the M5P (regression with decision trees)
algorithm which performed best. Contrary to
what had been observed on the training data
using cross-validation, ?TCD M5P-resources-
only? performs better than ?TCD M5P-all? on
the test data.
29
PRHLT-UPV (R, S): The system addresses the
task using a regression algorithm with 475 fea-
tures, including the 17 the baseline features.
Most of the features are defined as word scores.
Among them, the features obtained form a
smoothed naive Bayes classifier have shown to
be particularly interesting. Different methods
to combine word-level scores into sentence-
level features were investigated. For model
building, SVM regression was used. Given
the large number of features, the training data
provided as part of the task was insufficient
yielding unstable systems with not so good per-
formance. Different feature selection methods
were implemented to determine a subset of rel-
evant features. The final submission used these
relevant features to train an SVM system whose
parameters were optimized with respect to the
final evaluation metrics.
UEDIN (R, S): The system uses the baseline fea-
tures along with some additional features: bi-
nary features for named entities in source using
Stanford NER Tagger; binary indicators for oc-
currence of quotes or parenthetical segments,
words in upper case and numbers; geometric
mean of target word probabilities and proba-
bility of worst scoring word under a Discrim-
inative Word Lexicon Model; Sparse Neural
Network directly mapping from source to tar-
get (using the vector space model) with source
and target side either filtered to relevant words
or hashed to reduce dimensionality; number of
times at least a 3-gram is seen normalized by
sentence length; and Levenshtein distance of
either source or translation to closest entry of
the SMT training corpus on word or character
level. An ensemble of neural networks opti-
mized for RMSE was used for prediction (scor-
ing) and ranking. The contribution of new fea-
tures was tested by adding them to the baseline
features using 5-fold cross-validation. Most
features did not result in any improvement over
the baseline. The final submission was a com-
bination of all feature sets that showed im-
provement.
SJTU (R, S): The task is treated as a regression
problem using the epsilon-SVM method. All
features are extracted from the official data, in-
volving no external NLP tools/resources. Most
of them come from the phrase table, decod-
ing data and SMT training data. The focus
is on special word relations and special phrase
patterns, thus several feature templates on this
topic are extracted. Since the training data is
not large enough to assign weights to all fea-
tures, methods for estimating common strings
or sequences of words are used. The training
data is divided in 3/4 for training and 1/4 for
development to filter ineffective features. Be-
sides the baseline features, the final submission
contains 18 feature templates and about 4 mil-
lion features in total.
WLV-SHEF (R, S): The systems integrates novel
linguistic features from the source and target
texts in an attempt to overcome the limitations
of existing shallow features for quality estima-
tion. These linguistically-informed features in-
clude part-of-speech information, phrase con-
stituency, subject-verb agreement and target
lexicon analysis, which are extracted using
parsers, corpora and auxiliary resources. Sys-
tems are built using epsilon-SVM regression
with parameters optimised using 5-fold cross-
validation on the training set and two differ-
ent feature sets: ?WLV-SHEF BL? uses the 17
baseline features plus 70 linguistically inspired
features, while ?WLV-SHEF FS? uses a larger
set of 70 linguistic plus 77 shallow features (in-
cluding the baseline). Although results indicate
that the models fall slightly below the baseline,
further analysis shows that linguistic informa-
tion is indeed informative and complementary
to shallow indicators.
DFKI (R, S): ?DFKI morphPOSibm1LM? (R) is
a simple linear interpolation of POS 6-gram
language model scores, morpheme 6-gram lan-
guage model scores, IBM 1 scores (both ?di-
rect? and ?inverse?) for POS 4-grams and for
morphemes. The parallel News corpora from
WMT10 is used as extra data to train the lan-
guage model and the IBM 1 model. ?DFKI cfs-
30
plsreg? and ?DFKI grcfs-mars? (S) use a col-
lection of 264 features generated containing
the baseline features and additional resources.
Numerous methods of feature selection were
tested using 10-fold cross validation on the
training data, reducing these to 23 feature sets.
Several regression and (discretized) classifica-
tion algorithms were employed to train predic-
tion models. The best-performing models in-
cluded features derived from PCFG parsing,
language quality checking and LM scoring, of
both source and target, besides features from
the SMT search graph and a few baseline fea-
tures. ?DFKI cfs-plsreg? uses a Best First
correlation-based feature selection technique,
trained with Partial Least Squares Regression,
while ?DFKI grcfs-mars? uses a Greedy Step-
wise correlation-based feature selection tech-
nique, trained with multivariate adaptive re-
gression splines.
DCU-SYMC (R, S): Systems are based on a clas-
sification approach using a set of features that
includes the baseline features. The manually
assigned quality scores provided for each MT
output in the training set were rounded in or-
der to apply classification algorithms on a lim-
ited set of classes (integer values from 1 to 5).
Three classifiers were combined by averaging
the predicted classes: SVM using sequential
minimal optimization and RBF kernel (parame-
ters optimized by grid search), Naive Bayes and
Random Forest. ?DCU-SYMC constrained? is
based on a set of 70 features derived only from
the data provided for the task. These include
a set of features which attempt to model trans-
lation adequacy using a bilingual topic model
built using Latent Dirichlet Allocation. ?DCU-
SYMC unconstrained? is based on 308 fea-
tures including the constrained ones and oth-
ers extracted using external tools: grammatical-
ity features extracted from the source segments
using the TreeTagger part-of-speech tagger, an
English precision grammar, the XLE parser and
the Brown re-ranking parser and features based
on part-of-speech tag counts extracted from the
MT output using a Spanish TreeTagger model.
Loria (S): Several numerical or boolean features
are computed from the source and target sen-
tences and used to train an SVM regression al-
gorithm with linear (?Loria SVMlinear?) and
radial basis function (?Loria SVMrbf?) as ker-
nel. For the radial basis function, a grid search
is performed to optimise the parameter ?. The
official submission use the baseline features
and a number of features proposed in previous
work (Raybaud et al., 2011), amounting to 66
features. A feature selection algorithm is used
in order to remove non-informative features.
No additional data other than that provided for
the shared task is considered. The training data
is split into a training part (1000 sentences) and
a development part (832 sentences) to learn the
regression model and optimise the parameters
of the regression and for feature selection.
UPC (R, S): The systems use several features on
top of the baseline features. These are mostly
based on different language models estimated
on reference and automatic Spanish transla-
tions of the news-v7 corpus. The automatic
translations are generated by the system used
for the shared task. N-gram LMs are esti-
mated on word forms, POS tags, stop words
interleaved by POS tags, stop-word patterns,
plus variants in which the POS tags are re-
placed with the stem or root of each target
word. The POS tags on the target side are ob-
tained by projecting source side annotations via
automatic alignments. The resulting features
are: the perplexity of each additional language
model, according to the two translations, and
the ratio between the two perplexities. Addi-
tionally, features that estimate the likelihood
of the projection of dependency parses on the
two translations are encoded. For learning, lin-
ear SVM regression is used. Optimization was
done via 5-fold cross-validation on a develop-
ment data. Features are encoded by means of
their z-scores, i.e. how many standard devia-
tions the observed value is above or below the
mean. A variant of the system, ?UPC-2? uses
an option of SVMLight that removes inconsis-
tent points from the training set and retrains the
model until convergence.
31
6.4 Results
Here we give the official results for the ranking and
scoring subtasks followed by a discussion that high-
lights the main findings of the task.
Ranking subtask
Table 12 gives the results for the ranking sub-
task. The table is sorted from best to worse using
the DeltaAvg metric scores (Equation 15) as pri-
mary key and the Spearman correlation scores as
secondary key.
The winning submissions for the ranking subtask
are SDLLW?s M5PbestDeltaAvg and SVM entries,
which have DeltaAvg scores of 0.63 and 0.61, re-
spectively. The difference with respect to all the
other submissions is statistically significant at p =
0.05, using pairwise bootstrap resampling (Koehn,
2004). The state-of-the-art baseline system has a
DeltaAvg score of 0.55 (Spearman rank correla-
tion of 0.58). Five other submissions have perfor-
mances that are not different from the baseline at a
statistically-significant level (p = 0.05), as shown
by the gray area in the middle of Table 12. Three
submissions scored higher than the baseline system
at p = 0.05 (systems above the middle gray area),
which indicates that this shared-task succeeded in
pushing the state-of-the-art performance to new lev-
els. The range of performance for the submissions
in the ranking task varies from a DeltaAvg of 0.65
down to a DeltaAvg of 0.15 (with Spearman values
varying from 0.64 down to 0.19).
In addition to the performance of the official sub-
mission, we report here results obtained by var-
ious oracle methods. The oracle methods make
use of various metrics that are associated in a or-
acle manner to the test input: the gold-label Ef-
fort metric for ?Oracle Effort?, the HTER metric
computed against the post-edited translations as ref-
erence for ?Oracle HTER?, and the BLEU metric
computed against the same post-edited translations
as reference for ?Oracle (H)BLEU?.11 The ?Oracle
Effort? DeltaAvg score of 0.95 gives an upperbound
in terms of DeltaAvg for the test set used in this
evaluation. It basically indicates that, for this set,
11We use the (H)BLEU notation to underscore the use of
Post-Edited translations as reference, as opposed to using ref-
erences that are not the product of a Post-Editing process, as for
the traditional BLEU metric.
the difference in PE effort between the top-quality
quantiles and the overall quality is 0.95 on average.
We would like to emphasize here that the DeltaAvg
metric does not have any a-priori range for its values.
The upperbound, for instance, is test-dependent, and
therefore an ?Oracle Effort? score is useful for un-
derstanding the performance level of real system-
submissions. The ?Oracle HTER? DeltaAvg score
of 0.77 is a more realistic upperbound for the cur-
rent set. Since the HTER metric is considered a
good approximation for the effort required in post-
editing, ranking the test set based on the HTER
scores (from lowest HTER to highest HTER) pro-
vides a good oracle comparison point. The oracle
based on (H)BLEU gives a lower DeltaAvg score,
which can be interpreted to mean that the BLEU
metric provides a lower correlation to post-editing
effort compared to HTER. We also note here that
there is room for improvement between the highest-
scoring submission (at DeltaAvg 0.63) and the ?Ora-
cle HTER? DeltaAvg score of 0.77. We are not sure
if this difference can be bridged completely, but hav-
ing measured a quantitative difference between the
current best-performance and a realistic upperbound
is an important achievement of this shared-task.
Scoring subtask
The results for the scoring task are presented in
Table 13, sorted from best to worse by using the
MAE metric scores (Equation 16) as primary key
and the RMSE metric scores (Equation 17) as sec-
ondary key.
The winning submission is SDLLW?s
M5PbestDeltaAvg, with an MAE of 0.61 and
an RMSE of 0.75 (the difference with respect to
all the other submissions is statistically significant
at p = 0.05, using pairwise bootstrap resam-
pling (Koehn, 2004)). The strong, state-of-the-art
quality-estimation baseline system is measured to
have an MAE of 0.69 and RMSE of 0.82, with six
other submissions having performances that are
not different from the baseline at a statistically-
significant level (p = 0.05), as shown by the gray
area in the middle of Table 13). Five submissions
scored higher than the baseline system at p = 0.05
(systems above the middle gray area), which
indicates that this shared-task also succeeded in
pushing the state-of-the-art performance to new
32
System ID DeltaAvg Spearman Corr
? SDLLW M5PbestDeltaAvg 0.63 0.64
? SDLLW SVM 0.61 0.60
UU bltk 0.58 0.61
UU best 0.56 0.62
TCD M5P-resources-only* 0.56 0.56
Baseline (17FFs SVM) 0.55 0.58
PRHLT-UPV 0.55 0.55
UEdin 0.54 0.58
SJTU 0.53 0.53
WLV-SHEF FS 0.51 0.52
WLV-SHEF BL 0.50 0.49
DFKI morphPOSibm1LM 0.46 0.46
DCU-SYMC unconstrained 0.44 0.41
DCU-SYMC constrained 0.43 0.41
TCD M5P-all* 0.42 0.41
UPC 1 0.22 0.26
UPC 2 0.15 0.19
Oracle Effort 0.95 1.00
Oracle HTER 0.77 0.70
Oracle (H)BLEU 0.71 0.62
Table 12: Official results for the ranking subtask of the WMT12 Quality Evaluation shared-task. The winning submis-
sions are indicated by a ? (the difference with respect to other systems is statistically significant with p = 0.05). The
systems in the gray area are not significantly different from the baseline system. Entries with * represent submissions
for which a bug-fix was applied after the submission deadline.
33
System ID MAE RMSE
? SDLLW M5PbestDeltaAvg 0.61 0.75
UU best 0.64 0.79
SDLLW SVM 0.64 0.78
UU bltk 0.64 0.79
Loria SVMlinear 0.68 0.82
UEdin 0.68 0.82
TCD M5P-resources-only* 0.68 0.82
Baseline (17FFs SVM) 0.69 0.82
Loria SVMrbf 0.69 0.83
SJTU 0.69 0.83
WLV-SHEF FS 0.69 0.85
PRHLT-UPV 0.70 0.85
WLV-SHEF BL 0.72 0.86
DCU-SYMC unconstrained 0.75 0.97
DFKI grcfs-mars 0.82 0.98
DFKI cfs-plsreg 0.82 0.99
UPC 1 0.84 1.01
DCU-SYMC constrained 0.86 1.12
UPC 2 0.87 1.04
TCD M5P-all 2.09 2.32
Oracle Effort 0.00 0.00
Oracle HTER (linear mapping into [1.5-5.0]) 0.56 0.73
Oracle (H)BLEU (linear mapping into [1.5-5.0]) 0.61 0.84
Table 13: Official results for the scoring subtask of the WMT12 Quality Evaluation shared-task. The winning submis-
sion is indicated by a ? (the difference with respect to the other submissions is statistically significant at p = 0.05).
The systems in the gray area are not different from the baseline system at a statistically significant level (p = 0.05).
Entries with * represent submissions for which a bug-fix was applied after the submission deadline.
34
levels in terms of absolute scoring. The range of
performance for the submissions in the scoring task
varies from an MAE of 0.61 up to an MAE of 0.87
(the outlier MAE of 2.09 is reportedly due to bugs).
We also calculate scoring Oracles using the meth-
ods used for the ranking Oracles. The difference is
that the HTER and (H)BLEU oracles need a way
of mapping their scores (which are usually in the
[0, 100] range) into the [1, 5] range. For the compar-
ison here, we did the mapping by excluding the 5%
top and bottom outlier scores, and then linearly map-
ping the remaining range into the [1.5, 5] range. The
?Oracle Effort? scores are not very indicative in this
case. However, the ?Oracle HTER? MAE score of
0.56 is a somewhat realistic lowerbound for the cur-
rent set (although the score could be decreased by a
smarter mapping from the HTER range to the Effort
range). We argue that since the HTER metric is con-
sidered a good approximation for the effort required
in post-editing, effort-like scores derived from the
HTER score provide a good way to compute oracle
scores in a deterministic manner. Note that again
the oracle based on (H)BLEU gives a worse MAE
score at 0.61, which support the interpretation that
the (H)BLEU metric provides a lower correlation
to post-editing effort compared to (H)TER. Over-
all, we consider the MAE values for these HTER
and (H)BLEU-based oracles to indicate high error
margins. Most notably the performance of the best
system gets the same MAE score as the (H)BLEU
oracle, at 0.61 MAE. We take this to mean that the
scoring task is more difficult compared to the rank-
ing task, since even oracle-based solutions get high
error scores.
6.5 Discussion
When looking back at the goals that we identified for
this shared-task, most of them have been success-
fully accomplished. In addition, we have achieved
additional ones that were not explicitly stated from
the beginning. In this section, we discuss the accom-
plishments of this shared-task in more detail, start-
ing from the defined goals and beyond.
Identify new and effective quality indicators
The vast majority of the participating systems use
external resources in addition to those provided for
the task, such as parsers, part-of-speech taggers,
named entity recognizers, etc. This has resulted in
a wide variety of features being used. Many of the
novel features have tried to exploit linguistically-
oriented features. While some systems did not
achieve improvements over the baseline while ex-
ploiting such features, others have (the ?UU? sub-
missions, for instance, exploiting both constituency
and dependency trees).
Another significant set of features that has been
previously overlooked is the feature set of the MT
decoder. Considering statistical engines, these fea-
tures are immediately available for quality predic-
tion from the internal trace of the MT decoder (in
a glass-box prediction scenario), and its contribu-
tion is significant. These features, which reflect the
?confidence? of the SMT system on the translations
it produces, have been shown to be complemen-
tary to other, system-independent (black-box) fea-
tures. For example, the ?SDLLW? submissions in-
corporate these features, and their feature selection
strategy consistently favored this feature set. The
power of this set of features alone is enough to yield
(when used with an M5P model) outputs that would
have been placed 4th in the ranking task and 5th
in the scoring task, a remarkable achievement. An-
other interesting feature used by the ?SDLLW? sub-
missions rely on pseudo-references, i.e., translations
produced by other MT systems for the same input
sentence.
Identify alternative machine learning techniques
Although SVM regression was used to compute the
baseline performance, the baseline ?system? pro-
vided for the task consisted solely of a software to
extract features, as opposed to a model built us-
ing the regression algorithm. The rationale behind
this decision was to encourage participants to exper-
iment with alternative methods for combining differ-
ent quality indicators. This was achieved to a large
extent.
The best-performing machine learning techniques
were found to be the M5P Regression Trees and the
SVM Regression (SVR) models. The merit of the
M5P Regression Trees is that it provides compact
models that are less prone to overfitting. In contrast,
the SVR models can easily overfit given the small
amount of training data available and the large num-
bers of features commonly used. Indeed, many of
35
the submissions that fell below the baseline perfor-
mance can blame overfitting for (part of) their sub-
optimal performance. However, SVR models can
achieve high performance through the use of tun-
ing and feature selection techniques to avoid overfit-
ting. Structured learning techniques were success-
fully used by the ?UU? submissions ? the second
best performing team ? to represent parse trees. This
seems an interesting direction to encode other sorts
of linguistic information about source and trans-
lation texts. Other interesting learning techniques
have been tried, such as Neural Networks, Par-
tial Least Squares Regression, or multivariate adap-
tive regression splines, but their performance does
not suggest they are strong candidates for learning
highly-performing quality-estimation models.
Test the suitability of evaluation metrics for qual-
ity estimation DeltaAvg, our proposed metric for
measuring ranking performance, proved suitable for
scoring the ranking subtask. Its high correlation with
the Spearman ranking metric, coupled with its ex-
trinsic interpretability, makes it a preferred choice
for future measurements. It is also versatile, in the
sense that the its valuation function V can change to
reflect different extrinsic measures of quality.
Establish the state of the art performance The
results on both the ranking and the scoring subtasks
established new state of the art levels on the test set
used in this shared task. In addition to these lev-
els, the oracle performance numbers also help under-
stand the current performance level, and how much
of a gap in performance there still exists. Addi-
tional data points regarding quality estimation per-
formance are needed to establish how stable this
measure of the performance gap is.
Contrast the performance of regression and
ranking techniques Most of the submissions in
the ranking task used the results provided by a re-
gression solution (submitted for the scoring task) to
infer the rankings. Also, optimizing for ranking per-
formance via a regression solution seems to result in
regression models that perform very well, as in the
case of the top-ranked submission.
6.6 Quality Estimation Conclusions
There appear to be significant differences between
considering the quality estimation task as a ranking
problem versus a scoring problem. The ranking-
based approach appears to be somewhat simpler
and more easily amenable to automatic solutions,
and at the same time provides immediate benefits
when integrated into larger applications (see, for in-
stance, the post-editing application described in Spe-
cia (2011)). The scoring-based approach is more dif-
ficult, as the high error rate even of oracle-based so-
lutions indicates. It is also well-known from human
evaluations of MT outputs that human judges also
have a difficult time agreeing on absolute-number
judgements to translations.
Our experience in creating the current datasets
confirms that, even with highly-trained profession-
als, it is difficult to arrive at consistent judge-
ments. We plan to have future investigations on
how to achieve more consistent ways of generating
absolute-number scores that reflect the quality of au-
tomated translations.
7 Summary
As in previous incarnations of this workshop we car-
ried out an extensive manual and automatic evalu-
ation of machine translation performance, and we
used the human judgements that we collected to val-
idate automatic metrics of translation quality. This
year was also the debut of a new quality estimation
task, which tries to predict the effort involved in hav-
ing post editors correct MT output. The quality es-
timation task differs from the metrics task in that it
does not involve reference translations.
As in previous years, all data sets generated by
this workshop, including the human judgments, sys-
tem translations and automatic scores, are publicly
available for other researchers to analyze.12
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-0022,
12http://statmt.org/wmt12/results.html
36
the US National Science Foundation under grant
IIS-0713448, and the CoSyne project FP7-ICT-4-
248531 funded by the European Commission. The
views and findings are the authors? alone. Thanks
for Adam Lopez for discussions about alternative
ways of ranking the overall system scores. The
Quality Estimation shared task organizers thank
Wilker Aziz for his help with the SMT models and
resources, and Mariano Felice for his help with the
system for the extraction of baseline features.
References
