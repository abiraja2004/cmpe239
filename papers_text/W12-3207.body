Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 66?75,
Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational Linguistics
Applying Collocation Segmentation to the ACL Anthology Reference Corpus
Vidas Daudaravic?ius
Vytautas Magnus University / Vileikos 8, Lithuania
v.daudaravicius@if.vdu.lt
Abstract
Collocation is a well-known linguistic phe-
nomenon which has a long history of research
and use. In this study I employ collocation
segmentation to extract terms from the large
and complex ACL Anthology Reference Cor-
pus, and also briefly research and describe
the history of the ACL. The results of the
study show that until 1986, the most signifi-
cant terms were related to formal/rule based
methods. Starting in 1987, terms related to
statistical methods became more important.
For instance, language model, similarity mea-
sure, text classification. In 1990, the terms
Penn Treebank, Mutual Information , statis-
tical parsing, bilingual corpus, and depen-
dency tree became the most important, show-
ing that newly released language resources ap-
peared together with many new research areas
in computational linguistics. Although Penn
Treebank was a significant term only tem-
porarily in the early nineties, the corpus is still
used by researchers today. The most recent
significant terms are Bleu score and semantic
role labeling. While machine translation as a
term is significant throughout the ACL ARC
corpus, it is not significant for any particu-
lar time period. This shows that some terms
can be significant globally while remaining in-
significant at a local level.
1 Introduction
Collocation is a well-known linguistic phenomenon
which has a long history of research and use. The
importance of the collocation paradigm shift is
raised in the most recent study on collocations (Sere-
tan, 2011). Collocations are a key issue for tasks like
natural language parsing and generation, as well as
real-life applications such as machine translation, in-
formation extraction and retrieval. Collocation phe-
nomena are simple, but hard to employ in real tasks.
In this study I introduce collocation segmentation as
a language processing method, maintaining simplic-
ity and clarity of use as per the n-gram approach. In
the beginning, I study the usage of the terms collo-
cation and segmentation in the ACL Anthology Ref-
erence Corpus (ARC), as well as other related terms
such as word, multi-word, and n-gram. To evaluate
the ability of collocation segmentation to handle dif-
ferent aspects of collocations, I extract the most sig-
nificant collocation segments in the ACL ARC. In
addition, based on a ranking like that of TF -IDF ,
I extract terms that are related to different phenom-
ena of natural language analysis and processing. The
distribution of these terms in ACL ARC helps to un-
derstand the main breakpoints of different research
areas across the years. On the other hand, there was
no goal to make a thorough study of the methods
used by the ACL ARC, as such a task is complex
and prohibitively extensive.
2 ACL Anthology Reference Corpus
This study uses the ACL ARC version 20090501.
The first step was to clean and preprocess the corpus.
First of all, files that were unsuitable for the analysis
were removed. These were texts containing charac-
ters with no clear word boundaries, i.e., each charac-
ter was separated from the next by whitespace. This
problem is related to the extraction of text from .pdf
66
format files and is hard to solve. Each file in the
ACL ARC represents a single printed page. The file
name encodes the document ID and page number,
e.g., the file name C04-1001 0007.txt is made up of
four parts: C is the publication type, (20)04 is the
year, 1001 is the document ID, and 0007 is the page
number. The next step was to compile files of the
same paper into a single document. Also, headers
and footers that appear on each document page were
removed, though they were not always easily rec-
ognized and, therefore, some of them remained. A
few simple rules were then applied to remove line
breaks, thus keeping each paragraph on a single line.
Finally, documents that were smaller than 1 kB were
also removed. The final corpus comprised 8,581
files with a total of 51,881,537 tokens.
3 Terms in the ACL ARC related to
collocations
The list of terms related to the term collocation
could be prohibitively lengthy and could include
many aspects of what it is and how it is used. For
simplicitys sake, a short list of related terms, includ-
ing word, collocation, multiword, token, unigram,
bigram, trigram, collocation extraction and segmen-
tation, was compiled. Table 2 shows when these
terms were introduced in the ACL ARC: some terms
were introduced early on, others more recently. The
term collocation was introduced nearly 50 years ago
and has been in use ever since. This is not unex-
pected, as collocation phenomena were already be-
ing studied by the ancient Greeks (Seretan, 2011).
Table 2 presents the first use of terms, showing that
the terms segmentation, collocation and multiword
are related to a similar concept of gathering consec-
utive words together into one unit.
Term Count Documents Introduced in
word 218813 7725 1965
segmentation 11458 1413 1965
collocation 6046 786 1965
multiword 1944 650 1969
token 3841 760 1973
trigram 3841 760 1973/87
bigram 5812 995 1988
unigram 2223 507 1989
collocation extraction 214 57 1992
Table 1: Term usage in ACL ARC
While the term collocation has been used for
many years, the first attempt to define what a col-
location is could be related to the time period when
statistics first began to be used in linguistics heavily.
Until that time, collocation was used mostly in the
sense of an expression produced by a particular syn-
tactic rule. The first definition of collocation in ACL
ARC is found in (Cumming, 1986).
(Cumming, 1986): By ?collocation? I mean lex-
ical restrictions (restrictions which are not pre-
dictable from the syntactic or semantic properties of
the items) on the modifiers of an item; for example,
you can say answer the door but not answer the
window. The phenomenon which I?ve called col-
location is of particular interest in the context of a
paper on the lexicon in text generation because this
particular type of idiom is something which a gener-
ator needs to know about, while a parser may not.
It is not the purpose of this paper to provide a def-
inition of the term collocation, because at the mo-
ment there is no definition that everybody would
agree upon. The introduction of unigrams, bigrams
and trigrams in the eighties had a big influence on
the use of collocations in practice. N -grams, as
a substitute to collocations, started being used in-
tensively and in many applications. On the other
hand, n-grams are lacking in generalization capabil-
ities and recent research tends to combine n-grams,
syntax and semantics (Pecina, 2005) .
The following sections introduce collocation seg-
mentation and apply it to extracting the most signif-
icant collocation segments to study the main break-
points of different research areas in the ACL ARC.
4 Collocation Segmentation
The ACL ARC contains many different segmenta-
tion types: discourse segmentation (Levow, 2004),
topic segmentation (Arguello and Rose, 2006), text
segmentation (Li and Yamanishi, 2000), Chinese
text segmentation (Feng et al., 2004), word segmen-
tation (Andrew, 2006). Segmentation is performed
by detecting boundaries, which may also be of sev-
eral different types: syllable boundaries (Mu?ller,
2006), sentence boundaries (Liu et al., 2004), clause
boundaries (Sang and Dejean, 2001), phrase bound-
aries (Bachenko and Fitzpatrick, 1990), prosodic
boundaries (Collier et al., 1993), morpheme bound-
67
Term Source and Citation
word (Culik, 1965) : 3. Translation ?word by word? .
?Of the same simplicity and uniqueness is the decomposition of the sentence S in its
single words w1 , w2 , ..., wk separated by interspaces, so that it is possible to write
s = (w1 w2 ... wk ) like at the text.?
A word is the result of a sentence decomposition.
segmentation (Sakai, 1965): The statement ?x is transformed to y? is a generalization of the original
fact, and this generalization is not always true. The text should be checked before a
transformational rule is applied to it. Some separate steps for this purpose will save
the machine time. (1) A text to be parsed must consist of segments specified by the
rule. The correct segmentation can be done by finding the tree structure of the text.
Therefore, the concatenation rules must be prepared so as to account for the structure
of any acceptable string.
Collocation (Tosh, 1965): We shall include features such as lexical collocation (agent-action
agreement) and transformations of semantic equivalence in a systematic description
of a higher order which presupposes a morpho-syntactic description for each lan-
guage [8, pp. 66-71]. The following analogy might be drawn: just as strings of
alphabetic and other characters are taken as a body of data to be parsed and classified
by a phrase structure grammar, we may regard the string of rule numbers generated
from a phrase structure analysis as a string of symbols to be parsed and classified in a
still higher order grammar [11; 13, pp. 67-83], for which there is as yet no universally
accepted nomenclature.
multi-word (Yang, 1969): When title indices and catalogs, subject indices and catalogs, business
telephone directories, scientific and technical dictionaries, lexicons and idiom-and-
phrase dictionaries, and other descriptive multi-word information are desired, the
first character of each non-trivial word may be selected in the original word sequence
to form a keyword. For example, the rather lengthy title of this paper may have a
keyword as SADSIRS. Several known information systems are named exactly in this
manner such as SIR (Raphael?s Semantic Information Retrieval), SADSAM (Lind-
say?s Sentence Appraiser and Diagrammer and Semantic Analyzing Machine), BIRS
(Vinsonhaler?s Basic Indexing and Retrieval System), and CGC (Klein and Simmons?
Computational Grammar Coder).
token (Beebe, 1973): The type/token ratio is calculated by dividing the number of discrete
entries by the total number of syntagms in the row.
trigram (Knowles, 1973): sort of phoneme triples (trigrams), giving list of clusters and third-
order information-theoretic values.
(D?Orta et al., 1987): Such a model it called trigram language model. It is based
on a very simple idea and, for this reason, its statistics can be built very easily only
counting all the sequences of three consecutive words present in the corpus. On the
other hand, its predictive power is very high.
bigram (van Berkelt and Smedt, 1988): Bigrams are in general too short to contain any
useful identifying information while tetragrams and larger n-gram are already close
to average word length.
(Church and Gale, 1989): Our goal is to develop a methodology for extending an
n-gram model to an (n+l)-gram model. We regard the model for unigrams as com-
pletely fixed before beginning to study bigrams.
unigram the same as bigram for (Church and Gale, 1989)
collocation
extraction
(McKeown et al., 1992): Added syntactic parser to Xtract, a collocation extraction
system, to further filter collocations produced, eliminating those that are not consis-
tently used in the same syntactic relation.
Table 2: Terms introductions in ACL ARC.
aries (Monson et al., 2004), paragraph boundaries
(Filippova and Strube, 2006), word boundaries (Ryt-
ting, 2004), constituent boundaries (Kinyon, 2001),
topic boundaries (Tur et al., 2001).
Collocation segmentation is a new type of seg-
mentation whose goal is to detect fixed word se-
quences and to segment a text into word sequences
called collocation segments. I use the definition of
a sequence in the notion of one or more. Thus, a
collocation segment is a sequence of one or more
consecutive words that collocates and have colloca-
bility relations. A collocation segment can be of any
68
Figure 1: The collocation segmentation of the sentence a collocation is a recurrent and conventional fixed expression
of words that holds syntactic and semantic relations . (Xue et al., 2006).
length (even a single word) and the length is not de-
fined in advance. This definition differs from other
collocation definitions that are usually based on n-
gram lists (Tjong-Kim-Sang and S., 2000; Choueka,
1988; Smadja, 1993). Collocation segmentation is
related to collocation extraction using syntactic rules
(Lin, 1998). The syntax-based approach allows the
extraction of collocations that are easier to describe,
and the process of collocation extraction is well-
controlled. On the other hand, the syntax-based ap-
proach is not easily applied to languages with fewer
resources. Collocation segmentation is based on a
discrete signal of associativity values between two
consecutive words, and boundaries that are used to
chunk a sequence of words.
The main differences of collocation segmentation
from other methods are: (1) collocation segmenta-
tion does not analyze nested collocations it takes
the longest one possible in a given context, while the
n-gram list-based approach cannot detect if a collo-
cation is nested in another one, e.g., machine trans-
lation system; (2) collocation segmentation is able to
process long collocations quickly with the complex-
ity of a bigram list size, while the n-gram list-based
approach is usually limited to 3-word collocations
and has high processing complexity.
There are many word associativity measures,
such as Mutual Information (MI), T-score, Log-
Likelihood, etc. A detailed overview of associativ-
ity measures can be found in (Pecina, 2010), and
any of these measures can be applied to colloca-
tion segmentation. MI and Dice scores are almost
similar in the sense of distribution of values (Dau-
daravicius and Marcinkeviciene, 2004), but the Dice
score is always in the range between 0 and 1, while
the range of the MI score depends on the corpus
size. Thus, the Dice score is preferable. This score
is used, for instance, in the collocation compiler
XTract (Smadja, 1993) and in the lexicon extraction
system Champollion (Smadja et al., 1996). Dice is
defined as follows:
D(xi?1;xi) =
2 ? f(xi?1;xi)
f(xi?1) + f(xi)
where f(xi?1;xi) is the number of co-occurrence
of xi?1 and xi, and f(xi?1) and f(xi) are the num-
bers of occurrence of xi?1 and xi in the training cor-
pus. If xi?1 and xi tend to occur in conjunction,
their Dice score will be high. The Dice score is
sensitive to low-frequency word pairs. If two con-
secutive words are used only once and appear to-
gether, there is a good chance that these two words
are highly related and form some new concept, e.g.,
a proper name. A text is seen as a changing curve of
Dice values between two adjacent words (see Figure
1). This curve of associativity values is used to de-
tect the boundaries of collocation segments, which
can be done using a threshold or by following cer-
tain rules, as described in the following sections.
69
length unique segments segment count word count corpus coverage
1 289,277 31,427,570 31,427,570 60.58%
2 222,252 8,594,745 17,189,490 33.13%
3 72,699 994,393 2,983,179 5.75%
4 12,669 66,552 266,208 0.51%
5 1075 2,839 14,195 0.03%
6 57 141 846 0.00%
7 3 7 49 0.00%
Total 598,032 41,086,247 51,881,537 100%
Table 3: The distribution of collocation segments
2 word segments CTFIDF 3 word segments CTFIDF
machine translation 10777 in terms of 4099
speech recognition 10524 total number of 3926
training data 10401 th international conference 3649
language model 10188 is used to 3614
named entity 9006 one or more 3449
error rate 8280 a set of 3439
test set 8083 note that the 3346
maximum entropy 7570 it is not 3320
sense disambiguation 7546 is that the 3287
training set 7515 associated with the 3211
noun phrase 7509 large number of 3189
our system 7352 there is a 3189
question answering 7346 support vector machines 3111
information retrieval 7338 are used to 3109
the user 7198 extracted from the 3054
word segmentation 7194 with the same 3030
machine learning 7128 so that the 3008
parse tree 6987 for a given 2915
knowledge base 6792 it is a 2909
information extraction 6675 fact that the 2876
4 word segments CTFIDF 5 word segments CTFIDF
if there is a 1690 will not be able to 255
human language technology conference 1174 only if there is a 212
is defined as the 1064 would not be able to 207
is used as the 836 may not be able to 169
human language technology workshop 681 a list of all the 94
could be used to 654 will also be able to 43
has not yet been 514 lexical information from a large 30
may be used to 508 should not be able to 23
so that it can 480 so that it can also 23
our results show that 476 so that it would not 23
would you like to 469 was used for this task 23
as well as an 420 indicate that a sentence is 17
these results show that 388 a list of words or 16
might be able to 379 because it can also be 16
it can also be 346 before or after the predicate 16
have not yet been 327 but it can also be 16
not be able to 323 has not yet been performed 16
are shown in table 320 if the system has a 16
is that it can 311 is defined as an object 16
if there is an 305 is given by an expression 16
Table 4: Top 20 segments for the segment length of two to five words.
70
4.1 Setting segment boundaries with a
Threshold
A boundary can be set between two adjacent words
in a text when the Dice value is lower than a cer-
tain threshold. We use a dynamic threshold which
defines the range between the minimum and the av-
erage associativity values of a sentence. Zero equals
the minimum associativity value and 100 equals the
average value of the sentence. Thus, the threshold
value is expressed as a percentage between the min-
imum and the average associativity values. If the
threshold is set to 0, then no threshold filtering is
used and no collocation segment boundaries are set
using the threshold. The main purpose of using a
threshold is to keep only strongly connected tokens.
On the other hand, it is possible to set the thresh-
old to the maximum value of associativity values.
This would make no words combine into more than
single word segments, i.e., collocation segmentation
would be equal to simple tokenization. In general,
the threshold makes it possible to move from only
single-word segments to whole-sentence segments
by changing the threshold from the minimum to the
maximum value of the sentence. There is no reason
to use the maximum value threshold, but this helps
to understand how the threshold can be used. (Dau-
daravicius and Marcinkeviciene, 2004) uses a global
constant threshold which produces very long collo-
cation segments that are like the clichs used in le-
gal documents and hardly related to collocations. A
dynamic threshold allows the problem of very long
segments to be reduced. In this study I used a thresh-
old level of 50 percent. An example of threshold is
shown in Figure 1. In the example, if the threshold
is 50 percent then segmentation is as follows: a |
collocation | is a | recurrent | and | conventional |
fixed | expression | of words that | holds | syntactic
| and | semantic relations | . To reduce the problem
of long segments even more, the Average Minimum
Law can also be used, as described in the following
section.
4.2 Setting segment boundaries with Average
Minimum Law
(Daudaravicius, 2010) introduces the Average Min-
imum Law (AML) for setting collocation segmen-
tation boundaries. AML is a simple rule which is
applied to three adjacent associativity values and is
expressed as follows:
boundary(xi?2, xi?1) =
=
?
?
?
True
D(xi?3;xi?2) + D(xi?1;xi)
2
< D(xi?2;xi?1)
False otherwise
The boundary between two adjacent words in the
text is set where the Dice value is lower than the av-
erage of the preceding and following Dice values.
In order to apply AML to the first two or last two
words, I use sequence beginning and sequence end-
ing as tokens and calculate the associativity between
the beginning of the sequence and the first word,
and the last word and the end of the sequence as
shown in Figure 1. AML can be used together with
Threshold or alone. The recent study of (Daudar-
avicius, 2012) shows that AML is able to produce
segmentation that gives the best text categorization
results, while the threshold degrades them. On the
other hand, AML can produce collocation segments
where the associativity values between two adjacent
words are very low (see Figure 1). Thus, for lexicon
extraction tasks, it is a good idea to use AML and a
threshold together.
5 Collocation segments from the ACL
ARC
Before the collocation segmentation, the ACL ARC
was preprocessed with lowercasing and tokeniza-
tion. No stop-word lists, taggers or parsers were
used, and all punctuation was kept. Collocation seg-
mentation is done on a separate line basis, i.e., for
each text line, which is usually a paragraph, the av-
erage and the minimum combinability values are de-
termined and the threshold is set at 50 percent, mid-
way between the average and the minimum. The Av-
erage Minimum Law is applied in tandem. The tool
CoSegment for collocation segmentation is available
at (http://textmining.lt/).
Table 3 presents the distribution of segments by
length, i.e., by the number of words. The length
of collocation segments varies from 1 to 7 words.
In the ACL ARC there are 345,455 distinct tokens.
After segmentation, the size of the segment list was
598,032 segments, almost double the length of the
single word list. The length of the bigram list is
71
4,484,358, which is more than 10 times the size of
the word list and 7 times that of the collocation seg-
ment list. About 40 percent of the corpus comprises
collocation segments of two or more words, showing
the amount of fixed language present therein. The
longest collocation segment is described in section
2 . 2 , which contains seven words (when punctu-
ation is included as words). This shows that collo-
cation segmentation with a threshold of 50 percent
and AML diverges to one-, two- or three-word seg-
ments. Despite that, the list size of collocation seg-
ments is much shorter than the list size of bigrams,
and shorter still than that of trigrams.
After segmentation, it was of interest to find the
most significant segments used in the ACL ARC.
For this purpose I used a modified TF-IDF which
is defined as follows:
CTFIDF (x) = TF (x)?ln
(
N ?D(x) + 1
D(x) + 1
)
where TF (x) is the raw frequency of segment x in
the corpus, N is the total number of documents in
the corpus, and D(x) is the number of documents
in which the segment x occurs. Table 4 presents the
top 20 collocation segments for two-, three-, four-
and five-word segments of items that contain alpha-
betic characters only. The term machine transla-
tion is the most significant in CTFIDF terms. This
short list contains many of the main methods and
datasets used in daily computational linguistics re-
search, such as: error rate, test set, maximum en-
tropy, training set, parse tree, unknown words, word
alignment, Penn Treebank, language models, mutual
information, translation model, etc. These terms
show that computational linguistics has its own ter-
minology, methods and tools to research many top-
ics.
Finally, 76 terms of two or more words in length
with the highest CTFIDF values were selected. The
goal was to try to find how significant terms were
used yearly in the ACL ARC. The main part of the
ACL ARC was compiled using papers published af-
ter 1995. Therefore, for each selected term, the av-
erage CTFIDF value of each document for each year
was calculated. This approach allows term usage
throughout the history of the ACL to be analysed,
and reduces the influence of the unbalanced amount
of published papers. Only those terms whose aver-
age CTFIDF in any year was higher than 20 were
kept. For instance, the term machine translation had
to be removed, as it was not significant throughout
all the years. Each term was ranked by the year
in which its average CTFIDF value peaked. The
ranked terms are shown in Table 5. For instance,
the peak of the CTFIDF average of the term sta-
tistical parsing occurred in 1990, of the term lan-
guage model in 1987, and of the term bleu score
in 2006. The results (see Table 5) show the main
research trends and time periods of the ACL com-
munity. Most of the terms with CTFIDF peaks
prior to 1986 are related to formal/rule-based meth-
ods. Beginning in 1987, terms related to statistical
methods become more important. For instance, lan-
guage model, similarity measure, and text classifi-
cation. The year 1990 stands out as a kind of break-
through. In this year, the terms Penn Treebank, Mu-
tual Information, statistical parsing, bilingual cor-
pus, and dependency tree became the most impor-
tant terms, showing that newly released language re-
sources were supporting many new research areas
in computational linguistics. Despite the fact that
Penn Treebank was only significant temporarily, the
corpus is still used by researchers today. The most
recent important terms are Bleu score and semantic
role labeling.
This study shows that collocation segmentation
can help in term extraction from large and complex
corpora, which helps to speed up research and sim-
plify the study of ACL history.
6 Conclusions
This study has shown that collocation segmentation
can help in term extraction from large and complex
corpora, which helps to speed up research and sim-
plify the study of ACL history. The results show that
the most significant terms prior to 1986 are related
to formal/rule based research methods. Beginning in
1987, terms related to statistical methods (e.g., lan-
guage model, similarity measure, text classification)
become more important. In 1990, a major turning
point appears, when the terms Penn Treebank, Mu-
tual Information, statistical parsing, bilingual cor-
pus, and dependency tree become the most impor-
tant, showing that research into new areas of compu-
72
tational linguistics is supported by the publication of
new language resources. The Penn Treebank, which
was only significant temporarily, it still used today.
The most recent terms are Bleu score and semantic
role labeling. While machine translation as a term
is significant throughout the ACL ARC, it is not sig-
nificant in any particular time period. This shows
that some terms can be significant globally, but in-
significant at a local level.
References
