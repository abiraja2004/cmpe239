Proceedings of the 7th Workshop on Statistical Machine Translation, pages 410?421,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Twitter Translation using Translation-Based Cross-Lingual Retrieval
Laura Jehl and Felix Hieber and Stefan Riezler
Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany
{jehl,hieber,riezler}@cl.uni-heidelberg.de
Abstract
Microblogging services such as Twitter have
become popular media for real-time user-
created news reporting. Such communica-
tion often happens in parallel in different lan-
guages, e.g., microblog posts related to the
same events of the Arab spring were written
in Arabic and in English. The goal of this
paper is to exploit this parallelism in order
to eliminate the main bottleneck in automatic
Twitter translation, namely the lack of bilin-
gual sentence pairs for training SMT systems.
We show that translation-based cross-lingual
information retrieval can retrieve microblog
messages across languages that are similar
enough to be used to train a standard phrase-
based SMT pipeline. Our method outper-
forms other approaches to domain adaptation
for SMT such as language model adaptation,
meta-parameter tuning, or self-translation.
1 Introduction
Among the various social media platforms, mi-
croblogging services such as Twitter1 have become
popular communication tools. This is due to the easy
accessibility of microblogging platforms via inter-
net or mobile phones, and due to the need for a fast
mode of communication that microblogging satis-
fies: Twitter messages are short (limited to 140 char-
acters) and simultaneous (due to frequent updates by
prolific microbloggers). Twitter users form a social
network by ?following? the updates of other users,
either reciprocal or one-way. The topics discussed
in Twitter messages range from private chatter to im-
portant real-time witness reports.
1http://twitter.com/
Events such as the Arab spring have shown the
power and also the shortcomings of this new mode
of communication. Microblogging services played a
crucial role in quickly spreading the news about im-
portant events, furthermore they were useful in help-
ing organizers plan their protest. The fact that news
on microblogging platforms is sometimes ahead of
newswire is one of the most interesting facets of
this new medium. However, while Twitter messag-
ing is happening in multiple languages, most net-
works of ?friends? and ?followers? are monolingual
and only about 40% of all messages are in English2.
One solution to sharing news quickly and interna-
tionally was crowdsourcing manual translations, for
example at Meedan3, a nonprofit organization built
to share news and opinion between the Arabic and
English speaking world, by translating articles and
blogs, using machine translation and human expert
corrections.
The goal of our research is to automate this trans-
lation process, with a further aim of providing rapid
crosslingual data access for downstream applica-
tions. The automated translation of microblogging
messages is facing two main problems. First, there
are no bilingual sentence pair data from microblog-
ging domains available. Second, the colloquial, non-
standard language of many microblogging messages
makes it very difficult to adapt a machine translation
system trained on any of the available bilingual re-
sources such as transcriptions from political organi-
zations or news text.
The approach presented in this paper aims to ex-
ploit the fact that microblogging often happens in
2http://semiocast.com/publications/2011_
11_24_Arabic_highest_growth_on_Twitter
3http://news.meedan.net
410
parallel in different languages, e.g., microblog posts
related to the same events of the Arab spring were
published in parallel in Arabic and in English. The
central idea is to crawl a large set of topically related
Arabic and English microblogging messages, and
use Arabic microblog messages as search queries in
a cross-lingual information retrieval (CLIR) setup.
We use the probabilistic translation-based retrieval
technique of Xu et al. (2001) that naturally inte-
grates translation tables for cross-lingual retrieval.
The retrieval results are then used as input to a stan-
dard SMT pipeline to train translation models, start-
ing from unsupervised induction of word alignments
(Och and Ney, 2000) to phrase-extraction (Och and
Ney, 2004) and phrase-based decoding (Koehn et al.,
2007). We investigate several filtering techniques
for retrieval and phrase extraction (Munteanu and
Marcu, 2006; Snover et al., 2008) and find a straight-
forward application of phrase extraction from sym-
metrized alignments to be optimal. Furthermore, we
compare our approach to related domain adaptation
techniques for SMT and find our approach to yield
large improvements over all related techniques.
Finally, a side-product of our research is a cor-
pus of around 1,000 Arabic Twitter messages with
3 manual English translations each, which were cre-
ated using crowdsourcing techniques. This corpus
is used for development and testing in our experi-
ments.
2 Related Work
SMT for user-generated noisy data has been pio-
neered at the 2011 Workshop on Statistical Ma-
chine Translation that featured a translation task of
Haitian Creole emergency SMS messages4. This
task is very similar to the problem of Twitter transla-
tion since SMS contain noisy, abbreviated language.
The research papers related to the featured transla-
tion task deploy several approaches to domain adap-
tation, including crowdsourcing (Hu et al., 2011)
or extraction of parallel sentences from comparable
data (Hewavitharana et al., 2011).
The use of crowdsourcing to evaluate machine
translation and to build development sets was pi-
oneered by Callison-Burch (2009) and Zaidan and
4http://www.statmt.org/wmt11/
featured-translation-task.html
Callison-Burch (2009). Crowdsourcing has its lim-
its when it comes to generating parallel training data
on the scale of millions of parallel sentences. In
our work, we use crowdsourcing via Amazon Me-
chanical Turk5 to create a development and test cor-
pus that includes 3 English translations for each of
around 1,000 Arabic microblog messages.
There is a substantial amount of previous work on
extracting parallel sentences from comparable data
such as newswire text (Fung and Cheung, 2004;
Munteanu and Marcu, 2005; Tillmann and ming Xu,
2009) and on finding parallel phrases in non-parallel
sentences (Munteanu and Marcu, 2006; Quirk et al.,
2007; Cettolo et al., 2010; Vogel and Hewavitha-
rana, 2011). The approach that is closest to our
work is that of Munteanu and Marcu (2006): They
use standard information retrieval together with sim-
ple word-based translation for CLIR, and extract
phrases from the retrieval results using a clean bilin-
gual lexicon and an averaging filter. In this ap-
proach, filtering and cleaning techniques in align-
ment and phrase extraction have to compensate for
low-quality retrieval results. In our approach, the fo-
cus is on high-quality retrieval.
As our experimental results show, the main im-
provement of our technique is a decrease in out-of-
vocabulary (OOV) rate at an increase of the per-
centage of correctly translated unigrams and bi-
grams. Similar work on solving domain adaptation
for SMT by mining unseen words has been pre-
sented by Snover et al. (2008) and Daum? and Ja-
garlamudi (2011). Both approaches show improve-
ments by adding new phrase tables; however, both
approaches rely on techniques that require larger
comparable texts for mining unseen words. Since
in our case documents are very short (they consist
of 140 character sequences), these techniques are
not applicable. However, the advantage of the fact
that microblog messages resemble sentences is that
we can apply standard word- and phrase-alignment
techniques directly to the retrieval results.
Further approaches to domain adaptation for SMT
include adaptation using in-domain language mod-
els (Bertoldi and Federico, 2009), meta-parameter
tuning on in-domain development sets (Koehn and
Schroeder, 2007), or translation model adaptation
5http://www.turk.com
411
using self-translations of in-domain source language
texts (Ueffing et al., 2007). In our experiments we
compare our approach to these domain adaptation
techniques.
3 Cross-Lingual Retrieval via Statistical
Translation
3.1 Retrieval Model
In our approach, comparable candidates for domain
adaptation are selected via cross-lingual retrieval.
In a probabilistic retrieval framework, we estimate
the probability of a relevant document microblog
message D given a query microblog message Q,
P (D|Q). Following Bayes rule, this can be sim-
plified to ranking documents according to the like-
lihood P (Q|D) if we assume a uniform prior over
documents.
score(Q,D) = P (D|Q) = P (D)P (Q|D)P (Q) (1)
Our model is defined as follows:
score(Q,D) = P (Q|D) =
?
q?Q
P (q|D) (2)
P (q|D) = ?Pmix(q|D)
? ?? ?
mixture model
+(1? ?) PML(q|C)
? ?? ?
query collection backoff
(3)
Pmix(q|D) = ?
?
d?D
T (q|d)PML(d|D)
? ?? ?
translation model
(4)
+(1? ?)PML(q|D)
? ?? ?
self-translation
Our retrieval model is related to monolingual re-
trieval models such as the language-modeling ap-
proach of Ponte and Croft (1998) and the monolin-
gual statistical translation approach of Berger and
Lafferty (1999). Xu et al. (2001) extend the former
approaches to the cross-lingual setting by adding a
term translation table. They describe their model in
terms of a Hidden Markov Model with two states
that generate query terms: First, a document state
generates terms d in the document language and then
translates them into a query term q. Second, a back-
off state generates query terms q directly in the query
language. In the document state the probability of
emitting q depends on all d that translate to q, ac-
cording to a translation distribution T . This is esti-
mated by marginalizing out d as
?
d T (q|d)P (d|D).
In the backoff state the probability PML(q|C) of
emitting a query term is estimated as the relative
frequency of this term within a corpus in the query
language. The probability of transitioning into the
document state or the backoff state is given by ? and
1? ?.
We view this model from a smoothing perspective
where the backoff state is linearly interpolated with
the translation probability using a mixture weight
? to control the weighting between both terms.
Furthermore, we expand Xu et al. (2001)?s gen-
erative model to incorporate the concept of ?self-
translation?, introduced by Xue et al. (2008) in a
monolingual question-answering context: Twitter
messages across languages usually share relevant
terms such as hashtags, named entities or user men-
tions. Therefore, we model the event of a query
term literally occurring in the document in a sepa-
rate model that is itself linearly interpolated with a
parameter ? with the translation model.
We implemented the model based on a Lucene6
index, which allows efficient storage of term-
document and document-term vectors. To mini-
mize retrieval time, we consider only those doc-
uments as retrieval candidates where at least one
term translates to a query term, according to the
translation table T . Stopwords were removed for
both queries and documents. Compared to com-
mon inverted index retrieval implementations, our
model is quite slow since the document-term vectors
have to be loaded. However, multi-threading sup-
port and batch retrieval on a Hadoop cluster made
the model tractable. On the upside, the translation-
based model allows greater precision in finding
the candidates for comparable microblog messages
than simpler approaches that use a combination of
tfidf matching and n-best query term expansion:
The translation-based retrieval exploits all possi-
ble alignments between query and document terms
which is particularly important for short documents
such as microblog messages.
3.2 In-Domain Phrase Extraction
To prepare the extraction of phrases from retrieval
results, we conducted cross-lingual retrieval in both
directions: retrieving Arabic documents using En-
glish microblog messages as queries and vice versa.
6http://lucene.apache.org/core/
412
For each run we kept the top N retrieved documents.
Each document was then paired with its query to
generate pseudo-parallel data.
We tried two approaches for using this data to
improve our translations. The first, more restric-
tive method makes use of the word alignments we
obtained from 5.8 million clean parallel training
data from the NIST evaluation campaign. The re-
trieval step generates word-alignments in the direc-
tion D ? Q. After retrieval, the reverse alignment
for each query-document pair is also generated by
using a translation table in the direction Q ? D. An
alignment point between a query term q and a docu-
ment term d is created, iff T (q|d) or T (d|q) exist in
the translation tables D ? Q or Q ? D. Based on
these word-alignments, we extract phrases by apply-
ing the grow-diag-final-and heuristic and using Och
and Ney (2004)?s phrase extraction algorithm as im-
plemented in Moses7 (Koehn et al., 2007). We con-
ducted experiments using different constraints on
the number of alignment points required for a pair
to be considered as well as the value of N . Our first
technique resembles the technique of Munteanu and
Marcu (2006) who also perform phrase extraction
by combining clean alignment lexica for initial sig-
nals with heuristics to smooth alignments for final
fragment extraction.
While we obtained some gains using our heuris-
tics, we are aware that our method is severely re-
stricted in that it only learns new words which are
in the vicinity of known words. We therefore also
tried the bolder approach of treating our data as
parallel and running unsupervised word alignment8
(Och and Ney, 2000) directly on the query-document
pairs to obtain new world alignments and build a
phrase table. In contrast to previous work (Snover
et al., 2008; Daum? and Jagarlamudi, 2011), we can
take advantage of the sentence-like character of mi-
croblog messages and treat queries and retrieval re-
sults similar to sentence aligned data.
For both extraction methods, the standard five
translation features from the new phrase table
(phrase translation probability and lexical weight-
ing in both directions, phrase penalty) were added to
the translation features in Moses. We tried different
7http://statmt.org/moses/
8http://code.google.com/p/giza-pp/
al-Gaddafi, al-Qaddhafi, assad, babrain, bahrain,
egypt, gadaffi, gaddaffi, gaddafi, Gheddafi, homs,
human rights, human-rights, humanrights, libia, li-
bian, libya, libyan, lybia, lybian, lybya, lybyan,
manama, Misrata, nabeelrajab, nato, oman, Pos-
itiveLibyaTweets, Qaddhafi, sirte, syria, tripoli,
tripolis, yemen;
Table 1: Keywords used for Twitter crawl.
modes of combining new and original phrase table,
namely using either one or using the new phrase ta-
ble as backoff in case no phrase translation is found
in the original phrase table.
4 Data
4.1 Twitter Crawl
We crawled Twitter messages from September 20,
2011 until January 23, 2012 via the Streaming API9
in keyword-tracking mode, obtaining 25.5M Twit-
ter messages (tweets) in various languages. Table 1
shows the list of keywords that were chosen to re-
trieve microblog messages related to the events of
the Arab spring.10
In order to separate the microblog message cor-
pus by languages, we applied a Naive Bayes lan-
guage identifier11. This yielded a distribution with
the six most common languages (of 52) being Ara-
bic (57%), English (33%), Somali (2%), Spanish
(2%), Indonesian (1.5%), German (0.7%). We kept
only microblog messages classified as English or
Arabic with confidence greater 0.9. Keyword-based
crawling creates a strong bias towards the domain
of the keywords and it does not guarantee that all
microblog messages regarding a certain topic or re-
gion are retrieved or that all retrieved messages are
related to the Arab Spring and human righs in the
middle east. Additionally, retweets artificially in-
9https://dev.twitter.com/docs/
streaming-api/
10The Twitter Streaming API allows up to 400 tracking key-
words that are matched to uppercase, lowercase and quoted
variations of the keywords. Partial matching such as ?tripolis?
matching ?tripoli? as well as Arabic Unicode characters are not
supported. We extended our keywords over time by analyzing
the crawl, e.g., by introducing spelling variants and hashtags.
11Language Detection Library for Java, by
Shuyo Nakatani (http://code.google.com/p/
language-detection/).
413
Arabic English
tweets + retweets 14,565,513 8,501,788
tweets 6,614,126 5,129,829
avg. retweet/tweet 11.62 7.27
unique users 180,271 865,202
avg. tweets/user 36.6 5.9
Table 2: Twitter corpus statistics
flate the size of the data, although there are no new
terms added. Therefore, we removed all duplicate
retweets that did not introduce additional terms to
the original tweet. Table 2 explains the shrinkage
of the dataset after removing retweets - compared
to English users, a smaller number of Arabic users
produced a much larger number of retweets. Inter-
estingly, 56,087 users tweet a substantial amount in
both languages. This suggests that users spread mes-
sages simultaneously in Arabic and English.
4.2 Creating a Small Parallel Twitter Corpus
using Crowdsourcing
For the evaluation of our method, a small amount
of parallel in-domain data was required. Since there
are no corpora of translated microblog messages, we
decided to use Amazon Mechanical Turk12 to cre-
ate our own evaluation set, following the exploratory
work of Zaidan and Callison-Burch (2011b). We
randomly selected 2,000 Arabic microblog mes-
sages. Hashtags, user mentions and URLs were re-
moved from each microblog message beforehand,
because they do not need to be translated and would
just artificially inflate scores at test time. The mi-
croblog messages were then manually cleaned and
pruned. We discarded messages which contained
almost no text or large portions of other languages
and removed remaining Twitter markup. In the end,
1,022 microblog messages were used in the Me-
chanical Turk task. We split the data into batches
of ten sentences which comprised one HIT (human
intelligence task). Each HIT had to be completed by
three workers. In order to have some control over
translation quality, we inserted one control sentence
per HIT, taken from the LDC-GALE Phase 1 Arabic
Blog Parallel Text. Turkers were rewarded 10 cents
per translation. Following Zaidan and Callison-
Burch (2011b), all Arabic sentences were converted
12http://www.turk.com
into images in order to prevent turkers from past-
ing them into online machine translation engines.
Our final corpus consists of 1,022 translated mi-
croblog messages with three translations each. An
example containing translations for one of the sen-
tences which we inserted for quality checking pur-
poses, along with the reference translation, is given
in table 3. It can be seen that translators sometimes
made grammar mistakes or odd word choices. They
also tended to omit punctuation marks. However,
translations also contained reasonable translation al-
ternatives (such as ?gathered? or ?collected?). We
also asked translators to insert an ?unknown? token
whenever they were unable to translate a word. Our
HIT setup did not allow workers to skip a sentence,
forcing them to complete an entire batch. In order to
account for translation variants we decided to use all
three translations obtained via Mechanical Turk as
multiple references instead of just keeping the top
translation. We randomly split our small parallel
corpus, using half of the microblog messages for de-
velopment and half for testing.
4.3 Preprocessing
Besides removal of Twitter markup, several addi-
tional preprocessing steps such as digit normaliza-
tion were applied to the data. We also decided to ap-
ply the Buckwalter Arabic transliteration scheme13
to avoid encoding difficulties. Habash and Sadat
(2006) have shown that tokenization is helpful for
translating Arabic. We therefore decided to ap-
ply a more involved tokenization scheme than sim-
ple whitespace splitting to our data. As the re-
trieval relies on translation tables, all data need
to be tokenized the same way. We are aware
of the MADA+TOKAN Arabic morphological an-
alyzer and tokenizer (Habash and Rambow, 2005),
however, this toolkit produces very in-depth analy-
ses of the data and thus led to difficulties when we
tried to scale it to millions of sentences/microblog
messages. That is why we only used MADA for
transliteration and chose to implement the simpler
approach by Lee et al. (2003) for tokenization. This
approach only requires a small set of annotated data
to obtain a list of prefixes and suffixes and uses n-
13http://www.qamus.org/transliteration.
htm
414
REFERENCE breaking the silence, a campaign group made up of israeli soldiers, gathered anonymous accounts from 26 soldiers.
TRANSLATION1 and breaking silence is a group of israeli soldiers that had unknown statistics from 26 soldiers israeli
TRANSLATION2 breaking the silence by a group of israeli soldiers who gathered unidentified statistics from 26 israeli soldier.
TRANSLATION3 breaking the silence is a group of israeli soldiers that collected unknown statistics of 26 israeli soldiers
Table 3: Example turker translations.
gram-models to determine the most likely prefix?-
stem-suffix? split of a word.14
5 Twitter Translation Experiments
We conducted a series of experiments to evaluate
our strategy of using CLIR and phrase-extraction to
extract comparable data in the Twitter domain. We
also explored more standard ways of domain adap-
tation such as using English microblog messages to
build an in-domain language model, or generating
synthetic bilingual corpora from monolingual data.
All experiments were conducted using the Moses
machine translation system15 (Koehn et al., 2007)
with standard settings. Language models were
built using the SRILM toolkit16 (Stolcke, 2002).
For all experiments, we report lowercased BLEU-
4 scores (Papineni et al., 2001) as calculated by
Moses? multi-bleu script. For assessing signifi-
cance, we apply the approximate randomization test
(Noreen, 1989; Riezler and Maxwell, 2005). We
consider pairwise differing results scoring a p-value
< 0.05 as significant.
Our baseline model was trained using 5,823,363
million parallel sentences in Modern Standard
Arabic (MSA) (198,500,436 tokens) and English
(193,671,201 tokens) from the NIST evaluation
campaign. This data contains parallel text from dif-
ferent domains, including UN reports, newsgroups,
newswire, broadcast news and weblogs.
5.1 Domain Adaption using Monolingual
Resources
As a first step, we used the available in-domain
data for a combination of domain adaptation tech-
14The n-gram-model required for tokenization was trained on
5.8 million Modern Standard Arabic sentences from the NIST
evaluation campaign. This data had previously been tokenized
with the same method, trained to match the Penn Arabic Tree-
bank, v3.
15http://statmt.org/moses/
16http://www.speech.sri.com/projects/
srilm/
niques similar to Bertoldi and Federico (2009).
There were three different adaptation measures:
First, the turker-generated development set was used
for optimizing the weights of the decoding meta-
parameters, as introduced by Koehn and Schroeder
(2007). Second, the English microblog messages in
our crawl were used to build an in-domain language
model. This adaptation technique was first proposed
by Zhao et al. (2004). Third, the Arabic portion of
our crawl was used to synthetically generate addi-
tional parallel training data. This was accomplished
by machine-translating the Arabic microblog mes-
sages with the best system after performing the first
two adaptation steps. Since decoding is very time-
intensive, only 1 million randomly selected Ara-
bic microblog messages were used to generate syn-
thetic parallel data. This new data was then used
to train another phrase table. Such self-translation
techniques have been introduced by Ueffing et al.
(2007). All results were evaluated against a base-
line of using only NIST data for translation model,
language model and weight optimization.
Our results are shown in table 4. Using an in-
domain development set while leaving everything
else untouched led to an improvement of approxi-
mately 1 BLEU point. Three experiments involv-
ing the Twitter language model confirm Bertoldi
and Federico (2009)?s findings that the language
model was most helpful. The BLEU-score could
be improved by 1.5 to 2 points in all experiments.
When using an in-domain language model, there
was no significant difference between deploying an
in-domain or out-of-domain development set. We
also compared the effect of using only the in-domain
language model to that of adding the in-domain
language model as an extra feature while keeping
the NIST language model.17 There was no signif-
17The weights for both language models were optimized
along with all other translation feature weights, rather than run-
ning an extra optimization step to interpolate between both lan-
guage models, since Koehn and Schroeder (2007) showed that
415
Run Translation Model Language Model Dev Set BLEU %
1 NIST NIST NIST 13.90
2 NIST NIST Twitter 14.83?
3 NIST Twitter NIST 15.98?
4 NIST Twitter Twitter 15.68?
5 NIST Twitter & NIST Twitter 16.04?
6 self-train Twitter & NIST Twitter 15.79?
7 self-train & NIST Twitter & NIST Twitter 15.94?
Table 4: Domain adaptation experiments. Asterisks indicate significant improvements over baseline (1).
Run Twitter Phrases extraction method # sentence pairs # extracted phrases BLEU %
8 top 3 retrieval results heuristics 14,855,985 6,508,141 17.04?
9 top 1 retrieval results GIZA++ 5,141,065 54,260,537 18.73??
10 retrieval intersection GIZA++ 3,452,566 29,091,009 18.85??
11 retrieval intersection as backoff GIZA++ 3,452,566 29,091,009 18.93??
Table 5: CLIR domain adaptation experiments. All weights were optimized on the Twitter dev set and used
the Twitter and NIST language models. One Asterisk indicates a significant improvement over baseline run
(5) from table 4. Two Asterisks indicate a significant improvement over run (8).
icant difference between both runs. However, for
further adaptation experiments we used the system
with the highest absolute BLEU score. In our case,
using synthetically generated data was not help-
ful, yielding similar results as the language model
experiments above. As has been observed before
by Bertoldi and Federico (2009), it did not matter
whether the synthetic data were used on their own
or in addition to the original training data.
5.2 Domain Adaptation using
Translation-based CLIR
Meta-parameters ?, ? ? [0, 1] of the retrieval model
were tuned in a mate-finding experiment: Mate-
finding refers to the task of retrieving the single rel-
evant document for a query. In our case, each source
tweet in the crowdsourced development set had ex-
actly one ?mate?, namely the crowdsourced transla-
tion that was ranked best in a further crowdsourced
ranking task. Using the retrieval model described
in section 3 we achieved precision@1 scores above
95% in finding the translations of a tweet when ?
and ? were set to 0.9. We fixed these parameter set-
tings for all following experiments. The translation
table was taken from the baseline experiments in ta-
ble 4. During retrieval, we kept up to 10 highest
scoring documents per query.
both strategies yielded the same results.
We first employed heuristic phrase extraction
based on the word alignments generated from the
NIST data as described above. To avoid learning
too much noise, maximum phrase length was re-
stricted to 3 (the default is 7). To evaluate the effects
of choosing more restrictive or more lax settings,
we ran experiments varying the following configu-
rations:
1. Constraints on alignment points:
? no constraints,
? 3+ alignment points in each direction,
? 3+ alignment points in both directions,
? 5+ alignment points in both directions.
2. Constraints on retrieval ranking:
? top 10 results,
? top 3 results,
? top 1 results,
? retrieval intersection (results found in both
retrieval directions)
We obtained improvements for all combinations
of these configurations. However, we observed that
requiring 5 common alignment points was too strict,
since few pairs met this constraint. We also noticed
that using only the top 3 retrieval results was benefi-
cial to performance, suggesting that more compara-
ble microblog messages were indeed ranked higher.
416
Using extraction heuristics we gained maximally 1.0
BLEU using the top 3 retrieval results and requiring
at least 3 alignment points in both alignment direc-
tions (see first line in table 5). However, other con-
figurations produced very similar results.
While heuristics led to small incremental im-
provements, we achieved a much larger improve-
ment by training a new phrase table from scratch us-
ing GIZA++. Again, we restricted maximum phrase
length to 3 words. In order to keep phrase table
size manageable, we had to restrict retrieval to top-
1 results or only use retrieval results in the inter-
section of retrieval directions. Best results are ob-
tained when combining phrase tables extracted from
GIZA++ alignments in the intersection of retrieval
results with NIST phrase tables in backoff mode (see
last line in table 5).
6 Error Analysis
Our cross-lingual retrieval approach succeeded in
finding nearly parallel tweets, confirming our hy-
pothesis that such data actually exists. Examples are
given in table 6.
Table 7 shows a more detailed breakdown of our
translation scores. First, standard adaptation meth-
ods increased n-gram precision, suggesting that us-
ing in-domain adaptation data caused the system to
choose more suitable words. As expected, there was
no reduction in OOVs, since using an in-domain
language model and development set does not in-
troduce new vocabulary. Heuristic phrase extrac-
tion again produced small improvements in n-gram
precision while reducing the number of unknown
words. Learning a new phrase table with GIZA++
produced substantial improvements both in OOV-
rate and in n-gram precision.
Nevertheless, even the scores of the adapted sys-
tem are still fairly low and translation quality as
judged by inspection of the output can be very poor.
This suggests that the language used on Twitter still
poses a great challenge, due to its variety of styles
as well as the users? tendency to use non-standard
spelling and colloquial or dialectal expressions. Our
development set contained many different genres,
from Qu?ran verses over news headlines to personal
chatter. Another difficulty was posed by dialectal
Arabic content. To gain an impression of the amount
of dialectal content in our data, we used the Arabic
Online Commentary Dataset created by Zaidan and
Callison-Burch (2011a) to classify our test set. Ta-
ble 8 shows the distribution of dialects in our test
data according to language model probability. This
distribution should be viewed with a grain of salt,
since the shortness of tweets might cause unreliable
results when using a model based on word frequen-
cies for classification. Still, the results suggest that
there is a high proportion of dialectal content and
spelling variation in our data, causing a large num-
ber of OOVs. For example, the preposition ??,
meaning ?in? is often written as Y?. Our phrase
table trained only on standard Arabic data as well as
our extraction heuristic failed to translate this fre-
quently occurring word. Only when retraining a
phrase table with GIZA++ did we translate it cor-
rectly.
Dialect # Sentences
Egyptian 141
Levantine 147
Gulf 78
Modern Standard Arabic 145
Table 8: Dialectal content in our test set as classified
by the AOC dataset.
Table 9 gives examples of translations generated
using different adaptation methods in comparison to
the references and the Google translation service to
illustrate strengths and weaknesses of our approach.
Example 1 shows a case where unknown words were
learned through translation model adaptation. Note
that even the Google translator did not recognize
the word ?ys? which was transliterated as
?Msellat?. Zaidan and Callison-Burch (2011a) point
out that dialectal variants are often transliterated
by Google. Note also, that the unadapted transla-
tion erroneously translated the place name ?sitra? as
?jacket?, a mistake which was also made in two of
the references and by Google. The same happened
to the place name ?wadyan?, which could also be
taken as meaning ?and religions?. This error was
enforced by our preprocessing step incorrectly split-
ting off the prefix ?w? which often carries the mean-
ing ?and?. In addition to that, the two runs which
used translation model adaptation each dropped a
part of the input sentence (?in sitra?, ?firing?). We
417
ARABIC TWEET fO?  Y?  ?yybyl?   w?d?? ??AyF ?? @q?  ?  d??? ?s?rf?  Hy?r?  
 ?  
GOOGLE TRANSLATION AFP confirms that the French President Gaddafi Libyans tried to call and forgiveness
ENGLISH TWEET french president assures that will be taken to court and tells the libyans to forgive each other
ARABIC TWEET Hym?  ??   ? rO? Y? ?wmm?  A?rJ ?ym ??C ? A?E Crq? ?AO?  ?y\n EAh
GOOGLE TRANSLATION NTRA decide to increase the number of all mobile operators in Egypt a commencement from Thursday
ENGLISH TWEET ntra decide to increase the number of all mobile operators in starting from thursday
ARABIC TWEET ?CA? ?lV ??rV ?? r?An? ?w? dm  Yl? ?y?  dyhK? 
GOOGLE TRANSLATION Shahid Amin AA Day January through gunshot
ENGLISH TWEET martyr amin ali ahmed on jan by gunshot
Table 6: Examples of nearly parallel tweets found by our retrieval method.
Adaptation method OOV-rate %/absolute unigram precision %/absolute bigram precision %/absolute output length (words)
None 22.56/2216 51.1/5020 20.2/1882 9832
LM and Dev 20.05/2220 51.4/5442 22.1/2227 10595
Retrieval (heuristic) 17.47/1790 53.5/5484 23.6/2299 10246
Retrieval (GIZA++) 4.22/439 56.1/5834 26.1/2575 10395
Table 7: OOV-rate and precision for different adaptation methods.
attribute this to that fact that the phrase table extrac-
tion often produced one-to-many alignments when
only one alignment point was known. In Example 2
GIZA++ extraction clearly outperformed heuristic
phrase extraction. This example also shows that our
method is good at learning proper names. While
the first two examples resemble news text, Exam-
ple 3 is a more informal message. It is particularly
interesting to note that with GIZA++ extraction the
term ?shabiha? is learned, which is commonly used
in Syria to mean ?thugs? and specifically refers to
armed civilians who assault protesters against Bashir
Al-Assad?s regime. Example 4 also shows substan-
tial OOV reduction. However, the term ? rtns
 r??  (?in Opera Central?, the location of Telecom
Egypt) is incorrectly translated as ?really opera?.
7 Conclusion
We presented an approach to translation of mi-
croblog messages from the Twitter domain. The
main obstacle to state-of-the-art SMT of such data
is the complete lack of sentence-parallel training
data. We presented a technique that uses translation-
based CLIR to find relevant Arabic Twitter messages
given English Twitter queries, and applies a standard
pipeline for unsupervised training of phrase-based
SMT to retrieval results. We found this straight-
forward technique to outperform more conservative
techniques to extract phrases from comparable data
and also to outperform techniques using monolin-
gual resources for language model adaptation, meta-
parameter tuning, or self-translation.
The greatest benefit of our approach is a signifi-
cant reduction of OOV terms at a simultaneous im-
provement of correct unigram and bigram transla-
tions. Despite this positive net effect, we still find
a considerable amount of noise in the automati-
cally extracted phrase tables. Noise reduction by
improved pre-processing and by more sophisticated
training will be subject to future work. Furthermore,
we would like to investigate a tighter integration of
CLIR and SMT training by using forced decoding
techniques for CLIR and by a integrating a feedback
loop into retrieval and training.
Acknowledgments
We would like to thank Julia Ostertag for several it-
erations of manual error analysis of Arabic transla-
tion output.
418
EXAMPLE 1
SRC ?w?d?  ?ys? ?lW? Tlrt? ?A?  ? ?tq 	?K?   w? ?rtF
GOOGLE Riot troops stormed the jacket and religions foot and launches Msellat tears
NO ADAPTATION jacket riot forces storm and religions foot ?ys? ?lW? tears
LM AND DEV sitra and religions of the foot of the riot forces storm ?ys? ?lW? tears
RETRIEVAL (HEURISTIC) in sitra riot police storming and religions of tear gas on foot
RETRIEVAL (GIZA++) the riot police stormed and religions of the foot firing tear gas
REF0 vest riot forces break into wadyan by foot and trough gas tear
REF1 sotra the riot forces enter on foot and shoot tear bombs
REF2 the cover for riot police enters wadian walking and shoot tear bombs
EXAMPLE 2
SRC Yq?w`?  ?tq? ?? ?wy?  dtyF A?A?
GOOGLE Obama will speak today the death of al-Awlaki
NO ADAPTATION dtyF A?A? today killed Yq?w`? 
LM AND DEV dtyF A?A? friday for the killing of Yq?w`? 
RETRIEVAL (HEURISTIC) A?A? today on the killing of
RETRIEVAL (GIZA++) obama today on the al awlaki killing
REF0 obama will talk today about the killing of al - awlaki
REF1 obama is talking today about el awlaqi death
REF2 obama will speak today about the killing of al - awlaqi
EXAMPLE 3
SRC (: ?wy?ts? ?Am ?? TybK? 
GOOGLE Cbihh in Hama are crying :)
NO ADAPTATION TybK?  mired in calling for help : )
LM AND DEV TybK?  in hama calling for help : )
RETRIEVAL (HEURISTIC) inside the protectors of the calling for help : )
RETRIEVAL (GIZA++) shabiha in hama calling for help : )
REF0 the gangsters in hama are asking for help
REF1 the gangs in hamah are peading :)
REF2 the thugs in hama are calling for help :)
EXAMPLE 4
SRC  r??  ? rtns T?r? Y? T?rK?  Hy?C ??zt? ?AO?? T?rOm?A ?wl?A? :: ???r?
GOOGLE Freedom :: Telecom Egypt workers holding company?s president in a room Psontral Opera
NO ADAPTATION : : free workers ?AO?? T?rOm?A holding company chairman  r??  ? rtns Y? chamber
LM AND DEV : : workers free ?AO?? T?rOm?A holding company chairman Y? r??  ? rtns room
RETRIEVAL (HEURISTIC) free : : afcd T?rOm?A hold ceo hostage ppl is the president of the chamber of  r??  ? rtns
RETRIEVAL (GIZA++) egypt : : workers telecom workers are holding the head of the company in the chamber of really opera
REF0 freedom :: workers in the egyptian for communication are holding the company president in a room in the opera central
REF1 freedom , workers in egypt for calls detain the head of the company in a room in opera central
REF2 hurriya :: workers in telecom egypt detaining the president of the company in a room in the opera central
Table 9: Example output using different adaptation methods.
References
