NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 5?6,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Position Paper: Towards Standardized Metrics and Tools 
for Spoken and Multimodal Dialog System Evaluation 
 
 
Sebastian M?ller, Klaus-Peter Engelbrecht, 
Florian Kretzschmar, Stefan Schmidt, Benjamin Weiss 
Quality and Usability Lab, Telekom Innovation Laboratories, TU Berlin 
Ernst-Reuter-Platz 7 
10587 Berlin, Germany 
sebastian.moeller@telekom.de 
 
  
 
Abstract 
We argue that standardized metrics and auto-
matic evaluation tools are necessary for 
speeding up knowledge generation and devel-
opment processes for dialog systems. 
1 Introduction 
The Spoken Dialogue Challenge launched by 
CMU (Black et al., 2011) provides a common plat-
form for dialog researchers in order to test the per-
formance of their systems and components against 
the state-of-the-art. Still, evaluations are individual 
undertakings in most areas, as common metrics 
and procedures which would be applicable for a 
range of systems are sparse. In the following, it is 
argued that significant progress can be made if 
three prerequisites are available: 
? Common metrics for quantifying user and sys-
tem interaction behavior and perceived quality 
? Reliable models for predicting user judgments 
on the basis of automatically-extracted or an-
notated interaction metrics 
? Methods for realistically simulating user be-
havior in response to dialog systems 
The state-of-the-art and necessary research in these 
three areas is outlined in the following paragraphs. 
The Spoken Dialogue Challenge can contribute to 
validating such metrics and models. 
2 Common Metrics  
Whereas early assessment and evaluation cycles 
were based on ad-hoc selected metrics, approaches 
have been made to come up with a standard set of 
metrics for quantifying interactions between users 
and systems which would make evaluation exer-
cises comparable. The International Telecommuni-
cation Union (ITU-T) has standardized two sets of 
metrics: ITU-T Suppl. 24 to P-Series (2005) for 
spoken dialog systems, and ITU-T Suppl. 25 to P-
Series Rec. (2011) for multimodal dialog systems. 
These metrics describe system performance (e.g. in 
terms of error rates) and user/system interaction 
behavior (e.g. in terms of meta-communication 
acts, durations) in a quantitative way, and can thus 
serve as an input to the models discussed below. 
Input is welcome to stabilize these metrics, so that 
they are of more use to researchers and system de-
velopers. The proper conjunction between such 
metrics and standardized annotation schemes (e.g., 
Bunt et al., 2010) will strengthen the establishment 
and spreading of a specific set of metrics. 
When it comes to user-perceived quality, Hone 
and Graham (2000) have made a first attempt to 
come up with a validated questionnaire (SASSI), 
which, however, lacks a scale to assess speech out-
put quality. The approach has been put forward in 
ITU-T Rec. P.851 (2003) by including speech out-
put and dialog managing capabilities. A framework 
structure was preferred over a fixed (and validated) 
questionnaire, in order to more flexibly address the 
needs of researchers and developers. This approach 
still needs to be extended towards multimodal sys-
tems, where modality appropriateness, preference 
and perceived performance have to be considered. 
ITU-T welcomes contributions on this topic. 
5
For practical usage, it is desirable to have evalu-
ation methods which provide diagnostic value to 
the system developer, so that the sources of misbe-
havior can be identified. The diagnosis can be 
based on perceptual dimensions (effectiveness, 
efficiency, mental effort, etc.) or on technical char-
acteristics (error rates, vocabulary coverage, etc.) 
or both. Approaches in this direction are welcome 
and would significantly increase the usefulness of 
evaluation exercises for the system developers. 
3 User-perceived Quality Prediction  
The first approach to predict user judgments on the 
basis of interaction metrics is the well-known 
PARADISE model (Walker et al., 1997). The main 
challenge to date is the low generalizability of such 
models. The reason is that many of the underlying 
input parameters are interdependent, and that a 
simple linear combination does not account for 
more complex relationships (e.g. there might be an 
optimum length for a dialog, which cannot be easi-
ly described by a purely linear model). 
However, other algorithms such as non-linear 
regression, classification trees or Markov models, 
have not shown a significantly improved perfor-
mance (M?ller et al., 2008; Engelbrecht, 2011). 
The latter are however adequate to describe the 
evolution of user opinion during the dialog, and 
thus might have principled advantages over models 
which use aggregated interaction performance met-
rics as an input. 
4 User Behavior Simulation 
During system development, it would be useful to 
anticipate how users would interact with a dialog 
system. Reflected to the system developer, such 
anticipations help to identify usability problems 
already before real users interact with the system. 
Whereas user behavior simulation has frequently 
been used for training statistical dialog managers, 
only few approaches are documented which apply 
them to system evaluation. Early approaches main-
ly selected possible utterances from a set of col-
lected data. The MeMo workbench (Engelbrecht, 
2011) tried to combine statistical selection of prob-
able interaction paths with the knowledge of usa-
bility experts about what typically influences user 
behavior. Such knowledge can also be generated 
by a conversational analysis and categorization 
(Schmidt et al., 2010). 
A different approach has been followed in the 
SpeechEval project (M?ller et al., 2009) where 
statistical dialog managers have been trained on a 
large diverse dataset to generate utterances on a 
conceptual level. The system is then amended with 
ASR and TTS to allow for a speech-based black-
box interaction with telephone-based dialog sys-
tems. Combined with diagnostic quality prediction 
models, such tools can support system developers 
to evaluate different dialog strategies early in the 
design cycle and at low costs, and thus avoid dis-
satisfied users. The approach still has to be extend-
ed towards multimodal dialog systems. 
References  
