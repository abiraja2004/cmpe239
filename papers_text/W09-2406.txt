Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 37?45,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Large-scale Semantic Networks: Annotation and Evaluation
Va?clav Nova?k
Institute of Formal and Applied Linguistics
Charles University in Prague, Czech Republic
novak@ufal.mff.cuni.cz
Sven Hartrumpf
Computer Science Department
University of Hagen, Germany
Sven.Hartrumpf@FernUni-Hagen.de
Keith Hall?
Google Research
Zu?rich, Switzerland
kbhall@google.com
Abstract
We introduce a large-scale semantic-network
annotation effort based on the MutliNet for-
malism. Annotation is achieved via a pro-
cess which incorporates several independent
tools including a MultiNet graph editing tool,
a semantic concept lexicon, a user-editable
knowledge-base for semantic concepts, and a
MultiNet parser. We present an evaluation
metric for these semantic networks, allowing
us to determine the quality of annotations in
terms of inter-annotator agreement. We use
this metric to report the agreement rates for a
pilot annotation effort involving three annota-
tors.
1 Introduction
In this paper we propose an annotation frame-
work which integrates the MultiNet semantic net-
work formalism (Helbig, 2006) and the syntactico-
semantic formalism of the Prague Dependency Tree-
bank (Hajic? et al, 2006) (PDT). The primary goal of
this task is to increase the interoperability of these
two frameworks in order to facilitate efforts to an-
notate at the semantic level while preserving intra-
sentential semantic and syntactic annotations as are
found in the PDT.
The task of annotating text with global semantic
interactions (e.g., semantic interactions within some
discourse) presents a cognitively demanding prob-
lem. As with many other annotation formalisms,
?Part of this work was completed while at the Johns Hop-
kins University Center for Language and Speech Processing in
Baltimore, MD USA.
we propose a technique that builds from cognitively
simpler tasks such as syntactic and semantic anno-
tations at the sentence level including rich morpho-
logical analysis. Rather than constraining the se-
mantic representations to those compatible with the
sentential annotations, our procedure provides the
syntacitco-semantic tree as a reference; the annota-
tors are free to select nodes from this tree to create
nodes in the network. We do not attempt to measure
the influence this procedure has on the types of se-
mantic networks generated. We believe that using a
soft-constraint such as the syntactico-semantic tree,
allows us to better generate human labeled seman-
tic networks with links to the interpretations of the
individual sentence analyses.
In this paper, we present a procedure for com-
puting the annotator agreement rate for MultiNet
graphs. Note that a MultiNet graph does not rep-
resent the same semantics as a syntactico-semantic
dependency tree. The nodes of the MultiNet graph
are connected based on a corpus-wide interpretation
of the entities referred to in the corpus. These global
connections are determined by the intra-sentential
interpretation but are not restricted to that inter-
pretation. Therefore, the procedure for computing
annotator agreement differs from the standard ap-
proaches to evaluating syntactic and semantic de-
pendency treebanks (e.g., dependency link agree-
ment, label agreement, predicate-argument structure
agreement).
As noted in (Bos, 2008), ?Even though the de-
sign of annotation schemes has been initiated for
single semantic phenomena, there exists no anno-
tation scheme (as far as I know) that aims to inte-
37
grate a wide range of semantic phenomena all at
once. It would be welcome to have such a resource
at ones disposal, and ideally a semantic annotation
scheme should be multi-layered, where certain se-
mantic phenomena can be properly analysed or left
simply unanalysed.?
In Section 1 we introduce the theoretical back-
ground of the frameworks on which our annotation
tool is based: MultiNet and the Tectogrammatical
Representation (TR) of the PDT. Section 2 describes
the annotation process in detail, including an intro-
duction to the encyclopedic tools available to the an-
notators. In Section 3 we present an evaluation met-
ric for MultiNet/TR labeled data. We also present an
evaluation of the data we have had annotated using
the proposed procedure. Finally, we conclude with
a short discussion of the problems observed during
the annotation process and suggest improvements as
future work.
1.1 MultiNet
The representation of the Multilayered Extended
Semantic Networks (MultiNet), which is described
in (Helbig, 2006), provides a universal formalism
for the treatment of semantic phenomena of natu-
ral language. To this end, they offer distinct ad-
vantages over the use of the classical predicate
calculus and its derivatives. For example, Multi-
Net provides a rich ontology of semantic-concept
types. This ontology has been constructed to be
language independent. Due to the graphical inter-
pretation of MultiNets, we believe manual anno-
tation and interpretation is simpler and thus more
cognitively compatible. Figure 1 shows the Multi-
Net annotation of a sentence from the WSJ corpus:
?Stephen Akerfeldt, currently vice president fi-
nance, will succeed Mr. McAlpine.?
In this example, there are a few relationships that il-
lustrate the representational power of MultiNet. The
main predicate succeed is a ANTE dependent of the
node now, which indicates that the outcome of the
event described by the predicate occurs at some time
later than the time of the statement (i.e., the succes-
sion is taking place after the current time as captured
by the future tense in the sentence). Intra-sentential
coreference is indicated by the EQU relationship.
From the previous context, we know that the vice
president is related to a particular company, Magna
International Inc. The pragmatically defined rela-
tionship between Magna International Inc. and vice
president finance is captured by the ATTCH (con-
ceptual attachment) relationship. This indicates that
there is some relationship between these entities for
which one is a member of the other (as indicated by
the directed edge). Stephen Akerfeldt is the agent of
the predicate described by this sub-network.
The semantic representation of natural language
expressions by means of MultiNet is generally in-
dependent of the considered language. In contrast,
the syntactic constructs used in different languages
to express the same content are obviously not iden-
tical. To bridge the gap between different languages
we employ the deep syntactico-semantic representa-
tion available in the Functional Generative Descrip-
tion framework (Sgall et al, 1986).
1.2 Prague Dependency Treebank
The Prague Dependency Treebank (PDT) presents a
language resource containing a deep manual analy-
sis of texts(Sgall et al, 2004). The PDT contains
annotations on three layers:
Morphological A rich morphological annotation is
provided when such information is available in
the language. This includes lemmatization and
detailed morphological tagging.
Analytical The analytical layer is a dependency
analysis based purely on the syntactic interpre-
tation.
Tectogrammatical The tectogrammatical annota-
tion provides a deep-syntactic (syntactico-
semantic) analysis of the text. The formal-
ism abstracts away from word-order, function
words (syn-semantic words), and morphologi-
cal variation.
The units of each annotation level are linked with
corresponding units on the preceding level. The
morphological units are linked directly with the
original tokenized text. Linking is possible as most
of these interpretations are directly tied to the words
in the original sentence. In MultiNet graphs, addi-
tional nodes are added and nodes are removed.
The PDT 2.0 is based on the long-standing
Praguian linguistic tradition, adapted for the current
38
Figure 1: MultiNet annotation of sentence ?Stephen Akerfeldt, currently vice president finance, will succeed Mr.
McAlpine.? Nodes C4 and C8 are re-used from previous sentences. Node C2 is an unexpressed (not explicitly stated
in the text) annotator-created node used in previous annotations.
computational-linguistics research needs. The theo-
retical basis of the tectogrammatical representation
lies in the Functional Generative Description of lan-
guage systems (Sgall et al, 1986). Software tools
for corpus search, lexicon retrieval, annotation, and
language analysis are included. Extensive documen-
tation in English is provided as well.
2 Integrated Annotation Process
We propose an integrated annotation procedure
aimed at acquiring high-quality MultiNet semantic
annotations. The procedure is based on a combi-
nation of annotation tools and annotation resources.
We present these components in the this section.
2.1 Annotation Tool
The core annotation is facilitated by the cedit
tool1, which uses PML (Pajas and S?te?pa?nek, 2005),
an XML file format, as its internal representa-
tion (Nova?k, 2007). The annotation tool is an
application with a graphical user interface imple-
mented in Java (Sun Microsystems, Inc., 2007). The
1The cedit annotation tool can be downloaded from
http://ufal.mff.cuni.cz/?novak/files/cedit.zip.
cedit tool is platform independent and directly con-
nected to the annotators? wiki (see Section 2.4),
where annotators can access the definitions of indi-
vidual MultiNet semantic relations, functions and at-
tributes; as well as examples, counterexamples, and
discussion concerning the entity in question. If the
wiki page does not contain the required information,
the annotator is encouraged to edit the page with
his/her questions and comments.
2.2 Online Lexicon
The annotators in the semantic annotation project
have the option to look up examples of MultiNet
structures in an online version of the semantically
oriented computer lexicon HaGenLex (Hartrumpf et
al., 2003). The annotators can use lemmata (instead
of reading IDs formed of the lemma and a numer-
ical suffix) for the query, thus increasing the recall
of related structures. English and German input is
supported with outputs in English and/or German;
there are approximately 3,000 and 25,000 seman-
tic networks, respectively, in the lexicon. An exam-
ple sentence for the German verb ?borgen.1.1? (?to
borrow?) plus its automatically generated and val-
39
Figure 2: HaGenLex entry showing an example sentence
for the German verb ?borgen.1.1? (?to borrow?). The
sentence is literally ?The man borrows himself money
from the friend.?
idated semantic representation is displayed in Fig-
ure 2. The quality of example parses is assured by
comparing the marked-up complements in the ex-
ample to the ones in the semantic network. In the
rare case that the parse is not optimal, it will not be
visible to annotators.
2.3 Online Parser
Sometimes the annotator needs to look up a phrase
or something more general than a particular noun
or verb. In this case, the annotator can use
the workbench for (MultiNet) knowledge bases
(MWR (Gno?rlich, 2000)), which provides conve-
nient and quick access to the parser that translates
German sentences or phrases into MultiNets.
2.4 Wiki Knowledge Base
Awiki (Leuf and Cunningham, 2001) is used collab-
oratively to create and maintain the knowledge base
used by all the annotators. In this project we use
Dokuwiki (Badger, 2007). The entries of individ-
ual annotators in the wiki are logged and a feed of
changes can be observed using an RSS reader. The
cedit annotation tool allows users to display appro-
priate wiki pages of individual relation types, func-
tion types and attributes directly from the tool using
their preferred web browser.
3 Network Evaluation
We present an evaluation which has been carried
out on an initial set of annotations of English arti-
cles from The Wall Street Journal (covering those
annotated at the syntactic level in the Penn Tree-
bank (Marcus et al, 1993)). We use the annotation
from the Prague Czech-English Dependency Tree-
bank (Cur???n et al, 2004), which contains a large por-
tion of the WSJ Treebank annotated according to the
PDT annotation scheme (including all layers of the
FGD formalism).
We reserved a small set of data to be used to train
our annotators and have excluded these articles from
the evaluation. Three native English-speaking anno-
tators were trained and then asked to annotate sen-
tences from the corpus. We have a sample of 67
sentences (1793 words) annotated by two of the an-
notators; of those, 46 sentences (1236 words) were
annotated by three annotators.2 Agreement is mea-
sured for each individual sentences in two steps.
First, the best match between the two annotators?
graphs is found and then the F-measure is computed.
In order to determine the optimal graph match be-
tween two graphs, we make use of the fact that
the annotators have the tectogrammatical tree from
which they can select nodes as concepts in theMulti-
Net graph. Many of the nodes in the annotated
graphs remain linked to the tectogrammatical tree,
therefore we have a unique identifier for these nodes.
When matching the nodes of two different annota-
tions, we assume a node represents an identical con-
cept if both annotators linked the node to the same
tectogrammatical node. For the remaining nodes,
we consider all possible one-to-one mappings and
construct the optimal mapping with respect to the F-
measure.
Formally, we start with a set of tectogrammatical
trees containing a set of nodes N . The annotation is
a tuple G = (V,E, T,A), where V are the vertices,
E ? V ? V ?P are the directed edges and their la-
bels (e.g., agent of an action: AGT ? P ), T ? V ?N
is the mapping from vertices to the tectogrammati-
cal nodes, and finally A are attributes of the nodes,
which we ignore in this initial evaluation.3 Analo-
gously, G? = (V ?, E?, T ?, A?) is another annotation
2The data associated with this experiment can be down-
loaded from http://ufal.mff.cuni.cz/?novak/files/data.zip. The
data is in cedit format and can be viewed using the cedit editor
at http://ufal.mff.cuni.cz/?novak/files/cedit.zip.
3We simplified the problem also by ignoring the mapping
from edges to tectogrammatical nodes and the MultiNet edge
attribute knowledge type.
40
of the same sentence and our goal is to measure the
similarity s(G,G?) ? [0, 1] of G and G?.
To measure the similarity we need a set ? of ad-
missible one-to-one mappings between vertices in
the two annotations. A mapping is admissible if
it connects vertices which are indicated by the an-
notators as representing the same tectogrammatical
node:
? =
{
? ? V ? V ?
??? (1)
?
n?N
v?V
v??V ?
((
(v,n)?T?(v?,n)?T ?
)
?(v,v?)??
)
? ?v?V
v?,w??V ?
((
(v,v?)???(v,w?)??
)
?(v?=w?)
)
? ?v,w?V
v??V ?
((
(v,v?)???(w,v?)??
)
?(v=w)
)}
In Equation 1, the first condition ensures that ? is
constrained by the mapping induced by the links to
the tectogrammatical layer. The remaining two con-
ditions guarantee that ? is a one-to-one mapping.
We define the annotation agreement s as:
sF (G,G?) = max??? (F (G,G
?, ?))
where F is the F1-measure:
Fm(G,G?, ?) = 2 ?m(?)|E|+ |E?|
wherem(?) is the number of edges that match given
the mapping ?.
We use four versions of m, which gives us four
versions of F and consequently four scores s for ev-
ery sentence:
Directed unlabeled: mdu(?) =?????
{
(v,w,?)?E
????v?,w??V ?,???P
((
v?, w?, ??
)
? E?
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
Undirected unlabeled: muu(?) =?????
{
(v,w,?)?E
????v?,w??V ?,???P
(
((v?, w?, ??) ? E? ? (w?, v?, ??) ? E?)
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
Directed labeled: mdl(?) =
?????
{
(v,w,?)?E
????v?,w??V ?
((
v?, w?, ?
)
? E?
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
Undirected labeled: mul(?) =
?????
{
(v,w,?)?E
????v?,w??V ?
(
((v?, w?, ?) ? E? ? (w?, v?, ?) ? E?)
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
These four m(?) functions give us four possible
Fm measures, which allows us to have four scores
for every sentence: sdu, suu, sdl and sul.
Figure 3 shows that the inter-annotator agreement
is not significantly correlated with the position of the
sentence in the annotation process. This suggests
that the annotations for each annotator had achieved
a stable point (primarily due to the annotator training
process).
10 20 30 40 50
0.2
0.4
0.6
0.8
1.0
Sentence length
Inte
r?a
nno
tato
r F?
mea
sure
 ? U
ndir
ecte
d U
nlab
eled
Annotators
CB?CWSM?CWSM?CB
Figure 4: Inter-annotator agreement depending on the
sentence length. Each point represents a sentence.
Figure 4 shows that the agreement is not corre-
lated with the sentence length. It means that longer
41
0 10 20 30 40
0.2
0.4
0.6
0.8
1.0
Index
Und
irec
ted
 Un
labe
led 
F?m
eas
ure
Annotators
CB?CWSM?CWSM?CB
0 10 20 30 40
0.0
0.2
0.4
0.6
Index
Und
irec
ted
 La
bele
d F
?m
eas
ure
Annotators
CB?CWSM?CWSM?CB
Figure 3: Inter-annotator agreement over time. Left: unlabeled, right: labeled. Each point represents a sentence; CB,
CW, and SM are the annotators? IDs.
sentences are not more difficult than short sentences.
The variance decreases with the sentence length as
expected.
In Figure 5 we show the comparison of directed
and labeled evaluations with the undirected unla-
beled case. By definition the undirected unlabeled
score is the upper bound for all the other scores.
The directed score is well correlated and not very
different from the undirected score, indicating that
the annotators did not have much trouble with de-
termining the correct direction of the edges. This
might be, in part, due to support from the formal-
ism and its tool cedit: each relation type is speci-
fied by a semantic-concept type signature; a relation
that violates its signature is reported immediately to
the annotator. On the other hand, labeled score is
significantly lower than the unlabeled score, which
suggests that the annotators have difficulties in as-
signing the correct relation types. The correlation
coefficient between suu and sul (approx. 0.75) is
also much lower than than the correlation coefficient
between suu and sdu (approx. 0.95).
Figure 6 compares individual annotator pairs. The
scores are similar to each other and also have a sim-
ilar distribution shape.
Undirected Unlabeled F?measure
Den
sity
0.0
0.5
1.0
1.5
2.0
2.5
0.2 0.4 0.6 0.8 1.0
CB ? CW 0.0
0.5
1.0
1.5
2.0
2.5
SM ? CB0.0
0.5
1.0
1.5
2.0
2.5
SM ? CW
Figure 6: Comparison of individual annotator pairs.
A more detailed comparison of individual anno-
tator pairs is depicted in Figure 7. The graph shows
that there is a significant positive correlation be-
tween scores, i.e. if two annotators can agree on the
42
0.2 0.4 0.6 0.8 1.0
0.2
0.4
0.6
0.8
Undirected Unlabeled F?measure
Dire
cte
d U
nlab
eled
 F?
me
asu
re
Annotators
CB?CWSM?CWSM?CB
0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
Undirected Unlabeled F?measure
Und
irec
ted
 La
bele
d F
?m
eas
ure
Annotators
CB?CWSM?CWSM?CB
Figure 5: Left: Directed vs. undirected inter-annotator agreement. Right: Labeled vs. unlabeled inter-annotator agree-
ment. Each point represents a sentence.
annotation, the third is likely to also agree, but this
correlation is not a very strong one. The actual cor-
relation coefficients are shown under the main diag-
onal of the matrix.
Sample Annotators Agreement F-measure
suu sdu sul sdl
Smaller CB-CW 61.0 56.3 37.1 35.0
Smaller SM-CB 54.9 48.5 27.1 25.7
Smaller SM-CW 58.5 50.7 31.3 30.2
Smaller average 58.1 51.8 31.8 30.3
Larger CB-CW 64.6 59.8 40.1 38.5
Table 1: Inter-annotator agreement in percents. The re-
sults come from the two samples described in the first
paragraph of Section 3.
Finally, we summarize the raw result in Table 1.
Note that we report simple annotator agreement
here.
4 Conclusion and Future Work
We have presented a novel framework for the anno-
tation of semantic network for natural language dis-
course. Additionally we present a technique to eval-
uate the agreement between the semantic networks
annotated by different annotators.
Our evaluation of an initial dataset reveals that
given the current tools and annotation guidelines, the
annotators are able to construct the structure of the
semantic network (i.e., they are good at building the
directed graph). They are not, however, able to con-
sistently label the semantic relations between the se-
mantic nodes. In our future work, we will investigate
the difficulty in labeling semantic annotations. We
would like to determine whether this is a product of
the annotation guidelines, the tool, or the formalism.
Our ongoing research include the annotation of
inter-sentential coreference relationships between
the semantic concepts within the sentence-based
graphs. These relationships link the local structures,
allowing for a complete semantic interpretation of
the discourse. Given the current level of consistency
in structural annotation, we believe the data will be
useful in this analysis.
43
CB_CW
0.2 0.4 0.6 0.8
ll
l
l
l
l
l
ll
l
l
l
l
l l
l l
l
l
l
l
l
l
l l
l
l l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
0.2
0.4
0.6
0.8
1.0
ll
l
l
l
l
l
ll
l
l
l
l
ll
l l
l
l
l
l
l
l
l l
l
ll
l
l
ll
l
l
ll
l
l
l
l
l
l
l
l
l
l
0.2
0.4
0.6
0.8
0.34 SM_CW
ll
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
0.2 0.4 0.6 0.8 1.0
0.55 0.56
0.2 0.4 0.6 0.8
0.2
0.4
0.6
0.8
SM_CB
Undirected Unlabeled F?measure with Correlation Coefficients
Figure 7: Undirected, unlabeled F-measure correlation of annotator pairs. Each cell represents two different pairs of
annotators; cells with graphs show scatter-plots of F-scores for the annotator pairs along with the optimal linear fit;
cells with values show the correlation coefficient (each point in the plot corresponds to a sentence). For example,
the top row, right-most column, we are comparing the F-score agreement of annotators CB and CW with that of the
F-score agreement of annotators SM and CB. This should help identify an outlier in the consistency of the annotations.
Acknowledgment
This work was partially supported by Czech
Academy of Science grants 1ET201120505 and
1ET101120503; by Czech Ministry of Educa-
tion, Youth and Sports projects LC536 and
MSM0021620838; and by the US National Science
Foundation under grant OISE?0530118. The views
expressed are not necessarily endorsed by the spon-
sors.
References
Mike Badger. 2007. Dokuwiki ? A Practical Open
Source Knowledge Base Solution. Enterprise Open
Source Magazine.
Johan Bos. 2008. Let?s not Argue about Semantics. In
European Language Resources Association (ELRA),
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, may.
Jan Cur???n, Martin C?mejrek, Jir??? Havelka, and Vladislav
Kubon?. 2004. Building parallel bilingual syntacti-
cally annotated corpus. In Proceedings of The First
International Joint Conference on Natural Language
Processing, pages 141?146, Hainan Island, China.
Carsten Gno?rlich. 2000. MultiNet/WR: A Knowledge
Engineering Toolkit for Natural Language Informa-
tion. Technical Report 278, University Hagen, Hagen,
Germany.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr Sgall,
Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, and Marie
Mikulova?. 2006. Prague Dependency Treebank 2.0.
44
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia, Pennsylvania.
Sven Hartrumpf, Hermann Helbig, and Rainer Osswald.
2003. The Semantically Based Computer Lexicon Ha-
GenLex ? Structure and Technological Environment.
Traitement Automatique des Langues, 44(2):81?105.
Hermann Helbig. 2006. Knowledge Representation and
the Semantics of Natural Language. Springer, Berlin,
Germany.
Bo Leuf and Ward Cunningham. 2001. The Wiki Way.
Quick Collaboration on the Web. Addison-Wesley,
Reading, Massachusetts.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313?330.
Va?clav Nova?k. 2007. Cedit ? semantic networks man-
ual annotation tool. In Proceedings of Human Lan-
guage Technologies: The Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL-HLT), pages 11?12,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Petr Pajas and Jan S?te?pa?nek. 2005. A Generic XML-
Based Format for Structured Linguistic Annotation
and Its Application to Prague Dependency Treebank
2.0. Technical Report 29, UFAL MFF UK, Praha,
Czech Republic.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. D. Reidel, Dordrecht, The Netherlands.
Petr Sgall, Jarmila Panevova?, and Eva Hajic?ova?. 2004.
Deep syntactic annotation: Tectogrammatical repre-
sentation and beyond. In Adam Meyers, editor, Pro-
ceedings of the HLT-NAACL 2004 Workshop: Fron-
tiers in Corpus Annotation, pages 32?38, Boston,
Massachusetts, May. Association for Computational
Linguistics.
Sun Microsystems, Inc. 2007. Java Platform, Standard
Edition 6. http://java.sun.com/javase/6/webnotes/
README.html.
45
