16 
1965 International Conference on Computational Linguistics 
SETS OF GRAMMARS BETWEEN CONTEXT-FREE 
AND CONTEXT-SENSITIVE 
Peter Kugel 
Technical Operations Research 
South Avenue 
Burlington, Mass. 
U.S.A. 
, : ;~ '  ' e / %,' # . 
A B S T R A C T  
We discuss some sets of g rammars  whose generative power lies 
between that of the set of context-free grammars  and that of the set of context- 
sensitive grammars .  These sets are developed by subjecting enerators 
of context-sensitive grammars  to abstract versions of a "hardware" restr ict ion 
to which the users  of natural anguages, unlike the descr ibers  of natural angu- 
ages, might be subject. 
Kugel 1 
The notion of a formal grammar was first introduced to provide formal 
models of techniques used by the descr ibers of natural anguages (linguists) 
(1). Later, formal grammars  have been used as models of the capabilities 
of users of natural anguages (See (2) for a review). Language users differ 
from language descr ibers in being subject o restr ict ions on the amount of 
nhardwareW that they have available to them and the amount of time that they 
have to perform their operations. Where the linguist has available (at least 
theoretically) an unlimited amount of material  with which (pencil) and on which 
(paper) to store his intermediate r sults, it is probable that the internal organi- 
zation of the natural anguage user may not permit him the use of such unlimited 
re source s. 
Therefore, when one uses a formal grammar as a model of the language 
user, one may consider the effects of subjecting such grammars  to abstract 
versions of certain types of hardware limitations. One model in this vein is 
t hat of Yngve (3) which considers the natural anguage user to be like a device 
capable of dealing with context-free languages and then subjects it to further 
limitations. However, there are reasons for thinking that natural anguage 
users may have available to them powers beyond those of the context-free 
grammars.  According to current views, these additional powers are those that 
are required to construct transformational grammars.  Among these one might 
include the ability to permute the order of elements in a string and the ability 
to erase elements (4). 
The ability to effect the permutation of elements is a property of context- 
sensitive grammars.  However, context-sensitive grammars  have additional 
drawbacks as models for the capabilities of the users of natural anguages (1). 
Permitt ing erasure as an element the generation of a phrase marker has the 
difficulty that it is not always clear whether the resulting rewrit ing systems 
. 
generate only recursive sets of strings. These considerations suggest hat one 
Thus, any semi-Thue system (For a definition see (5), p. 84) can be looked 
at as a context-free grammar which permits the shortening of strings (erasure). 
But semi-Thue systems are capable of generating non-recursive sets of strings 
((5), Theorem 2.6, p. 93). 
Kugel 2 
might want some context-sensit ivity and some erasure but not enough to pro- 
duce the undesirable features of context-sensit ive grammar  or of semi-Thue 
systems. 
One way of getting at such grammars  might be to consider a device for 
generating context-sensit ive languages and subjecting it to abstract versions of 
t he types of hardware l imitations to which the users of natural anguage users 
might be sjubect. 
Assume that users  of natural languages are information processing systems 
organized in the manner of the present-day digital computer. They have a 
storage unit (memory),a processing unit, and some input/output equipment. One 
way of suggesting the roles of these parts is to say that they correspond roughly 
to those parts of the handling of a natural anguage that are described by the 
semantic, syntactic and phonetic omponents of a language description respect -  
ively. Since our concern in this paper is largely with the syntactic omponent, 
we will consider l imitations on the effects of l imitations on the processing unit. 
Suppose that the processing unit has the machinery for applying the rewrit ing 
rules of context-sensit ive grammar ,  but that this application has to be done by 
changing the state of something like a reg ister  in the arithmetic unit of a present-  
day computer. Such a reg ister  can be looked at as a sequence of pigeon holes into 
which symbols can be placed. A rule then is applied to change the contents of the 
pigeonholes and the results  are returned to the memory or output. To say that 
the reg is ters  have a given size is to say that there is only a fixed number of such 
. 
pigeon-holes . Such an assumption finds a formal analogue in the notion of a 
formal grammar  as a restr ict ion on the length of the str ings that can appear on 
either side of the arrow in a rewrit ing rule. To say that a reg is ter  has only n 
pigeon-holes i  to say that the str ings on either side of the arrow can contain at 
** 
most n symbols. However, such a restr ict ion does not accomplish much that 
We are also assuming that there is no way of doing anything like multiple 
precision arithmetic. 
** 
Or, equivalently, that the string on the right hand side of the arrow can 
contain at most n symbols. 
Kugel 3 
/ 
is of interest, for it is easy to prove that: 
Theorem 1: The set of all g rammars  that contain strings of no more than 
two symbols on either side of their rewrit ing rules has the generative power of 
the set of context-sensit ive grammars .  The set of all g rammars  that contain 
no more than three symbols in any rewrit ing rule has the generative power of 
. 
the set of context-f lee grammars .  
It is c lear from an examination of the proof of the f irst part of this theorem 
that the restr ict ion on the length of the strings used in stating rules of the 
grammar is overcome by introducing new letters. Such an introduction of 
additional letters is common in proofs of theorems about formal g rammars  and 
it is reasonable so long as one is considering these grammars  as models of the 
procedures used by language descr ibers  who have available to them a medium 
(pencil marks on paper} which is unlimited not only in amount, but which permits 
an unlimited variety of symbols within a given space (at least in theory}. 
The fact that language users  might have to represent their grammatical  
catagories in a discrete rather  than continuous medium suggests that one might 
l imit the number of available (distinct) symbols that can appear in a rule of 
grammar.  However, this restr ict ion also is of no great interest since we can 
prove the following: 
Theorem 2: There is a sense in which the generative power of grammars  whose 
rules can be expressed using only two distinct symbols in its vocabulary is equiv- 
alent to the set of all context-sensit ive grammars.  
Suppose, therefore, that one attempts to limit both of these simultaneously. 
Thus, let us define a "grammar of size (m, p)S as a grammar  whose rules are 
constructed of strings (on either side of the rewrit ing ru le 's  arrows) such that 
no string contains more than m occurrences of letters and such that the non- 
$ 
Definitions and proofs of theorems can be found in the appendix. 
Explicated in the appendix. 
Kugel 4 
terminal  vocabulary of the grammars  contains no more than p distinct letters.  
Let us f irst consider such grammars  as augmented simply by dictionaries. 
These grammars  turn out to be curious hybrids. For one thing, given a size, 
there is only a finite number of g rammars  of that size (if one equates traight-  
forward re letter ings of the same grammars) .  Furthermore:  
Theorem 3: The set of g rammars  of size (m, p) with dictionaries, for 
sufficiently large m and p, cannot generate all context-free languages and can 
generate some languages which are not context f lee.  
Nevertheless, it is obvious that the union of the grammars  of size 
(m, p) for all values of m and p has the generative power of the set of all 
context-sensit ive grammars  (since any context-sensit ive grammar  has some 
finite size). 
These grammars  are not part icularly interesting because we have put 
l imits on the amount of recurs ion that can appear in them. This can be over-  
come by permitting some recurs ion either in a pre-  or post-processor,  l imiting 
recurs ion to context-free ru les only. Thus, we are led to consider systems 
consisting of three parts in tandem. The f irst part is a context-free grammar,  
the second part is a g rammar  of size (m, p), and the third is a dictionary. 
Although such systems appear to be rather ad ho% one can give some arguments 
for considering them. The arguments for the two grammars  in tandem are 
roughly those for a context-free grammar  followed by a transformational 
component. If we allow erasure  in the final processing we can permit our inter-  
mediate string generated by the context-free grammar  to be the phrase marker  
in something approximating Polish notation. Thus, the phrase marker:  
/ \  
Kugel 5 
could be represented by the string SACxDyBz. The context-sensitive grammar 
of restr icted size could operate on these markers in the manner of a trans- 
formational component. The dictionary would contain rules of the form X--*, A--*, 
etc. ,  to erase the non-terminal symbols. This argument suggests that if one 
wants such a system as a model for a natural anguage user one might consider 
different primitive operations in the part of the system that was to represent the 
transformational component. Thus, using the suggestion of (4) one might permit 
not only what we have been calling grammar ules but also rules which permute 
the order of strings directly such as rules of the form: XYZ--*ZYX. By making 
these primitive one makes them cost less of the "size" of the underlying rammar.  
The argument for allowing something like a dictionary is that something of this 
sort appears to be required for the phonetic omponent of a language description 
anyway. 
Let us call such systems "grammar systems of size (m, p). " Those systems 
which have primitive permutation rules we might call "permutation systems. " 
We can prove: 
Theorem 4: Grammar systems define infinite hierarchies of languages 
L 0. .. L i. .. such that (a) L 0 is the set of context-free languages; (2) L i~  Lj 
for j sufficiently greater than i and (3) the sum of the L i for all i is the set of 
context- sensitive language s.
We have suggested that if a natural anguage user is organized like a present- 
day digital computer he might find that the size of the registers in what corresponds 
to his "processing unit" might have an effect on the kinds of languages with which 
he could deal. We have given a rather prel iminary sketch of how this might 
occur. Such effects appear however, to be crit ically dependent on the "machine 
code" of such a system, and in view of the current lack of knowledge as to what 
this code might be, it is not clear whether the kinds of notions that we have 
discussed have any applications in computational linguistics, even if the under- 
lying notion of some sort of a "register" limitation applies to the competence 
of natural anguage users.  
Kugel 6 
APPENDIX 
This appendix contains definit ions of some of the te rms used in the body 
of the paper and proofs of the theorems.  We begin by defining some basic 
notions. A rewr i t ing  rule is a ru le of the form PhQ-*PHQ where  P, Q, h, and 
H are (possibly empty*) str ings.  If h is a single le t ter  and H is a non-empty 
str ing of le t te rs  a rewr i t ing  ru le is ca l led a grammar  ru le .  A grammar  (or a 
context-sens i t ive grammar  ) G is a single letter  S together with a finite set of 
g rammar  ru les .  The @phabet of G is the set of all l e t te rs  in rewr i t ing ru les  
of G. The non- termina l  vocabulary of G is the set of all l e t te rs  appearing on the 
left hand side of some grammar  rule in G. The termina l  vocabulary of G is the 
alphabet of G minus the non- termina l  vocabulary of G. We wil l  assume that S 
is always in the non- termina l  vocabulary of G. 
A set of rewr i t ing  ru les  which contains no non- termina l  le t ters  on the r ight-  
hand side of a rewr i t ing rule is cal led a dict ionary.  If P and Q in all the grammar  
ru les  of G are  empty,then G is a context- f ree grammar .  A der ivat ion of a str ing 
S n in a grammar  G is a sequence of str ings S 1, . . . ,  S n such that S 1 is S and such 
that S i +1 is the resu l t  of replac ing some sequence of le t te rs  L in S i by a sequence 
of le t ters  L' such that L - -L '  is one of the grammar  ru les  of G. The language 
generated by a grammar  G is the set of all str ings M such that there exists a 
der ivat ion of M in G, and such that M cons ists  of only le t ters  in the terminal  
vocabulary of G. Two sets of g rammars  that generate the same sets of languages 
are said to have t_he same generat ive power. 
Theorem h (a) The set of g rammars  , none of whose ru les  contain more 
than four le t ters ,  has the same generat ive power as the set of context-sensi t ive 
grammars .  (b) The set of g rammars  none of whose ru les  contain more than 
three le t ters  has the generat ive power of the set of all context f ree grammars .  
PhQ, however,  is not empty (i. e . ,  not all of P, h, and Q can be empty). 
Kugel 7 
Proof: (a) In order  to prove this part of the theorem we need only present  
an effective procedure for replacing each of the ru les  of an arb i t rary  context-  
sensit ive grammar  G with a set of ru les  containing no more  than two let ters  on 
e i ther  side of the arrow, and such that the generat ive power of the resul t ing 
grammar  G' remains  the same. Consider an arb i t rary  rule of the form L--R, 
where L=a 1 . . .a  i . . . a j  andR=al . . .a . l _ lb  1 . . .bka i+ 1 . . .a j .  Replace this by 
the following new ru les  in which the c and d are new let ters  not in the alphabet m m 
of G: 
Rules 
a la2_a lc  2 
c2a3-'*c2c 3 
Cnan+ ~'CnCn + 1 
c i _ l'ai-*Ci_lCi 
aj _ laj--,-dj laj 
aj 2dj - l"*dj - 2dj - 1 
andn + l"*dndn + 1 
$ 
cidi - 1-'*didi - 1 
Effect of Added Rules 
a l '  " ' ai(ai+ 1" " "aj )-~alc 2. .. ci(a i + 1" " ' aj) 
(ale2" " ' )ciai+ 1" " "aj-"(c 1" ' ' )di" ' "dj _ laj 
$ 
In schematiz ing the effects of a sequence of ru les  we have assumed an 
order  in their  application. However, where the order  of application is arb i t rary  
parts of the str ings might be dif ferent if the order  of application were  different. 
These parts are indicated by surrounding them with parentheses.  
Kugel 8 
Rules Effect of Added Rules 
di-'bld'i + 1 
d ) d'i+ n-~bn- 1 i+n+l 
d'i+k_ l-'b k 
( . . . )d i ( .  .. ) - * ( . . . )b l . . .bc ( . . . )  
c2-*a2:' ' l 
ci - l~a i  - 1 
a lc  2. ?. c i _ 1 (.. ? )-*a 1. .. a i _ l ( .  ? ? ) 
di+ l-~ai+. 1 1 
d.--a. J J 
( " ' )d i+ l ' "d j  -'~ ( ' ' ' )a i+ l ' ' '  aj 
The equivalence in the other direct ion (i. e . ,  the fact that all four letter  
g rammars  are at most  context sensit ive) is obvious. 
(b) Because of the definit ion of a "grammar  ule n ru les  containing three 
le t ters  can only be of the form a-*bc (and not ab~c)  so c lear ly  all three letter  
ru les  are context- f ree.  To produce a three letter  equivalent of a longer context-  
f ree rule, say a---a 1. . a one replaces it by the ru les  a-~ala~, .. ~ , -~ a' 1' 
? n ' - i  - i  i + ' " ' 
a'-*a where the a? are new letters. n n } 
Theorem 2: The set of g rammars  containing only two letter  together wi tha 
dict ionary has the generat ive power of the set of all context sensit ive grammars .  
Proof: Let the two le t ters  be 0 and 1. Again, it is only necessary  to provide 
an effective procedure for replacing any rule in a given context-sens i t ive grammar  
with a new set of ru les  containing only two letter,  plus some dict ionary rules.  
Kugel 9 
Suppose that G contains m rules and that the alphabet of G contains n letters. 
Let each rule be of the form Li-*R i (for the i-th rule). We construct G' as 
follows: 
To replace each rule Li--*R i we add new rules as follows: 
Rewrite each letter a. in L i by the string: \] 
~/jth position 
011... 1100011... O... 110 (= a!) 
m t imes n t imes 
lth position 
The first replacing rule takes the revised L i into 0111..~0... 11000... The 
effect of this is to tag the string as being subjected to rule number i. The second 
replacing rule takes the 0 in the n-tuple of ones of the letter being replaced, and 
it turns it into a 1. If the only effect of the rule is to simply replace this letter 
by another letter, the rest  of the new rules place the 0 in the n-tuple appropriately 
and then erase the tag in the left-most m--tuple to signal the end of the applica- 
tion of the rule. If the replaced letter is expanded then the replacements are 
added one letter at a time and the process is finished off by "untagging" the left- 
most m-tuple in the replacement for L i. 
The dictionary has the job of translating back into the vocabulary of G. It 
lacks any procedures for dealing with letters whose m-tuple is not all one's so 
that no intermediate product of a rule can be terminal. The dictionary is simply 
the set of rules a!-~a, for each a. in the terminal vocabulary of G. It is clear that 
J J J 
G' generates exactly the same language as G. 
This proof suggests a problem that might be of some interest. In devising 
a procedure for reducing the number of letters in what is, in some sense, a 
program, one is required to add new rules. These rules introduce intermediate 
products (strings), and the basic problem in the proof was that of devising a way 
in which these intermediate products can be prevented from being caught up by 
rules other than those that are intended to apply to them. We have used an 
extremely straightforward technique for doing this but this technique is costly 
in the size of the required strings. 
Kugel 10 
One might ask what more efficient general procedures there are for such 
reduction. A reason for asking this question (other than a theoretic interest} is 
that the world as seen by a biological organism can be looked at as consisting 
of an arbi t rary alphabet, the units (or letters} of v&ich are the basic percepts 
of that organism. However, the organismTs brain might have a fixed alphabet 
into which the processing of this (probably larger} alphabet has to be encoded. 
Such encoding would probably have to be done by an algorithm that avoided this 
crossing of intermediate products. 
We define a grammar  of size (m, p) as a set of g rammar  ules which has 
a non-terminal  vocabulary of no more than m letters and such that no rule 
contains a string of more than p occurrences of letters on the right hand 
side of the arrow. 
Theorem 3: The set of g rammars  of size (m, p) plus an arb i t rary  number of 
dictionary rules for sufficiently large m and p, cannot generate all context-free 
languages and can generate some languages that are not context-free. 
Proof: Consider the language that consists of the str ings 
bi repeated an arbitrary number of t imes 
aib i. ?. bia i
for some range r of i, (1 ..< i :.< r}. If r > im then this language cannot be 
i=1 
generated by a grammar  of size (m, p) since all the recurs ion must be in the 
i=p 
context-sensit ive part. But there are only ~, im distinct left hand sides of 
i=1 
such rules so that the grammar  must generate some string of the form 
aib i. . .  bia i for i ? j. Since any context sensitive grammar  is a g rammar  of 
size (m, p) and since Chomsky has proved that not all context-sensit ive languages 
are context free (6), it is obvious that there are languages generated by grammars  
of size (m, p} for sufficiently large (m, p) that are not context-free. We define a 
grammar  system of size (m, p) as three rewrit ing systems, the f irst of which is 
a context-free grammar,  the second of which is a g rammar  of size (m, p} and the 
third of which is a dictionary. The language generated by such a system is defined 
in the obvious way. 
Kugel 11 
Theorem 4: The sets of languages generated by grammar systems of size 
(mxp),where mxp = y, define a hierarchy of languages L such that (a) L 0 is the 
Y 
set of context-free languages, (b) L. ~ L. for j sufficiently greater than i, and 
1 j 
(c) such that the sum of the L is the set of context-sensit ive languages. 
Y 
Proof: The set of languages whose strings are of the form PhP, where h 
is a fixed string and P are arbitrary strings on a given alphabet A, are context- 
sensitive and not context-free (6). Therefore, in a grammar system which 
generates such a language,the part that generates such strings must be in the 
context-sensitive part. Although the dictionary can introduce arbitrary new 
letters it cannot insure that if the substitution for some given letter a i is to be 
aj at one time and a k at another, that the substitutions in a given string will be 
uniform (i. e. ,  always aj and never ak) for the entire length of an arbitrari ly long 
string. Therefore, the rules of the context-sensitive part of the grammar 
system generating PhP must have different letters (or distinct strings representing 
different letters) in the left-hand side of its rules. But in the grammar generating 
the copy of a given string P there must be at least one rule to produce the effect 
of copying each letter of A. If we let the alphabet of A be larger than mxp, then 
this cannot occur in a grammar of size (m, p). 
Therefore, for every grammar of size (m, p) there is a context-sensit ive 
language that cannot be generated by a grammar system limited to a grammar of 
that size. But clearly this language can be generated by a system having a 
grammar of some finite size. This proves part (b). Part (a) of the theorem is 
proved by observing that the set of context free languages are generated by a 
grammar system with a grammar of size (0, 0). This is so because the content- 
sensitive part is empty and the amount of erasure that can be produced by any 
dictionary is always finite and therefore its effect can be incorporated into a 
context-free grammar.  Part (c) of the theorem is obvious. 
Kugel 12 
R E F E R E N C E S  
l o  
2. 
. 
. 
e 
6. 
Chomsky, N., Syntactic Structures, Mouton, 1957. 
Miller, G.A. and Chomsky, N., "Finitary Models of Language Users" 
in Handbook of Mathematical.Psychology r (Luce, Bush and 
Galanter eds. ) Volume 2, John Wiley, 1963. 
Yngve, V.H., "A Model and an Hypothesis for Language Structure", 
Proceedings of the American Philosophical Society, Volume 104, 
pp. 444-466, 1960. 
Fraser, J .B . ,  "Some Remarks on Elementary Transformations", in 
Quarterly Progress Report of the Research Laboratory of 
Electronics (M. I. T. ), No. 71. 
Davis, M., Computability and Unsolvabilityj McGraw-Hil l ,  1958. 
Chomsky, N., "On Certain Formal Properties of Grammars", Infor- 
mation and Control, Volume 2, pp, 137-167, 1959. 
