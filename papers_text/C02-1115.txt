A Maximum Entropy-based Word Sense Disambiguation system

Armando Suarez Manuel Palomar
Departamento de Lenguajes y Sistemas Informaticos
Universidad de Alicante
Apartado de correos, 99
E-03080 Alicante, Spain
farmando, mpalomarg@dlsi.ua.es
http://www.dlsi.ua.es/armando/publicaciones.html
Abstract
In this paper, a supervised learning system of
word sense disambiguation is presented. It is
based on conditional maximum entropy models.
This system acquires the linguistic knowledge
from an annotated corpus and this knowledge
is represented in the form of features. Several
types of features have been analyzed using the
SENSEVAL-2 data for the Spanish lexical sam-
ple task. Such analysis shows that instead of
training with the same kind of information for
all words, each one is more eectively learned
using a dierent set of features. This best-
feature-selection is used to build some systems
based on dierent maximum entropy classiers,
and a voting system helped by a knowledge-
based method.
1 Introduction
Word sense disambiguation (WSD) is an open
research eld in natural language processing
(NLP). The task of WSD consists in assign-
ing the correct sense to words using an elec-
tronic dictionary as the source of word deni-
tions. This is a hard problem that is receiving
a great deal of attention from the research com-
munity.
Currently, there are two main methodologi-
cal approaches in this research area: knowledge-
based methods and corpus-based methods. The
former approach relies on previously acquired
linguistic knowledge, and the latter uses tech-
niques from statistics and machine learning to
induce models of language usage from large
samples of text (Pedersen, 2001). Learning can
be supervised or unsupervised. With supervised

This paper has been partially supported by the
Spanish Government (CICYT) under project number
TIC2000-0664-C02-02.
learning, the actual status (here, sense label)
for each piece of data in the training example is
known, whereas with unsupervised learning the
classication of the data in the training example
is not known (Manning and Schutze, 1999).
At SENSEVAL-2, researchers showed the lat-
est contributions to WSD. Some supervised sys-
tems competed in the Spanish lexical sample
task. The Johns Hopkins University system
(Yarowsky et al, 2001) combined, by means of
a voting-based classier, several WSD subsys-
tems based on dierent methods: decision lists
(Yarowsky, 2000), cosine-based vector models,
and Bayesian classiers. The University of
Maryland system (UMD-SST) (Cabezas et al,
2001) used support vector machines.
Pedersen (2002) proposes a baseline method-
ology for WSD based on decision tree learning
and naive Bayesian classiers, using simple lex-
ical features. Several systems that combine dif-
ferent classiers using distinct sets of features
competed at SENSEVAL-2, both in the English
and Spanish lexical sample tasks.
This paper presents a system that implements
a corpus-based method of WSD. The method
used to perform the learning over a set of sense-
disambiguated examples is that of maximum en-
tropy (ME) probability models. Linguistic in-
formation is represented in the form of feature
vectors, which identify the occurrence of certain
attributes that appear in contexts containing
linguistic ambiguities. The context is the text
surrounding an ambiguity that is relevant to the
disambiguation process. The features used may
be of a distinct nature: word collocations, part-
of-speech (POS) labels, keywords, topic and
domain information, grammatical relationships,
and so on. Instead of training with the same
kind of information for all words, which under-
estimates which information is more relevant to
each word, our research shows that each word is
more eectively learned using a dierent set of
features. Therefore, a more accurate feature se-
lection can be done testing several combinations
of features by means of a n-fold cross-validation
over the training data.
At SENSEVAL-2, Stanford University pre-
sented a metalearner (Ilhan et al, 2001) com-
bining simple classiers (naive-Bayes, vector
space, memory-based and others) that use vot-
ing and conditional ME models. Garca Varea
et al (2001) do machine translation tasks using
ME to perform some kind of semantic classi-
cation, but they also rely on another statistical
training procedure to dene word classes.
In the following discussion, the ME frame-
work will be described. Then, feature imple-
mentation and the complete set of feature de-
nitions used in this work will be detailed. Next,
evaluation results using several combinations of
these features will be shown. Finally, some con-
clusions will be presented, along with a brief
discussion of work in progress and future work
planned.
2 The Maximum Entropy
Framework
ME modeling provides a framework for integrat-
ing information for classication frommany het-
erogeneous information sources (Manning and
Schutze, 1999). ME probability models have
been successfully applied to some NLP tasks,
such as POS tagging or sentence boundary de-
tection (Ratnaparkhi, 1998).
The WSD method used in this work is based
on conditional ME models. It has been im-
plemented using a supervised learning method
that consists of building word-sense classiers
using a semantically tagged corpus. A classi-
er obtained by means of an ME technique con-
sists of a set of parameters or coe?cients which
are estimated using an optimization procedure.
Each coe?cient is associated with one feature
observed in the training data. The main pur-
pose is to obtain the probability distribution
that maximizes the entropy, that is, maximum
ignorance is assumed and nothing apart from
the training data is considered. Some advan-
tages of using the ME framework are that even
knowledge-poor features may be applied accu-
rately; the ME framework thus allows a virtu-
ally unrestricted ability to represent problem-
specic knowledge in the form of features (Rat-
naparkhi, 1998).
Let us assume a set of contexts X and a set
of classes C. The function cl : X ! C chooses
the class c with the highest conditional proba-
bility in the context x: cl(x) = argmax
c
p(cjx).
Each feature is calculated by a function that is
associated to a specic class c
0
, and it takes the
form of equation (1), where cp(x) is some ob-
servable characteristic in the context
1
. The con-
ditional probability p(cjx) is dened by equation
(2), where 
i
is the parameter or weight of the
feature i, K is the number of features dened,
and Z(x) is a value to ensure that the sum of
all conditional probabilities for this context is
equal to 1.
f(x; c) =

1 if c
0
= c and cp(x) = true
0 otherwise
(1)
p(cjx) =
1
Z(x)
K
Y
i=1

f
i
(x;c)
i
(2)
The implementation of this ME framework
for WSD was done in C++ and included the
learning module, the classication module, the
evaluation module, and the corpus translation
module. The rst two modules comprise the
main components.
The learning module produces the classiers
for each word using a corpus that is syntacti-
cally and semantically annotated. This module
has two subsystems. The rst subsystem con-
sists of two component actions: in a rst step,
the module processes the learning corpus in or-
der to dene the functions that will apprise the
linguistic features of each context; in a second
step, the module then lls in the feature vectors.
The second subsystem of the learning module
performs the estimation of the coe?cients and
stores the classication functions. For example,
let us assume that we want to build a classier
for noun interest and that POS label of the pre-
vious word is the type of feature to use and the
training corpus has these three samples:
1
The ME approach is not limited to binary features,
but the optimization procedure used for the estimation
of the parameters, the Generalized Iterative Scaling pro-
cedure, uses this kind of functions.
... the widespread interest#1 in the ...
... the best interest#5 of both ...
... persons expressing interest#1 in the ...
The learning module performs a sequen-
tial processing of this corpus looking for pairs
<POS-label, sense>. Then, <adjective,#1>,
<adjective,#5>, and <noun,#1> are used to
dene three functions (each context have a vec-
tor of three features). Next, each vector is lled
in with the result of the evaluation of each func-
tion. Finally, the optimization procedure calcu-
lates the coe?cients and the output is a classi-
er for the word interest.
The classication module carries out the dis-
ambiguation of new contexts using the previ-
ously stored classication functions. When ME
does not have enough information about a spe-
cic context, several senses may achieve the
same maximum probability and thus the clas-
sication cannot be done properly. In these
cases, the most frequent sense in the corpus is
assigned. However, this heuristic is only neces-
sary for minimum number of contexts or when
the set of linguistic attributes processed is very
small.
3 Feature Implementation
An important issue in the implementation of
this ME framework is the form of the functions
that calculate each feature. These functions are
dened in the training phase and depend upon
the data in the corpus.
A usual denition of features would substi-
tute cp(x) in equation (1) with an expression
like info(x,i) = a, where info(x,i) informs about
a property that can be found at position i in
a context x, and a is a predened value. For
example, if we consider that 0 is the position of
the word to be learned and that i is related to 0,
then POS(x,-1) = `adjective'. Therefore, equa-
tion (1) is used to generate a function for each
possible value (sense; a) at position i. Hence-
forth, we will refer to this type of features as
\non-relaxed" features, against \relaxed" fea-
tures described below. In the example of the
previous section, three \non-relaxed" functions
could be dened.
Other expressions, such as info(x,i) 2 W
(c
0
;i)
,
may be substituted for the term cp(x) as a
way to reduce the number of possible features.
In the expression above, for example, W
(c
0
;i)
is the set of attributes present in the learning
examples at position i. Again, if we assume
that POS(x; 1), then for each sense of the
ambiguous word, the system builds a set with
the POS tags occurring in their previous posi-
tions. So this kind of function reduces the num-
ber of features to one per each sense at posi-
tion i. In the example of the previous section,
two \relaxed" functions could be dened from
<fadjective,noung,#1> and <adjective,#5>.
Due to the nature of the disambiguation task,
the number of times that a feature generated by
the rst type of function (\non-relaxed") is ac-
tivated is very low, and feature vectors have a
large number of values equal to 0. The new
function drastically reduces the number of fea-
tures, with a minimum of degradation in eval-
uation results. At the same time, new features
can be incorporated into the learning process
with a minimum impact on e?ciency.
4 Description of Features
The set of features dened for the training of the
system is described in gure 1, and is based on
the feature selection made by Ng and Lee (1996)
and Escudero et al (2000). Features are auto-
matically dened as explained before and de-
pend on the data in the training corpus. These
features are based on words, collocations, and
POS tags in the local context. Both \relaxed"
and \non-relaxed" functions are used.
Figure 1: List of types of features
 0: ambiguous-word shape
 s : words at positions 1, 2, 3
 p : POS-tags of words at positions 1, 2, 3
 b : lemmas of collocations at positions ( 2; 1),
( 1;+1), (+1;+2)
 c: collocations at positions ( 2; 1), ( 1;+1),
(+1;+2)
 km: lemmas of nouns at any position in con-
text, occurring at least m% times with a sense
 r : grammatical relation of the ambiguous word
 d : the word that the ambiguous word depends
on
 L: lemmas of content-words at positions 1,
2, 3 (\relaxed" denition)
 W : content-words at positions 1, 2, 3
(\relaxed" denition)
 S, B, C, P, and D : \relaxed" versions
Actually, each item in gure 1 groups several
sets of features. The majority of them depend
on nearest words (for example, s comprises all
possible features dened by the words occur-
ring in each sample at positions w
 3
, w
 2
, w
 1
,
w
+1
, w
+2
, w
+3
related to the ambiguous word).
Types nominated with capital letters are based
on the \relaxed" function form, that is, these
features consists of a simply recognition of an
attribute as belonging to the training data.
Keyword features (km) are vaguely inspired
by Ng and Lee (1996). A nouns selection is
done using frequency information for nouns co-
occurring with a particular sense. For example,
in a set of 100 examples of sense four of the noun
interest, if the noun bank is found ten times or
more (m = 10%), then a feature is dened for
each possible sense of interest.
Moreover, new features have also been de-
ned using other grammatical properties: rela-
tionship features (r) that refer to the grammati-
cal relationship of the ambiguous word (subject,
object, complement, ...) and dependency fea-
tures (d and D) extract the word related to the
ambiguous one through the dependency parse
tree.
5 Evaluation
In this section we present the results of our
evaluation over the training and test data of
the SENSEVAL-2 Spanish lexical sample task.
This corpus was parsed using Conexor Func-
tional Dependency Grammar parser for Spanish
(Tapanainen and Jarvinen, 1997).
Table 1 shows the ve best results using sev-
eral sets of features. The classiers were built
Table 1: Evaluation on SENSEVAL-2 Spanish data
ALL Nouns
0.671 0LWSBCk5 0.683 LWSBCk5
0.666 LWSBCk5 0.682 0LWSBCk5
0.663 sbcpdk5 0.666 0LWSBCPDk5
0.662 0LWSBCPDk5 0.666 0LWsBCPDk5
0.662 0LWsBCPDk5 0.666 0LWSBCPDk5
Verbs Adjectives
0.595 sk5 0.783 LWsBCp
0.584 sbcprdk3 0.778 0sprd
0.583 sbcpdk5 0.777 0sbcprdk5
0.580 sbcpk5 0.777 0sbcprdk10
0.580 0sbcprdk3 0.772 0spdk5
Table 2: 3-fold cross-validation results on
SENSEVAL-2 Spanish training data
Features Functions Accur MFS
autoridad,N sbcp 548 0.589 0.503
bomba,N 0LWSBCk5 176 0.762 0.707
canal,N sbcprdk3 1258 0.579 0.307
circuito,N 0LWSBCk5 482 0.536 0.392
corazon,N 0Sbcpk5 210 0.781 0.607
corona,N sbcp 420 0.722 0.489
gracia,N 0sk5 542 0.634 0.295
grano,N 0LWSBCr 102 0.681 0.483
hermano,N 0Sprd 152 0.731 0.602
masa,N LWSBCk5 206 0.756 0.455
naturaleza,N sbcprdk3 1213 0.527 0.424
operacion,N 0LWSBCk5 399 0.543 0.377
organo,N 0LWSBCPDk5 271 0.715 0.515
partido,N 0LWSBCk5 111 0.839 0.524
pasaje,N sk5 389 0.685 0.451
programa,N 0LWSBCr 137 0.587 0.486
tabla,N sk5 282 0.663 0.488
actuar,V sk5 772 0.514 0.293
apoyar,V 0sbcprdk3 1257 0.730 0.635
apuntar,V 0LWsBCPDk5 729 0.661 0.478
clavar,V sbcprdk3 1026 0.561 0.449
conducir,V LWsBCPD 482 0.534 0.358
copiar,V 0sbcprdk3 1231 0.457 0.338
coronar,V sk5 739 0.698 0.327
explotar,V 0LWSBCk5 643 0.593 0.318
saltar,V LWsBC 518 0.403 0.132
tocar,V 0sbcprdk3 1888 0.583 0.313
tratar,V sbcpk5 1421 0.527 0.208
usar,V 0Sprd 222 0.732 0.669
vencer,V sbcprdk3 1063 0.696 0.618
brillante,A sbcprdk3 1199 0.756 0.512
ciego,A 0spdk5 478 0.812 0.565
claro,A 0Sprd 177 0.919 0.854
local,A 0LWSBCr 64 0.798 0.750
natural,A sbcprdk10 949 0.471 0.267
popular,A sbcprdk10 2624 0.865 0.632
simple,A LWsBCPD 522 0.776 0.621
verde,A LWSBCk5 556 0.601 0.317
vital,A Sbcp 591 0.774 0.441
from the training data and evaluated over the
test data. These values mean the maximum ac-
curacy that the system can achieve at this mo-
ment with a xed set of features for all words.
Nevertheless, there are clear dierences between
nouns, verbs and adjectives.
Our main goal is to nd a method to auto-
matically obtain the best feature selection from
the training data. Such method consists of a
n-fold cross-validation testing several combina-
tions of features over the training data and the
analysis of the results obtained for each word.
Table 2 shows the best results obtained us-
ing a 3-fold cross-validation evaluation method
on training data. Several feature combinations
have been tested in order to nd the best set
for each selected word. The purpose was to
achieve the most relevant information for each
word from the corpus rather than applying the
same combination of features to all of them.
Therefore, column Features is the feature se-
lection with the best result. Strings in each
row represent the whole set of features used
in the training of each classier. For example,
autoridad obtains its best result using nearest
words, collocations of two lemmas, collocations
of two words, and POS information; s, b, c and
p features respectively (see gure 1). Functions
is the number of functions generated from fea-
tures, and Accur (for \accuracy") the number of
correctly classied contexts divided by the total
number of contexts. Column MFS is the accu-
racy obtained when the most frequent sense is
selected.
In order to perform the three tests on each
word, some preprocessing of the corpus was
done. For each word, all senses were uniformly
distributed in the three folds (each fold contains
one third of examples of each sense). Those
senses that had fewer than three examples in
the original corpus le were rejected and not
processed.
The data summarized in Table 2 reveal that
utilization of \relaxed" features in the ME
method is useful; both \relaxed" and \non-
relaxed" functions are used, even for the same
word. For example, adjective vital obtains the
best result with \Sbcp" (the \relaxed" ver-
sion of words in a window ( 3:: + 3), colloca-
tions of two lemmas and two words in a win-
dow ( 2:: + 2), and POS labels, in a window
( 3::+3) too); we can assume that single word
information is less important than collocations
in order to disambiguate vital correctly.
Ambiguous word shape (0 features) is useful
for nouns, verbs and adjectives, but many of the
words do not use it for its best feature selection.
In general, these words have not a relevant re-
lationship between shape and senses. On the
other hand, POS information (p and P features)
is selected less often. When comparing lemma
features against word features (e.g., L versus
W , and B versus C), they are complementary
in the majority of cases. Grammatical relation-
ships (r features) and word-word dependencies
(d and D features) seem very useful too if com-
bined with other types of attributes. Moreover,
keywords (km features) are used very often, pos-
sibly due to the source and size of contexts of
SENSEVAL-2 data.
Table 3: Best feature selections per POS
ALL Nouns
0.613 sbcprdk3 0.609 LWSBCk5
0.609 sbcprdk5 0.609 sbcprdk5
0.605 0sbcprdk3 0.609 sbcprdk3
0.604 sbcprdk10 0.608 sk5
0.602 sbcpdk5 0.602 0sbcprdk3
Verbs Adjectives
0.575 sbcprdk3 0.706 0spdk5
0.568 sbcpdk5 0.701 0sbcprdk10
0.567 sbcprdk5 0.699 sbcprdk10
0.567 sbcpk5 0.699 0sbcprdk5
0.560 sbcprdk10 0.696 LWsBCp
Table 3 shows the rst ve best feature se-
lections for all words, and for nouns, verbs, and
adjectives. Data in this table and in table 2
were used to build four dierent sets of classi-
ers in order to compare their accuracy: MEx
uses the overall best feature selection for all
words; MEbfs the best selection of features for
Table 4: Evaluation of ME systems
ALL Nouns
0.677 MEbfs.pos 0.683 MEbfs.pos
0.676 vME 0.678 vME
0.667 MEbfs 0.661 MEbfs
0.658 MEx 0.646 MEx
Verbs Adjectives
0.583 vME 0.774 vME
0.583 MEbfs.pos 0.772 MEbfs.pos
0.583 MEx 0.771 MEbfs
0.580 MEbfs 0.756 MEx
MEx: sbcprdk3 for all words
MEbfs: each word with its
best feature selection
MEbfs.pos: LWSBCk5 for nouns,
sbcprdk3 for verbs,
and 0spdk5 for adjectives
vME: majority voting between MEx,
MEbfs.pos, and MEbfs
each word; MEbfs.pos uses the best selection
of each POS for all words of that POS; nally,
vME is a majority voting system that has as
input the answers of the three systems.
Table 4 shows the comparison of these four
systems, the less e?cient is MEx that ap-
plies the same set of types of features to all
words. However, the best feature selection per
word (MEbfs) is not the best, probably be-
cause deeper analysis and more training exam-
ples are necessary. The best choice seems to se-
lect a xed set of types of features for each POS
(MEbfs.pos). This last system obtains an ac-
curacy slightly better than the best evaluation
result in table 1, that is, a best-feature-selection
strategy from training data guarantees a suc-
cessful disambiguation.
In general, verbs are di?cult to learn and
the accuracy of the method for them too low;
in our opinion, more information (knowledge-
based perhaps) is needed to build their classi-
ers, but the types of features used could be un-
suitable too. The voting system (vME), based
on the agreement between the other three sys-
tems, does not improve the accuracy.
Finally, the results of the ME method were
compared with the systems that competed at
SENSEVAL-2 in the Spanish lexical sample task
(table 5)
23
. If the ME systems described previ-
ously are ranked within this scoring table, nouns
and adjectives obtain a excellent results; verbs
obtain worse results.
Table 5 also includes an enrichment of vME:
vME+SM. This new voting system adds an-
other classier, specication marks (Montoyo
and Palomar, 2001), a knowledge-based method
that uses the semantic relationships between
words stored in WordNet and EuroWordNet
(Vossen, 1998). Because it works merely with
nouns, vME+SM improves accuracy for them
only, but obtains the same score than JHU(R).
Overall score reaches the second place.
6 Conclusions
AWSD system based on maximum entropy con-
ditional probability models has been presented.
2
Systems: JHU and JHU(R) by Johns Hopkins Uni-
versity; CSS244 by Stanford University; UMD-SST by
University of Maryland; Duluth systems by University
of Manitoba; UA by University of Alicante.
3
SENSECAL-2 data can be downloaded from
http://www.sle.sharp.co.uk/senseval2/
It is a supervised learning method that needs a
corpus previously annotated with sense labels.
Using the training data of SENSEVAL-2 for
the Spanish lexical sample task, several com-
binations of features were analyzed in order to
identify which were the best. This information
is the basis of various sets of classiers, as well
as two majority voting systems. The results ob-
tained by these systems show that selecting best
feature sets guarantees the success of the disam-
biguation method.
As we work to improve the ME method with a
deeper analysis of the feature selection strategy,
we are also working to develop a cooperative
strategy between several other methods as well,
both knowledge-based and corpus-based.
Future research will incorporate domain in-
formation as an additional information source
for the system. WordNet Domains (Magnini
and Strapparava, 2000) is an enrichment of
WordNet with domain labels. These attributes
will be incorporated into the learning of the sys-
tem in the same way that features were incorpo-
rated, as described above, except that domain
disambiguation will be evaluated as well; that
is, WordNet senses (synsets) will be substituted
for domains labels, thereby reducing the num-
ber of possible classes into which contexts can
be classied.
References
Clara Cabezas, Philip Resnik, and Jessica
Stevens. 2001. Supervised Sense Tagging us-
ing Support Vector Machines. In Preiss and
Yarowsky (Preiss and Yarowsky, 2001), pages
59{62.
Gerard Escudero, Lluis Marquez, and Ger-
man Rigau. 2000. Boosting applied to
word sense disambiguation. In Proceedings
of the 12th Conference on Machine Learning
ECML2000, Barcelona, Spain.
Ismael Garca-Varea, Franz J. Och, Hermann
Ney, and Francisco Casacuberta. 2001. Re-
ned lexicon models for statistical machine
translation using a maximum entropy ap-
proach. In Proceedings of 39th Annual Meet-
ing of the Association for Computational Lin-
guistics, pages 204{211.
H. Tolga Ilhan, Sepandar D. Kamvar, Dan
Klein, Christopher D. Manning, and Kristina
Toutanova. 2001. Combining Heterogeneus
Table 5: Comparing with SENSEVAL-2 systems
ALL Nouns Verbs Adjectives
0.713 jhu(R) 0.702 jhu(R) 0.643 jhu(R) 0.802 jhu(R)
0.684 vME+SM 0.702 vME+SM 0.609 jhu 0.774 vME
0.682 jhu 0.683 MEbfs.pos 0.595 css244 0.772 MEbfs.pos
0.677 MEbfs.pos 0.681 jhu 0.584 umd-sst 0.772 css244
0.676 vME 0.678 vME 0.583 vME 0.771 MEbfs
0.670 css244 0.661 MEbfs 0.583 MEbfs.pos 0.764 jhu
0.667 MEbfs 0.652 css244 0.583 MEx 0.756 MEx
0.658 MEx 0.646 MEx 0.580 MEbfs 0.725 duluth 8
0.627 umd-sst 0.621 duluth 8 0.515 duluth 10 0.712 duluth 10
0.617 duluth 8 0.612 duluth Z 0.513 duluth 8 0.706 duluth 7
0.610 duluth 10 0.611 duluth 10 0.511 ua 0.703 umd-sst
0.595 duluth Z 0.603 umd-sst 0.498 duluth 7 0.689 duluth 6
0.595 duluth 7 0.592 duluth 6 0.490 duluth Z 0.689 duluth Z
0.582 duluth 6 0.590 duluth 7 0.478 duluth X 0.687 ua
0.578 duluth X 0.586 duluth X 0.477 duluth 9 0.678 duluth X
0.560 duluth 9 0.557 duluth 9 0.474 duluth 6 0.655 duluth 9
0.548 ua 0.514 duluth Y 0.431 duluth Y 0.637 duluth Y
0.524 duluth Y 0.464 ua
Classiers for Word-Sense Disambigua-
tion. In Preiss and Yarowsky (Preiss and
Yarowsky, 2001), pages 87{90.
Bernardo Magnini and C. Strapparava. 2000.
Experiments in Word Domain Disambigua-
tion for Parallel Texts. In Proceedings of the
ACL Workshop on Word Senses and Multi-
linguality, Hong Kong, China.
Christopher D. Manning and Hinrich Schutze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cam-
bridge, Massachusetts.
Andres Montoyo and Manuel Palomar. 2001.
Specication Marks for Word Sense Disam-
biguation: New Development. In Alexan-
der F. Gelbukh, editor, CICLing, volume
2004 of Lecture Notes in Computer Science,
pages 182{191. Springer.
Hwee Tou Ng and Hian Beng Lee. 1996. Inte-
grating multiple knowledge sources to disam-
biguate word senses: An exemplar-based ap-
proach. In Arivind Joshi and Martha Palmer,
editors, Proceedings of the 34th Annual Meet-
ing of the ACL, San Francisco. Morgan Kauf-
mann Publishers.
Ted Pedersen. 2001. A decision tree of bigrams
is an accurate predictor of word sense. In
Proceedings of the 2nd Annual Meeting of the
North American Chapter of the ACL, pages
79{86, Pittsburgh, July.
Ted Pedersen. 2002. A baseline methodology
for word sense disambiguation. In Alexan-
der F. Gelbukh, editor, CICLing, volume
2276 of Lecture Notes in Computer Science,
pages 126{135. Springer.
Judita Preiss and David Yarowsky, edi-
tors. 2001. Proceedings of SENSEVAL-2,
Toulouse, France, July. ACL-SIGLEX.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Res-
olution. Ph.D. thesis, University of Pennsyl-
vania.
Pasi Tapanainen and Timo Jarvinen. 1997. A
non-projective dependency parser. In Pro-
ceedings of the Fifth Conference on Applied
Natural Language Processing, pages 64{71,
April.
Piek Vossen. 1998. EuroWordNet: Building
a Multilingual Database with WordNets for
European Languages. The ELRA Newsletter,
3(1).
David Yarowsky, Silviu Cucerzan, Radu Flo-
rian, Charles Schafer, and Richard Wi-
centowski. 2001. The Johns Hopkins
SENSEVAL-2 System Description. In Preiss
and Yarowsky (Preiss and Yarowsky, 2001),
pages 163{166.
David Yarowsky. 2000. Hierarchical decision
lists for word sense disambiguation. Comput-
ers and the Humanities, 34(2):179{186.
