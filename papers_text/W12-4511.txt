Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 100?106,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
A Multigraph Model for Coreference Resolution
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va Mu?jdricza-Maydt, Michael Strube
Natural Language Processing Group
Heidelberg Institute for Theoretical Studies gGmbH
Heidelberg, Germany
(sebastian.martschat|jie.cai|michael.strube)@h-its.org
Abstract
This paper presents HITS? coreference reso-
lution system that participated in the CoNLL-
2012 shared task on multilingual unrestricted
coreference resolution. Our system employs a
simple multigraph representation of the rela-
tion between mentions in a document, where
the nodes correspond to mentions and the
edges correspond to relations between the
mentions. Entities are obtained via greedy
clustering. We participated in the closed tasks
for English and Chinese. Our system ranked
second in the English closed task.
1 Introduction
Coreference resolution is the task of determining
which mentions in a text refer to the same entity.
This paper describes HITS? system for the CoNLL-
2012 Shared Task on multilingual unrestricted coref-
erence resolution, where the goal is to build a system
for coreference resolution in an end-to-end multilin-
gual setting (Pradhan et al, 2012). We participated
in the closed tasks for English and Chinese and fo-
cused on English. Our system ranked second in the
English closed task.
Being conceptually similar to and building upon
Cai et al (2011b), our system is based on a directed
multigraph representation of a document. A multi-
graph is a graph where two nodes can be connected
by more than one edge. In our model, nodes rep-
resent mentions and edges are built from relations
between the mentions. The entities to be inferred
correspond to clusters in the multigraph.
Our model allows for directly representing any
kind of relations between pairs of mentions in a
graph structure. Inference over this graph can har-
ness structural properties and the rich set of encoded
relations. In order to serve as a basis for further
work, the components of our system were designed
to work as simple as possible. Therefore our system
relies mostly on local information between pairs of
mentions.
2 Architecture
Our system is implemented on top of the BART
toolkit (Versley et al, 2008). To compute the coref-
erence clusters in a document, we first extract a set
of mentions M = {m1, . . . ,mn} ordered according
to their position in the text (Section 2.1). We then
build a directed multigraph where the set of nodes
is M and edges are induced by relations between
mentions (Section 2.4). The relations we use in our
system are coreference indicators like string match-
ing or alias (Section 3). For every relation R, we
compute a weight wR using the training data (Sec-
tion 2.3). We then assign the weight wR to any edge
that is induced by the relation R. Depending on dis-
tance and connectivity properties of the graph the
weights may change (Section 2.4.1). Given the con-
structed graph with edge weights, we go through the
mentions according to their position in the text and
perform greedy clustering (Section 2.6). For Chi-
nese, we employ spectral clustering (Section 2.5) as
adopted in Cai et al (2011b) before the greedy clus-
tering step to reduce the number of candidate an-
tecedents for a mention. The components of our sys-
tem are described in the following subsections.
100
2.1 Mention Extraction
Noun phrases are extracted from the provided parse
and named entity annotation layers. For embedded
mentions with the same head, we only keep the men-
tion with the largest span.
2.1.1 English
For English we identify eight different mention
types: common noun, proper noun, personal pro-
noun, demonstrative pronoun, possessive pronoun,
coordinated noun phrase, quantifying noun phrase
(some of ..., 17 of ...) and quantified noun phrase
(the armed men in one of the armed men). The head
for a common noun or a quantified noun is com-
puted using the SemanticHeadFinder from the Stan-
ford Parser1. The head for a proper noun starts at
the first token tagged as a noun until a punctuation,
preposition or subclause is encountered. Coordina-
tions have the CC tagged token as head and quanti-
fying noun phrases have the quantifier as head.
In a postprocessing step we filter out adjectival
use of nations and named entities with semantic
class Money, Percent or Cardinal. We discard men-
tions whose head is embedded in another mention?s
head. Pleonastic pronouns are identified and dis-
carded via a modified version of the patterns used
by Lee et al (2011).
2.1.2 Chinese
For Chinese we detect four mention types: com-
mon noun, proper noun, pronoun and coordination.
The head detection for Chinese is provided by the
SunJurafskyChineseHeadFinder from the Standford
Parser, except for proper nouns whose head is set to
the mention?s rightmost token.
The remaining processing is similar to the men-
tion detection for English.
2.2 Preprocessing
We extract the information in the provided an-
notation layers and transform the predicted con-
stituent parse trees into dependency parse trees.
We work with two different dependency represen-
tations, one obtained via the converter implemented
1http://nlp.stanford.edu/software/
lex-parser.shtml
in Stanford?s NLP suite2, the other using LTH?s
constituent-to-dependency conversion tool3. For
pronouns, we determine number and gender using
tables containing a mapping of pronouns to their
gender and number.
2.2.1 English
For English, number and gender for common
nouns are computed via a comparison of head
lemma to head and using the number and gender
data of Bergsma and Lin (2006). Quantified noun
phrases are always plural. We compute semantic
classes via a WordNet (Fellbaum, 1998) lookup.
2.2.2 Chinese
For Chinese, we simply determine number and
gender by searching for the corresponding desig-
nators, since plural mentions mostly end with ?,
while ?? (sir) and ?? (lady) often suggest gen-
der information. To identify demonstrative and defi-
nite noun phrases, we check whether they start with
a definite/demonstrative indicator (e.g. ? (this) or
? (that)). We use lists of named entities extracted
from the training data to determine named entities
and their semantic class in development and testing
data.
2.3 Computing Weights for Relations
We compute weights for relations using simple de-
scriptive statistics on training documents. Since this
is a robust approach to learning weights for the type
of graph model we employ (Cai et al, 2011b; Cai
et al, 2011a), we use only a fraction of the available
training data. We took a random subset consisting of
around 20% for English and 15% for Chinese of the
training data. For every document in this set and ev-
ery relation R, we go through the set M of extracted
mentions and compute for every pair (mi,mj) with
i > j whether R holds for this pair. The weight wR
for R is then the number of coreferent pairs with R
divided by the number of all pairs with R.
2.4 Graph Construction
The set of relations we employ consists of two sub-
sets: negative relations R? which enforce that no
2http://nlp.stanford.edu/software/
stanford-dependencies.shtml
3http://nlp.cs.lth.se/software/treebank_
converter/
101
edge is built between two mentions, and positive re-
lations R+ that induce edges. Again, we go through
M in a left-to-right fashion. If for two mentions mi,
mj with i > j a negative relation R? holds, no edge
between mi and mj can be built. Otherwise we add
an edge from mi to mj for every positive relation
R+ such that R+(mi,mj) is true. The structure ob-
tained by this construction is a directed multigraph.
We handle copula relations similar to Lee et al
(2011): if mi is this and the pair (mi,mj) is in a
copula relation (like This is the World), we remove
mj and replace mj in all edges involving it by mi.
For Chinese, we handle ?role appositives? as intro-
duced by Haghighi and Klein (2009) analogously.
2.4.1 Assigning Weights to Edges
Initially, any edge (mi,mj) induced by the rela-
tion R has the weight wR computed as described
in Section 2.3. If R is a transitive relation, we di-
vide the weight by the number of mentions con-
nected by this relation. This corresponds to the way
edge weights are assigned during the spectral em-
bedding in Cai et al (2011b). If R is a relation sen-
sitive to distance like compatibility between a com-
mon/proper noun and a pronoun, the weight is al-
tered according to the distance between mi and mj .
2.4.2 An Example
We demonstrate the graph construction by a sim-
ple example. Consider a document consisting of the
following three sentences.
Barack Obama and Nicolas Sarkozy met
in Toronto yesterday. They discussed the
financial crisis. Sarkozy left today.
Let us assume that our system identifies Barack
Obama (m1), Nicolas Sarkozy (m2), Barack Obama
and Nicolas Sarkozy (m3), They (m4) and Sarkozy
(m5) as mentions. We consider these mentions and
the relations N Number, P Nprn Prn, P Alias and
P Subject described in Section 3. The graph con-
structed according to the algorithm described in this
section is displayed in Figure 1.
Observe the effect of the negative relation
N Number: while P Nprn Prn holds for the pair
Barack Obama (m1) and They (m4), the mentions
do not agree in number. Hence N Number holds for
this pair and no edge from m4 to m1 can be built.
m2 m5
m3 m4
P Alias
P Nprn Prn
P Subject
Figure 1: An example graph. Nodes represent mentions,
edges are induced by relations between the mentions.
2.5 Spectral Clustering
For Chinese we apply spectral clustering before the
final greedy clustering phase. In order to be able to
apply spectral clustering, we make the graph undi-
rected and merge parallel edges into one edge, sum-
ming up all weights. Due to the way edge weights
are computed, the resulting undirected simple graph
corresponds to the graph Cai et al (2011b) use as
input to the spectral clustering algorithm. Spectral
clustering is now performed as in Cai et al (2011b).
2.6 Greedy Clustering
To describe our clustering algorithm, we use some
additional terminology: if there exists an edge from
m to n we say that m is a parent of n and that n is a
child of m.
In the last step, we go through the mentions from
left to right. Let mi be the mention in focus. For
English, we consider all children of mi as possible
antecedents. For Chinese we restrict the possible an-
tecedents to all children that are in the same cluster
obtained by spectral clustering.
If mi is a pronoun, we determine mj such that
the sum over all weights of edges from mi to mj is
maximized. We then assign mi and mj to the same
entity. In English, if mi is a parent of a noun phrase
m that embeds mj , we instead assign mi and m to
the same entity.
For Chinese, all other noun phrases are assigned
to the same entity as all their children in the cluster
obtained by spectral clustering. For English, we are
more restrictive: definites and demonstratives are as-
signed to the same cluster as their closest (according
to the position of the mentions in the text) child.
Negative relations may also be applied as con-
straints in this phase. Before assigning mi to the
same entity as a set of mentions C, we check for
102
every m ? C and every negative relation R?
that we want to incorporate as a constraint whether
R?(mi,m) holds. If yes, we do not assign mi to the
same entity as the mentions in C.
2.7 Complexity
Our algorithms for weight computation, graph con-
struction and greedy clustering look at all pairs of
mentions in a document and perform simple calcu-
lations, which leads to a time complexity of O
(
n2
)
per document, where n is the number of mentions
in a document. When performing spectral cluster-
ing, this increases to O
(
n3
)
. Since we deal with
at most a few hundred mentions per document, the
cubic running time is not an issue.
3 Relations
In our system relations serve as templates for build-
ing or disallowing edges between mentions. We
distinguish between positive and negative relations:
negative relations disallow edges between mentions,
positive relations build edges between mentions.
Negative relations can also be used as constraints
during clustering, while positive relations may also
be applied as ?weak? relations: in this case, we only
add the induced edge when the two mentions under
consideration are already included in the graph after
considering all the non-weak relations.
Most of the relations presented here were already
used in our system for last year?s shared task (Cai et
al., 2011b). The set of relations was enriched mainly
to resolve pronouns in dialogue and to resolve pro-
nouns that do not carry much information by them-
selves like it and they.
3.1 Negative Relations
(1) N Gender, (2) N Number: Two mentions do
not agree in gender or number.
(3) N SemanticClass: Two mentions do not agree
in semantic class (only the Object, Date and Per-
son top categories derived from WordNet (Fell-
baum, 1998) are used).
(4) N ItDist: The anaphor is it or they and the sen-
tence distance to the antecedent is larger than
one.
(5) N BarePlural: Two mentions that are both bare
plurals.
(6) N Speaker12Prn: Two first person pronouns
or two second person pronouns with different
speakers, or one first person pronoun and one
second person pronoun with the same speaker.
(7) N DSprn: Two first person pronouns in direct
speech assigned to different speakers.
(8) N ContraSubjObj: Two mentions are in the
subject and object positions of the same verb,
and the anaphor is a non-possessive pronoun.
(9) N Mod: Two mentions have the same syntac-
tic heads, and the anaphor has a pre- or post-
modifier which does not occur in the antecedent
and does not contradict the antecedent.
(10) N Embedding: Two mentions where one em-
beds the other, which is not a reflexive or posses-
sive pronoun.
(11) N 2PrnNonSpeech: Two second person pro-
nouns without speaker information and not in di-
rect speech.
3.2 Positive Relations
(12) P StrMatch Npron, (13) P StrMatch Pron:
After discarding stop words, if the strings of
mentions completely match and are not pro-
nouns, the relation P StrMatch Npron holds.
When the matched mentions are pronouns,
P StrMatch Pron holds.
(14) P HeadMatch: If the syntactic heads of men-
tions match.
(15) P Nprn Prn: If the antecedent is not a pro-
noun and the anaphor is a pronoun. This relation
is restricted to a sentence distance of 1.
(16) P Alias: If mentions are aliases of each other
(i.e. proper names with partial match, full names
and acronyms, etc.).
(17) P Speaker12Prn: If the speaker of the second
person pronoun is talking to the speaker of the
first person pronoun. The mentions contain only
first or second person pronouns.
(18) P DSPrn: If one mention is subject of a speak
verb, and the other mention is a first person pro-
noun within the corresponding direct speech.
(19) P ReflPrn: If the anaphor is a reflexive pro-
noun, and the antecedent is the subject of the
sentence.
103
(20) P PossPrn: If the anaphor is a possessive pro-
noun, and the antecedent is the subject of the
sentence or the subclause.
(21) P GPEIsA: If the antecedent is a Named En-
tity of GPE entity type and the anaphor is a def-
inite expression of the same type.
(22) P PossPrnEmbedding: If the anaphor is a
possessive pronoun and is embedded in the an-
tecedent.
(23) P VerbAgree: If the anaphor is a pronoun and
has the same predicate as the antecedent.
(24) P Subject & (25) P Object: If both mentions
are subjects/objects (applies only if the anaphor
is it or they).
(26) P SemClassPrn: If the anaphor is a pronoun,
the antecedent is not a pronoun, and both have
semantic class Person.
For English, we used all relations except for (21) and
(26). Relations (1), (2) and (10) were incorporated
as constraints during greedy clustering. For Chinese,
we used relations (1) ? (6), (12) ? (15), (21) and (26).
(26) was incorporated as a weak relation.
4 Results
We submitted to the closed tasks for English and
Chinese. The results on the English development
set and testing set are displayed in Table 1 and Table
2 respectively. To indicate the progress we achieved
within one year, Table 3 shows the performance of
our system on the CoNLL ?11 development data set
compared to last year?s results (Cai et al, 2011b).
The Overall number is the average of MUC, B3
and CEAF (E), MD is the mention detection score.
Overall, we gained over 5% F1 some of which can
be attributed to improved mention detection.
Metric R P F1
MD 73.96 75.69 74.81
MUC 64.93 68.69 66.76
B3 68.42 75.77 71.91
CEAF (M) 61.23 61.23 61.23
CEAF (E) 49.61 45.60 47.52
BLANC 77.81 80.75 79.19
Overall 62.06
Table 1: Results on the English CoNLL ?12 development
set
Metric R P F1
MD 74.23 76.10 75.15
MUC 65.21 68.83 66.97
B3 66.50 74.69 70.36
CEAF (M) 59.61 59.61 59.61
CEAF (E) 48.64 44.72 46.60
BLANC 73.29 78.94 75.73
Overall 61.31
Table 2: Results on the English CoNLL ?12 testing set
Metric R P F1 2011 F1
MD 70.84 73.08 71.94 66.28
MUC 60.80 65.09 62.87 55.19
B3 68.37 75.89 71.94 68.52
CEAF (M) 60.42 60.42 60.42 54.44
CEAF (E) 50.40 46.11 48.16 43.19
BLANC 75.44 79.26 77.19 72.13
Overall 60.99 55.63
Table 3: Results on the English CoNLL ?11 development
set compared to Cai et al (2011b)
Table 4 and Table 5 display our results on Chinese
development data and testing data respectively.
Metric R P F1
MD 52.45 71.50 60.51
MUC 45.90 67.07 54.50
B3 58.94 84.26 69.36
CEAF (M) 53.60 53.60 53.60
CEAF (E) 50.73 34.24 40.89
BLANC 66.17 83.11 71.45
Overall 54.92
Table 4: Results on the Chinese CoNLL ?12 development
set
Metric R P F1
MD 48.49 74.02 58.60
MUC 42.71 67.80 52.41
B3 55.37 85.24 67.13
CEAF (M) 51.30 51.30 51.30
CEAF (E) 51.81 32.46 39.92
BLANC 63.96 82.81 69.18
Overall 53.15
Table 5: Results on the Chinese CoNLL ?12 testing set
Because none of our team members has knowl-
edge of the Arabic language we did not attempt to
104
run our system on the Arabic datasets and therefore
our official score for this language is considered to
be 0. The combined official score of our submission
is (0.0 + 53.15 + 61.31)/3 = 38.15. In the closed
task our system was the second best performing sys-
tem for English and the eighth best performing sys-
tem for Chinese.
5 Error analysis
We did not attempt to resolve event coreference and
did not incorporate world knowledge which is re-
sponsible for many recall errors our system makes.
Since we use a simple greedy strategy for clus-
tering that goes through the mentions left-to-right,
errors in clustering propagate, which gives rise to
cluster-level inconsistencies. We observed a drop in
performance when using more negative relations as
constraints. A more sophisticated clustering strat-
egy that allows a more refined use of constraints is
needed.
5.1 English
Our detection of copula and appositive relations is
quite inaccurate, which is why we limit the incor-
poration of copulas to cases where the antecedent is
this and left appositives out.
We aim for high precision regarding the usage of
the negative relation N Modifier. This leads to some
loss in recall. For example, our system does not as-
sign the just-completed Paralympics and the 12-day
Paralympics to the same entity. Such cases require a
more involved reasoning scheme to decide whether
the modifiers are actually contradicting each other.
Non-referring pronouns constitute another source
of errors. While we improved detection of pleonas-
tic it compared to last year?s system, a lot of them
are not filtered out. Our system also does not distin-
guish well between generic and non-generic uses of
you and we, which hurts precision.
5.2 Chinese
Since each Chinese character carries its own mean-
ing, there are multiple ways to express the same en-
tity by combining different characters into a word.
Both syntactic heads and modifiers can be replaced
by similar words or by abbreviated versions. From
??? (outside people) to ???? (outside eth-
nic group) the head is replaced, while from ??
? (Diana) to ?? ?? ? ?? (charming Di
Princess) the name is abbreviated.
Modifier replacement is more difficult to cope
with, our system does not recognize that ?? ?
??? (starting-over counting-votes job) and??
?? (verifying-votes job) are coreferent. It is also
not trivial to separate characters from words (e.g. by
separating ? and ?) to resolve such cases, since
it will introduce too much noise as a consequence.
In order to tackle this problem, a smart scheme to
propagate similarities from partial words to the en-
tire mentions and a knowledge base upon which re-
liable similarities can be retrieved are necessary.
In contrast to English there is no strict enforce-
ment of using definite noun phrases when referring
to an antecedent in Chinese. Both ???? (the
talk) and?? (talk) can corefer with the antecedent
??????????? (Clinton?s talk during
Hanoi election). This makes it very difficult to dis-
tinguish generic expressions from referential ones.
In the submitted version of our system, we simply
ignore the nominal anaphors which do not start with
definite articles or demonstratives.
6 Conclusions
In this paper we presented a graph-based model for
coreference resolution. It captures pairwise relations
between mentions via edges induced by relations.
Entities are obtained by graph clustering. Discrim-
inative information can be incorporated as negative
relations or as constraints during clustering.
We described our system?s architecture and the re-
lations it employs, highlighting differences and sim-
ilarities to our system from last year?s shared task.
Designed to work as a basis for further work, our
system works mainly by exploring the relationship
between pairs of mentions. Due to its modular archi-
tecture, our system can be extended by components
taking global information into account, for example
for weight learning or clustering.
We focused on the closed task for English in
which our system achieved competitive perfor-
mance, being ranked second out of 15 participants.
Acknowledgments. This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first and the second authors have been
supported by a HITS PhD. scholarship.
105
References
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, Sydney, Australia, 17?
21 July 2006, pages 33?40.
Jie Cai, E?va Mu?jdricza-Maydt, Yufang Hou, and Michael
Strube. 2011a. Weakly supervised graph-based coref-
erence resolution for clinical data. In Proceedings of
the 5th i2b2 Shared Tasks and Workshop on Chal-
lenges in Natural Language Processing for Clinical
Data, Washington, D.C., 20-21 October 2011. To ap-
pear.
Jie Cai, E?va Mu?jdricza-Maydt, and Michael Strube.
2011b. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Shared
Task of the 15th Conference on Computational Natu-
ral Language Learning, Portland, Oreg., 23?24 June
2011, pages 56?60.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singapore,
6?7 August 2009, pages 1152?1161.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Shared Task of the 15th Conference on Compu-
tational Natural Language Learning, Portland, Oreg.,
23?24 June 2011, pages 28?34.
Sameer Pradhan, Alessandro Moschitti, and Nianwen
Xue. 2012. CoNLL-2012 Shared Task: Modeling
multilingual unrestricted coreference in OntoNotes. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012. This volume.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Companion Volume to the Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, Columbus, Ohio, 15?20 June 2008, pages
9?12.
106
