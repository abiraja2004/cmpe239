Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1335?1343,
Beijing, August 2010
Resolving Surface Forms to Wikipedia Topics 
Yiping Zhou   Lan Nie   Omid Rouhani-Kalleh   Flavian Vasile   Scott Gaffney 
Yahoo! Labs at Sunnyvale 
{zhouy,lannie,omid,flavian,gaffney}@yahoo-inc.com 
 
Abstract 
Ambiguity of entity mentions and con-
cept references is a challenge to mining 
text beyond surface-level keywords. We 
describe an effective method of disambi-
guating surface forms and resolving them 
to Wikipedia entities and concepts. Our 
method employs an extensive set of fea-
tures mined from Wikipedia and other 
large data sources, and combines the fea-
tures using a machine learning approach 
with automatically generated training da-
ta. Based on a manually labeled evalua-
tion set containing over 1000 news ar-
ticles, our resolution model has 85% pre-
cision and 87.8% recall. The performance 
is significantly better than three baselines 
based on traditional context similarities 
or sense commonness measurements. Our 
method can be applied to other languages 
and scales well to new entities and con-
cepts. 
1 Introduction 
Ambiguity in natural language is prevalent and, 
as such, it can be a difficult challenge for infor-
mation retrieval systems and other text mining 
applications. For example, a search for ?Ford? in 
Yahoo! News retrieves about 40 thousand ar-
ticles containing Ford referring to a company 
(Ford Motors), an athlete (Tommy Ford), a place 
(Ford City), etc. Due to reference ambiguity, 
even if we knew the user was only interested in 
the company, they would still have to contend 
with articles referring to the other concepts as 
well. 
In this paper we focus on the problem of re-
solving references of named-entities and con-
cepts in natural language through their textual 
surface forms. Specifically, we present a method 
of resolving surface forms in general text docu-
ments to Wikipedia entries. The tasks of resolu-
tion and disambiguation are nearly identical; we 
make the distinction that resolution specifically 
applies when a known set of referent concepts 
are given a priori. Our approach differs from oth-
ers in multiple aspects including the following.  
1) We employ a rich set of disambiguation 
features leveraging mining results from large-
scale data sources. We calculate context-
sensitive features by extensively mining the cat-
egories, links and contents of the entire Wikipe-
dia corpus. Additionally we make use of context-
independent data mined from various data 
sources including Web user-behavioral data and 
Wikipedia. Our features also capture the one-to-
one relationship between a surface form and its 
referent.  
2) We use machine learning methods to train 
resolution models with a large automatically la-
beled training set. Both ranking-based and classi-
fication-based resolution approaches are ex-
plored.  
3) Our method disambiguates both entities and 
word senses. It scales well to new entities and 
concepts, and it can be easily applied to other 
languages.  
We propose an extensive set of metrics to eva-
luate not only overall resolution performance but 
also out-of-Wikipedia prediction. Our systems 
for English language are evaluated using real-
world test sets and compared with a number of 
baselines. Evaluation results show that our sys-
tems consistently and significantly outperform 
others across all test sets. 
The paper is organized as follows. We first de-
scribe related research in Section 2, followed by 
an introduction of Wikipedia in Section 3. We 
then introduce our learning method in Section 4 
and our features in Section 5. We show our expe-
rimental results in Section 6, and finally close 
with a discussion of future work. 
1335
2 Related Work 
Named entity disambiguation research can be 
divided into two categories: some works (Bagga 
and Baldwin, 1998; Mann and Yarowsky, 2003;  
Pedersen et al, 2005; Fleischman and Hovy, 
2004; Ravin and Kazi, 1999) aim to cluster am-
biguous surface forms to different groups, with 
each representing a unique entity; others (Cucer-
zan, 2007; Bunescu and Pa?ca, 2006; Han and 
Zhao, 2009; Milne and Witten, 2008a; Milne and 
Witten, 2008b) resolve a surface form to an enti-
ty or concept extracted from existing knowledge 
bases. Our work falls into the second category. 
Looking specifically at resolution, Bunescu 
and Pasca (2006) built a taxonomy SVM kernel 
to enrich a surface form?s representation with 
words from Wikipedia articles in the same cate-
gory. Cucerzan (2007) employed context vectors 
consisting of phrases and categories extracted 
from Wikipedia. The system also attempted to 
disambiguate all surface forms in a context si-
multaneously, with the constraint that their re-
solved entities should be globally consistent on 
the category level as much as possible. Milne and 
Witten (2008a, 2008b) proposed to use Wikipe-
dia?s link structure to capture the relatedness 
between Wikipedia entities so that a surface form 
is resolved to an entity based on its relatedness to 
the surface form?s surrounding entities. Besides 
relatedness, they also define a commonness fea-
ture that captures how common it is that a sur-
face form links to a particular entity in general. 
Han and Zhao (2009) defined a novel alignment 
strategy to calculate similarity between surface 
forms based on semantic relatedness in the con-
text.  
Milne and Witten?s work is most related to 
what we propose here in that we also employ 
features similar to their relatedness and com-
monness features. However, we add to this a 
much richer set of features which are extracted 
from Web-scale data sources beyond Wikipedia, 
and we develop a machine learning approach to 
automatically blend our features using complete-
ly automatically generated training data. 
3 Wikipedia 
Wikipedia has more than 200 language editions, 
and the English edition has more than 3 million 
articles as of March 2009. Newsworthy events 
are often added to Wikipedia within days of oc-
currence; Wikipedia has bi-weekly snapshots 
available for download.  
Each article in Wikipedia is uniquely identi-
fied by its title which is usually the most com-
mon surface form of an entity or concept. Each 
article includes body text, outgoing links and 
categories. Here is a sample sentence in the ar-
ticle titled ?Aristotle? in wikitext format. ?To-
gether with Plato and [[Socrates]] (Plato's 
teacher), Aristotle is one of the most important 
founding figures in [[Western philosophy]].? 
Near the end of the article, there are category 
links such as ?[[Category:Ancient Greek mathe-
maticians]]?. The double brackets annotate out-
going links to other Wikipedia articles with the 
specified titles. The category names are created 
by authors. Articles and category names have 
many-to-many relationships. 
In addition to normal articles, Wikipedia also 
has special types of articles such as redirect ar-
ticles and disambiguation articles. A redirect ar-
ticle?s title is an alternative surface form for a 
Wikipedia entry. A disambiguation article lists 
links to similarly named articles, and usually its 
title is a commonly used surface form for mul-
tiple entities and concepts.  
4 Method of Learning 
Our goal is to resolve surface forms to entities or 
concepts described in Wikipedia. To this end, we 
first need a recognizer to detect surface forms to 
be resolved. Then we need a resolver to map a 
surface form to the most probable entry in Wiki-
pedia (or to out-of-wiki) based on the context. 
Recognizer: We first create a set of Wikipedia 
(article) entries E = {e1, e2, ?} to which we want 
to resolve surface forms. Each entry?s surface 
forms are mined from multiple data sources. 
Then we use simple string match to recognize 
surface forms from text documents.  
Among all Wikipedia entries, we exclude 
those with low importance. In our experiments, 
we removed the entries that would not interest 
general Web users, such as stop words and punc-
tuations. Second, we collect surface forms for 
entries in E using Wikipedia and Web search 
query click logs based on the following assump-
tions:  
1336
? Each Wikipedia article title is a surface form 
for the entry. Redirect titles are taken as alter-
native surface forms for the target entry.  
? The anchor text of a link from one article to 
another is taken as an alternative surface form 
for the linked-to entry.  
? Web search engine queries resulting in user 
clicks on a Wikipedia article are taken as alter-
native surface forms for the entry. 
As a result, we get a number of surface forms 
for each entry ei. If we let sij denote the j-th sur-
face form for entry i, then we can represent our 
entry dictionary as EntSfDict = {<e1, (s11, s12, 
?)>, <e2, (s21, s22, ?)>, ?}.  
Resolver: We first build a labeled training set 
automatically, and then use supervised learning 
methods to learn models to resolve among Wiki-
pedia entries. In the rest of this section we de-
scribe the resolver in details. 
4.1 Automatically Labeled Data 
To learn accurate models, supervised learning 
methods require training data with both large 
quantity and high quality, which often takes lots 
of human labeling effort. However, in Wikipedia, 
links provide a supervised mapping from surface 
forms to article entries. We use these links to 
automatically generate training data. If a link's 
anchor text is a surface form in EntSfDict, we 
extract the anchor text as surface form s and the 
link's destination article as Wikipedia entry e, 
then add the pair (s, e) with a positive judgment 
to our labeled example set. Continuing, we use 
EntSfDict to find other Wikipedia entries for 
which s is a surface form and create negative 
examples for these and add them to our labeled 
example set. If e does not exist in EntSfDict (for 
example, if the link points to a Wikipedia article 
about a stop word), then a negative training ex-
ample is created for every Wikipedia entry to 
which s may resolve. We use oow (out-of-wiki) 
to denote this case. 
Instead of article level coreference resolution, 
we only match partial names with full names 
based on the observation that surface forms for 
named entities are usually capitalized word se-
quences in English language and a named entity 
is often mentioned by a long surface form fol-
lowed by mentions of short forms in the same 
article. For each pair (s, e) in the labeled example 
set, if s is a partial name of a full name s? occur-
ring earlier in the same document, we replace (s, 
e) with (s?, e) in the labeled example set.  
Using this methodology we created 2.4 million 
labeled examples from only 1% of English Wiki-
pedia articles. The abundance of data made it 
possible for us to experiment on the impact of 
training set size on model accuracy.  
4.2 Learning Algorithms 
In our experiments we explored both Gradient 
Boosted Decision Trees (GBDT) and Gradient 
Boosted Ranking (GBRank) to learn resolution 
models. They both can easily combine features 
of different scale and with missing values. Other 
supervised learning methods are to be explored 
in the future.  
GBDT: We use the stochastic variant of 
GBDTs (Friedman, 2001) to learn a binary logis-
tic regression model with the judgments as the 
target. GBDTs compute a function approxima-
tion by performing a numerical optimization in 
the function space. It is done in multiple stages, 
with each stage modeling residuals from the 
model of the last stage using a small decision 
tree. A brief summary is given in Algorithm 1. In 
the stochastic version of GBDT, one sub-samples 
the training data instead of using the entire train-
ing set to compute the loss function. 
Algorithm 1 GBDTs 
Input: training data N
iii yx 1)},{( =  , loss function 
L[y, f(x)] , the number of nodes for each tree J 
, the number of trees M . 
1: Initialize f(x)=f0 
2: For m = 1 to M 
2.1:   For i = 1 to N, compute the negative 
gradient by taking the derivative of the 
loss with respect to f(x) and substitute 
with 
iy and )(1 imi xf ? . 
2.2:    Fit a J-node regression tree to the 
components of the negative gradient. 
2.3:   Find the within-node updates m
ja  for j 
= 1 to J by performing J univariate op-
timizations of the node contributions to 
the estimated loss. 
2.4:   Do the update m
ji
m
ii
m
i arxfxf ?+= ? )()( 1 , 
where j is the node that xi belongs to, r 
is learning rate. 
3: End for 
4: Return fM 
1337
In our setting, the loss function is a negative 
binomial log-likelihood, xi is the feature vector 
for a surface-form and Wikipedia-entry pair (si, 
ei), and yi is +1 for positive judgments and -1 is 
for negative judgments. 
GBRank: From a given surface form?s judg-
ments we can infer that the correct Wikipedia 
entry is preferred over other entries. This allows 
us to derive pair-wise preference judgments from 
absolute judgments and train a model to rank all 
the Wikipedia candidate entries for each surface 
form. Let },...,1),()(|),{( '' NixlxlxxS iiii =?=  be the set of 
preference judgments, where xi and xi' are the 
feature vectors for two pairs of surface-forms and 
Wikipedia-entry, l(xi) and l(xi') are their absolute 
judgments respectively. GBRank (Zheng et al, 
2007) tries to learn a function h such that 
)()( 'ii xhxh ? for Sxx ii ?),( ' . A sketch of the algorithm 
is given in Algorithm 2. 
Algorithm 2 GBRank 
1: Initialize h=h0 
2: For k=1 to K 
2.1:   Use hk-1 as an approximation of h and 
compute 
})()(|),{( '11
' ?+??= ??+ ikikii xhxhSxxS
})()(|),{( '11
' ?+<?= ??? ikikii xhxhSxxS  
where ))()(( 'ii xlxl ?=??  
2.2:   Fit a regression function gk using 
GBDT and the incorrectly predicted 
examples 
}),(|))(,(),)(,{( '1
''
1
?
?? ??+ Sxxxhxxhx iiikiiki ??  
2.3:   Do the update 
)1/())()(()( 1 ++= ? kxgxkhxh kkk ? , where ?  is  
learning rate. 
3: End for 
4: Return hK 
We use a tuning set independent from the 
training set to select the optimal parameters for 
GBDT and GBRank. This includes the number 
of trees M, the number of nodes J, the learning 
rate r, and the sampling rate for GBDT; and for 
GBRank we select  K, ? and ?.  
The feature importance measurement given by 
GBDT and GBRank is computed by keeping 
track of the reduction in the loss function at each 
feature variable split and then computing the to-
tal reduction of loss along each explanatory fea-
ture variable. We use it to analyze feature effec-
tiveness. 
4.3 Prediction 
After applying a resolution model on the given 
test data, we obtain a score for each surface-form 
and Wikipedia-entry pair (s, e). Among all the 
pairs containing s, we find the pair with the high-
est score, denoted by (s, e~ ). 
It?s very common that a surface form refers to 
an entity or concept not defined in Wikipedia. So 
it?s important to correctly predict whether the 
given surface form cannot be mapped to any Wi-
kipedia entry in EntSfDict. 
We apply a threshold to the scores from reso-
lution models. If the score for (s, e~ ) is lower than 
the threshold, then the prediction is oow (see 
Section 4.1), otherwise e~  is predicted to be the 
entry referred by s. We select thresholds based 
on F1 (see Section 6.2) on a tuning set that is 
independent from our training set and test set. 
5 Features 
For each surface-form and Wikipedia-entry pair 
(s, e), we create a feature vector including fea-
tures capturing the context surrounding s and 
features independent of the context. They are 
context-dependent and context-independent fea-
tures respectively. Various data sources are 
mined to extract these features, including Wiki-
pedia articles, Web search query-click logs, and 
Web-user browsing logs. In addition, (s, e) is 
compared to all pairs containing s based on 
above features and the derived features are called 
differentiation features.  
5.1 Context-dependent Features 
These features measure whether the given sur-
face form s resolving to the given Wikipedia en-
try e would make the given document more co-
herent. They are based on 1) the vector represen-
tation of e, and 2) the vector representation of the 
context of s in a document d. 
Representation of e: By thoroughly mining 
Wikipedia and other large data sources we ex-
tract contextual clues for each Wikipedia entry 
e and formulate its representation in the follow-
ing ways. 
1) Background representation. The overall 
background description of e is given in the cor-
responding Wikipedia article, denoted as Ae. Na-
turally, a bag of terms and surface forms in Ae 
can represent e. So we represent e by a back-
1338
ground word vector Ebw and a background sur-
face form vector Ebs, in which each element is 
the occurrence count of a word or a surface form 
in Ae?s first paragraph. 
2) Co-occurrence representation. The terms 
and surface forms frequently co-occurring with e 
capture its contextual characteristics. We first 
identify all the Wikipedia articles linking to Ae. 
Then, for each link pointing to Ae we extract the 
surrounding words and surface forms within a 
window centered on the anchor text. The window 
size is set to 10 words in our experiment. Finally, 
we select the words and surface forms with the 
top co-occurrence frequency, and represent e by 
a co-occurring word vector Ecw and a co-
occurring surface form vector Ecs, in which each 
element is the co-occurrence frequency of a se-
lected word or surface form.  
3) Relatedness representation. We analyzed 
the relatedness between Wikipedia entries from 
different data sources using various measure-
ments, and we computed over 20 types of rela-
tedness scores in our experiments. In the follow-
ing we discuss three types as examples. The first 
type is computed based on the overlap between 
two Wikipedia entries? categories. The second 
type is mined from Wikipedia inter-article links. 
(In our experiments, two Wikipedia entries are 
considered to be related if the two articles are 
mutually linked to each other or co-cited by 
many Wikipedia articles.) The third type is 
mined from Web-user browsing data based on 
the assumption that two Wikipedia articles co-
occurring in the same browsing session are re-
lated. We used approximately one year of Yahoo! 
user data in our experiments. A number of differ-
ent metrics are used to measure the relatedness. 
For example, we apply the algorithm of Google 
distance (Milne and Witten, 2008b) on Wikipe-
dia links to calculate the Wikipedia link-based 
relatedness, and use mutual information for the 
browsing-session-based relatedness. In summary, 
we represent e by a related entry vector Er for 
each type of relatedness, in which each element 
is the relatedness score between e and a related 
entry. 
Representation of s: We represent a surface 
form?s context as a vector, then calculate a con-
text-dependent feature for a pair <s,e> by a simi-
larity function Sim from two vectors. Here are 
examples of context representation. 
1) s is represented by a word vector Sw and a 
surface form vector Ss, in which each element is 
the occurrence count of a word or a surface form 
surrounding s. We calculate each vector?s simi-
larity with the background and co-occurrence 
representation of e, and it results in Sim(Sw, Ebw) , 
Sim(Sw, Ecw) , Sim(Ss, Ebs) and Sim(Ss, Ecs) . 
2) s is represented by a Wikipedia entry vector 
Se, in which each element is a Wikipedia entry to 
which a surrounding surface form s could re-
solve. We calculate its similarity with the rela-
tedness representation of e, and it results in 
Sim(Se, Er). 
In the above description, similarity is calcu-
lated by dot product or in a summation-of-
maximum fashion. In our experiments we ex-
tracted surrounding words and surface forms for 
s from the whole document or from the text win-
dow of 55 tokens centered on s, which resulted in 
2 sets of features. We created around 50 context-
dependent features in total. 
5.2 Context-independent Features  
These features are extracted from data beyond 
the document containing s. Here are examples. 
? During the process of building the dictionary 
EntSfDict as described in Section 4, we count 
how often s maps to e and estimate the proba-
bility of s mapping to e for each data source. 
These are the commonness features. 
? The number of Wikipedia entries that s could 
map to is a feature about the ambiguity of s. 
? The string similarity between s and the title of 
Ae is used as a feature. In our experiments 
string similarity was based on word overlap. 
5.3 Differentiation Features 
Among all surface-form and Wikipedia-entry 
pairs that contain s, at most one pair gets the pos-
itive judgment. Based on this observation we 
created differentiation features to represent how 
(s, e) is compared to other pairs for s.  They are 
derived from the context-dependent and context-
independent features described above. For exam-
ple, we compute the difference between the 
string similarity for (s, e) and the maximum 
string similarity for all pairs containing s. The 
derived feature value would be zero if (s, e) has 
larger string similarity than other pairs contain-
ing s. 
1339
6 Experimental Results 
In our experiments we used the Wikipedia snap-
shot for March 6th, 2009.  Our dictionary 
EntSfDict contains 3.5 million Wikipedia entries 
and 6.5 million surface forms. 
A training set was created from randomly se-
lected Wikipedia articles using the process de-
scribed in Section 4.1. We varied the number of 
Wikipedia articles from 500 to 40,000, but the 
performance did not increase much after 5000. 
The experimental results reported in this paper 
are based on the training set generated from 5000 
articles. It contains around 1.4 million training 
examples. There are approximately 300,000 sur-
face forms, out of which 28,000 are the oow case.  
Around 400 features were created in total, and 
200 of them were selected by GBDT and 
GBRank to be used in our resolution models. 
6.1 Evaluation Datasets 
Three datasets from different data sources are 
used in evaluation. 
1) Wikipedia hold-out set. Using the same 
process for generating training data and exclud-
ing the surface forms appearing in the training 
data, we built the hold-out set from approximate-
ly 15,000 Wikipedia articles, containing around 
600,000 labeled instances. There are 400,000 
surface forms, out of which 46,000 do not re-
solve to any Wikipedia entry. 
2) MSNBC News test set. This entity disam-
biguation data set was introduced by Cucerzan 
(2007). It contains 200 news articles collected 
from ten MSNBC news categories as of January 
2, 2007. Surface forms were manually identified 
and mapped to Wikipedia entities. The data set 
contains 756 surface forms. Only 589 of them are 
contained in our dictionary EntSfDict, mainly 
because EntSfDict excludes surface forms of out-
of-Wikipedia entities and concepts. Since the 
evaluation task is focused on resolution perfor-
mance rather than recognition, we exclude the 
missing surface forms from the labeled example 
set. The final dataset contains 4,151 labeled in-
stances. There are 589 surface forms and 40 of 
them do not resolve to any Wikipedia entry. 
3) Yahoo! News set. One limitation of the 
MSNBC test set is the small size. We built a 
much larger data set by randomly sampling 
around 1,000 news articles from Yahoo! News 
over 2008 and had them manually annotated. The 
experts first identified person, location and or-
ganization names, then mapped each name to a 
Wikipedia article if the article is about the entity 
referred to by the name. We didn?t include more 
general concepts in this data set to make the ma-
nual effort easier. This data set contains around 
100,000 labeled instances. The data set includes 
15,387 surface forms and 3,532 of them cannot 
be resolved to any Wikipedia entity. We random-
ly split the data set to 2 parts of equal size. One 
part is used to tune parameters of GBDT and 
GBRank and select thresholds based on F1 value. 
The evaluation results presented in this paper is 
based on the remaining part of the Yahoo! News 
set. 
6.2 Metrics 
The possible outcomes from comparing a resolu-
tion system?s prediction with ground truth can be 
categorized into the following types. 
? True Positive (TP), the predicted e was correct-
ly referred to by s.   
? True Negative (TN), s was correctly predicted 
as resolving to oow.  
? Mismatch (MM), the predicted e was not cor-
rectly referred to by s and should have been e? 
from EntSfDict. 
? False Positive (FP), the predicted e was not 
correctly referred to by s and should have been 
oow. 
? False Negative (FN), the predicted oow is not 
correct and should have been e? from 
EntSfDict. 
Similar to the widely used metrics for classifi-
cation systems, we use following metrics to eva-
luate disambiguation performance.  
MMFPTP
TP
precision ++=
                 
MMFNTP
TP
recall ++=
 
recallprecision
recallprecision
F +
??= 21   
MMFNTNFPTP
TNTP
accuracy ++++
+=  
In the Yahoo! News test set, 23.5% of the sur-
face forms do not resolve to any Wikipedia en-
tries, and in the other two test sets the percentag-
es of oow are between 10% and 20%. This de-
monstrates it is necessary in real-world applica-
tions to explicitly measure oow prediction. We 
propose following metrics. 
FNTN
TN
oowprecision +=_
                 
FPTN
TN
oowrecall +=_
 
oowrecalloowprecision
oowrecalloowprecision
oowF
__
__2
_1 +
??=  
1340
6.3 Evaluation Results 
With our training set we trained one resolution 
model using GBDT (named as WikiRes-c) and 
another resolution model using GBRank (named 
as WikiRes-r). The models were evaluated along 
with the following systems. 
1) Baseline-r: each surface form s is randomly 
mapped to oow or a candidate entry for s in 
EntSfDict. 
2) Baseline-p: each surface form s is mapped 
to the candidate entry e for s with the highest 
commonness score. The commonness score is 
linear combination of the probability of s being 
mapped to e estimated from different data 
sources. The commonness score is among the 
features used in WikiRes-c and WikiRes-r.  
3) Baseline-m: we implemented the approach 
brought by Cucerzan (2007) based on our best 
understanding. Since we use a different version 
of Wikipedia and a different entity recognition 
approach, the evaluation result differs from the 
result presented in their paper. But we believe 
our implementation follows the algorithm de-
scribed in their paper. 
In Table 1 we present the performance for 
each system on the Yahoo! News test set and the 
MSNBC test set. The performance of WikiRes-c 
and WikiRes-r are computed after we apply the 
thresholds selected on the tuning set described in 
Section 6.1. In the upper half of Table 1, the 
three baselines use the thresholds that lead to the 
best F1 on the Yahoo! News test set. In the lower 
half of Table 1, the three baselines use the thre-
sholds that lead to the best F1 on the MSNBC 
test set. 
Among the three baselines, Baseline-r has the 
lowest performance. Baseline-m uses a few con-
text-sensitive features and Baseline-p uses a con-
text-independent feature. These two types of fea-
tures are both useful, but Baseline-p shows better 
performance, probably because the surface forms 
in our test sets are dominated by common senses. 
In our resolution models, these features are com-
bined together with many other features calcu-
lated from different large-scale data sources and 
on different granularity levels. As shown in Ta-
ble 1, both of our resolution solutions substan-
tially outperform other systems.  Furthermore, 
WikiRes-c and WikiRes-r have similar perfor-
mance. 
 
 Precision Recall F1 Accuracy p-value 
Yahoo! News Test Set 
Baseline-r 47.023 60.831 53.043 47.023 0 
Baseline-p 73.869 88.157 80.383 73.175 5.2e-78 
Baseline-m 62.240 80.517 70.208 62.240 1.3e-160
WikiRes-r 83.406 88.858 86.046 80.717 0.012 
WikiRes-c 85.038 87.831 86.412 81.463 --- 
MSNBC Test Set 
Baseline-r 60.272 64.545 62.335 60.272 8.9e-19 
Baseline-p 82.292 86.182 84.192 82.003 0.306 
Baseline-m 78.947 84.545 81.651 78.947 0.05 
WikiRes-r 88.785 86.364 87.558 84.550 0.102 
WikiRes-c 88.658 85.273 86.932 83.192 --- 
Table 1. Performance on the Yahoo! News Test 
Set and the MSNBC Test set 
 
Figure 1. Precision-recall on the Yahoo! News 
Test Set and the MSNBC Test Set 
 
We compared WikiRes-c with each competitor 
and from the statistical significance test results in 
the last column of Table 1 we see that on the Ya-
hoo! News test set WikiRes-c significantly out-
performs others. The p-values for the MSNBC 
test set are much higher than for the Yahoo! 
News test set because the MSNBC test set is 
much smaller. 
Attempting to address this point, we see that 
the F1 values of WikiRes on the MSNBC test set 
and on the Yahoo! News test set only differs by a 
couple percentage points, although, these test 
sets were created independently. This suggests 
the objectivity of our method for creating the 
Yahoo! News test set and provides a way to 
measure resolution model performance on what 
1341
would occur in a general news corpus in a statis-
tically significant manner. 
In Figure 1 we present the precision-recall 
curves on the Yahoo! News and the MSNBC test 
sets. We see that our resolution models are sub-
stantially better than the other two baselines at 
any particular precision or recall value on both 
test sets. Baseline-r is not included in the com-
parison since it does not have the tradeoff be-
tween precision and recall. We find the preci-
sion-recall curve of WikiRes-r is very similar to 
WikiRes-c at the lower precision area, but its re-
call is much lower than other systems after preci-
sion reaches around 90%. So, in Figure 1 the 
curves of WikiRes-r are truncated at the high pre-
cision area. 
In Table 2 we compare the performance of 
out-of-Wikipedia prediction. The comparison is 
done on the Yahoo! News test set only, since 
there are only 40 surface forms of oow case in 
the MSNBC test set. Each system?s threshold is 
the same as that used for the upper half of Table 
1. The results show our models have substantial-
ly higher precision and recall than Baseline-p and 
Baseline-m. From the statistical significance test 
results in the last column, we can see that Wi-
kiRes-c significantly outperforms Baseline-p and 
Baseline-m. Also, our current approaches still 
have room to improve in the area of out-of-
Wikipedia prediction.  
We also evaluated our models on a Wikipedia 
hold-out set. The model performance is greater 
than that obtained from the previous two test sets 
because the hold-out set is more similar to the 
training data source itself. Again, our models 
perform better than others. 
From the feature importance lists of our 
GBDT model and GBRank model, we find that 
the commonness features, the features based on 
Wikipedia entries? co-occurrence representation 
and the corresponding differentiation features are 
the most important. 
 
 Precision Recall F1 p-value 
Baseline-p 64.907 22.152 33.03 1.6e-20 
Baseline-m 47.207 44.78 45.961 1.3e-34 
WikiRes-r 68.166 52.994 59.630 0.084 
WikiRes-c 67.303 59.777 63.317 ---
Table 2. Performance of Out-of-Wikipedia Pre-
diction on the Yahoo! News Test Set 
7 Conclusions 
We have described a method of learning to re-
solve surface forms to Wikipedia entries. Using 
this method we can enrich the unstructured doc-
uments with structured knowledge from Wikipe-
dia, the largest knowledge base in existence. The 
enrichment makes it possible to represent a doc-
ument as a machine-readable network of senses 
instead of just a bag of words. This can supply 
critical semantic information useful for next-
generation information retrieval systems and oth-
er text mining applications. 
Our resolution models use an extensive set of 
novel features and are leveraged by a machine 
learned approach that depends only on a purely 
automated training data generation facility. Our 
methodology can be applied to any other lan-
guage that has Wikipedia and Web data available 
(after modifying the simple capitalization rules in 
Section 4.1). Our resolution models can be easily 
and quickly retrained with updated data when 
Wikipedia and the relevant Web data are 
changed. 
For future work, it will be important to inves-
tigate other approaches to better predict oow. 
Adding global constraints on resolutions of the 
same term at multiple locations in the same doc-
ument may also be important. Of course, devel-
oping new features (such as part-of-speech, 
named entity type, etc) and improving training 
data quality is always critical, especially for so-
cial content sources such as those from Twitter. 
Finally, directly demonstrating the degree of ap-
plicability to other languages is interesting when 
accounting for the fact that the quality of Wiki-
pedia is variable across languages. 
References 
Bagga, Amit and Breck Baldwin. 1998. Entity-based 
cross-document coreferencing using the Vector 
Space Model. Proceedings of the 17th interna-
tional conference on Computational linguis-
tics. 
Bunescu, Razvan and Marius Pa?ca. 2006. Using En-
cyclopedic Knowledge for Named Entity Disam-
biguation. Proceedings of the 11th Conference 
of the European Chapter of the Association of 
Computational Linguistics (EACL-2006). 
Cucerzan, Silviu. 2007. Large-Scale Named Entity 
Disambiguation Based on Wikipedia Data. Pro-
1342
ceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language 
Processing and Computational Natural Lan-
guage Learning. 
Fleischman, Ben Michael and Eduard Hovy. 2004. 
Multi-Document Person Name Resolution. Pro-
ceesing of the Association for Computational 
Linguistics. 
Friedman, J. H. 2001. Stochastic gradient boosting. 
Computational Statistics and Data Analysis, 
38:367?378. 
Han, Xianpei and Jun Zhao 2009. Named Entity Dis-
ambiguation by Leveraging Wikipedia Semantic 
Knowledge. Proceedings of the 31st annual in-
ternational ACM SIGIR conference on Re-
search and development in information re-
trieval. 
Mann, S. Gidon and David Yarowsky. 2003. Unsu-
pervised Personal Name Disambiguation. Pro-
ceedings of the seventh conference on Natural 
language learning at HLT-NAACL 2003. 
Milne, David and Ian H. Witten. 2008a. Learning to 
Link with Wikipedia. In Proceedings of the 
ACM Conference on Information and Know-
ledge Management (CIKM'2008). 
Milne, David and Ian H. Witten. 2008b. An effective, 
low-cost measure of semantic relatedness obtained 
from Wikipedia links. Proceedings of the first 
AAAI Workshop on Wikipedia and Artificial 
Intelligence. 
 Pedersen, Ted, Amruta Purandare and Anagha Kul-
karni. 2005. Name Discrimination by Clustering 
Similar Contexts. Proceedings of the Sixth In-
ternational Conference on Intelligent Text 
Processing and Computational Linguistics 
(2005). 
Ravin, Y. and Z. Kazi. 1999. Is Hillary Rodham Clin-
ton the President? In Association for Computa-
tional Linguistics Workshop on Coreference 
and its Applications. 
Yarowsky, David. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. Pro-
ceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics, 
pages 189-196.  
Zheng, Zhaohui, K. Chen, G. Sun, and H. Zha. 2007. 
A regression framework for learning ranking func-
tions using relative relevance judgments. Proceed-
ings of the 30th annual international ACM 
SIGIR conference on Research and develop-
ment in information retrieval, pages 287-294. 
 
 
1343
