Coling 2008: Companion volume ? Posters and Demonstrations, pages 19?22
Manchester, August 2008
Phrasal Segmentation Models for Statistical Machine Translation
Graeme Blackwood, Adri
`
a de Gispert, William Byrne
Machine Intelligence Laboratory
Department of Engineering, Cambridge University
Trumpington Street, Cambridge, CB2 1PZ, U.K.
{gwb24|ad465|wjb31}@cam.ac.uk
Abstract
Phrasal segmentation models define a
mapping from the words of a sentence
to sequences of translatable phrases. We
discuss the estimation of these models
from large quantities of monolingual train-
ing text and describe their realization as
weighted finite state transducers for incor-
poration into phrase-based statistical ma-
chine translation systems. Results are re-
ported on the NIST Arabic-English trans-
lation tasks showing significant comple-
mentary gains in BLEU score with large
5-gram and 6-gram language models.
1 Introduction
In phrase-based statistical machine transla-
tion (Koehn et al, 2003) phrases extracted from
word-aligned parallel data are the fundamental
unit of translation. Each phrase is a sequence
of contiguous translatable words and there is no
explicit model of syntax or structure.
Our focus is the process by which a string of
words is segmented as a sequence of such phrases.
Ideally, the segmentation process captures two as-
pects of natural language. Firstly, segmentations
should reflect the underlying grammatical sentence
structure. Secondly, common sequences of words
should be grouped as phrases in order to preserve
context and respect collocations. Although these
aspects of translation are not evaluated explicitly,
phrases have been found very useful in transla-
tion. They have the advantage that, within phrases,
words appear as they were found in fluent text.
However, reordering of phrases in translation can
lead to disfluencies. By defining a distribution over
possible segmentations, we hope to address such
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
disfluencies. A strength of our approach is that it
exploits abundantly available monolingual corpora
that are usually only used for training word lan-
guage models.
Most prior work on phrase-based statistical lan-
guage models concerns the problem of identifying
useful phrasal units. In (Ries et al, 1996) an iter-
ative algorithm selectively merges pairs of words
as phrases with the goal of minimising perplex-
ity. Several criteria including word pair frequen-
cies, unigram and bigram log likelihoods, and a
correlation coefficient related to mutual informa-
tion are compared in (Kuo and Reichl, 1999). The
main difference between these approaches and the
work described here is that we already have a defi-
nition of the phrases of interest (i.e. the phrases of
the phrase table extracted from parallel text) and
we focus instead on estimating a distribution over
the set of possible alternative segmentations of the
sentence.
2 Phrasal Segmentation Models
Under the generative model of phrase-based statis-
tical machine translation, a source sentence s
I
1
gen-
erates sequences u
K
1
= u
1
, . . . , u
K
of source lan-
guage phrases that are to be translated. Sentences
cannot be segmented into phrases arbitrarily: the
space of possible segmentations is constrained by
the contents of the phrase table which consists of
phrases found with translations in the parallel text.
We start initially with a distribution in which seg-
mentations assume the following dependencies:
P (u
K
1
,K|s
I
1
) = P (u
K
1
|K, s
I
1
)P (K|I). (1)
The distribution over the number of phrases K is
chosen to be uniform, i.e. P (K|I) = 1/I, K ?
{1, 2, . . . , I}, and all segmentations are considered
equally likely. The probability of a particular seg-
mentation is therefore
P (u
K
1
|K, s
I
1
) =
{
C(K, s
I
1
) if u
K
1
= s
I
1
0 otherwise
(2)
19
where C(K, s
I
1
) is chosen to ensure normalisation
and the phrases u
1
, . . . , u
K
are found in the phrase
table. This simple model of segmentation has been
found useful in practice (Kumar et al, 2006).
Our goal is to improve upon the uniform seg-
mentation of equation (2) by estimating the phrasal
segmentation model parameters from naturally oc-
curing phrase sequences in a large monolingual
training corpus. An order-n phrasal segmentation
model assigns a probability to a phrase sequence
u
K
1
according to
P (u
K
1
|K, s
I
1
) =
K
?
k=1
P (u
k
|u
k?1
1
,K, s
I
1
) ?
{
C(K, s
I
1
)
?
K
k=1
P (u
k
|u
k?1
k?n+1
) if u
K
1
= s
I
1
0 otherwise
(3)
where the approximation is due to the Markov as-
sumption that only the most recent n ? 1 phrases
are useful when predicting the next phrase. Again,
each u
k
must be a phrase with a known transla-
tion. For a fixed sentence s
I
1
, the normalisation
term C(K, s
I
1
) can be calculated. In translation,
however, calculating this quantity becomes harder
since the s
I
1
are not fixed. We therefore ignore
the normalisation and use the unnormalised like-
lihoods as scores.
2.1 Parameter Estimation
We focus on first-order phrasal segmentation mod-
els. Although we have experimented with higher-
order models we have not yet found them to yield
improved translation.
Let f(u
k?1
, u
k
) be the frequency of occurrence
of a string of words w
j
i
in a very large training
corpus that can be split at position x such that
i < x ? j and the substrings w
x?1
i
and w
j
x
match
precisely the words of two phrases u
k?1
and u
k
in
the phrase table. The maximum likelihood proba-
bility estimate for phrase bigrams is then their rel-
ative frequency:
?
P (u
k
|u
k?1
) =
f(u
k?1
, u
k
)
f(u
k?1
)
. (4)
These maximum likelihood estimates are dis-
counted and smoothed with context-dependent
backoff such that
P (u
k
|u
k?1
) =
{
?(u
k?1
, u
k
)
?
P (u
k
|u
k?1
) if f(u
k?1
, u
k
) > 0
?(u
k?1
)P (u
k
) otherwise
(5)
where ?(u
k?1
, u
k
) discounts the maximum like-
lihood estimates and the context-specific backoff
weights ?(u
k?1
) are chosen to ensure normalisa-
tion.
3 The Transducer Translation Model
The Transducer Translation Model (TTM) (Kumar
and Byrne, 2005; Kumar et al, 2006) is a gener-
ative model of translation that applies a series of
transformations specified by conditional probabil-
ity distributions and encoded as Weighted Finite
State Transducers (Mohri et al, 2002).
The generation of a target language sentence
t
J
1
starts with the generation of a source lan-
guage sentence s
I
1
by the source language model
P
G
(s
I
1
). Next, the source language sentence is
segmented according to the uniform phrasal seg-
mentation model distribution P
W
(u
K
1
,K|s
I
1
) of
equation (2). The phrase translation and reorder-
ing model P
?
(v
R
1
|u
K
1
) generates the reordered se-
quence of target language phrases v
R
1
. Finally,
the reordered target language phrases are trans-
formed to word sequences t
J
1
under the target
segmentation model P
?
(t
J
1
|v
R
1
). These compo-
nent distributions together form a joint distribu-
tion over the source and target language sentences
and their possible intermediate phrase sequences
as P (t
J
1
, v
R
1
, u
K
1
, s
I
1
).
In translation under the generative model, we
start with the target sentence t
J
1
in the foreign lan-
guage and search for the best source sentence s?
I
1
.
Encoding each distribution as a WFST leads to a
model of translation as a series of compositions
L = G ?W ? ? ? ? ? T (6)
in which T is an acceptor for the target language
sentence and L is the word lattice of translations
obtained during decoding. The most likely trans-
lation s?
I
1
is the path in L with least cost.
The above approach generates a word lattice L
under the unweighted phrasal segmentation model
of equation (2). In the initial experiments reported
here, we apply the weighted phrasal segmentation
model via lattice rescoring. We take the word lat-
tice L and compose it with the unweighted trans-
ducer W to obtain a lattice of phrases L ?W ; this
lattice contains phrase sequences and translation
scores consistent with the initial translation. We
also extract the complete list of phrases relevant to
each translation.
20
We then wish to apply the phrasal segmentation
model distribution of equation (3) to this phrase
lattice. The conditional probabilities and backoff
structure defined in equation (5) can be encoded
as a weighted finite state acceptor (Allauzen et al,
2003). In this acceptor, ?, states encode histories
and arcs define the bigram and backed-off unigram
phrase probabilities. We note that the raw counts
of equation (4) are collected prior to translation
and the first-order probabilities are estimated only
for phrases found in the lattice.
The phrasal segmentation model is composed
with the phrase lattice and projected on the in-
put to obtain the rescored word lattice L
?
=
(L ?W ) ??. The most likely translation after ap-
plying the phrasal segmentation model is found as
the path in L
?
with least cost. Apart from likeli-
hood pruning when generating the original word
lattice, the model scores are included correctly in
translation search.
4 System Development
We describe experiments on the NIST Arabic-
English machine translation task and apply phrasal
segmentation models in lattice rescoring.
The development set mt02-05-tune is formed
from the odd numbered sentences of the NIST
MT02?MT05 evaluation sets; the even numbered
sentences form the validation set mt02-05-test.
Test performance is evaluated using the NIST sub-
sets from the MT06 evaluation: mt06-nist-nw for
newswire data and mt06-nist-ng for newsgroup
data. Results are also reported for the MT08 evalu-
ation. Each set contains four references and BLEU
scores are computed for lower-case translations.
The uniformly segmented TTM baseline system
is trained using all of the available Arabic-English
data for the NIST MT08 evaluation
1
. In first-pass
translation, decoding proceeds with a 4-gram lan-
guage model estimated over the parallel text and a
965 million word subset of monolingual data from
the English Gigaword Third Edition. Minimum
error training (Och, 2003) under BLEU optimises
the decoder feature weights using the development
set mt02-05-tune. In the second pass, 5-gram and
6-gram zero-cutoff stupid-backoff (Brants et al,
2007) language models estimated using 4.7 billion
words of English newswire text are used to gener-
ate lattices for phrasal segmentation model rescor-
ing. The phrasal segmentation model parameters
1
http://www.nist.gov/speech/tests/mt/2008/
mt02-05-tune mt02-05-test
TTM+MET 48.9 48.6
+6g 51.9 51.7
+6g+PSM 52.7 52.7
Table 2: BLEU scores for phrasal segmentation
model rescoring of 6-gram rescored lattices.
are trained using a 1.8 billion word subset of the
same monolingual training data used to build the
second-pass word language model. A phrasal seg-
mentation model scale factor and phrase insertion
penalty are tuned using the development set.
5 Results and Analysis
First-pass TTM translation lattices generated with
a uniform segmentation obtain baseline BLEU
scores of 48.9 for mt02-05-tune and 48.6 for
mt02-05-test. In our experiments we demon-
strate that phrasal segmentation models continue
to improve translation even for second-pass lat-
tices rescored with very large zero-cutoff higher-
order language models. Table 1 shows phrasal seg-
mentation model rescoring of 5-gram lattices. The
phrasal segmentation models consistently improve
the BLEU score: +1.1 for both the development
and validation sets, and +1.4 and +0.4 for the in-
domain newswire and out-of-domain newsgroup
test sets. Rescoring MT08 gives gains of +0.9 on
mt08-nist-nw and +0.3 on mt08-nist-ng.
For a limited quantity of training data it is not
always possible to improve translation quality sim-
ply by increasing the order of the language model.
Comparing tables 1 and 2 shows that the gains in
moving from a 5-gram to a 6-gram are small. Even
setting aside the practical difficulty of estimating
and applying such higher-order language models,
it is doubtful that further gains could be had simply
by increasing the order. That the phrasal segmenta-
tion models continue to improve upon the 6-gram
lattice scores suggests they capture more than just
a longer context and that they are complementary
to word-based language models.
The role of the phrase insertion penalty is to
encourage longer phrases in translation. Table 3
shows the effect of tuning this parameter. The
upper part of the table shows the BLEU score,
brevity penalty and individual n-gram precisions.
The lower part shows the total number of words
in the output, the number of words translated as
a phrase of the specified length, and the average
number of words per phrase. When the insertion
21
mt02-05-tune mt02-05-test mt06-nist-nw mt06-nist-ng mt08-nist-nw mt08-nist-ng
TTM+MET 48.9 48.6 46.1 35.2 48.4 33.7
+5g 51.5 51.5 48.4 36.7 49.1 36.4
+5g+PSM 52.6 52.6 49.8 37.1 50.0 36.7
Table 1: BLEU scores for phrasal segmentation model rescoring of 5-gram rescored lattices.
PIP -4.0 -2.0 0.0 2.0 4.0
BLEU 48.6 50.1 51.1 49.9 48.7
BP 0.000 0.000 0.000 -0.034 -0.072
1g 82.0 83.7 84.9 85.7 86.2
2g 57.3 58.9 59.9 60.5 61.1
3g 40.8 42.2 43.1 43.6 44.2
4g 29.1 30.3 31.1 31.5 32.0
words 70550 66964 63505 60847 58676
1 58840 46936 25040 15439 11744
2 7606 12388 18890 19978 18886
3 2691 4890 11532 13920 14295
4 860 1820 5016 6940 8008
5 240 450 1820 2860 3500
6+ 313 480 1207 1710 2243
w/p 1.10 1.21 1.58 1.86 2.02
Table 3: Effect of phrase insertion penalty (PIP)
on BLEU score, brevity penalty (BP), individual
n-gram precisions, phrase length distribution, and
average words per phrase (w/p) for mt02-05-tune.
penalty is too low, single word phrases dominate
the output and any benefits from longer context or
phrase-internal fluency are lost. As the phrase in-
sertion penalty increases, there are large gains in
precision at each order and many longer phrases
appear in the output. At the optimal phrase in-
sertion penalty, the average phrase length is 1.58
words and over 60% of the translation output is
generated from multi-word phrases.
6 Discussion
We have defined a simple model of the phrasal seg-
mentation process for phrase-based SMT and esti-
mated the model parameters from naturally occur-
ring phrase sequence examples in a large training
corpus. Applying first-order models to the NIST
Arabic-English machine translation task, we have
demonstrated complementary improved transla-
tion quality through exploitation of the same abun-
dantly available monolingual data used for training
regular word-based language models.
Comparing the in-domain newswire and out-
of-domain newsgroup test set performance shows
the importance of choosing appropriate data for
training the phrasal segmentation model param-
eters. When in-domain data is of limited avail-
ability, count mixing or other adaptation strategies
may lead to improved performance.
Acknowledgements
This work was supported in part under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-06-
C-0022.
References
Allauzen, Cyril, Mehryar Mohri, and Brian Roark.
2003. Generalized algorithms for constructing sta-
tistical language models. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 557?564.
Brants, Thorsten, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on EMNLP and CoNLL,
pages 858?867.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference for Computational
Linguistics on Human Language Technology, pages
48?54, Morristown, NJ, USA.
Kumar, Shankar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In Proceedings of the conference on HLT
and EMNLP, pages 161?168.
Kumar, Shankar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer translation
template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Kuo, Hong-Kwang Jeff and Wolfgang Reichl. 1999.
Phrase-based language models for speech recogni-
tion. In Sixth European Conference on Speech Com-
munication and Technology, pages 1595?1598.
Mohri, Mehryar, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech
recognition. In Computer Speech and Language,
volume 16, pages 69?88.
Och, Franz Josef. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 160?167, Morristown, NJ, USA.
Ries, Klaus, Finn Dag Bu, and Alex Waibel. 1996.
Class phrase models for language modeling. In Pro-
ceedings of the 4th International Conference on Spo-
ken Language Processing.
22
