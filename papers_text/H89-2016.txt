Data Col lect ion And Evaluat ion 
David S. Pal lett  
Nat ional  Inst i tute of Standards and Technology 
ABSTRACT 
This session focussed on two inter-re lated issues: (I) per formance 
assessment for spoken language systems and (2) exper ience to date 
in speech corpora col lect ion for these systems. The session 
inc luded formal  p resentat ions  f rom representat ives  of SRI 
International,  MIT's Laboratory for Computer Science, BBN Systems 
and Technologies Corporation, and Carnegie Mel lon Univers i ty 's  
School of Computer Science. 
SESSION OVERVIEW 
Mater ial  presented by Patti Price et al of SRI Internat ional  
descr ibed  col lect ion of more than 12 hours of human-human 
interact ive problem solving in the air travel p lanning domain. SRI 
has made use of this data to def ine an initial vocabulary  and to 
def ine an interface for this domain. Recent efforts to conduct 
"Wizard" s imulat ions of human-system interact ions in this domain 
were described. Price noted that in the natura l ly  occurr ing 
dialogues it is rare that a database query occurs. Rather, the 
user wil l  state a plan (e.g., "I need to make a reservation") and 
then provide, in small steps, the pieces of information necessary 
for the "agent" to help accompl ish the plan. Breaking the dia logue 
into small pieces of information, asking for frequent confirmation, 
and having the agent sometimes take an act ive role all seem to play 
a role in making the dialogue efficient. These f indings suggest to 
Price that both arguments of naturalness (a strong mot ivat ing 
factor for the use of natural language in the first place) and 
eff ic iency argue for human-system interact ions that wil l  y ield 
large numbers of sentences that are not database queries. In view 
of this, it was argued that performance assessment procedures must 
go beyond considerat ion of database query-answer pairs and include 
other mechanisms for assessment, such as the template-based method 
of the MUCK-2 approach. 
In the first of the two papers by the group at MIT's Laboratory 
for Computer Science, Zue et al descr ibe the col lect ion and 
pre l iminary analysis of a spontaneous speech corpus using a 
s imulated human-system dialogue with the VOYAGER spoken language 
system. The Voyager system is made up of three components: (i) the 
SUMMIT speech recognit ion system which converts the speech into a 
set of word hypotheses, (2) the TINA natural  language component, 
which provides a l inguist ic interpretat ion and a parse tree that is 
t ranslated into a query language form, and (3) a modi f ied vers ion 
of the direct ion assistance program (developed by J im Davis of 
115 
MIT's Media Laboratory). Spontaneous speech data were recorded 
from I00 subjects, and each subject was also recorded reading 
orthographic transcriptions of their spontaneous speech (minus 
false starts, hesitations and filled pauses). In col lecting the 
spontaneous speech, a simulation was used that replaced the SUMMIT 
speech recognition component with a human typing the orthographic 
transcription of the spontaneous speech into the remainder of the 
system. The speech data, consisting of nearly i0,000 utterances, 
were subsequently digit ized and transcribed. Comparison of 
corresponding spontaneous and read speech data show that the 
spontaneous utterances \[i.e., sentences\] are longer than their read 
counterparts, and that there is much more variabi l i ty in the 
spontaneous speech. Pauses that are more frequent and longer 
account for much of the longer duration characterizing spontaneous 
speech. Non-speech vocalizations found in the spontaneous speech 
include mouth clicks, breath noise and fil led pauses such as "um", 
"uh", or "ah". False starts occurred in almost 4% of the 
spontaneous sentences. The words following false starts in the 
spontaneous speech included "back ups" (to the same as the 
\[apparent\]  in tended word),  a d i f fe rent  word in the same 
\[syntactic\] category, a word from a new category, or a back up to 
repeat \[several\] words already uttered. This study concludes that 
the process of data collection in this simulation was relatively 
straightforward, and that incremental data col lection can "be quite 
effective for development of spoken language systems". 
In the second paper from Zue's group, a number of performance 
assessment issues are raised. It is suggested that spoken 
language systems should be evaluated along several dimensions. The 
dimensions include (i) accuracy of the system and its various 
modules (e.g., phonetic, word and sentence accuracy as well as 
l ingu is t i c  and task completion accuracy), (2) coverage and 
habitability, (3) flexibility, and (4) eff iciency (e.g. task 
completion time). Zue et al note that evaluations of accuracy 
inevitably involve the use of references involving varying degrees 
of subjectivity. At higher levels, system outputs may involve more 
abst rac t  information, complicating the process of automatic 
comparison with a reference output. The prel iminary evaluation of 
the MIT Voyager system includes evaluation of the Summit speech 
recognition component. Using a 570 word lexicon and a word-pair  
grammar with a test set perplexity of 22 to constrain the search 
space, word accuracies of approximately 86% and sentence accuracies 
of 49% for sentences of about 8 words per sentence are reported. 
Analyses of natural language performance focussed on coverage in 
terms of percentage of sentences that could be parsed and 
perplexity. Overall system performance has been evaluated by 
several means, including a panel of naive users to judge the 
appropriateness of the responses of the system as well as the 
queries made by the subjects. Although data were avai lable only 
for a small number of subjects, it appeared that "appropriate" 
responses together with "verbose, but otherwise correct" responses 
116 
accounted for approx imate ly  85% of the responses. About  87% of the 
user quer ies were judged reasonable. The issue of e f f ic iency was 
not addressed, since the system under d iscuss ion operates in about 
12 t imes real time, prec luding real - t ime interact ive dialogues. 
The paper by Boisen, Ramshaw and Bates from BBN descr ibes "an 
automatic, essent ia l ly  domain- independent"  means of evaluat ing 
spoken language systems that provide answers to quer ies from a 
database. This proposal  was developed out of an unders tand ing  that 
some consensus has been achieved on a number of issues including: 
(i) "Common evaluat ion involves work ing on a common domain (or 
domains). A common corpus of development quer ies (in both spoken 
and t ranscr ibed form), and answers to those quer ies in some 
canonical  format, are therefore required.",  (2) "One basis for 
system evaluat ion wil l  be answers to quer ies from a common 
database, perhaps in addit ion to other measures."  (3) "Automatic 
evaluat ion methods should be used whenever  they are feasible". The 
proposal  for evaluat ion on a DARPA common task has as a key 
component a program des ignated a "Comparator" that compares 
canonical  answers to the answers suppl ied by a spoken language 
system. Answers are to be expressed in the form of a "Common 
Answer Spec i f icat ion (CAS)", as descr ibed in the proposal.  The 
proposed comparator  is a Common LISP program for compar ing system 
output expressed in CAS format with canonical  answers. Much as Zue 
et al note, Boisen et al note that "evaluat ion requires human 
judgement, and therefore the best we can expect from a program is 
comparison, not evaluation". BBN has prepared a small  corpus of 
queries and their  answers for the (proposed) "Common Personnel  
Database" to i l lustrate the use of the CAS format and as a check on 
the c lar i ty  and completeness of the CAS. Finally, Boisen et al 
note that the co l lect ion of any corpus "for SLS development  and 
test ing wil l  be more useful  if it is easi ly  sub-d iv ided into easier 
and harder  cases", and they propose candidate categor izat ions,  
start ing from a default  case in which no extra-sentent ia l  context 
is required, to the more d i f f icu l t  categor ies involv ing "local" 
extra-sentent ia l  reference, el l ips is  cases, non- local  references 
and \[even\] more complex cases. It is argued that these pr inc ip les  
of ca tegor i za t ion  shou ld  be fo l lowed in implement ing SLS 
evaluations. 
In BBN's second paper in this session, Derr and Schwartz descr ibed 
the development of a new grammar that can be used in assess ing the 
performance of speech recognit ion systems. It is a "stat ist ical  
f i rst -order class grammar" that has been developed for two 
di f ferent  task domains (the DARPA Resource Management  domain and a 
2000 word personnel  database domain). Derr and Schwartz argue that 
the exist ing two grammatical  condit ions (the "no grammar" or null 
grammar and the word-pa i r  grammar cases) "suffer from several  
inadequacies".  The null grammar provides "only a worst -case 
recognit ion test point", whi le  the word-pa i r  grammar not only 
excludes "many reasonable word sequences", but the use of the word- 
pair  grammar y ie lds such high recognit ion per formance that re l iable 
measurement  of system improvements (i.e. s tat is t ica l ly  s igni f icant  
inferences of improvements) cannot be obtained wi thout  use of very 
117 
large development and evaluation test sets. Given a priori 
assignment of words to classes, the statistics of BBN's class 
grammar were counted directly from training data by counting the 
number of transitions from each class to each other class. Using 
99 classes for the lexicon of the DARPA Resource Management task 
domain, the class grammar provides a perplexity of approximately 
75 and recognition error rates that are in-between the results for 
the word-pair and null grammars. The increased error rate is 
noteworthy not for the fact that recognition performance is 
degraded per se, but for the fact that, using this grammar, 
incrementa l  improvements  may be shown to be statist ical ly 
signif icant using smaller test sets and less test and machine 
time. \[Following this presentation, it was observed that 
incremental improvements could also be shown to be statist ical ly 
significant with the null grammar, but without the apparent 
benefit of higher word accuracies (i.e., the performance using the 
class grammar is shown to be somewhat better than the worst-case 
results for the null grammar). The critical issue is one of 
de f in ing  the des iderata  for constraining grammars and the 
relationship of these grammars to those that are in some sense 
"natural" to the task sub-language.\] Further advantages outl ined 
for this approach include the fact that it is readily adapted to 
new task domains and that it is "tunable" (i.e., can be set to 
provide varying perplexity) by varying the number of classes in 
the grammar. 
The paper by Rudnicky et al from CMU presents results of a study 
of a spoken language interface involving a complex problem- 
solving task. A group of users was asked to perform 40 spreadsheet 
tasks, each successive task being carried out in a different 
modality (speech or typing). The voice spreadsheet consists of the 
UNIX-based spreadsheet program "SC" interfaced to a recognizer 
embodying the Sphinx speech recognition technology. The CMU study 
is noteworthy for the fact that, of the systems discussed in this 
session, it is the only system to provide near-real time 
interactions without the intervention of a "Wizard". For a task 
vocabulary of 271 words \[and a constraining grammar with a 
perplexity of 52\], word accuracies of 92.7% to 94.9% were achieved 
for spontaneous and read speech. Analyses conducted by Rudnicky et 
al. include discussions of semantic accuracy, grammatical i ty and 
language habitability. Spontaneous speech events are discussed in 
three categor ies :  lexical ,  ext ra - lex ica l ,  and non-lexical. 
Detailed analyses also include "the time it takes to do things". 
Since the implementation used at CMU for these studies processed 
speech in about 2 times real time, it is perhaps not surprising 
that total task time was greater for speech input than keyboard. 
However,  by accounting for processing "overhead" times and 
proposing a halving of the present error rate, Rudnicky et al 
estimate that task completion times for speech and keyboard should 
be "equivalent". Current efforts are directed toward achieving a 
true real-time implementation and improving system accuracy. 
118 
