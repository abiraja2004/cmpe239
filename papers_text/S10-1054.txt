Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 242?247,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
UBA: Using Automatic Translation and Wikipedia for
Cross-Lingual Lexical Substitution
Pierpaolo Basile
Dept. of Computer Science
University of Bari ?Aldo Moro?
Via E. Orabona, 4
70125 Bari (ITALY)
basilepp@di.uniba.it
Giovanni Semeraro
Dept. of Computer Science
University of Bari ?Aldo Moro?
Via E. Orabona, 4
70125 Bari (ITALY)
semeraro@di.uniba.it
Abstract
This paper presents the participation of the
University of Bari (UBA) at the SemEval-
2010 Cross-Lingual Lexical Substitution
Task. The goal of the task is to substi-
tute a word in a language L
s
, which oc-
curs in a particular context, by provid-
ing the best synonyms in a different lan-
guage L
t
which fit in that context. This
task has a strict relation with the task of
automatic machine translation, but there
are some differences: Cross-lingual lexi-
cal substitution targets one word at a time
and the main goal is to find as many good
translations as possible for the given tar-
get word. Moreover, there are some con-
nections with Word Sense Disambiguation
(WSD) algorithms. Indeed, understand-
ing the meaning of the target word is nec-
essary to find the best substitutions. An
important aspect of this kind of task is
the possibility of finding synonyms with-
out using a particular sense inventory or a
specific parallel corpus, thus allowing the
participation of unsupervised approaches.
UBA proposes two systems: the former is
based on an automatic translation system
which exploits Google Translator, the lat-
ter is based on a parallel corpus approach
which relies on Wikipedia in order to find
the best substitutions.
1 Introduction
The goal of the Cross-Lingual Lexical Substitu-
tion (CLLS) task is to substitute a word in a lan-
guage L
s
, which occurs in a particular context,
by providing the best substitutions in a different
language L
t
. In SemEval-2010 the source lan-
guage L
s
is English, while the target language L
t
is Spanish. Clearly, this task is related to Lexical
Substitution (LS) (McCarthy and Navigli, 2007)
which consists in selecting an alternative word for
a given one in a particular context by preserving
its meaning. The main difference between the LS
task and the CLLS one is that in LS source and
target languages are the same. CLLS is not a easy
task since neither a list of candidate words nor a
specific parallel corpus are supplied by the orga-
nizers. However, this opens the possibility of us-
ing several knowledge sources, instead of a single
one fixed by the task organizers. Therefore, the
system must identify a set of candidate words in
L
t
and then select only those words which fit the
context. From another point of view, the cross-
lingual nature of the task allows to exploit auto-
matic machine translation methods, hence the goal
is to find as many good translations as possible for
the given target word. A thorough description of
the task can be found in (Mihalcea et al, 2010;
Sinha et al, 2009).
To easily understand the task, an example fol-
lows. Consider the sentence:
During the siege, George Robertson had
appointed Shuja-ul-Mulk , who was a bright
boy only 12 years old and the youngest
surviving son of Aman-ul-Mulk, as the ruler
of Chitral.
In the previous sentence the target word is
?bright?. Taking into account the meaning of
the word ?bright? in this particular context, the
best substitutions in Spanish are: ?inteligente?,
?brillante? and ?listo?.
We propose two systems to tackle the problem
of CLLS: the first is based on an automatic trans-
lation system which exploits the API of Google
Translator
1
, the second is based on a parallel cor-
pus approach which relies on Wikipedia. In par-
ticular, in the second approach we use a struc-
tured version of Wikipedia called DBpedia (Bizer
1
http://code.google.com/p/google-api-translate-java/
242
et al, 2009). Both systems adopt several lexical
resources to select the list of possible substitutions
for a given word. Specifically, we use three differ-
ent dictionaries: Google Dictionary, Babylon Dic-
tionary and Spanishdict. Then, we combine the
dictionaries into a single one, as described in Sec-
tion 2.1.
The paper is organized as follows: Section 2 de-
scribes the strategy we adopted to tackle the CLLS
task, while results of an experimental session we
carried out in order to evaluate the proposed ap-
proaches are presented in Section 3. Conclusions
are discussed in Section 4.
2 Methodology
Generally speaking, the problem of CLLS can be
coped with a strategy which consists of two steps,
as suggested in (Sinha et al, 2009):
? candidate collection: in this step several re-
sources are queried to retrieve a list of po-
tential translation candidates for each target
word and part of speech;
? candidate selection: this step concerns the
ranking of potential candidates, which are the
most suitable ones for each instance, by using
information about the context.
Regarding the candidate collection, we exploit
three dictionaries: Google Dictionary, Babylon
Dictionary and Spanishdict. Each dictionary is
modeled using a strategy described in Section 2.1.
We use the same approach to model each dictio-
nary in order to make easy both the inclusion of fu-
ture dictionaries and the integration with the can-
didate selection step.
Candidate selection is performed in two dif-
ferent ways. The first one relies on the auto-
matic translation of the sentence in which the tar-
get word occurs, in order to find the best substitu-
tions. The second method uses a parallel corpus
built on DBpedia to discover the number of doc-
uments in which the target word is translated by
one of the potential translation candidates. Details
about both methods are reported in Section 2.2
2.1 Candidate collection
This section describes the method adopted to re-
trieve the list of potential translation candidates for
each target word and part of speech.
Our strategy combines several bi-lingual dictio-
naries and builds a single list of candidates for
each target word. The involved dictionaries meet
the following requirements:
1. the source language L
s
must be English and
the target one L
t
must be Spanish;
2. each dictionary must provide information
about the part of speech;
3. the dictionary must be freely available.
Moreover, each candidate has a score s
ij
com-
puted by taking into account its rank in the list
of possible translations supplied by the i ? th
dictionary. Formally, let us denote by D =
{d
1
, d
2
, . . . , d
n
} the set of n dictionaries and by
L
i
= {c
1
, c
2
, . . . , c
m
i
} the list of potential candi-
dates provided by d
i
. The score s
ij
is computed
by the following equation:
s
ij
= 1?
j
m
i
j ? {1, 2, . . . ,m
i
} (1)
Since each list L
i
has a different size, we adopt
a score normalization strategy based on Z-score
to merge the lists in a unique one. Z-score nor-
malizes the scores according to the average ? and
standard deviation ?. Given the list of scores
L = {s
1
, s
2
, . . . , s
n
}, ? and ? are computed on
L and the normalized score is defined as:
s
i
=
s
i
? ?
?
(2)
Then, all the lists L
i
are merged in a single list
M . The list M contains all the potential candi-
dates belonging to all the dictionaries with the re-
lated score. If a candidate occurs in more than one
dictionary, only the occurrence with the maximum
score is chosen.
At the end of the candidate collection step the
list M of potential translation candidates for each
target word is computed. It is important to point
out that the list M is sorted and supplies an initial
rank, which can be then modified by the candidate
selection step.
2.2 Candidate selection
While the candidate collection step is common to
the two proposed systems, the problem of candi-
date selection is faced by using different strategies
in the two systems.
243
The first system, called unibaTranslate,
uses a method based on google-api-translate-java
2
.
The main idea behind unibaTranslate is to
look for a potential candidate in the translation of
the target sentence. Sometimes, no potential can-
didates occur into the translation. When this hap-
pens the system uses some heuristics to discover a
possible translation.
For example, given the target word ?raw? and
the potential candidates M ={puro, crudo, sin re-
finar, de baja calidad, agrietado, al natural, bozal,
asado, frito and bruto}, the two possible scenarios
are:
1. a potential candidate occurs into the transla-
tion:
? S
en
: The raw honesty of that basic
crudeness makes you feel stronger in a
way.
? S
es
: La cruda honestidad de esa
crudeza de base que te hace sentir mas
fuerte en un camino.
2. no potential candidates occur into the trans-
lation, but a correct translation of the target
word is provided:
? S
en
: Many institutional investors are
now deciding that they are getting a raw
deal from the company boards of Aus-
tralia.
? S
es
: Muchos inversores institucionales
estan ahora decidiendo que estan recibi-
endo un trato injusto de los directorios
de las empresas de Australia.
In detail, the strategy can be split in several
steps:
1. Retrieve the list M of potential translation
candidates using the method described in
Section 2.1.
2. Translate the target sentence S
en
from En-
glish to Spanish, using the google-api-
translate-java, which results into the sentence
S
es
.
3. Enrich M by adding multiword expressions.
To implement this step, the two bigrams
which contain the target word and the only
trigram in which the target word is the 2
nd
term are taken into to account.
2
http://code.google.com/p/google-api-translate-java/
Coming back to the first sentence in the previ-
ous example, the following n-grams are built:
?the raw?, ?raw honesty? and ?the raw hon-
esty?. For each n-gram, candidate transla-
tions are looked for using Google Dictionary.
If translations are found, they are added toM
with an initial score equal to 0.
4. Fix a window W
3
of n words to the right and
to the left of the target word, and perform the
following steps:
(a) for each candidate c
k
in M , try to find
c
k
in W . If c
k
occurs in W , then add 2
to the score of c
k
in M ;
(b) if no exact match is found in the previ-
ous step, perform a new search by com-
paring c
k
with the words in W using
the Levenshtein distance
4
(Levenshtein,
1966). If the Levenshtein distance is
greater than 0.8, then add 2 to the score
of c
k
in M .
5. If no exact/partial match is found in the pre-
vious steps, probably the target word is trans-
lated with a word which does not belong to
M . To overcome this problem, we implement
a strategy able to discover a possible transla-
tion in S
es
which is not in M . This approach
involves three steps:
(a) for each word w
i
in S
en
, a list of poten-
tial translations P
i
is retrieved;
(b) if a word in P
i
is found in S
es
, the word
is removed from S
es
5
;
(c) at this point, S
es
contains a list R of
words with no candidate translations. A
score is assigned to those words by tak-
ing into account their position in S
es
with respect to the position of the target
word in S
en
, using the following equa-
tion:
1?
|pos
c
? pos
t
|
L
max
(3)
where pos
c
is the translation candidate
position in S
es
, pos
t
is the target word
position in S
en
and L
max
is the maxi-
mum length between the length of S
en
and S
es
.
3
The window W is the same for both S
en
and S
es
.
4
A normalized Levenshtein distance is adopted to obtain
a value in [0, 1].
5
A partial match based on normalized Levenshtein dis-
tance is implemented.
244
Moreover, the words not semanti-
cally related to the potential candidates
(found using Spanish WordNet
6
) are re-
moved fromR. In detail, for each candi-
date in M a list of semantically related
words in Spanish WordNet
7
is retrieved
which results in a set WN of related
words. Words in R but not in WN are
removed from R. In the final step, the
list R is sorted and the first word in R is
added toM assigning a score equal to 2.
6. In the last step, the list M is sorted. The out-
put of this process is the ranked list of poten-
tial candidates.
It is important to underline that both S
en
and
S
es
are tokenized, part-of-speech tagged and lem-
matized. Lemmatization plays a key role in the
matching step, while part-of-speech tagging is
needed to query both the dictionaries and the
Spanish WordNet. We adopt META (Basile et al,
2008) and FreeLing (Atserias et al, 2006) to per-
form text processing for English and Spanish re-
spectively.
The second proposed system, called
unibaWiki, is based on the idea of automati-
cally building a parallel corpus from Wikipedia.
We use a structured version of Wikipedia called
DBpedia (Bizer et al, 2009). The main idea be-
hind DBpedia is to extract structured information
from Wikipedia and then to make this information
available. The main goal is to have access easily
to the large amount of information in Wikipedia.
DBpedia opens new and interesting ways to use
Wikipedia in NLP applications.
In CLLS task, we use the extended abstracts of
English and Spanish provided by DBpedia. For
each extended abstract in Spanish which has the
corresponding extended abstract in English, we
build a document composed by two fields: the for-
mer contains the English text (text
en
) and the lat-
ter contains the Spanish text (text
es
). We adopt
Lucene
8
as storage and retrieval engine to make
the documents access fast and easy.
The idea behind unibaWiki is to count, for
each potential candidate, the number of docu-
ments in which the target word occurs in text
en
and the potential candidate occurs in text
es
. A
6
http://www.lsi.upc.edu/?nlp/projectes/ewn.html
7
The semantic relations of hyperonymy, hyponymy and
?similar to? are exploited.
8
http://lucene.apache.org/
score equal to the number of retrieved documents
is assigned, then the candidates are sorted accord-
ing to that score.
Given the list M of potential candidates and the
target word t, for each c
k
? M we perform the
following query:
text
en
: t AND text
es
: c
k
where the field name is followed by a colon and
by the term you are looking for.
It is important to underline here that multiword
expressions require a specific kind of query. For
each multiword expression we adopt the Phrase-
Query which is able to retrieve documents that
contain a specific sequence of words instead of a
single keyword.
2.3 Implementation
To implement the candidate collection step we de-
veloped a Java application able to retrieve infor-
mation from dictionaries. For each dictionary, a
different strategy has been adopted. In particular:
1. Google Dictionary: Google Dictionary web-
site is queried by using the HTTP protocol
and the answer page is parsed;
2. Spanishdict: the same strategy adopted for
Google Dictionary is used for the Spanishdict
website
9
;
3. Babylon Dictionary: the original file avail-
able from the Babylon website
10
is converted
to obtain a plain text file by using the Unix
utility dictconv. After that, an application
queries the text file in an efficient way by
means of a hash map.
Both candidate selection systems are developed
in Java. Regarding the unibaWiki system, we
adopt Lucene to index DBpedia abstracts. The
output of Lucene is an index of about 680 Mbytes,
277,685 documents and about 1,500,000 terms.
3 Evaluation
The goal of the evaluation is to measure the sys-
tems? ability to find correct Spanish substitutions
for a given word. The dataset supplied by the or-
ganizers contains 1,000 instances in XML format.
9
http://www.spanishdict.com/
10
www.babylon.com
245
Moreover, the organizers provide trial data com-
posed by 300 instances to help the participants
during the development of their systems.
The systems are evaluated using two scoring
types: best scores the best guessed substitution,
while out-of-ten (oot) scores the best 10 guessed
substitutions. For each scoring type, precision (P)
and recall (R) are computed. Mode precision (P-
mode) and mode recall (R-mode) calculate preci-
sion and recall against the substitution chosen by
the majority of the annotators (if there is a ma-
jority), respectively. Details about evaluation and
scoring types are provided in the task guidelines
(McCarthy et al, 2009).
Results of the evaluation using trial data are
reported in Table 1 and Table 2. Our systems
are tagged as UBA-T and UBA-W, which de-
note unibaTranslate and unibaWiki, re-
spectively. Systems marked as BL-1 and BL-2
are the two baselines provided by the organiz-
ers. The baselines use Spanishdict dictionary to
retrieve candidates. The system BL-1 ranks the
candidates according to the order returned on the
online query page, while the BL-2 rank is based on
candidate frequencies in the Spanish Wikipedia.
Table 1: best results (trial data)
System P R P-mode R-Mode
BL-1 24.50 24.50 51.80 51.80
BL-2 14.10 14.10 28.38 28.38
UBA-T 26.39 26.39 59.01 59.01
UBA-W 22.18 22.18 48.65 48.65
Table 2: oot results (trial data)
System P R P-mode R-Mode
BL-1 38.58 38.58 71.62 71.62
BL-2 37.83 37.83 68.02 68.02
UBA-T 44.16 44.16 78.38 78.38
UBA-W 45.15 45.15 72.52 72.52
Results obtained using trial data show that our
systems are able to overcome the baselines. Only
the best score achieved by UBA-W is below BL-1.
Moreover, our strategy based on Wikipedia (UBA-
W) works better than the one proposed by the or-
ganizers (BL-2).
Results of the evaluation using test data are re-
ported in Table 3 and Table 4, which include all the
participants. Results show that UBA-T obtains the
highest recall using best scoring strategy. More-
over, both systems UBA-T and UBA-W achieve
the highest R-mode and P-mode using oot scoring
strategy. It is worthwhile to point out that the pres-
ence of duplicates affect recall (R) and precision
(P), but not R-mode and P-mode. For this reason
some systems, such as SWAT-E, obtain very high
recall (R) and low R-mode using oot scoring. Du-
plicates are not produced by our systems, but we
performed an a posteriori experiment in which du-
plicates are allowed. In that experiment, the first
candidate provided by UBA-T has been duplicated
ten times in the results. Using that strategy, UBA-T
achieves a recall (and precision) equal to 271.51.
This experiment proves that also our system is able
to obtain the highest recall when duplicates are al-
lowed into the results. Moreover, it is important to
underline here that we do not know how other par-
ticipants generate duplicates in their results. We
adopted a trivial strategy to introduce duplicates.
Table 3: best results (test data)
System P R P-mode R-Mode
BL-1 24.34 24.34 50.34 50.34
BL-2 15.09 15.09 29.22 29.22
UBA-T 27.15 27.15 57.20 57.20
UBA-W 19.68 19.68 39.09 39.09
USPWLV 26.81 26.81 58.85 58.85
Colslm 27.59 25.99 59.16 56.24
WLVUSP 25.27 25.27 52.81 52.81
SWAT-E 21.46 21.46 43.21 43.21
UvT-v 21.09 21.09 43.76 43.76
CU-SMT 21.62 20.56 45.01 44.58
UvT-g 19.59 19.59 41.02 41.02
SWAT-S 18.87 18.87 36.63 36.63
ColEur 19.47 18.15 40.03 37.72
IRST-1 22.16 15.38 45.95 33.47
IRSTbs 22.51 13.21 45.27 28.26
TYO 8.62 8.39 15.31 14.95
Table 4: oot results (test data)
System P R P-mode R-Mode
BL-1 44.04 44.04 73.53 73.53
BL-2 42.65 42.65 71.60 71.60
UBA-T 47.99 47.99 81.07 81.07
UBA-W 52.75 52.75 83.54 83.54
USPWLV 47.60 47.60 79.84 79.84
Colslm 46.61 43.91 69.41 65.98
WLVUSP 48.48 48.48 77.91 77.91
SWAT-E 174.59 174.59 66.94 66.94
UvT-v 58.91 58.91 62.96 62.96
UvT-g 55.29 55.29 73.94 73.94
SWAT-S 97.98 97.98 79.01 79.01
ColEur 44.77 41.72 71.47 67.35
IRST-1 33.14 31.48 58.30 55.42
IRSTbs 29.74 8.33 64.44 19.89
TYO 35.46 34.54 59.16 58.02
FCC-LS 23.90 23.90 31.96 31.96
Finally, Table 5 reports some statistics about
UBA-T and the number of times (N) the candi-
246
date translation is taken from Spanish WordNet
(Spanish WN) or multiword expressions (Multi-
word exp.). The number of instances in which
the candidate is a correct substitution is reported
in column C. Analyzing the results we note that
most errors are due to part-of-speech tagging. For
example, given the following sentence:
S
en
: You will still be responsible for the shipping
and handling fees, and for the cost of return-
ing the merchandise.
S
es
: Usted seguira siendo responsable de los gas-
tos de envio y manipulacion y, para los gastos
de devolucion de la mercancia.
where the target word is the verb return. In this
case the verb is used as noun and the algorithm
suggests correctly devolucion (noun) as substitu-
tion instead of devolver (verb). The gold standard
provided by the organizers contains devolver as
substitution and there is no match between devolu-
cion and devolver during the scoring.
Table 5: UBA-T statistics.
Strategy N C
Spanish WN 34 11
Multiword exp. 21 11
4 Conclusions
We described our participation at SemEval-2
Cross-Lingual Lexical Substitution Task, propos-
ing two systems called UBA-T and UBA-W. The
first relies on Google Translator, the second
is based on DBpedia, a structured version of
Wikipedia. Moreover, we exploited several dictio-
naries to retrieve the list of candidate substitutions.
UBA-T achieves the highest recall among all
the participants to the task. Moreover, the results
proved that the method based on Google Transla-
tor is more effective than the one based on DBpe-
dia.
Acknowledgments
This research was partially funded by Regione
Puglia under the contract POR PUGLIA 2007-
2013 - Asse I Linea 1.1 Azione 1.1.2 - Bando
?Aiuti agli Investimenti in Ricerca per le PMI? -
Fondo per le Agevolazioni alla Ricerca, project ti-
tle: ?Natural Browsing?.
References
Jordi Atserias, Bernardino Casas, Elisabet Comelles,
Meritxell Gonz?alez, Llu??s Padr?o, and Muntsa Padr?o.
2006. FreeLing 1.3: Syntactic and semantic services
in an open-source NLP library. In Proceedings of
the 5th International Conference on Language Re-
sources and Evaluation (LREC06), pages 48?55.
Pierpaolo Basile, Marco de Gemmis, Anna Lisa Gen-
tile, Leo Iaquinta, Pasquale Lops, and Giovanni Se-
meraro. 2008. META - MultilanguagE Text Ana-
lyzer. In Proceedings of the Language and Speech
Technnology Conference - LangTech 2008, Rome,
Italy, February 28-29, pages 137?140.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. DBpedia-A crys-
tallization point for the Web of Data. Web Seman-
tics: Science, Services and Agents on the World Wide
Web, Issue 7:154?165.
Vladimir Iosifovich Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions, and re-
versals. In Soviet Physics-Doklady, volume 10.
Diana McCarthy and Roberto Navigli. 2007.
SemEval-2007 Task 10: English Lexical Sub-
stitution Task. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 48?53, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics.
Diana McCarthy, Rada Sinha, and Ravi Mihal-
cea. 2009. Cross Lingual Lexical Sub-
stitution. http://lit.csci.unt.edu/DOCS/task2clls-
documentation.pdf.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. SemEval-2010 Task 2: Cross-Lingual
Lexical Substitution. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010). Association for Computational
Linguistics.
Ravi Sinha, Diana McCarthy, and Rada Mihalcea.
2009. SemEval-2010 Task 2: cross-lingual lexical
substitution. In SEW ?09: Proceedings of the Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 76?81, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
247
