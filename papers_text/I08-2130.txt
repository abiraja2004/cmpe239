Combining Context Features by Canonical Belief Network for Chinese
Part-Of-Speech Tagging
Hongzhi Xu and Chunping Li
School of Software, Tsinghua University
Key Laboratory for Information System Security, Ministry of Education China
xuhz05@mails.tsinghua.edu.cn
cli@tsinghua.edu.cn
Abstract
Part-Of-Speech(POS) tagging is the essen-
tial basis of Natural language process-
ing(NLP). In this paper, we present an al-
gorithm that combines a variety of context
features, e.g. the POS tags of the words next
to the word a that needs to be tagged and the
context lexical information of a by Canoni-
cal Belief Network to together determine the
POS tag of a. Experiments on a Chinese cor-
pus are conducted to compare our algorithm
with the standard HMM-based POS tagging
and the POS tagging software ICTCLAS3.0.
The experimental results show that our algo-
rithm is more effective.
1 Introduction
Part-Of-Speech(POS) tagging is the essential basis
of Natural language processing(NLP). It is the pro-
cess in which each word is assigned to a correspond-
ing POS tag that describes how this word be used in
a sentence. Typically, the tags can be syntactic cat-
egories, such as noun, verb and so on. For Chinese
language, word segmentation must be done before
POS tagging, because, different from English sen-
tences, there is no distinct boundary such as white
space to separate different words(Sun, 2001). Also,
Chinese word segmentation and POS tagging can be
done at the same time(Ng, 2004)(Wang, 2006).
There are two main approaches for POS tagging:
rule-based and statistical algorithms(Merialdo,
1994). Rule based POS tagging methods ex-
tratct rules from training corpus and use these
rules to tag new sentences(Brill, 1992)(Brill,
1994). Statistic-based algorithms based on Belief
Network(Murphy, 2001) such as Hidden-Markov-
Model(HMM)(Cutting, 1992)(Thede, 1999), Lex-
icalized HMM(Lee, 2000) and Maximal-Entropy
model(Ratnaparkhi, 1996) use the statistical infor-
mation of a manually tagged corpus as background
knowledge to tag new sentences. For example, the
verb is mostly followed by a noun, an adverb or
nothing, so if we are sure that a word a is a verb,
we could say the word b following a has a large
probability to be a noun. This could be helpful
specially when b has a lot of possible POS tags or it
is an unknown word.
Formally, this process relates to Pr(noun|verb),
Pr(adverb|verb) and Pr(nothing|verb), that can
be estimated from the training corpus. HMM-
based tagging is mainly based on such statistical
information. Lexicalized HMM tagging not only
considers the POS tags information to determine
whether b is noun, adverb or nothing, but also
considers the lexical information a itself. That
is, it considers the probabilities Pr(noun|a, verb),
Pr(adverb|a, verb) and Pr(nothing|a, verb) for
instance. Since combining more context informa-
tion, Lexicalized HMM tagging gets a better perfor-
mance(Lee, 2000).
The main problem of Lexicalized HMM is that
it suffers from the data sparseness, so parameter
smoothing is very important. In this paper, we
present a new algorithm that combines several con-
text information, e.g. the POS tags information and
lexical information as features by Canonical Belief
Network(Turtle, 1991) to together determine the tag
907
of a new word. The experiments show that our algo-
rithm really performs well. Here, we don?t explore
Chinese word segmentation methods, and related in-
formation can be found in(Sun, 2001).
The rest of the paper is organized as follows. In
section 2 and section 3, we describe the standard
HMM-based tagging and Lexicalized HMM tagging
respectively which are relevant to our algorithm. In
section 4, we describe the Belief Network as a pre-
liminary. In section 5, we present our algorithm that
is based on Canonical Belief Network. Section 6 is
the experiments and their results. In section 7, we
have the conclusion and the future work.
2 Standard Hidden Markov Model
The problem of POS tagging can be formally de-
fined as: given an observation(sentence) w =
{w1, w2, ..., wT } and a POS tag set TS =
{t1, t2, ..., tM}, the task is to find a tag sequence
t = {t1, t2, ..., tT }, where ti ? TS, that is the most
possible one to explain the observation. That is to
find t to maximize the probability Pr(t|w). It can
be rewritten by Bayesian rule as follows.
Pr(t|w) = Pr(w|t)? Pr(t)Pr(w)
As for any sequence t, the probability Pr(w) is con-
stant, we could ignore Pr(w). For Pr(t), it can be
decomposed by the chain rule as follows.
Pr(t) = Pr(t1, t2, ..., tT )
= Pr(t1)? Pr(t2|t1)? Pr(t3|t1, t2)?
...? Pr(tT |t1, t2, ..., tT?1)
Through this formula, we could find that the calcu-
lation is impossible because of the combination ex-
plosion of different POS tags. Generally, we use a
n-gram especially n = 2 model to calculate Pr(t)
approximately as follows.
Pr(t) = Pr(t1|t0)? Pr(t2|t1)? Pr(t3|t2)?
...? Pr(tT |tT?1)
where t0 is nothing. For Pr(w|t), with an indepen-
dent assumption, it can be calculated approximately
as follows.
Pr(w|t) = Pr(w1|t1)? Pr(w2|t2)? Pr(w3|t3)
...? Pr(wT |tT )
Usually, the probability Pr(ti|ti?1) is called tran-
sition probability, and Pr(wi|ti) is called the emis-
sion probability. They both can be estimated from
the training set. This means that the tag ti of word
wi is only determined by the tag ti?1 of word wi?1.
So, we could find the best sequence through a for-
ward(left to right) process.
If we state all possible POS tags(stats) of each
word and connect all possible ti?1 with all possi-
ble ti and each edge is weighted by Pr(ti|ti?1),
we could get a Directed Acyclic Graph(DAG). The
searching process(decoding) that is involved in find-
ing t that maximizes Pr(t|w) can be explained as
finding the path with the maximal probability. For
this sub task, Viterbi is an efficient algorithm that
can be used(Allen, 1995).
3 Lexicalized Hidden Markov Model
Lexicalized HMM is an improvement to the stan-
dard HMM. It substitutes the probability Pr(ti|ti?1)
with Pr(ti|ti?J,i?1, wi?L,i?1), and the probability
Pr(wi|ti) with Pr(wi|ti?K,i, wi?I,i?1). In other
words, the tag of word wi is determined by the tags
of the J words right before wi and L words right be-
fore wi. It uses more context information of wi to
determine its tag.
However, it will suffer from the data sparse-
ness especially when the values of J , L, K and
I are large, which means it needs an explosively
larger training corpus to get a reliable estimation of
these parameters, and smoothing techniques must be
adopted to mitigate the problem. Back-off smooth-
ing is used by Lexicalized HMM. In the back-off
model, if a n-gram occurs more than k times in
training corpus, then the estimation is used but dis-
counted, or the estimation will use a shorter n-gram
e.g. (n-1)-gram estimation as a back-off probabil-
ity. So, it is a recursive process to estimate a n-gram
parameter.
4 Belief Network
Belief Network is a probabilistic graphical model,
which is also a DAG in which nodes represent
random variables, and the arcs represent condi-
tional independence assumptions. For example, the
probability Pr(A,B) = Pr(A) ? Pr(B|A) can
be depicted as Figure 1(a), and if we decompose
908
%$&
D
%$&
E
%
$
$
%
F G
Figure 1: Some Belief Networks.
Pr(A,B) = Pr(B) ? Pr(A|B), it can be de-
picted as Figure 1(b). Similarly, the probability
Pr(A,B,C) = Pr(A)? Pr(B|A)? Pr(C|A,B)
can be depicted as Figure 1(c).
As we have analyzed above, such decomposition
would need us to estimate a large amount of pa-
rameters. In the belief network, a conditional in-
dependence relationship can be stated as follows: a
node is independent of its ancestors given its par-
ents, where the ancestor/parent relationship is with
respect to some fixed topological ordering of the
nodes. For example, if we simplify the graph Figure
1(c) to graph Figure 1(d), it is equivalent to the de-
composition: Pr(A,B,C) = Pr(A)?Pr(B|A)?
Pr(C|B), which is actually the same as that of
HMM. More details about Belief Network can found
in(Murphy, 2001).
5 Canonical Belief Network Based
Part-Of-Speech Tagging
5.1 Canonical Belief Network
Canonical Belief Network was proposed by Turtle
in 1991(Turtle, 1991), and it was used in informa-
tion retrieval tasks. Four canonical forms are pre-
sented to combine different features, that is and, or,
wsum and sum to simplify the probability combi-
nation further. With the and relationship, it means
that if a node in a DAG is true, then all of its parents
must be true. With the or relationship, it means that
if a node in a DAG is true, then at least one of its par-
ents is true. With the wsum relationship, it means
that if a node in a DAG is true, it is determined by all
of its parents and each parent has a different weight.
With the sum relationship, it means that if a node in
a DAG is true, it is determined by all of its parents
and each parent has an equal weight.
For example, we want to evaluate the probabil-
ity Pr(D|A) or Pr(D = true|A = true), and
$
&%
'
DQG
D
$
&%
'
RU
E
$
&%
'
ZVXP
F
$
&%
'
VXP
G
Figure 2: Canonical Belief Networks for
Pr(A,B,C,D).
node D has two parents B and C, we could use
the four canonical forms to evaluate Pr(D|A) as
shown in Figure 2. Suppose that Pr(B|A) = p1
and Pr(C|A) = p2, with the four canonical form
and, or, wsum and sum, we could get the follow-
ing estimations respectively.
Pand(D|A) = p1 ? p2
Por(D|A) = 1? (1? p1)? (1? p2)
Pwsum(D|A) = w1p1 + w2p2
Psum(D|A) = (p1 + p2)/2
The standard Belief Network actually supposes that
all the relationships are and. However, in real world,
it is not the case. For example, we want to evaluate
the probability that a person will use an umbrella,
and there are two conditions that a person will use
it: raining or a violent sunlight. If we use the stan-
dard Belief Network, it is impossible to display such
situation, because it could not be raining and sunny
at the same time. The or relationship could easily
solve this problem.
5.2 Algorithm Description
Definition: A feature is defined as the context in-
formation of a tag/word, which can be POS tags,
words or both. For example, {Ti?J , ..., Ti?1} is a
feature of tag ti, {Ti?J , ..., Ti} is a feature of word
wi, {Ti?J , ..., Ti?1,Wi?L, ...,Wi?1} is a feature of
tag ti, {Ti?K , ..., Ti,Wi?I , ...,Wi?1} is a feature of
word wi.
In our algorithm, we select 6 features for tag ti,
and select 2 features for word wi, which are shown
in Table 1. We can see that f1t , f2t and f3t are actually
the n-gram features used in HMM, f4t , f5t and f6t are
actually features used by lexicalized HMM.
We adopt the canonical form or to combine them
as shown in Figure 3, and use the canonical form
909
Features
f1t : Ti?3, Ti?2, Ti?1
f2t : Ti?2, Ti?1
ti f3t : Ti?1
f4t : Ti?3, Ti?2, Ti?1, Wi?3, Wi?2, Wi?1
f5t : Ti?2, Ti?1, Wi?2, Wi?1
f6t : Ti?1, Wi?1
wi f1w: Ti?1, Ti
f2w: Ti
Table 1: Features used for ti and wi.
and to combine features of ti and wi. Because we
think that the POS tag of a new word can be de-
termined if any one of the features can give a high
confidence or implication of a certain POS tag. The
probabilities Pr(f it |ti?1), i = 1, ..., 6. are all 1,
which means that all the features in the Canonical
Belief Network are considered to estimate the tag
ti of word wi when we have already estimated the
tag ti?1 of word wi?1. So, the transition probability
could be calculated as follows.
ptransi?1,i = 1?
6?
j=1
[1? Pr(ti|f jt )]
In the same way, the probabilities Pr(f iw|ti), i =
1, 2. are all 1. The emission probability could be
calculated as follows.
pomiti = 1?
2?
j=1
[1? Pr(wi|f jw)]
Let?s return to the POS tagging problem which
needs to find a tag sequence t that maximizes the
probability Pr(t|w), given a word sequence w de-
fined in Section 2. It is involved in evaluating two
probabilities Pr(t) and Pr(w|t). With the Canon-
ical Belief Network we just defined, they could be
calculated as follows.
Pr(t) = ?Ti=1 ptransi?1,i
Pr(w|t) = ?Ti=1 pomiti
Pr(w, t) = Pr(t)? Pr(w|t)
The canonical form or would not suffer from
the data sparseness even though it refers to 4-
gram, because if a 4-gram feature(f1t for example)
doesn?t appear in the training corpus, the probability
IW
RU
IWIW IWIW IW
WL
WL
RU
IZ
ZL
IZ
D E
WL
Figure 3: Canonical Belief Networks used in our al-
gorithm.
Pr(ti|f1t ) is estimated as zero, which means the fea-
ture contributes nothing to determine the probability
that word wi gets a tag ti, which is actually deter-
mined by a lower n-grams. Cases are the same for
3-gram, 2-gram and so on. In a special case, when a
4-gram (f4t for example) appears in the training cor-
pus and appears only once, the probability Pr(ti|f1t )
will be 1, which means that the sentence or phrase
we need to tag may have appeared in the training
corpus, so we can tag the sentence or phrase with
reference to the appeared sentence or phrase in the
training corpus. This is an intuitional comprehen-
sion of our algorithm and its motivation.
Decoding: The problem of using high n-gram
is the combination explosion especially for high
grams. For example, consider the feature , suppose
one word has 3 possible tags on average, then we
have to evaluate 33 = 27 cases for f1t , further, dif-
ferent features could get different combinations and
the number of combinations will be 272?92?32 =
531441. To solve the problem, we constrain all fea-
tures to be consistent. For example, the tag ti?1 of
feature f1t must be same as that of feature f2t , f3t ,
f4t , f5t and f6t at one combination. The following
features are not consistent, because the ti?1 in f1t is
V BP , while the ti?1 in f4t is NN .
f1t = JJ,NNS, V BP
f4t = JJ,NNS,NN, little, boys, book
This will decrease the total combination to 33 = 27.
We use a greedy search scheme that is based on the
classic decoding algorithm Viterbi. Suppose that the
Viterbi algorithm has reached the state ti?1, to cal-
culate the best path from the start to ti, we only use
the tags on the best path from the start to ti?1 to cal-
culate the probability. This decreases the total com-
910
bination to 3(the number of possible tags of ti?1),
which is the same as that of standard HMM.
6 Experiments
Dataset: We conduct our experiments on a Chinese
corpus consisting of all news from January, 1998 of
People?s Daily, tagged with the tag set of Peking
University(PKU), which contains 46 POS tags1. For
the corpus, we randomly select 90% as the training
set and the remaining 10% as the test set. The cor-
pus information is shown in Table 2, where unknown
words are the words that appear in test set but not in
training set. The experiments are run on a machine
with 2.4GHZ CPU, and 1GB memory.
Training set Test set
Words 1021592 112321
Sentences 163419 17777
Unknow words 2713
Table 2: Chinese corpus information.
Unknown Words: In our experiments, we first
store all the words with their all possible POS tags
in a dictionary. So, our algorithm gets all possible
tags of a word through a dictionary. As for the word
in the test set that doesn?t appear in the training set,
we give the probability Pr(wi|f jw) value 1, with all
j. This processing is quite simple, however, it is
enough to observe the relative performances of dif-
ferent POS taggers.
For Chinese word segmentation, we use the seg-
mentation result of ICTCLAS3.02. The segmenta-
tion result is shown in Table 3. Sen-Prec is the ratio
of the sentences that are correctly segmented among
all sentences in the test set.
Precision Recall F1 Sen-Prec
0.9811 0.9832 0.9822 0.9340
Table 3: Segmentation Result by ICTCLAS.
Open Test: We compare the POS tagging per-
formance of our algorithm with the standard HMM,
1http://icl.pku.edu.cn/Introduction/corpustagging.htm
2ICTCLAS3.0 is a commercial software developed by Insti-
tute of Computing Technology, Chinese Academy of Science,
that is used for Chinese word segmentation and POS tagging.
and ICTCLAS3.0. The experimental result is shown
in Table 4. Prec-Seg is the POS tagging precision
on the words that are correctly segmented. Prec-
Sen is the ratio of the sentences that are correctly
tagged among all sentences in the test set. Prec-Sen-
Seg is the ratio of sentences that are correctly tagged
among the sentences that are correctly segmented.
With the experiments, we can see that, our algo-
rithm always gets the best performance. The ICT-
CLAS3.0 doesn?t perform very well. However, this
is probably because of that the tag set used by ICT-
CLAS3.0 is different from that of PKU. Even though
it provides a mapping scheme from their tags to
PKU tags, they may be not totally consistent. The
published POS tagging precision of ICTCLAS3.0 is
94.63%, also our algorithm is a little better. This has
proved that our algorithm is more effective for POS
tagging task.
ICTCLAS HMM CBN
Precision 0.9096 0.9388 0.9465
Recall 0.9115 0.9408 0.9485
F1 0.9105 0.9398 0.9475
Prec-Seg 0.9271 0.9569 0.9647
Prec-Sen 0.6342 0.7404 0.7740
Prec-Sen-Seg 0.6709 0.7927 0.8287
Table 4: Open test comparison result on Chinese
corpus.
Close Test: As we have analyzed above in Sec-
tion 5.2 that our algorithm takes advantage of more
information in the training set. When a sentence or a
phrase appears in the training set, it will help a lot to
tag the new sentence correctly. To test whether this
case really happens, we conduct a new experiment
that is the same as the first one except that the test
set is also added to the training set. The experimen-
tal result is shown in Table 5. We can see that the
performance of our algorithm is greatly improved,
while the HMM doesn?t improve much, which fur-
ther proves our analysis.
Even though our algorithm gives a satisfying per-
formance, it may be able to be improved by adopt-
ing smoothing techniques to take advantage of more
useful features, e.g. to make the probabilities such
as Pr(ti|f1t ), Pr(ti|f2t ) not be zero. In addition, the
adoption of techniques to deal with unknown words
911
ICTCLAS HMM CBN
Precision 0.9096 0.9407 0.9658
Recall 0.9115 0.9427 0.9678
F1 0.9105 0.9417 0.9668
Prec-Seg 0.9271 0.9588 0.9843
Prec-Sen 0.6342 0.7476 0.8584
Prec-Sen-Seg 0.6709 0.8004 0.9191
Table 5: Close test comparison result on Chinese
corpus.
and techniques to combine with rules may also im-
prove the performance of our algorithm. If we have
a larger training corpus, it may be better to remove
some confusing features such as f3t and f2w, because
they contain weak context information and this is
why a higher n-gram model always performs better
than a lower n-gram model when the training corpus
is large enough. However, this should be validated
further.
7 Conclusion and Future Work
In this paper, we present a novel algorithm that
combines useful context features by Canonical Be-
lief Network to together determine the tag of a new
word. The ?or? node can allow us to use higher n-
gram model although the training corpus may be not
sufficient. In other words, it can overcome the data
sparseness problem and make use of more informa-
tion from the training corpus. We conduct experi-
ments on a Chinese popular corpus to evaluate our
algorithm, and the results have shown that it is pow-
erful even in case that we don?t deal with the un-
known words and smooth the parameters.
We think that our algorithm could also be used
for tagging English corpus. In addition, we only ex-
tract simple context information as features. We be-
lieve that there exists more useful features that can
be used to improve our algorithm. For example, the
syntax analysis could be combined as a new fea-
ture, because a POS sequence may be illegal even
though it gets the maximal probability through our
algorithm. Yet, these will be our future work.
Acknowledgement This work was supported by
Chinese 973 Research Project under grant No.
2002CB312006.
References
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-Of-Speech Tagging. In Proc. of the Empiri-
cal Methods in Natural Language Processing Confer-
ence(EMNLP?96), 133-142.
Bernard Merialdo. 1994. Tagging English Text with
a Probabilistic Model. Computational Linguistics,
20(2):155?172.
Doug Cutting, Julian Kupied, Jan Pedersen and Penelope
Sibun. 1992. A Practical part-of-speech tagger. In
Proceedings of the 3rd Conference on Applied Natural
Language Processing(ANLP?92), 133-140.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. In Proc. of the 30th Conference on Applied Com-
putational Linguistics(ACL?92), Trento, Italy, 112-
116.
Eric Brill. 1994. Some Advances in Transformation-
Based Part of Speech Tagging. In Proc. of
the 12th National Conference on Artificial Intelli-
gence(AAAI?94), 722-727.
Howard Turtle and W. Bruce Croft. 1991. Evaluation of
an Inference Network-Based Retrieval Model. ACM
Transactions on Information Systems, 9(3):187-222.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once?
Word-Based or Character-Based?. In Proc. of the Em-
pirical Methods in Natural Language Processing Con-
ference(EMNLP?04).
James Allen. 1995. Natural Language Understanding.
The Benjamin/Cummings Publishing Company.
Kevin P. Murphy. 2001. An introduction to graphical
models. Technical report, Intel Research Technical
Report.
Maosong Sun and Jiayan Zou. 2001. A critical appraisal
of the research on Chinese word segmentation(In Chi-
nese). Contemporary Linguistics, 3(1):22-32.
Mengqiu Wang and Yanxin Shi. 2006. Using Part-of-
Speech Reranking to Improve Chinese Word Segmen-
tation. In Proc. of the 5th SIGHAN Workshop on Chi-
nese Language Processing, 205-208.
Sang-Zoo Lee, Jun-ichi Tsujii and Hae-Chang Rim.
2000. Lexicalized Hidden Markov Models for Part-of-
Speech Tagging. In Proc. of 18th International Con-
ference on Computational Linguistics(COLING?00),
Saarbrucken, Germany, 481-487.
Scott M. Thede and Mary P. Harper. 1999. A Second-
Order Hidden Markov Model for Part-of-Speech Tag-
ging. In Proc. of the 37th Conference on Applied
Computational Linguistics(ACL?99), 175-182.
912
