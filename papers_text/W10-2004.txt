Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 27?35,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
HHMM Parsing with Limited Parallelism
Tim Miller
Department of Computer Science
and Engineering
University of Minnesota, Twin Cities
tmill@cs.umn.edu
William Schuler
University of Minnesota, Twin Cities
and The Ohio State University
schuler@ling.ohio-state.edu
Abstract
Hierarchical Hidden Markov Model
(HHMM) parsers have been proposed as
psycholinguistic models due to their broad
coverage within human-like working
memory limits (Schuler et al, 2008) and
ability to model human reading time
behavior according to various complexity
metrics (Wu et al, 2010). But HHMMs
have been evaluated previously only with
very wide beams of several thousand
parallel hypotheses, weakening claims to
the model?s efficiency and psychological
relevance. This paper examines the effects
of varying beam width on parsing accu-
racy and speed in this model, showing that
parsing accuracy degrades gracefully as
beam width decreases dramatically (to 2%
of the width used to achieve previous top
results), without sacrificing gains over a
baseline CKY parser.
1 Introduction
Probabilistic parsers have been successful at ac-
curately estimating syntactic structure from free
text. Typically, these systems work by consider-
ing entire sentences (or utterances) at once, using
dynamic programming to obtain globally optimal
solutions from locally optimal sub-parses.
However, these methods usually do not attempt
to conform to human-like processing constraints,
e.g. leading to center embedding and garden path
effects (Chomsky and Miller, 1963; Bever, 1970).
For systems prioritizing accurate parsing perfor-
mance, there is little need to produce human-like
errors. But from a human modeling perspective,
the success of globally optimized whole-utterance
models raises the question of how humans can ac-
curately parse linguistic input without access to
this same global optimization. This question cre-
ates a niche in computational research for models
that are able to parse accurately while adhering as
closely as possible to human-like psycholinguistic
constraints.
Recent work on incremental parsers includes
work on Hierarchical Hidden Markov Model
(HHMM) parsers that operate in linear time by
maintaining a bounded store of incomplete con-
stituents (Schuler et al, 2008). Despite this seem-
ing limitation, corpus studies have shown that
through the use of grammar transforms, this parser
is able to cover nearly all sentences contained in
the Penn Treebank (Marcus et al, 1993) using a
small number of unconnected memory elements.
But this bounded-memory parsing comes at a
price. The HHMM parser obtains good coverage
within human-like memory bounds only by pur-
suing an ?optionally arc-eager? parsing strategy,
nondeterministically guessing which constituents
can be kept open for attachment (occupying an ac-
tive memory element), or closed for attachment
(freeing a memory element for subsequent con-
stituents). Although empirically determining the
number of parallel competing hypotheses used in
human sentence processing is difficult, previous
results in computational models have shown that
human-like behavior can be elicited at very low
levels of parallelism (Boston et al, 2008b; Brants
and Crocker, 2000), suggesting that large num-
bers of active hypotheses are not needed. Previ-
ously, the HHMM parser has only been evaluated
on large beam widths, leaving this aspect of its
psycholinguistic plausibility untested.
In this paper, the performance of an HHMM
parser will be evaluated in two experiments that
27
vary the amount of parallelism allowed during
parsing, measuring the degree to which this de-
grades the system?s accuracy. In addition, the
evaluation will compare the HHMM parser to an
off-the-shelf probabilistic CKY parser to evaluate
the actual run time performance at various beam
widths. This serves two purposes, evaluating one
aspect of the plausibility of this parsing frame-
work as a psycholinguistic model, and evaluating
its potential utility as a tool for operating on un-
segmented text or speech.
2 Related Work
There are several criteria a parser must meet
in order to be plausible as a psycholinguistic
model of the human sentence-processing mecha-
nism (HSPM).
Incremental operation is perhaps the most obvi-
ous. The HSPM is able to process sentences in-
crementally, meaning that at each point in time of
processing input, it has some hypothesis of the in-
terpretation of that input, and each subsequent unit
of input serves to update that hypothesis.
The next criterion for psycholinguistic plausi-
bility is processing efficiency. The HSPM not
only operates incrementally, but in standard op-
eration it does not lag behind a speaker, even if,
for example, the speaker continues speaking at ex-
tended length without pause. Standard machine
approaches, such as chart parsers based on the
CKY algorithm, operate in worst-case cubic run
time on the length of input. Without knowing
where an utterance or sentence might end, such an
algorithm will take more time with each succes-
sive word and will eventually fall behind.
The third criterion is a reasonable limiting of
memory resources. This constraint means that the
HSPM, while possibly considering multiple hy-
potheses in parallel, is not limitlessly so, as evi-
denced by the existence of garden path sentences
(Bever, 1970; Lewis, 2000). If this were not the
case, garden-path sentences would not cause prob-
lems, as reaching the disambiguating word would
simply result in a change in the favored hypothe-
sis. In fact, garden path sentences typically cannot
be understood on a first pass and must be reread,
indicating that the correct analysis is attainable
and yet not present in the set of parallel hypotheses
of the first pass.
While parsers meeting these three criteria can
claim to not violate any psycholinguistic con-
straints, there has been much recent work in
testing psycholinguistically-motivated parsers to
make forward predictions about human sentence
processing, in order to provide positive evidence
for certain probabilistic parsing models as valid
psycholinguistic models of sentence processing.
This work has largely focused on correlating mea-
sures of parsing difficulty in computational models
with delays in reading time in human subjects.
Hale (2001) introduced the surprisal metric for
probabilistic parsers, which measures the log ra-
tio of the total probability mass at word t ? 1
and word t. In other words, it measures how
much probability was lost in incorporating the
next word into the current hypotheses. Boston et
al. (2008a) show that surprisal is a significant pre-
dictor of reading time (as measured in self-paced
reading experiments) using a probabilistic depen-
dency parser. Roark et al (2009) dissected parsing
difficulty metrics (including surprisal and entropy)
to separate out the effects of syntactic and lexical
difficulties, and showed that these new metrics are
strong predictors of reading difficulty.
Wu et al (2010) evaluate the same Hierarchical
Hidden Markov Model parser used in this work in
terms of its ability to reproduce human-like results
for various complexity metrics, including some of
those mentioned above, and introduce a new met-
ric called embedding difference. This metric is
based on the idea of embedding depth, which is
the number of elements in the memory store re-
quired to hold a given hypothesis. Using more
memory elements corresponds to center embed-
ding in phrase structure trees, and presumably cor-
relates to some degree with complexity. Average
embedding for a time step is computed by com-
puting the weighted average number of required
memory elements (weighted by probability) for all
hypotheses on the beam. Embedding difference is
simply the change in this value when the next word
is encountered.
Outside of Wu et al, the most similar work
from a modeling perspective is an incremen-
tal parser implemented using Cascaded Hidden
Markov Models (CHMMs) (Crocker and Brants,
2000). This model is superficially similar to the
Hierarchical Hidden Markov Models described
below in that it relies on multiple levels of interde-
pendent HMMs to account for hierarchical struc-
ture in an incremental model. Crocker and Brants
use the system to parse ambiguous sentences (such
28
as the athlete realized his goals were out of reach)
and examine the relative probabilities of two plau-
sible analyses at each time step. They then show
that the shifting of these two probabilities is con-
sistent with empirical evidence about how humans
perceive these sentences word by word.
However, as will be described below, the
HHMM has advantages over the CHMM from
a psycholinguistic modeling perspective. The
HHMM uses a limited memory store contain-
ing only four elements which is consistent with
many estimates of human short term memory lim-
its (Cowan, 2001; Miller, 1956). In addition to
modeling memory limits, the limited store acts as
a fixed-depth stack that ensures linear asymptotic
parsing time, and a grammar transform allows for
wide coverage of speech and newspaper corpora
within that limited memory store (Schuler et al,
2010).
3 Hierarchical Hidden Markov Model
Parser
Hidden Markov Models (HMMs) have long been
used to successfully model sequence data in which
there is a latent (hidden) variable at each time step
that generates the observed evidence at that time
step. These models are used for such applications
as part-of-speech tagging, and speech recognition.
Hierarchical Hidden Markov Models (HH-
MMs) are an extension of HMMs which can rep-
resent sequential data containing hierarchical rela-
tions. In HHMMs, complex hidden variables may
output evidence for several time steps in sequence.
This process may recurse, though a finite depth
is required to make any guarantees about perfor-
mance. Murphy and Paskin (2001) showed that
this model could be framed as a Dynamic Bayes
Network (DBN), so that inference is linear on the
length of the input sequence.
In the HHMM parser used here, the complex
hidden variables are syntactic states that gener-
ate sub-sequences of other syntactic states, even-
tually generating pre-terminals and words. This
section will describe how the trees must be trans-
formed, and then mapped to HHMM states. This
section will then continue with a formal definition
of an HHMM, followed by a description of how
this model can parse natural language, and finally
a discussion of what different aspects of the model
represent in terms of psycholinguistic modeling.
3.1 Right-Corner Transform
In order to parse with an HHMM, phrase struc-
ture trees need to be mapped to a hierarchical se-
quence of states of nested HMMs. Since Mur-
phy and Paskin showed that the run time complex-
ity of the HHMM is exponential on the depth of
the nested HMMs, it is important to minimize the
depth of the model for optimal performance. In
order to do this, a tree transformation known as
a right-corner transform is applied to the phrase
structure trees comprising the training data, to
transform right-expanding sequences of complete
constituents into left-expanding sequences of in-
complete constituents A?/A?, consisting of an in-
stance of an active constituent A? lacking an in-
stance of an awaited constituent A? yet to be rec-
ognized. This transform can be defined as a syn-
chronous grammar that maps every context-free
rule expansion in a source tree (in Chomsky Nor-
mal Form) to a corresponding expansion in a right-
corner transformed tree:1
? Beginning case: the top of a right-expanding
sequence in an ordinary phrase structure tree
is mapped to the bottom of a left-expanding
sequence in a right-corner transformed tree:
A?
A??0
?
A??1
?
?
A?
A?/A??1
A??0
?
?
(1)
? Middle case: each subsequent branch in
a right-expanding sequence of an ordinary
phrase structure tree is mapped to a branch in
a left-expanding sequence of the transformed
tree:
A?
? A???
A????0
?
A????1
?
?
A?
A?/A????1
A?/A???
?
A????0
?
?
(2)
? Ending case: the bottom of a right-expanding
sequence in an ordinary phrase structure tree
1Here, ? and ? are tree node addresses, consisting of se-
quences of zeros, representing left branches, and ones, repre-
senting right branches, on a path from the root of the tree to
any given node.
29
a) A?
A?0
a ?
00 A?01
A?010
a ?
01
00
a ?
01
01
A?011
a ?
01
10
a ?
01
11
A?1
A?10
A?100
a ?
10
00
a ?
10
01
A?101
a ?
10
10
a ?
10
11
a ?
11
b) A?
A?/A?11
A?/A?1
A?0
A?0/A?0111
A?0/A?011
A?0/A?01
a ?
00
A?010
A?010/A?0101
a ?
01
00 a ?
01
01
a ?
01
10 a ?
01
11
A?10
A?10/A?1011
A?10/A?101
A?100
A?100/A?1001
a ?
10
00 a ?
10
01
a ?
10
10 a ?
10
11
a ?
11
Figure 1: Sample right-corner transform of
schematized tree before (a) and after (b) applica-
tion of transform.
is mapped to the top of a left-expanding se-
quence in a right-corner transformed tree:
A?
? A???
a???
?
A?
A?/A???
?
A???
a???
(3)
The application of this transform is exemplified in
Figure 1.
3.2 Hierarchical Hidden Markov Models
Right-corner transformed trees are mapped to ran-
dom variables in a Hierarchical Hidden Markov
Model (Murphy and Paskin, 2001).
A Hierarchical Hidden Markov Model
(HHMM) is essentially a factored version of
a Hidden Markov Model (HMM), configured to
recognize bounded recursive structures (i.e. trees).
Like HMMs, HHMMs use Viterbi decoding to
obtain sequences of hidden states s?1..T given
sequences of observations o1..T (words or audio
features), through independence assumptions
in a transition model ?A and an observation
model ?B (Baker, 1975; Jelinek et al, 1975):
s?1..T
def= argmax
s1..T
T
?
t=1
P?A(st | st?1) ? P?B(ot | st)
(4)
HHMMs then factor the hidden state transition?A
into a reduce and shift phase (Equation 5), then
into a bounded set of depth-specific operations
(Equation 6):
P?A(st|st?1) =
?
rt
P?R(rt|st?1)?P?S(st|rt st?1)
(5)
def=
?
r1..Dt
D
?
d=1
P?R,d(r
d
t | rd+1t sdt?1sd?1t?1 )?
P?S,d(s
d
t |rd+1t rdt sdt?1sd?1t )
(6)
which allow depth-specific variables to reduce
(through ?R-Rdn,d), transition (?S-Trn,d), and ex-
pand (?S-Exp,d) like tape symbols in a pushdown
automaton with a bounded memory store, depend-
ing on whether the variable below has reduced
(rdt ?RG) or not (rdt 6?RG):2
P?R,d(r
d
t | rd+1t sdt?1sd?1t?1 )
def=
{
if rd+1t 6?RG : Jrdt =r?K
if rd+1t ?RG : P?R-Rdn,d(rdt | rd+1t sdt?1 sd?1t?1 )
(7)
P?S,d(s
d
t | rd+1t rdt sdt?1sd?1t )
def=
?
?
?
if rd+1t 6?RG, rdt 6?RG : Jsdt =sdt?1K
if rd+1t ?RG, rdt 6?RG : P?S-Trn,d(sdt | rd+1t rdt sdt?1sd?1t )
if rd+1t ?RG, rdt ?RG : P?S-Exp,d(sdt | sd?1t )
(8)
where s0t = s? and rD+1t = r? for constants s?
(an incomplete root constituent), r? (a complete
lexical constituent) and r? (a null state resulting
from reduction failure) s.t. r??RG and r? 6?RG.
Right-corner transformed trees, as exemplified
in Figure 1(b), can then be aligned to HHMM
states as shown in Figure 2, and used to train an
HHMM as a parser.
Parsing with an HHMM simply involves pro-
cessing the input sequence, and estimating a most
likely hidden state sequence given this observed
input. Since the output is to be the best possible
parse, the Viterbi algorithm is used, which keeps
track of the highest probability state at each time
step, where the state is the store of incomplete syn-
tactic constituents being processed. State transi-
tions are computed using the models above, and
each state at each time step keeps a back pointer to
the state it most probably came from. Extracting
the highest probability parse requires extracting
2Here, J?K is an indicator function: J?K = 1 if ? is true, 0
otherwise.
30
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7
a
?0101
a
?0110
a
?0111
a
?1000
a
?1001
a
?1010
? ? ? ? ? ?
? ? ? ?
A
?100 /A
?1001
A
?10 /A
?101
A
?10 /A
?1011
?
A
?0 /A
?011
A
?0 /A
?0111
A
? /A
?1
A
? /A
?1
A
? /A
?1
A
? /A
?1
Figure 2: Mapping of schematized right-corner
tree into HHMM memory elements.
the most likely sequence, deterministically map-
ping that sequence back to a right-corner tree, and
reversing the right-corner transform to produce an
ordinary phrase structure tree.
Unfortunately exact inference is not tractable
with this model and dataset. The state space is
too large to manage for both space and time rea-
sons, and thus approximate inference is carried
out, through the use of a beam search. At each
time step, only the top N most probable hypoth-
esized states are maintained. Experiments de-
scribed in (Schuler, 2009) suggest that there does
not seem to be much lost in going from exact in-
ference using the CKY algorithm to a beam search
with a relatively large width. However, the op-
posite experiment, examining the effect of going
from a relatively wide beam to a very narrow beam
has not been thoroughly studied in this parsing ar-
chitecture.
4 Optionally Arc-eager Parsing
The right-corner transform described in Sec-
tion 3.1 saves memory because it transforms any
right-expanding sequence with left-child subtrees
into a left-expanding sequence of incomplete con-
stituents, with the same sequence of subtrees as
right children. The left-branching sequences of
siblings resulting from this transform can then be
composed bottom-up through time by replacing
each left child category with the category of the
resulting parent, within the same memory element
(or depth level). For example, in Figure 3(a) a
left-child category NP/NP at time t=4 is composed
with a noun new of category NP/NNP (a noun
phrase lacking a proper noun yet to come), result-
ing in a new parent category NP/NNP at time t=5
replacing the left child category NP/NP in the top-
most d=1 memory element.
This in-element composition preserves ele-
ments of the bounded memory store for use in pro-
cessing descendants of this composed constituent,
yielding the human-like memory demands re-
ported in (Schuler et al, 2008). But whenever
an in-element composition like this is hypothe-
sized, it isolates an intermediate constituent (in
this example, the noun phrase ?new york city?)
from subsequent composition. Allowing access
to this intermediate constituent ? for example,
to allow ?new york city? to become a modifier
of ?bonds?, which itself becomes an argument of
?for? ? requires an analysis in which the interme-
diate constituent is stored in a separate memory
element, shown in Figure 3(b). This creates a lo-
cal ambiguity in the parser (in this case, from time
step t=4) that may have to be propagated across
several words before it can be resolved (in this
case, at time step t=7). This is essentially an am-
biguity between arc-eager (in-element) and arc-
standard (cross-element) composition strategies,
as described by Abney and Johnson (1991). In
contrast, an ordinary (purely arc-standard) parser
with an unbounded stack would only hypothesize
analysis (b), avoiding this ambiguity.3
The right-corner HHMM approach described
in this paper relies on a learned statistical model
to predict when in-element (arc-eager) compo-
sitions will occur, in addition to hypothesizing
parse trees. The model encodes a mixed strategy:
with some probability arc-eager or arc-standard
for each possible expansion. Accuracy results on
a right-corner HHMM model trained on the Penn
Wall Street Journal Treebank suggest that this kind
of optionally arc-eager strategy can be reliably sta-
tistically learned.
By placing firm limits on the number of open
incomplete constituents in working memory, the
Hierarchical HMM parser maintains parallel hy-
potheses on the beam which predict whether each
constituent will host a subsequent attachment or
not. Empirical results described in the next section
3It is important to note that neither the right-corner nor
left-corner parsing strategy by itself creates this ambiguity.
The ambiguity arises from the decision to use this option-
ally arc-eager strategy to reduce memory store allocation in
a bounded memory parser. Implementations of left-corner
parsers such as that of Henderson (2004) adopt a arc-standard
strategy, essentially always choosing analysis (b) above, and
thus do not introduce this kind of local ambiguity. But in
adopting this strategy, such parsers must maintain a stack
memory of unbounded size, and thus are not attractive as
models of human parsing in short-term memory (Resnik,
1992).
31
a)
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7
strong
dem
and
for
new
york
city
? ? ? ? ? ?
? ? ? ? ? ?
?
NP/NN
NP/PP
NP/NP
NP/NNP
NP/NNP
NP(dem
.)
b)
d=1
d=2
d=3
word
t=1 t=2 t=3 t=4 t=5 t=6 t=7
strong
dem
and
for
new
york
city
? ? ? ? ? ?
? ? ? ?
NNP/NNP
NNP/NNP
NP(city)
?
NP/NN
NP/PP
NP/NP
NP/NP
NP/NP
NP(dem
.)/NP(?)
Figure 3: Alternative analyses of ?strong demand for new york city ...?: a) using in-element composition,
compatible with ?strong demand for new york city is ...? (in which the demand is for the city); and b)
using cross-element (or delayed) composition, compatible with either ?strong demand for new york city
is ...? (in which the demand is for the city) or ?strong demand for new york city bonds is ...? (in which a
forthcoming referent ? in this case, bonds ? is associated with the city, and is in demand). In-element
composition (a) saves memory but closes off access to the noun phrase headed by ?city?, and so is not
incompatible with the ?...bonds? completion. Cross-element composition (b) requires more memory,
but allows access to the noun phrase headed by ?city?, so is compatible with either completion. This
ambiguity is introduced at t=4 and propagated until at least t=7. An ordinary, non-right-corner stack
machine would exclusively use analysis (b), avoiding ambiguity.
show that this added demand on parallelism does
not substantially degrade parsing accuracy, even at
very narrow beam widths.
5 Experimental Evaluation
The parsing model described in Section 3 has
previously been evaluated on the standard task
of parsing the Wall Street Journal section of the
Penn Treebank. This evaluation was optimized
for accuracy results, and reported a relatively wide
beam width of 2000 to achieve its best results.
However, most psycholinguistic models of the hu-
man sentence processing mechanism suggest that
if the HSPM does work in parallel, it does so with
a much lower number of concurrent hypotheses
(Boston et al, 2008b). Viewing the HHMM pars-
ing framework as a psycholinguistic model, a nec-
essary (though not sufficient) condition for it being
a valid model is that it be able to maintain rela-
tively accurate parsing capabilities even at much
lower beam widths.
Thus, the first experiments in this paper evalu-
ate the degradation of parsing accuracy depending
on beam width of the HHMM parser. Experiments
were conducted again on the WSJ Penn Treebank,
using sections 02-21 to train, and section 23 as the
test set. Punctuation was included in both train-
ing and testing. A set of varied beam widths were
considered, from a high of 2000 to a low of 15.
This range was meant to roughly correspond to
the range of parallelism used in other similar ex-
periments, using 2000 as a high end due to its us-
age in previous parsing experiments. However, it
should be noted that in fact the highest value of
2000 is already an approximate search ? prelim-
inary experiments showed that exhaustive search
with the HHMM would require more than 100000
elements per time step (exact values may be much
higher but could not be collected because they ex-
hausted system memory).
The HHMM parser was compared to a custom
built (though standard) probabilistic CKY parser
implementation trained on the CNF trees used as
input to the right-corner transform, so that the
CKY parser was able to compete on a fair foot-
ing. The accuracy results of these experiments are
shown in Figure 4.
These results show fairly graceful decline in
parsing accuracy with a beam width starting at
2000 elements down to about 50 beam elements.
This beam width is much less than 1% of the ex-
haustive search, though it is around 1% of what
might be considered the highest reasonable beam
width for efficient parsing. The lowest beam
widths attempted, 15, 20, and 25, result in ac-
curacy below that of the CKY parser. The low-
est beam width attempted, 15, shows the sharpest
decline in accuracy, putting the HHMM system
nearly 8 points below the CKY parser in terms of
accuracy.
This compares reasonably well to results by
32
 70
 72
 74
 76
 78
 80
 82
 84
 0  100  200  300  400  500
La
be
le
d 
F-
Sc
or
e
Beam Width
Figure 4: Plot of parsing accuracy (labeled F-
score) vs. beam widths for an HHMM parser
(curved line). Top line is HHMM accuracy with
beam width of 2000 (upper bound). The bottom
line is CKY parser results. Points correspond to
beam widths of 15, 20, 25, 50, 100, 250, and 500.
Brants and Crocker (2000) showing that an in-
cremental chart-parsing algorithm can parse accu-
rately with pruning down to 1% of normal memory
usage. While that parsing algorithm is difficult to
compare directly to this HHMM parser, the reduc-
tion in beam width in this system to 50 beam el-
ements from an already approximated 2000 beam
elements shows similar robustness to approxima-
tion. Accuracy comparisons should be taken with
a grain of salt due to additional annotations per-
formed to the Treebank before training, but the
HHMM parser with a beam width of 50 obtains
approximately the same accuracy as the Brants
and Crocker incremental CKY parser pruning to
3% of chart size. At 1% pruning, Brants and
Crocker achieved around 75% accuracy, which
falls between the HHMM parser at beam widths
of 20 and 25.
Results by Boston et al (2008b) are also dif-
ficult to compare directly due to a difference in
parsing algorithm and different research priority
(that paper was attempting to correlate parsing dif-
ficulty with reading difficulty). However, that pa-
per showed that a dependency parser using less
than ten beam elements (and as few as one) was
just as capable of predicting reading difficulty as
the parser using 100 beam elements.
A second experiment was conducted to eval-
uate the HHMM for its time efficiency in pars-
ing. This experiment is intended to address two
questions: Whether this framework is efficient
 0
 2
 4
 6
 8
 10
 12
 14
 10  20  30  40  50  60  70
Se
co
nd
s 
pe
r s
en
te
nc
e
Sentence Length
CKY
HHMM
Figure 5: Plot of parsing time vs. sentence length
for HHMM and CKY parsers.
enough to be considered a viable psycholinguis-
tic model, and whether its parsing time and accu-
racy remain competitive with more standard cu-
bic time parsing technologies at low beam widths.
To evaluate this aspect, the HHMM parser was
run at low beam widths on sentences of varying
lengths. The baseline was the widely-used Stan-
ford parser (Klein and Manning, 2003), run in
?vanilla PCFG? mode. This parser was used rather
than the custom-built CKY parser from the pre-
vious experiment, to avoid the possibility that its
implementation was not efficient enough to pro-
vide a realistic test. The HHMMparser was imple-
mented as described in the previous section. These
experiments were run on a machine with a single
2.40 GHz Celeron CPU, with 512 MB of RAM. In
both implementations the parser timing includes
only time spent actually parsing sentences, ignor-
ing the overhead incurred by reading in model files
or training.
Figure 5 shows a plot of parsing time versus
sentence length for the HHMM parser for a beam
width of 20. Sentences shorter than 10 words were
not included for visual clarity (both parsers are ex-
tremely fast at that length). At this beam width,
the performance of the HHMM parser (labeled F-
score) was 74.03%, compared to 71% for a plain
CKY parser. As expected, the HHMM parsing
time increases linearly with sentence length, while
the CKY parsing time increases super-linearly.
(However, due to high constants in the run time
complexity of the HHMM, it was not a priori clear
that the HHMM would be faster for any sentence
of reasonable length.)
33
The results of this experiment show that the
HHMM parser is indeed competitive with a proba-
bilistic CKY parser, in terms of parsing efficiency,
even while parsing with higher accuracy. At sen-
tences longer that 26 words (including punctua-
tion), the HHMM parser is faster than the CKY
parser. This advantage is clear for segmented text
such as the Wall Street Journal corpus. However,
this advantage is compounded when considering
unsegmented or ambiguously segmented text such
as transcribed speech or less formal written text, as
the HHMM parser can also make decisions about
where to put sentence breaks, and do so in linear
time.4
6 Conclusion and Future Work
This paper furthers the case for the HHMM as a
viable psycholinguistic model of the human pars-
ing mechanism by showing that performance de-
grades gracefully as parallelism decreases, provid-
ing reasonably accurate parsing even at very low
beam widths. In addition, this work shows that
an HHMM parser run at low beam widths is com-
petitive in speed with parsers that don?t work in-
crementally, because of its asymptotically linear
runtime.
This is especially surprising given that the
HHMM uses parallel hypotheses on the beam to
predict whether constituents will remain open for
attachment or not. Success at low beam widths
suggests that this optionally arc-eager prediction
is something that is indeed relatively predictable
during parsing, lending credence to claims of psy-
cholinguistic relevance of HHMM parsing.
Future work should explore further directions
in improving parsing performance at low beam
widths. The lowest beam value experiments
presented here generally parsed fairly accurately
when they completed, but were already encounter-
ing problems with unparseable sentences that neg-
atively affected parser accuracy. The large accu-
racy decrease between beam sizes of 20 and 15 is
likely to be mostly due to the lack of any correct
analysis on the beam when the sentence is com-
pleted.
It should be noted, however, that no adjustments
were made to the parser?s syntactic model with
these beam variations. This syntactic model was
optimized for accuracy at the standard beam width
4It does this probabilistically as a side effect of the pars-
ing, by choosing an analysis in which r0t ? RG (for any t).
of 2000, and thus contains some state splittings
that are beneficial at wide beam widths, but at
low beam widths are redundant and prevent oth-
erwise valid hypotheses from being maintained on
the beam. For applications in which speed is a
priority, future research can evaluate tradeoffs in
accuracy that occur at different beam widths with
a coarser-grained syntactic representation that al-
lows for more variation of hypotheses even on
very small beams.
Acknowledgments
This research was supported by National Science
Foundation CAREER/PECASE award 0447685.
The views expressed are not necessarily endorsed
by the sponsors.
References
Steven P. Abney and Mark Johnson. 1991. Memory
requirements and local ambiguities of parsing strate-
gies. J. Psycholinguistic Research, 20(3):233?250.
James Baker. 1975. The Dragon system: an overivew.
IEEE Transactions on Acoustics, Speech and Signal
Processing, 23(1):24?29.
Thomas G. Bever. 1970. The cognitive basis for lin-
guistic structure. In J. ?R. Hayes, editor, Cognition
and the Development of Language, pages 279?362.
Wiley, New York.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
Umesh Patil, and Shravan Vasishth. 2008a. Parsing
costs as predictors of reading difficulty: An evalua-
tion using the Potsdam Sentence Corpus. Journal of
Eye Movement Research, 2(1):1?12.
Marisa Ferrara Boston, John T. Hale, Reinhold Kliegl,
and Shravan Vasishth. 2008b. Surprising parser ac-
tions and reading difficulty. In Proceedings of ACL-
08: HLT, Short Papers, pages 5?8, Columbus, Ohio,
June. Association for Computational Linguistics.
Thorsten Brants and Matthew Crocker. 2000. Prob-
abilistic parsing and psychological plausibility. In
Proceedings of COLING ?00, pages 111?118.
Noam Chomsky and George A. Miller. 1963. Intro-
duction to the formal analysis of natural languages.
In Handbook of Mathematical Psychology, pages
269?321. Wiley.
Nelson Cowan. 2001. The magical number 4 in short-
term memory: A reconsideration of mental storage
capacity. Behavioral and Brain Sciences, 24:87?
185.
Matthew Crocker and Thorsten Brants. 2000. Wide-
coverage probabilistic sentence processing. Journal
of Psycholinguistic Research, 29(6):647?669.
34
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
159?166, Pittsburgh, PA.
James Henderson. 2004. Lookahead in determinis-
tic left-corner parsing. In Proc. Workshop on Incre-
mental Parsing: Bringing Engineering and Cogni-
tion Together, pages 26?33, Barcelona, Spain.
Frederick Jelinek, Lalit R. Bahl, and Robert L. Mercer.
1975. Design of a linguistic statistical decoder for
the recognition of continuous speech. IEEE Trans-
actions on Information Theory, 21:250?256.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423?430, Sapporo,
Japan.
Richard L. Lewis. 2000. Falsifying serial and paral-
lel parsing models: Empirical conundrums and an
overlooked paradigm. Journal of Psycholinguistic
Research, 29:241?248.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
George A. Miller. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. Psychological Review,
63:81?97.
Kevin P. Murphy and Mark A. Paskin. 2001. Lin-
ear time inference in hierarchical HMMs. In Proc.
NIPS, pages 833?840, Vancouver, BC, Canada.
Philip Resnik. 1992. Left-corner parsing and psy-
chological plausibility. In Proceedings of COLING,
pages 191?197, Nantes, France.
Brian Roark, Asaf Bachrach, Carlos Cardenas, and
Christophe Pallier. 2009. Deriving lexical and
syntactic expectation-based measures for psycholin-
guistic modeling via incremental top-down parsing.
Proceedings of the 2009 Conference on Empirical
Methods in Natural Langauge Processing, pages
324?333.
William Schuler, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2008. Toward a
psycholinguistically-motivated model of language.
In Proceedings of COLING, pages 785?792,
Manchester, UK, August.
William Schuler, Samir AbdelRahman, TimMiller, and
Lane Schwartz. 2010. Broad-coverage incremen-
tal parsing using human-like memory constraints.
Computational Linguistics, 36(1).
William Schuler. 2009. Parsing with a bounded
stack using a model-based right-corner transform.
In Proceedings of the North American Association
for Computational Linguistics (NAACL ?09), pages
344?352, Boulder, Colorado.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an
incremental right-corner parser. In Proceedings of
the 49th Annual Conference of the Association for
Computational Linguistics.
35
