Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 19?26,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Revisiting multi-tape automata for Semitic morphological analysis and
generation
Mans Hulden
University of Arizona
Department of Linguistics
mhulden@email.arizona.edu
Abstract
Various methods have been devised to pro-
duce morphological analyzers and gen-
erators for Semitic languages, ranging
from methods based on widely used finite-
state technologies to very specific solu-
tions designed for a specific language
or problem. Since the earliest propos-
als of how to adopt the elsewhere suc-
cessful finite-state methods to root-and-
pattern morphologies, the solution of en-
coding Semitic grammars using multi-tape
automata has resurfaced on a regular ba-
sis. Multi-tape automata, however, require
specific algorithms and reimplementation
of finite-state operators across the board,
and hence such technology has not been
readily available to linguists. This paper,
using an actual Arabic grammar as a case
study, describes an approach to encoding
multi-tape automata on a single tape that
can be implemented using any standard
finite-automaton toolkit.
1 Introduction
1.1 Root-and-pattern morphology and
finite-state systems
The special problems and challenges embodied by
Semitic languages have been recognized from the
early days of applying finite-state methods to nat-
ural language morphological analysis. The lan-
guage model which finite-state methods have been
most successful in describing?a model where
morphemes concatenate in mostly strict linear
order?does not translate congenially to the type
of root-and-pattern morphology found in e.g. Ara-
bic and Hebrew (Kataja and Koskenniemi, 1988;
Lavie et al, 1988).
In Arabic, as in most Semitic languages, verbs
have for a long time been analyzed as consist-
ing of three elements: a (most often) triconsonan-
tal root, such as ktb (H.
H ?), a vowel pattern
containing grammatical information such as voice
(e.g. the vowel a) and a derivational template,
such as CVCVC indicating the class of the verb, all
of which are interdigitated to build a stem, such
as katab (I.

J

?).1 This stem is in turn subject to
more familiar morphological constructions includ-
ing prefixation and suffixation, yielding informa-
tion such as number, person, etc, such as kataba
( I.

J?), the third person singular masculine perfect
form.
The difficulty of capturing this interdigitation
process is not an inherent shortcoming of finite-
state automata or transducers per se, but rather
a result of the methods that are commonly used
to construct automata. Regular expressions that
contain operations such as concatenation, union,
intersection, as well as morphotactic descriptions
through right-linear grammars offer an unwieldy
functionality when it comes to interleaving strings
with one another in a regulated way. But, one
could argue, since large scale morphological ana-
lyzers as finite-state automata/transducers have in-
deed been built (see e.g. Beesley (1996, 1998b,a)),
the question of how to do it becomes one of con-
struction, not feasibility.
1.2 Multitape automata
One early approach, suggested by Kay (1987) and
later pursued in different variants by Kiraz (1994,
2000) among others, was to, instead of modeling
morphology along the more traditional finite-state
transducer, modeling it with a n-tape automaton,
where tapes would carry precisely this interleaving
1Following autosegmental analyses, this paper assumes
the model where the vocalization is not merged with the pat-
tern, i.e. we do not list separate patterns for vocalizations
such as CaCaC as is assumed more traditionally. Which anal-
ysis to choose largely a matter of convenience, and the meth-
ods in this paper apply to either one.
19
that is called for in Semitic interdigitation. How-
ever, large-scale multitape solutions containing the
magnitude of information in standard Arabic dic-
tionaries such as Wehr (1979) have not been re-
ported.
To our knowledge, two large-scale morphologi-
cal analyzers for Arabic that strive for reasonable
completeness have been been built: one by Xerox
and one by Tim Buckwalter (Buckwalter, 2004).
The Xerox analyzer relies on complex extensions
to the finite-state calculus of one and two-tape
automata (transducers) as documented in Beesley
and Karttunen (2003), while Buckwalter?s system
is a procedural approach written in Perl which de-
composes a word and simultaneously consults lex-
ica for constraining the possible decompositions.
Also, in a similar vein to Xerox?s Arabic analyzer,
Yona and Wintner (2008) report on a large-scale
system for Hebrew built on transducer technology.
Most importantly, none of these very large systems
are built around multi-tape automata even though
such a construction from a linguistic perspective
would appear to be a fitting choice when dealing
with root-and-pattern morphology.
1.3 n-tape space complexity
There is a fundamental space complexity problem
with multi-tape automata, which is that when the
number of tapes grows, the required joint sym-
bol alphabet grows with exponential rapidity un-
less special mechanisms are devised to curtail this
growth. This explosion in the number of transi-
tions in an n-tape automaton can in many cases
be more severe than the growth in the number of
states of a complex grammar.
To take a simple, though admittedly slightly ar-
tificial example: suppose we have a 5-tape au-
tomaton, each tape consisting of the same alpha-
bet of, say 22 symbols {s1, . . . , s22}. Now, as-
sume we want to restrict the co-occurrence of s1
on any combination of tapes, meaning s1 can only
occur once on one tape in the same position, i.e.
we would be accepting any strings containing a
symbol such as s1:s2:s2:s2:s2 or s2:s2:s2:s2:s3
but not, s1:s2:s3:s4:s1. Without further treatment
of the alphabet behavior, this yields a multi-tape
automaton which has a single state, but 5,056,506
transitions?each transition naturally representing
a legal combination of symbols on the five tapes.
This kind of transition blow-up is not completely
inevitable: of course one can devise many tricks
to avoid it, such as adding certain semantics to
the transition notation?in our example by per-
haps having a special type of ?failure? transition
which leads to non-acceptance. For the above ex-
ample this would cut down the number of tran-
sitions from 5,056,506 to 97,126. The drawback
with such methods is that any changes will tend
to affect the entire finite-state system one is work-
ing with, requiring adaptations in almost every un-
derlying algorithm to construct automata. One is
then unable to leverage the power of existing soft-
ware designed for finite-state morphological anal-
ysis, but needs to build special-purpose software
for whatever multi-tape implementation one has in
mind.2
1.4 The appeal of the multi-tape solution
The reason multi-tape descriptions of natural lan-
guage morphology are appealing lies not only
in that such solutions seem to be able to han-
dle Semitic verbal interdigitation, but also in
that a multi-tape solution allows for a natural
alignment of information regarding segments and
their grammatical features, something which is
often missing in finite-state-based solutions to
morphological analysis. In the now-classical
way of constructing morphological analyzers, we
have a transducer that maps a string represent-
ing an unanalyzed word form, such as kataba
( I.

J

?) to a string representing an analyzed one,
e.g. ktb +FormI +Perfect +Act +3P
+Masc +Sg. Such transductions seldom pro-
vide grammatical component-wise alignment in-
formation telling which parts of the unanalyzed
words contribute to which parts of the grammat-
ical information. Particularly if morphemes signi-
fying a grammatical category are discontinuous,
this information is difficult to provide naturally
in a finite-automaton based system without many
tapes. A multi-tape solution, on the other hand,
2Two anonymous reviewers point out the work by Habash
et al (2005) and Habash and Rambow (2006) who report an
effort to analyze Arabic with such a multitape system based
on work by Kiraz (2000, 2001) that relies on custom algo-
rithms devised for a multitape alphabet. Although Habash
and Rambow do not discuss the space requirements in their
system, it is to be suspected that the number of transitions
grows quickly using such an method by virtue of the argu-
ment given above. These approaches also use a small number
of tapes (between 3 and 5), and, since the number of tran-
sitions can increase exponentially with the number of tapes
used, such systems do not on the face of it appear to scale
well to more than a handful of tapes without special precau-
tions.
20
Tinput k a t a b a
Troot k t b
Tform Form I
Tptrn C V C V C
Tpaff a
Taffp +3P
+Masc
+Sg
Tvoc a a
Tvocp +Act
. . .
Table 1: A possible alignment of 8 tapes to capture
Arabic verbal morphology.
can provide this information by virtue of its con-
struction. The above example could in an 8-tape
automaton encoding be captured as illustrated in
table 1, assuming here that Tinput is the input tape,
the content of which is provided, and the subse-
quent tapes are output tapes where the parse ap-
pears.
In table 1, we see that the radicals on the root
tape are aligned with the input, as is the pattern on
the pattern tape, the suffix -a on the suffix tape,
which again is aligned with the parse for the suf-
fix on the affix parse tape (affp), and finally the
vocalization a is aligned with the input and the pat-
tern. This is very much in tune with both the type
of analyses linguists seem to prefer (McCarthy,
1981), and more traditional analyses and lexicog-
raphy of root-and-pattern languages such as Ara-
bic.
In what follows, we will present an alternate
encoding for multi-tape automata together with
an implementation of an analyzer for Arabic ver-
bal morphology. The encoding simulates a multi-
tape automaton using a simple one-tape finite-state
machine and can be implemented using standard
toolkits and algorithms given in the literature. The
encoding also avoids the abovementioned blow-up
problems related to symbol combinations on mul-
tiple tapes.
2 Notation
We assume the reader is familiar with the basic
notation regarding finite automata and regular ex-
pressions. We will use the standard operators of
Kleene closure (L?), union (L1 ? L2), intersec-
tion (L1 ? L2), and assume concatenation when-
ever there is no overt operator specified (L1L2).
We use the symbol ? to specify the alphabet, and
the shorthand \a to denote any symbol in the al-
phabet except a. Slight additional notation will be
introduced in the course of elaborating the model.
3 Encoding
In our implementation, we have decided to encode
the multi-tape automaton functionality as consist-
ing of a single string read by a single-tape automa-
ton, where the multiple tapes are all evenly inter-
leaved. The first symbol corresponds to the first
symbol on tape 1, the second to the first on tape 2,
etc.:
T1 ? ? ?
. . .
Tn?1 ? ? ?
Tn ? ? ?
. . .
For instance, the two-tape correspondence:
T1 a
T2 b c
would be encoded as the string ab?c, ? being a spe-
cial symbol used to pad the blanks on a tape to
keep all tapes synchronized.
This means that, for example, for an 8-tape rep-
resentation, every 8th symbol from the beginning
is a symbol representing tape 1.
Although this is the final encoding we wish to
produce, we have added one extra temporary fea-
ture to facilitate the construction: every symbol on
any ?tape? is always preceded by a symbol indi-
cating the tape number drawn from an alphabet
T1, . . . , Tn. These symbols are removed eventu-
ally. That means that during the construction, the
above two-tape example would be represented by
the string T1aT2bT1?T2c. This simple redundancy
mechanism will ease the writing of grammars and
actually limit the size of intermediate automata
during construction.
4 Construction
4.1 Overview
We construct a finite-state n-tape simulation gram-
mar in two steps. Firstly we populate each ?tape?
with all grammatically possible strings. That
means that, for our Arabic example, the root tape
21
should contain all possible roots we wish to ac-
cept, the template tape all the possible templates,
etc. We call this language the Base. The second
step is to constrain the co-occurrence of symbols
on the individual tapes, i.e. a consonant on the root
tape must be matched by a consonant of the input
tape as well as the symbol C on the pattern tape,
etc. Our grammar then consists of all the permit-
ted combinations of tape symbols allowed by a)
the Base and b) the Rules. The resulting lan-
guage is simply their intersection, viz.:
Base ? Rules
4.2 Populating the tapes
We have three auxiliary functions, TapeL(X,Y),
TapeM(X,Y), and TapeA(X,Y), where the ar-
gument X is the tape number, and Y the language
we with to insert on tape X.3 TapeL(X,Y) cre-
ates strings where every symbol from the language
Y is preceded by the tape indicator TX and where
the entire tape is left-aligned, meaning there are
no initial blanks on that tape. TapeM is the same
function, except words on that tape can be pre-
ceded by blanks and succeeded by blanks. TapeA
allows for any alignment of blanks within words
or to the left or right. Hence, to illustrate this
behavior, TapeL(4,C V C V C) will produce
strings like:
XT4CXT4VXT4CXT4VXT4CY
where X is any sequence of symbols not contain-
ing the symbol T4, and Y any sequence possibly
containing T4 but where T4 is always followed by
?, i.e. we pad all tapes at the end to allow for syn-
chronized strings on other tapes containing more
material to the right.
Now, if, as in our grammar, tape 4 is the tem-
plate tape, we would populate that tape by declar-
ing the language:
TapeM(4,Templates)
assuming Templates is the language that ac-
cepts all legal template strings, e.g. CVCVC,
CVCCVC, etc.
Hence, our complete Base language (continu-
ing with the 8-tape example) is:
3See the appendix for exact definitions of these functions.
TapeL(1,Inputs) ?
TapeA(2,Roots) ?
TapeL(3,Forms) ?
TapeM(4,Templates) ?
TapeA(5,Affixes) ?
TapeM(6,Parses) ?
TapeA(7,Voc) ?
TapeL(8,VocParses) ?
(T1?T2?T3?T4?T5?T6?T7?T8?)?
This will produce the language where all strings
are multiples of 16 in length. Every other sym-
bol is the TX tape marker symbol and every other
symbol is the actual symbol on that tape (allowing
for the special symbol ? also to represent blanks on
a tape). Naturally, we will want to define Inputs
occurring on tape 1 as any string containing any
combination of symbols since it represents all pos-
sible input words we wish to parse. Similarly, tape
2 will contain all possible roots, etc. This Base
language is subsequently constrained so that sym-
bols on different tapes align correctly and are only
allowed if they represent a legal parse of the word
on the input tape (tape 1).
4.3 Constructing the rules
When constructing the rules that constrain the co-
occurrence of symbols on the various tapes we
shall primarily take advantage of the ? oper-
ator first introduced for two-level grammars by
Koskenniemi (1983).4 The semantics is as fol-
lows. A statement:
X ? L1 R1, . . . , Ln Rn
where X and Li, Ri are all regular languages
defines the regular language where every instance
of a substring drawn from the languageX must be
surrounded by some pair Li and Ri to the left and
right, respectively.5
Indeed, all of our rules will consist exclusively
of? statements.
To take an example: in order to constrain the
template we need two rules that effectively say that
every C and V symbol occurring in the template
4There is a slight, but subtle difference in notation,
though: the original two-level? operator constrained single
symbols only (such as a:b, which was considered at compile-
time a single symbol); here, the argument X refers to any
arbitrary language.
5Many finite-state toolkits contain this as a separate op-
erator. See Yli-Jyra? and Koskenniemi (2004) and Hulden
(2008) for how such statements can be converted into regular
expressions and finite automata.
22
tape must be matched by 1) a consonant on the root
tape and 2) a vowel on the input tape. Because of
our single-tape encoding the first rule translates to
the idea that every T4 C sequence must be directly
preceded by T2 followed by some consonant fol-
lowed by T3 and any symbol at all:
T4 C ? T2 Cons T3 ? (1)
and the second one translates to:
T4 V ? T1 Vow T2 ? T3 ? (2)
assuming that Vow is the language that contains
any vowel and Cons the language that contains
any consonant.
Similarly, we want to constrain the Forms
parse tape that contains symbols such as Form I,
Form II etc., so that if, for example, Form I oc-
curs on that tape, the pattern CVCVC must occur on
the pattern tape.6
T3 Form I? TapeM(4,C V C V C) (3)
and likewise for all the other forms. It should be
noted that most constraints are very strictly local
to within a few symbols, depending slightly on the
ordering and function of the tapes. In (1), for in-
stance, which constrains a symbol on tape 4 with
a consonant on tape 2, there are only 2 interven-
ing symbols, namely that of tape 3. The ordering
of the tapes thus has some bearing on both how
simple the rules are to write, and the size of the re-
sulting automaton. Naturally, tapes that constrain
each other are ideally placed in adjacent positions
whenever possible.
Of course, some long-distance constraints will
be inevitable. For example, Form II is generally
described as a CVCCVC pattern, where the extra
consonant is a geminate, as in the stem kattab,
where the t of the root associates with both C?s
in the pattern. To distinguish this C behavior
from that of Form X which is also commonly de-
scribed with two adjacent C symbols where, how-
ever, there is no such association (as in the stem
staktab) we need to introduce another symbol.
6To be more exact, to be able to match and parse both
fully vocalized words such as wadarasat (
I ? P

X

?), and un-
vocalized ones, such as wdrst ( I?PX?), we want the pattern
CVCVC to actually be represented by the regular expression
C (V) C (V) C, i.e. where the vowels are optional. Note,
however, that the rule that constrains T4 V above only re-
quires that the V matches if there indeed is one. Hence,
by declaring vowels in patterns (and vocalizations) to be op-
tional, we can always parse any partially, fully, or unvocalized
verb. Of course, fully unvocalized words will be much more
ambiguous and yield more parses.
This symbol C2 occurs in Form II, which becomes
CVCC2VC. We then introduce a constraint to the
effect that any C2-symbol must be matched on the
input by a consonant, which is identical to the pre-
vious consonant on the input tape.7 These long-
distance dependencies can be avoided to some ex-
tent by grammar engineering, but so long as they
do not cause a combinatorial explosion in the num-
ber of states of the resulting grammar automaton,
we have decided to include them for the sake of
clarity.
To give an overview of some of the subsequent
constraints that are still necessary, we include here
a few descriptions and examples (where the starred
(***) tape snippets exemplify illegal configura-
tions):
? Every root consonant has a matching conso-
nant on the input tape
T1 k a t a b a
T2 k t b
T1 k a t a b a
T2*** d r s
? A vowel in the input which is matched by a
V in the pattern, must have a corresponding
vocalization vowel
T1 k a t a b a
T4 C V C V C
T7 a a
T1 k a t a b a
T4 C V C V C
T7*** u i
? A position where there is a symbol in the in-
put either has a symbol in the pattern tape or
a symbol in the affix tape (but not both)
T1 k a t a b a
T4 C V C V C
T5 a
T1 k a t a b a
T4 C V C V C
T5***
7The idea to preserve the gemination in the grammar is
similar to the solutions regarding gemination and spreading
of Forms II, V, and IX documented in Beesley (1998b) and
Habash and Rambow (2006).
23
4.4 The final automaton
As mentioned above, the symbols {T1, . . . , Tn}
are only used during construction of the automa-
ton for the convenience of writing the grammar,
and shall be removed after intersecting the Base
language with the Rules languages. This is a sim-
ple substitution TX ? , i.e. the empty string.
Hence, the grammar is compiled as:
Grammar = h(Base ? Rules)
where h is a homomorphism that replaces TX
symbols with , the empty string.
5 Efficiency Considerations
Because the construction method proposed can
very quickly produce automata of considerable
size, there are a few issues to consider when de-
signing a grammar this way. Of primary concern
is that since one is constructing deterministic au-
tomata, long-distance constraints should be kept
to a minimum. Local constraints, which the ma-
jority of grammar rules encode, yield so-called k-
testable languages when represented as finite au-
tomata, and the state complexity of their inter-
section grows additively. For larger k, however,
growth is more rapid which means that, for ex-
ample, when one is designing the content of the
individual tapes, care should be taken to ensure
that segments or symbols which are related to each
other preferably align very closely on the tapes.
Naturally, this same goal is of linguistic interest as
well and a grammar which does not align gram-
matical information with segments in the input is
likely not a good grammar. However, there are a
couple of ways in which one can go astray. For
instance, in the running example we have pre-
sented, one of the parse tapes has included the
symbol +3P +Masc +Sg, aligned with the affix
that represents the grammatical information:
. . .
T5 a
T6 +3P
+Masc
+Sg
. . .
However, if it be the case that what the parse
tape reflects is a prefix or a circumfix, as will be
the case with the imperfective, subjunctive and
jussive forms, the following alignment would be
somewhat inefficient:
. . .
T5 t a
T6 +3P
+Fem
+Sg
. . .
This is because the prefix ta, which appears
early in the word, is reflected on tape 6 at the end
of the word, in effect unnecessarily producing a
very long-distance dependency and hence dupli-
cates of states in the automaton encoding the in-
tervening material. A more efficient strategy is to
place the parse or annotation tape material as close
as possible to the segments which have a bearing
on it, i.e.:
. . .
T5 t a
T6 +3P
+Fem
+Sg
. . .
This alignment can be achieved by a constraint
in the grammar to the effect that the first non-blank
symbol on the affix tape is in the same position as
the first non-blank symbol on the affix parse tape.
It is also worth noting that our implementation
does not yet restrict the co-occurrence of roots and
forms, i.e. it will parse any word in any root in the
lexicon in any of the forms I-VIII, X. Adding these
restrictions will presumably produce some growth
in the automaton. However, for the time being we
have also experimented with accepting any trilit-
eral root?i.e. any valid consonantal combination.
This has drastically cut the size of the resulting
automaton to only roughly 2,000 states without
much overgeneration in the sense that words will
not incorrectly be matched with the wrong root.
The reason for this small footprint when not hav-
ing a ?real? lexicon is fairly obvious?all depen-
dencies between the root tape and the pattern tape
and the input tape are instantly resolved in the span
of one ?column? or 7 symbols.
6 Algorithmic additions
Naturally, one can parse words by simply inter-
secting TapeL(1, word) ? Grammar, where
24
word is the word at hand and printing out all the
legal strings. Still, this is unwieldy because of
the intersection operation involved and for faster
lookup speeds one needs to consider an algorith-
mic extension that performs this lookup directly
on the Grammar automaton.
6.1 Single-tape transduction
For our implementation, we have simply modified
the automaton matching algorithm in the toolkit
we have used, foma8 to, instead of matching ev-
ery symbol, matching the first symbol as the ?in-
put?, then outputting the subsequent n (where n
is 7 in our example) legal symbols if the subse-
quent input symbols match. Because the grammar
is quite constrained, this produces very little tem-
porary ambiguity in the depth-first search traversal
of the automaton and transduces an input to the
output tapes in nearly linear time.
7 Future work
The transduction mechanism mentioned above
works well and is particularly easy to implement
when the first ?tape? is the input tape containing
the word one wants to parse, since one can simply
do a depth-first search until the the next symbol
on the input tape (in our running example with 8
tapes, that would be 7 symbols forward) and dis-
card the paths where the subsequent tape 1 sym-
bols do not match, resulting in nearly linear run-
ning time. However, for the generation problem,
the solution is less obvious. If one wanted to sup-
ply any of the other tapes with a ready input (such
as form, root, and a combination of grammatical
categories), and then yield a string on tape 1, the
problem would be more difficult. Naturally, one
can intersect various TapeX(n, content) languages
against the grammar, producing all the possible in-
put strings that could have generated such a parse,
but this method is rather slow and results only in
a few parses per second on our system. Devis-
ing a fast algorithm to achieve this would be desir-
able for applications where one wanted to, for in-
stance, generate all possible vocalization patterns
in a given word, or for IR purposes where one
would automatically apply vocalizations to Arabic
words.
8See the appendix.
8 Conclusion
We have described a straightforward method by
which morphological analyzers for languages that
exhibit root-and-pattern morphology can be built
using standard finite-state methods to simulate
multi-tape automata. This enables one to take
advantage of already widely available standard
toolkits designed for construction of single-tape
automata or finite-state transducers. The feasibil-
ity of the approach has been tested with a limited
implementation of Arabic verbal morphology that
contains roughly 2,000 roots, yielding automata of
manageable size. With some care in construction
the method should be readily applicable to larger
projects in Arabic and other languages, in partic-
ular to languages that exhibit root-and-pattern or
templatic morphologies.
References
Beesley, K. and Karttunen, L. (2003). Finite-State
Morphology. CSLI, Stanford.
Beesley, K. R. (1996). Arabic finite-state analysis
and generation. In COLING ?96.
Beesley, K. R. (1998a). Arabic morphology us-
ing only finite-state operations. In Proceedings
of the Workshop on Computational Approaches
to Semitic Languages COLING-ACL, pages 50?
57.
Beesley, K. R. (1998b). Consonant spreading in
Arabic stems. In ACL, volume 36, pages 117?
123. Association for Computational Linguis-
tics.
Beeston, A. F. L. (1968). Written Arabic: An ap-
proach to the basic structures. Cambridge Uni-
versity Press, Cambridge.
Buckwalter, T. (2004). Arabic morphological an-
alyzer 2.0. LDC.
Habash, N. and Rambow, O. (2006). MAGEAD:
A morphological analyzer and generator for the
Arabic dialects. Proceedings of COLING-ACL
2006.
Habash, N., Rambow, O., and Kiraz, G. (2005).
Morphological analysis and generation for Ara-
bic dialects. Proceedings of the Workshop
on Computational Approaches to Semitic Lan-
guages (ACL ?05).
Hulden, M. (2008). Regular expressions and pred-
icate logic in finite-state language processing.
25
In Piskorski, J., Watson, B., and Yli-Jyra?, A.,
editors, Proceedings of FSMNLP 2008.
Kataja, L. and Koskenniemi, K. (1988). Finite-
state description of Semitic morphology: a case
study of ancient Akkadian. In COLING ?88,
pages 313?315.
Kay, M. (1987). Nonconcatenative finite-state
morphology. In Proceedings of the third con-
ference on European chapter of the Association
for Computational Linguistics, pages 2?10. As-
sociation for Computational Linguistics.
Kiraz, G. A. (1994). Multi-tape two-level mor-
phology: A case study in Semitic non-linear
morphology. In COLING ?94, pages 180?186.
Kiraz, G. A. (2000). Multi-tiered nonlinear mor-
phology using multitape finite automata: A case
study on Syriac and Arabic. Computational Lin-
guistics, 26(1):77?105.
Kiraz, G. A. (2001). Computational nonlinear
morphology: with emphasis on Semitic lan-
guages. Cambridge University Press, Cam-
bridge.
Koskenniemi, K. (1983). Two-level morphology:
A general computational model for word-form
recognition and production. Publication 11,
University of Helsinki, Department of General
Linguistics, Helsinki.
Lavie, A., Itai, A., and Ornan, U. (1988). On the
applicability of two level morphology to the in-
flection of Hebrew verbs. In Proceedings of
ALLC III, pages 246?260.
McCarthy, J. J. (1981). A Prosodic Theory of Non-
concatenative Morphology. Linguistic Inquiry,
12(3):373?418.
van Noord, G. (2000). FSA 6 Reference Manual.
Wehr, H. (1979). A Dictionary of Modern Writ-
ten Arabic. Spoken Language Services, Inc.,
Ithaca, NY.
Yli-Jyra?, A. and Koskenniemi, K. (2004). Compil-
ing contextual restrictions on strings into finite-
state automata. The Eindhoven FASTAR Days
Proceedings.
Yona, S. and Wintner, S. (2008). A finite-state
morphological grammar of Hebrew. Natural
Language Engineering, 14(2):173?190.
9 Appendix
The practical implementation described in the pa-
per was done with the freely available (GNU Li-
cence) foma finite-state toolkit.9. However, all of
the techniques used are available in other toolk-
its as well, such as xfst (Beesley and Karttunen,
2003), or fsa (van Noord, 2000)), and translation
of the notation should be straightforward.
The functions for populating the tapes in section
4.2, were defined in foma as follows:
TapeL(X,Y) =
[[Y ? [[0?\X \X]* [0?X]
?
]*]2
[X E|\X \X]*]
TapeM(X,Y) = [[Y ? [0?[\X \X|X E]]*
[0?\X \X]* [0?X]
?
]*]2 [X E|\X \X]*]
TapeA(X,Y) = [[Y ?
[0?\X \X|X E]* 0?X
?
]*]2;
Here, TapeX is a function of two variables, X
and Y. Transducer composition is denoted by ?,
cross-product by ?, the lower projection of a re-
lation by L2, and union by |. Brackets indicate
grouping and ? any symbol. The notation \X de-
notes any single symbol, except X . The symbol ?
here is the special ?blank? symbol used to pad the
tapes and keep them synchronized.
9http://foma.sourceforge.net
26
