Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 446?450,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling Human Inference Process for  
Textual Entailment Recognition 
 
Hen-Hsen Huang Kai-Chun Chang Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan 
{hhhuang, kcchang}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw 
 
 
Abstract 
This paper aims at understanding what hu-
man think in textual entailment (TE) recogni-
tion process and modeling their thinking pro-
cess to deal with this problem. We first ana-
lyze a labeled RTE-5 test set and find that the 
negative entailment phenomena are very ef-
fective features for TE recognition. Then, a 
method is proposed to extract this kind of 
phenomena from text-hypothesis pairs auto-
matically. We evaluate the performance of 
using the negative entailment phenomena on 
both the English RTE-5 dataset and Chinese 
NTCIR-9 RITE dataset, and conclude the 
same findings. 
1 Introduction 
Textual Entailment (TE) is a directional relation-
ship between pairs of text expressions, text (T) 
and hypothesis (H). If human would agree that 
the meaning of H can be inferred from the mean-
ing of T, we say that T entails H (Dagan et al, 
2006). The researches on textual entailment have 
attracted much attention in recent years due to its 
potential applications (Androutsopoulos and Ma-
lakasiotis, 2010). Recognizing Textual Entail-
ment (RTE) (Bentivogli, et al, 2011), a series of 
evaluations on the developments of English TE 
recognition technologies, have been held seven 
times up to 2011. In the meanwhile, TE recogni-
tion technologies in other languages are also un-
derway (Shima, et al, 2011).   
Sammons, et al, (2010) propose an evaluation 
metric to examine the characteristics of a TE 
recognition system. They annotate text-
hypothesis pairs selected from the RTE-5 test set 
with a series of linguistic phenomena required in 
the human inference process. The RTE systems 
are evaluated by the new indicators, such as how 
many T-H pairs annotated with a particular phe-
nomenon can be correctly recognized. The indi-
cators can tell developers which systems are bet-
ter to deal with T-H pairs with the appearance of 
which phenomenon. That would give developers 
a direction to enhance their RTE systems. 
Such linguistic phenomena are thought as im-
portant in the human inference process by anno-
tators. In this paper, we use this valuable re-
source from a different aspect. We aim at know-
ing the ultimate performance of TE recognition 
systems which embody human knowledge in the 
inference process. The experiments show five 
negative entailment phenomena are strong fea-
tures for TE recognition, and this finding con-
firms the previous study of Vanderwende et al 
(2006). We propose a method to acquire the lin-
guistic phenomena automatically and use them in 
TE recognition.  
This paper is organized as follows. In Section 
2, we introduce linguistic phenomena used by 
annotators in the inference process and point out 
five significant negative entailment phenomena. 
Section 3 proposes a method to extract them 
from T-H pairs automatically, and discuss their 
effects on TE recognition. In Section 4, we ex-
tend the methodology to the BC (binary class 
subtask) dataset distributed by NTCIR-9 RITE 
task (Shima, et al, 2011) and discuss their ef-
fects on TE recognition in Chinese. Section 5 
concludes the remarks. 
2 Human Inference Process in TE 
We regard the human annotated phenomena as 
features in recognizing the binary entailment re-
lation between the given T-H pairs, i.e., EN-
TAILMENT and NO ENTAILMENT. Total 210 
T-H pairs are chosen from the RTE-5 test set by 
Sammons et al (2010), and total 39 linguistic 
phenomena divided into the 5 aspects, including 
knowledge domains, hypothesis structures, infer-
ence phenomena, negative entailment phenome-
446
na, and knowledge resources, are annotated on 
the selected dataset. 
2.1 Five aspects as features 
We train SVM classifiers to evaluate the perfor-
mances of the five aspects of phenomena as fea-
tures for TE recognition. LIBSVM RBF kernel 
(Chang and Lin, 2011) is adopted to develop 
classifiers with the parameters tuned by grid 
search. The experiments are done with 10-fold 
cross validation. 
For the dataset of Sammons et al (2010), two 
annotators are involved in labeling the above 39 
linguistic phenomena on the T-H pairs. They 
may agree or disagree in the annotation. In the 
experiments, we consider the effects of their 
agreement. Table 1 shows the results. Five as-
pects are first regarded as individual features, 
and are then merged together. Schemes ?Annota-
tor A? and ?Annotator B? mean the phenomena 
labelled by annotator A and annotator B are used 
as features respectively.  The ?A AND B? 
scheme, a strict criterion, denotes a phenomenon 
exists in a T-H pair only if both annotators agree 
with its appearance. In contrast, the ?A OR B? 
scheme, a looser criterion, denotes a phenome-
non exists in a T-H pair if at least one annotator 
marks its appearance. 
We can see that the aspect of negative entail-
ment phenomena is the most significant feature 
among the five aspects. With only 9 phenomena 
in this aspect, the SVM classifier achieves accu-
racy above 90% no matter which labeling 
schemes are adopted. Comparatively, the best 
accuracy in RTE-5 task is 73.5% (Iftene and 
Moruz, 2009). In negative entailment phenomena 
aspect, the ?A OR B? scheme achieves the best 
accuracy. In the following experiments, we adopt 
this labeling scheme. 
2.2 Negative entailment phenomena 
There is a large gap between using negative en-
tailment phenomena and using the second effec-
tive features (i.e., inference phenomena). Moreo-
ver, using the negative entailment phenomena as 
features only is even better than using all the 39 
linguistic phenomena. We further analyze which 
negative entailment phenomena are more signifi-
cant. 
There are nine linguistic phenomena in the as-
pect of negative entailment. We take each phe-
nomenon as a single feature to do the two-way 
textual entailment recognition. The ?A OR B? 
scheme is applied. Table 2 shows the experi-
mental results. 
 Annotator A Annotator B A AND B A OR B 
Knowledge  
Domains 
50.95% 52.38% 52.38% 50.95% 
Hypothesis  
Structures 
50.95% 51.90% 50.95% 51.90% 
Inference  
Phenomena 
74.29% 72.38% 72.86% 74.76% 
Negative  
Entailment  
Phenomena 
97.14% 95.71% 92.38% 97.62% 
Knowledge  
Resources 
69.05% 69.52% 67.62% 69.52% 
ALL  97.14% 92.20% 90.48% 97.14% 
Table 1: Accuracy of recognizing binary TE rela-
tion with the five aspects as features. 
 
Phenomenon ID Negative entailment  
Phenomenon  
Accuracy 
0 Named Entity mismatch 60.95% 
1 Numeric Quantity mismatch 54.76% 
2 Disconnected argument 55.24% 
3 Disconnected relation 57.62% 
4 Exclusive argument 61.90% 
5 Exclusive relation 56.67% 
6 Missing modifier 56.19% 
7 Missing argument 69.52% 
8 Missing relation 68.57% 
Table 2: Accuracy of recognizing TE relation 
with individual negative entailment phenomena. 
 
The 1st column is phenomenon ID, the 2nd col-
umn is the phenomenon, and the 3rd column is 
the accuracy of using the phenomenon in the bi-
nary classification. Comparing with the best ac-
curacy 97.62% shown in Table 1, the highest 
accuracy in Table 2 is 69.52%, when missing 
argument is adopted. It shows that each phenom-
enon is suitable for some T-H pairs, and merging 
all negative entailment phenomena together 
achieves the best performance.  
We consider all possible combinations of 
these 9 negative entailment phenomena, i.e., 
  
 +?+   
  =511 feature settings, and use each 
feature setting to do 2-way entailment relation 
recognition by LIBSVM. The notation   
  de-
notes a set of 
  
(   )   
 feature settings, each with 
n features.  
The model using all nine phenomena achieves 
the best accuracy of 97.62%. Examining the 
combination sets, we find phenomena IDs 3, 4, 5, 
7 and 8 appear quite often in the top 4 feature 
settings of each combination set. In fact, this set-
ting achieves an accuracy of 95.24%, which is 
the best performance in   
  combination set. On 
the one hand, adding more phenomena into (3, 4, 
5, 7, 8) setting does not have much performance 
difference.  
In the above experiments, we do all the anal-
yses on the corpus annotated with linguistic phe-
nomena by human. We aim at knowing the ulti-
447
mate performance of TE recognition systems 
embodying human knowledge in the inference. 
The human knowledge in the inference cannot be 
captured by TE recognition systems fully correct-
ly. In the later experiments, we explore the five 
critical features, (3, 4, 5, 7, 8), and examine how 
the performance is affected if they are extracted 
automatically. 
3 Negative Entailment Phenomena Ex-
traction 
The experimental results in Section 2.2 show that 
disconnected relation, exclusive argument, ex-
clusive relation, missing argument, and missing 
relation are significant. We follow the definitions 
of Sammons et al (2010) and show them as fol-
lows. 
(a) Disconnected Relation. The arguments and 
the relations in Hypothesis (H) are all matched 
by counterparts in Text (T). None of the argu-
ments in T is connected to the matching relation. 
(b) Exclusive Argument. There is a relation 
common to both the hypothesis and the text, but 
one argument is matched in a way that makes H 
contradict T. 
(c) Exclusive Relation. There are two or more 
arguments in the hypothesis that are also related 
in the text, but by a relation that means H contra-
dicts T. 
(d) Missing Argument. Entailment fails be-
cause an argument in the Hypothesis is not pre-
sent in the Text, either explicitly or implicitly. 
(e) Missing Relation. Entailment fails because 
a relation in the Hypothesis is not present in the 
Text, either explicitly or implicitly. 
To model the annotator?s inference process, 
we must first determine the arguments and the 
relations existing in T and H, and then align the 
arguments and relations in H to the related ones 
in T. It is easy for human to find the important 
parts in a text description in the inference process, 
but it is challenging for a machine to determine 
what words are important and what are not, and 
to detect the boundary of arguments and relations. 
Moreover, two arguments (relations) of strong 
semantic relatedness are not always literally 
identical.  
In the following, a method is proposed to ex-
tract the phenomena from T-H pairs automatical-
ly. Before extraction, the English T-H pairs are 
pre-processed by numerical character transfor-
mation, POS tagging, and dependency parsing 
with Stanford Parser (Marneffe, et al, 2006; 
Levy and Manning, 2003), and stemming with 
NLTK (Bird, 2006). 
3.1 A feature extraction method 
Given a T-H pair, we first extract 4 sets of noun 
phrases based on their POS tags, including {noun 
in H}, {named entity (nnp) in H}, {compound 
noun (cnn) in T}, and {compound noun (cnn) in 
H}.  Then, we extract 2 sets of relations, includ-
ing {relation in H} and {relation in T}, where 
each relation in the sets is in a form of Predi-
cate(Argument1, Argument2).  Some typical ex-
amples of relations are verb(subject, object) for 
verb phrases, neg(A, B) for negations, num(Noun, 
number) for numeric modifier, and tmod(C, tem-
poral argument) for temporal modifier. A predi-
cate has only 2 arguments in this representation. 
Thus, a di-transitive verb is in terms of two rela-
tions. 
Instead of measuring the relatedness of T-H 
pairs by comparing T and H on the predicate-
argument structure (Wang and Zhang, 2009), our 
method tries to find the five negative entailment 
phenomena based on the similar representation. 
Each of the five negative entailment phenomena 
is extracted as follows according to their defini-
tions. To reduce the error propagation which may 
be arisen from the parsing errors, we directly 
match those nouns and named entities appearing 
in H to the text T. Furthermore, we introduce 
WordNet to align arguments in H to T. 
(a) Disconnected Relation. If (1) for each a ? 
{noun in H}?{nnp in H}?{cnn in H}, we can 
find a ? T too, and (2) for each r1=h(a1,a2) ? 
{relation in H}, we can find a relation r2=h(a3,a4) 
? {relation in T} with the same header h, but 
with different arguments, i.e., a3?a1 and a4?a2, 
then we say the T-H pair has the ?Disconnected 
Relation?  phenomenon. 
(b) Exclusive Argument. If there exist a rela-
tion r1=h(a1,a2)?{relation in H}, and a relation 
r2=h(a3,a4)?{relation in T} where both relations 
have the same header h, but either the pair (a1,a3) 
or the pair (a2,a4) is an antonym by looking up 
WordNet, then we say the T-H pair has the ?Ex-
clusive Argument? phenomenon.   
(c) Exclusive Relation. If there exist a relation 
r1=h1(a1,a2)?{relation in T}, and a relation 
r2=h2(a1,a2)?{relation in H} where both relations 
have the same arguments, but h1 and h2 have the 
opposite meanings by consulting WordNet, then 
we say that the T-H pair has the ?Exclusive Rela-
tion? phenomenon. 
448
(d) Missing Argument. For each argument a1 
?{noun in H}?{nnp in H}?{cnn in H}, if there 
does not exist an argument a2?T such that a1=a2, 
then we say that the T-H pair has ?Missing Ar-
gument? phenomenon. 
(e) Missing Relation. For each relation 
r1=h1(a1,a2)?{relation in H}, if there does not 
exist a relation r2=h2(a3,a4)?{relation in T} such 
that h1=h2, then we say that the T-H pair has 
?Missing Relation? phenomenon. 
3.2 Experiments and discussion 
The following two datasets are used in English 
TE recognition experiments. 
(a) 210 pairs from part of RTE-5 test set. The 
210 T-H pairs are annotated with the linguistic 
phenomena by human annotators.  They are se-
lected from the 600 pairs in RTE-5 test set, in-
cluding 51% ENTAILMENT and 49% NO EN-
TAILMENT. 
(b) 600 pairs of RTE-5 test set. The original 
RTE-5 test set, including 50% ENTAILMENT 
and 50% NO ENTAILMENT.  
Table 3 shows the performances of TE recog-
nition. The ?Machine-annotated? and the ?Hu-
man-annotated? columns denote that the phe-
nomena annotated by machine and human are 
used in the evaluation respectively. Using ?Hu-
man-annotated? phenomena can be seen as the 
upper-bound of the experiments. The perfor-
mance of using machine-annotated features in 
210-pair and 600-pair datasets is 52.38% and 
59.17% respectively. 
Though the performance of using the phenom-
ena extracted automatically by machine is not 
comparable to that of using the human annotated 
ones, the accuracy achieved by using only 5 fea-
tures (59.17%) is just a little lower than the aver-
age accuracy of all runs in RTE-5 formal runs 
(60.36%) (Bentivogli, et al, 2009). It shows that 
the significant phenomena are really effective in 
dealing with entailment recognition. If we can 
improve the performance of the automatic phe-
nomena extraction, it may make a great progress 
on the textual entailment. 
 
Phenomena 210 pairs 600 pairs 
Machine- 
annotated 
Human- 
annotated 
Machine- 
annotated 
Disconnected Relation 50.95% 57.62% 54.17% 
Exclusive Argument 50.95% 61.90% 55.67% 
Exclusive Relation 50.95% 56.67% 51.33% 
Missing Argument 53.81% 69.52% 56.17% 
Missing Relation 50.95% 68.57% 52.83% 
All 52.38% 95.24% 59.17% 
Table 3: Accuracy of textual entailment recogni-
tion using the extracted phenomena as features. 
4 Negative Entailment Phenomena in 
Chinese RITE Dataset 
To make sure if negative entailment phenomena 
exist in other languages, we apply the methodol-
ogies in Sections 2 and 3 to the RITE dataset in 
NTCIR-9. We annotate all the 9 negative entail-
ment phenomena on Chinese T-H pairs according 
to the definitions by Sammons et al (2010) and 
analyze the effects of various combinations of 
the phenomena on the new annotated Chinese 
data. The accuracy of using all the 9 phenomena 
as features (i.e.,   
  setting) is 91.11%. It shows 
the same tendency as the analyses on English 
data. The significant negative entailment phe-
nomena on Chinese data, i.e., (3, 4, 5, 7, 8), are 
also identical to those on English data. The mod-
el using only 5 phenomena achieves an accuracy 
of 90.78%, which is very close to the perfor-
mance using all phenomena.  
We also classify the entailment relation using 
the phenomena extracted automatically by the 
similar method shown in Section 3.1, and get a 
similar result. The accuracy achieved by using 
the five automatically extracted phenomena as 
features is 57.11%, and the average accuracy of 
all runs in NTCIR-9 RITE task is 59.36% (Shima, 
et al, 2011). Compared to the other methods us-
ing a lot of features, only a small number of bi-
nary features are used in our method. Those ob-
servations establish what we can call a useful 
baseline for TE recognition. 
5 Conclusion 
In this paper we conclude that the negative en-
tailment phenomena have a great effect in deal-
ing with TE recognition. Systems with human 
annotated knowledge achieve very good perfor-
mance. Experimental results show that not only 
can it be applied to the English TE problem, but 
also has the similar effect on the Chinese TE 
recognition. Though the automatic extraction of 
the negative entailment phenomena still needs a 
lot of efforts, it gives us a new direction to deal 
with the TE problem.  
The fundamental issues such as determining 
the boundary of the arguments and the relations, 
finding the implicit arguments and relations, ver-
ifying the antonyms of arguments and relations, 
and determining their alignments need to be fur-
ther examined to extract correct negative entail-
ment phenomena. Besides, learning-based ap-
proaches to extract phenomena and multi-class 
TE recognition will be explored in the future.  
449
Acknowledgments 
 
This research was partially supported by Excel-
lent Research Projects of National Taiwan Uni-
versity under contract 102R890858 and 2012 
Google Research Award.  
References 
Ion Androutsopoulos and Prodromos Malakasiotis. 
2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence 
Research, 38:135-187.  
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang 
Dang, and Danilo Giampiccolo. 2011. The seventh 
PASCAL recognizing textual entailment challenge. 
In Proceedings of the 2011 Text Analysis 
Conference (TAC 2011), Gaithersburg, Maryland, 
USA.. 
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, 
Danilo Giampiccolo, and Bernardo Magnini. 2009. 
The fifth PASCAL recognizing textual entailment 
challenge. In Proceedings of the 2009 Text 
Analysis Conference (TAC 2009), Gaithersburg, 
Maryland, USA. 
Steven Bird. 2006. NLTK: the natural language 
toolkit. In Proceedings of the 21st International 
Conference on Computational Linguistics and 44th 
Annual Meeting of the Association for Computa-
tional Linguistics (COLING-ACL 2006), pages 69-
72. 
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: 
a Library for Support Vector Machines. ACM 
Transactions on Intelligent Systems and Technolo-
gy, 2:27:1-27:27. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Ido Dagan, Oren Glickman, and Bernardo Magnini. 
2006. The PASCAL Recognising Textual Entail-
ment Challenge.  Lecture Notes in Computer Sci-
ence, 3944:177-190. 
Adrian Iftene and Mihai Alex Moruz. 2009. UAIC 
Participation at RTE5. In Proceedings of the 2009 
Text Analysis Conference (TAC 2009), 
Gaithersburg, Maryland, USA. 
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? 
In Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics (ACL 
2003), pages 439-446. 
Marie-Catherine de Marneffe, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. In 
The Fifth International Conference on Language 
Resources and Evaluation (LREC 2006), pages 
449-454. 
Mark Sammons, V.G.Vinod Vydiswaran, and Dan 
Roth. 2010. Ask not what textual entailment can do 
for you... In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2010), pages 1199-1208, Uppsala, Swe-
den. 
Hideki Shima, Hiroshi Kanayama, Cheng-Wei Lee, 
Chuan-Jie Lin, Teruko Mitamura, Yusuke Miyao,  
Shuming Shi, and Koichi Takeda. 2011. Overview 
of NTCIR-9 RITE: Recognizing inference in text. 
In Proceedings of the NTCIR-9 Workshop Meeting, 
Tokyo, Japan. 
Lucy Vanderwende, Arul Menezes, and Rion Snow. 
2006. Microsoft Research at RTE-2: Syntactic 
Contributions in the Entailment Task: an imple-
mentation. In Proceedings of the Second PASCAL 
Challenges Workshop. 
Rui Wang and Yi Zhang. 2009. Recognizing Textual 
Relatedness with Predicate-Argument Structures. 
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pag-
es 784?792, Singapore.  
 
450
