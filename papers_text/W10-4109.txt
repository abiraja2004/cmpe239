Bigram HMM with Context Distribution Clustering for Unsupervised
Chinese Part-of-Speech tagging
Lidan Zhang
Department of Computer Science
the University of Hong Kong
Hong Kong
lzhang@cs.hku.hk
Kwok-Ping Chan
Department of Computer Science
the University of Hong Kong
Hong Kong
kpchan@cs.hku.hk
Abstract
This paper presents an unsupervised
Chinese Part-of-Speech (POS) tagging
model based on the first-order HMM.
Unlike the conventional HMM, the num-
ber of hidden states is not fixed and will
be increased to fit the training data. In
favor of sparse distribution, the Dirich-
let priors are introduced with variational
inference method. To reduce the emis-
sion variables, words are represented by
their contexts and clustered based on the
distributional similarities between con-
texts. Experiment results show the out-
put state sequence of HMM are highly
correlated to the latent annotations of
gold POS tags, in context of clustering
similarity measures. The other exper-
iments on a real application, unsuper-
vised dependency parsing, reveal that the
output sequence can replace the manu-
ally annotated tags without loss of accu-
racies.
1 Introduction
Recently latent variable model has shown great
potential in recovering the underlying structures.
For example, the task of POS tagging is to re-
cover the appropriate sequence structure given
the input word sequence (Goldwater and Grif-
fiths, 2007). One of the most popular exam-
ple of latent models is Hidden Markov Model
(HMM), which has been extensively studied for
many years (Rabiner, 1989). The key problem
of HMM is how to find an optimal hidden state
number and the topology appropriately.
In most cases, the topology of HMM is pre-
defined by exploiting the domain or empirical
knowledge. This topology will be fixed during
the whole process. Therefore how to select the
optimal topology for a certain application or a set
of training data is still a problem, because many
researches show that varying the size of the state
space greatly affects the performance of HMM.
Generally there are two ways to adjust the state
number: top-down and bottom-up methods. In
the bottom-up methods (Brand, 1999), the state
number is initialized with a relatively large num-
ber. During the training, the states are merged or
trimmed and ended with a small set of states. On
the other hand, the top-down methods (Siddiqi et
al., 2007) start from a small state set and split one
or some states until no further improvement can
be obtained. The bottom-up approaches require
huge computational cost in deciding the states to
be merged, which makes it impractical for appli-
cations with large state space. In this paper, we
focus on the latter approaches.
Another problem in HMM is that EM algo-
rithm might yield local maximum value. John-
son (2007) points out that training HMM with
EM gives poor results because it leads to a fairly
flat distribution of hidden states when the empiri-
cal distribution is highly skewed. A multinomial
prior, which favors sparse distribution, is a good
choice for natural language tasks. In this paper,
we proposed a new procedure for inferring the
HMM topology and estimating its parameters si-
multaneously. Gibbs sampling has been used in
infinite HMM (iHMM) (Beal et al, 2001; Fox et
al., 2008; Van Gael et al, 2008) for inference.
Unfortunately Gibbs sampling is slow and diffi-
cult to be converged. In this paper, we proposed
the variational Bayesian inference for the adap-
tive HMMmodel with Dirichlet prior. It involves
a modification to the Baum-Welch algorithm. In
each iteration, we replaced only one hidden state
with two new states until convergence.
To reduce the number of observation vari-
ables, the words are pre-clustered and repre-
sented by the exemplar within the same clus-
ter. It is a one-to-many clustering, because the
same word play different roles under different
contexts. We evaluate the similarity between the
distribution of contexts, with the assumption that
the context distribution implies syntactic pattern
of the given word (Zelling, 1968; Weeds and
Weir, 2003). With this clustering, more contex-
tual information can be considered without in-
creasing the model complexity. A relatively sim-
ple model is important for unsupervised task in
terms of computational burden and data sparse-
ness. This is the reason why we do not increase
the order of HMM(Kaji and Kitsuregawa, 2008;
Headden et al, 2008).
With unsupervised algorithms, there are two
aspects to be evaluated (Van Gael et al, 2009).
Fist one is how good the outcome clusters are.
We compare the HMM results with the manu-
ally POS tags and report the similarity measures
based on information theory. On the other hand,
we test how good the outputs act as an interme-
diate results. In many natural language tasks, the
inputs are word class, not the actual lexical item,
for reason of sparsity. In this paper, we choose
the unsupervised dependency parsing as the ap-
plication to investigate whether our clusters can
replace the manual labeled tags or not.
The paper is organized as below: in section 2,
we describe the definition of HMM and its vari-
ance inference. We present our dynamic HMM
in section 3. To overcome the context limitation
in the first-order HMM, we present our distribu-
tional similarity clustering in section 4. In sec-
tion 5, we reported the results of the mentioned
experiments while section 6 concludes the paper.
2 Terminology
The task of POS tagging is to assign a syntac-
tic category sequence to the input words. Let
S be defined as the set of all possible hidden
states, which are expected to be highly correlated
to POS tags. ? represents the set of all words.
Therefore the task is to find a sequence of tag
sequence S = s1...sn ? S given a sequence of
words (i.e. a sentence, W = w1...wn ? ?). The
optimal tags is to maximize the conditional prob-
ability p(S |W), which is equal to:
max
S
p(S |W) = max
S
p(S )p(W |S )
= max
S
p(W, S )
(1)
In this paper,we consider the first-order HMM,
where the POS tags are regarded as hidden states
and words as observed variables. According to
the Markov assumption, the best sequence of
tags S for a given sequence of words W is done
by maximizing (with s0 = 0) the joint probabil-
ity:
p(W, S ) =
n
?
i=1
p(si|si?1)p(wi|si) (2)
where w0 is the special boundary marker of sen-
tences.
2.1 Variational Inference for HMM
Let the HMM be modeled with parameter ? =
(A, B, pi), where A = {ai j} = {P(st = j|st?1 = i)}
is the transition matrix governing the dynamic of
the HMM. B = {bt(i)} = {P(wt = i|st}) is the state
emission matrix and pi = {pii} = {P(s1 = i)} as-
signs the initial probabilities to all hidden states.
In favor of sparse distributions, a natural choice
is to encode Dirichlet prior into parameters p(?).
In particular, we have:
p(A) =
N
?
i=1
Dir({ai1, ..., aiN} |u(A))
p(B) =
N
?
i=1
Dir({bi1, ..., biN} |u(B))
p(pi) = Dir({pi1, ..., piN} |u(pi))
(3)
where the Dirichlet distribution of order N with
hyperparameter vector u is defined as:
Dir(x|u) =
?(
?N
i=1 ui)
?N
i=1 ?(ui)
N
?
i=1
xui?1i . (4)
In this paper, we consider the symmetric
Dirichlet distribution with a fixed length, i.e.
u = [
?N
i=1 ui/N, ...,
?N
i=1 ui/N].
In the Bayesian framework, the model param-
eters are also regarded as hidden variables. The
marginal likelihood can be calculated by sum-
ming up all hidden variables. According to the
Jensen?s inequality, the lower bound of marginal
likelihood is defined as:
ln p(W) = ln
?
?
S
p(?)p(W, S |?)d?
?
?
?
S
q(?, S ) ln
p(W, S , ?)
q(?, S )
d?
= F
(5)
Generally, Variational Bayesian Inference
aims to find a tractable distribution q(?, s) that
maximizes the lower bound F . To make infer-
ence flexible, the posterior distribution can be
assumed to be factorized according to the mean-
field assumption. We have:
p(W, S , ?) ? q(S , ?) = q
?
(?)qS (S ) (6)
Then an extension of EM algorithm (called
Baum-Welch algorithm) can be used to alter-
nately optimize the qS and q?. The EM process
is described as follows:
? E Step: Forward-Backward algorithm to
find the optimal state sequence S (t+1) =
argmax p(S (t)|W, ?(t))
? M Step: The parameters ?(t+1) are re-
estimated given the optimal state S (t+1)
The E and M steps are repeated until a conver-
gence criteria is satisfied. Beal (2003) proved
that only need to do minor modifications in M
step (in 1) is needed, when Dirichlet prior is in-
troduced.
3 Adaptive Hidden Markov Model
As aforementioned, the key problem of HMM is
how to initialize the number of hidden states and
select the topology of HMM. In this paper, we
use the top-down scheme: starting from a small
number of states, only one state is chosen in each
step and splitted into two new states. This binary
split scheme is described in Figure 1.
Algorithm 1 Outline of our adpative HMM
Initialization: Initialize: t = 0, N(t)
repeat
Optimization: Find the optimal parameters
for current Nt
Candidate Generation: Split states and
generate candidate HMMs
Candidate Selection: Select the optimal
HMM from the candidates, whose hidden
state number is Nt+1
untilNo further improvement can be achieved
after splitting
In the following, we will discuss the details of
each step one by one.
3.1 Candidate Generation
Let N(t) represent the number of hidden states at
timestep t. The problem is how to choose the
states for splitting. A straightforward way is to
select all states and generate N(t) + 1 candidate
HMMs, including the original un-splitted one.
Obviously the exhaustive search is inefficient es-
pecially for large state space. To make the algo-
rithm more efficient, some constraints must be
set to narrow the search space.
Intuitively entropy implies uncertainty. So
hidden states with large conditional entropies are
desirable to be splitted. We can define the con-
ditional entropy of the state sequences given ob-
servation W as:
H(S |W) = ?
?
S
[P(S |W) log P(S |W)] (8)
Our assumption is the state to be splitted must
be the states sequence with the highest condi-
tional entropy value. This entropy can be recur-
sively calculated with complexity O(N2T ) (Her-
nando et al, 2005). Here N is the number of
A(t+1) = {a(t+1)i j } = exp[?(?
(A)
i j ) ? ?(
N
?
j=1
?
(A)
i j )] ; ?
(A)
i j = u
(A)
j + Eq(s)[ni j]
B(t+1) = {b(t+1)ik } = exp[?(?
(B)
ik ) ? ?(
T
?
k=1
?
(B)
ik )] ; ?
(B)
ik = u
(B)
k + Eq(s)[n
?
ik]
pi
(t+1)
= {pi(t+1)i } = exp[?(?
(pi)
i ) ? ?(
N
?
i=1
?
(pi)
j )]; ?
(pi)
i = u
(pi)
i + Eq(s)[n
??
i ]
(7)
Figure 1: Parameters update equations in M-step. Here E is the expectation with respect to the
model parameters. And ni j is the expected number of transition from state si to state s j; n?ik is the
expected number of times word wk occurs with state si; n??i is the occurrence of s0 = i
states and T is the length of sequence. Using
this entropy constraint, the size of candidate state
set is always smaller than the minimal value be-
tween N and T .
3.2 Candidate Selection
Given the above candidate set, the parameters of
each HMM are to be updated. Note that we just
update the parameters related to the split state,
whilst keep the others fixed. Suppose the i-th
hidden state is replaced by two new states. First
the transition matrix is enlarged from N(t) ? N(t)
dimension to (N(t) + 1) ? (N(t) + 1) dimension,
by inserting one column and row after the i-th
column and row. In the process of update, we
only change the items in the two (i and i + 1)
rows and columns. The other elements irrelevant
to the split state are not involved in the update
procedure. Similarly EM algorithm is used to
find the optimal parameters. Note that most of
the calculations can be skipped by making use
of the forward and backward probability matrix
achieved in the previous step. Therefore the con-
vergence is fast.
Given the candidate selection, we can use a
modified Baum-Welch algorithm to find optimal
states and parameters. Here we use the algorithm
in (Siddiqi et al, 2007) with some modifications
for the Dirichlet prior. In particular, in E step,
we follow their partial Forward-Background al-
gorithm to calculate E[ni j] and E[n?ik], if si or s j
is candidate state to be splitted. Then in M-step,
only rows and columns related to the candidate
state are updated according to equation (7). The
detailed description is given as appendix.
Finally it is natural to use variational bound
of marginal likelihood in equation (5) for model
scoring and convergence criterion.
4 Distributional Clustering
To reduce the number of observation variables,
the words are clustered before HMM training.
Intuitively, the words share the similar contexts
have similar syntactic property. The categories
of many words are varied in different contexts.
In other words, the cluster of a given word is
heavily dependent on the context it appears. For
example,?? can be a noun (meaning: discov-
ery) if it acts as an object, or a verb (meaning: to
discover) if it is followed with a noun. Further-
more the introduction of context can overcome
the limited context in the first-order HMM.
The underlying hypothesis of clustering based
on distributional similarity is that the words oc-
curring in similar contexts behave as similar syn-
tactic roles. In this work, the context of a word
is a trigram consist of the word immediately pre-
ceding the target and the word immediately fol-
lowing it. The similarity between two words
is measured by Pointwise Mutual Information
(PMI) between the context pair in which they ap-
pear:
PMI(wi,w j) = log
P(ci, c j)
P(ci)P(c j)
(9)
where ci denotes the context of wi. P(ci, c j) is
the co-occurrence probability of ci and c j, and
P(ci) =
?
j P(ci, c j) is the occurrence probabil-
ity of ci. In our experiments, the cutoff context
count is set to 10, which means the frequency
less than the threshold is labeled as the unknown
context.
The above distributional similarity can be
used as a distance measure. Hence any cluster-
ing algorithm can be adopted. In this paper, we
use the affinity propagation algorithm (Frey and
Dueck, 2007). Its parameter ?dampfact? is set
to 0.9, and the other parameters are set as de-
fault. After running the clustering algorithm, the
contexts are clustered into 1869 clusterings. It
is noted that one word might be classified into
several clusters , if its contexts are clustered into
several clusters.
5 Experiments
As aforementioned, the outputs of our HMM
model are evaluated in two ways, clustering met-
ric and parsing performance. The data used in all
experiments are the Chinese data set in CoNLL-
2007 shared task. The number of tokens in
training, development and test sets are 609,060,
49,620 and 73,153 respectively. We use all train-
ing data set for training the model, whose maxi-
mum length is 242.
The hyper parameters of Dirichlet priors are
initialized in a homogeneous way. The initial
hidden state is set to 40 in all experiments. After
several iterations, the hidden states number con-
verged to 247, which is much larger than the size
of the manually defined POS tags. Our expec-
tation is the refinement variables can reveal the
deep granularity of the POS tags.
5.1 Clustering Evaluation
In this paper, we use information theoretic based
metrics to quantify the information shared by
two clusters. The most common information-
based clustering metric is the variational of In-
formation (VI)(Meila?, 2007). Given the cluster-
ing resultCr and the gold clusteringCg, VI sums
up the conditional entropy of one cluster distri-
bution given the other one:
VI(Cr,Cg) = H(Cr) + H(Cg) ? 2I(Cr,Cg)
= H(Cr |Cg) + H(Cg|Cr)
(10)
where H(Cr) is the entropy associated with the
clustering Cr, and mutual information I(Cr,Cg)
quantifies the mutual dependence between two
clusterings, or say the shared information be-
tween two variables. It is easy to see that
VI? [0, log(N)], where N is the number of data
points. However, the standard VI is not normal-
ized, which favors clusterings with a small num-
ber of clusters. It can be normalized by divid-
ing by log(N), because the number of training
instances are fixed. However the normalized VI
score is misleadingly large, if the N is very large
which is the case in our task. In this paper only
un-normalized VI scores are reported to show the
score ranking.
To standardize the measures to have fixed
bounds, (Strehl and Ghosh, 2003) defined the
normalized Mutual Information (NMI) as:
NMI(Cr,Cg) =
I(Cr,Cg)
?
H(Cr)H(Cg)
(11)
NMI takes its lower bound of 0 if no information
is shared by two clusters and the upper bound
of 1 if two clusterings are identical. The NMI
however, still has problems, whose variation is
sensitive to the choice of the number of clusters.
Rosenberg and Hirschberg (2007) proposed
V-measure to combine two desirable properties
of clustering: homogeneity (h) and completeness
(c) as follows:
h = 1 ? H(Cg|Cr)/H(Cg)
c = 1 ? H(Cr |Cg)/H(Cr)
V = 2hc/(h + c)
(12)
Generally homogeneity and completeness
runs in opposite way, whose harmonic mean (i.e.
V-measure) is a comprise score, just like F-score
for the precision and recall.
Let us first examine the contextual word clus-
tering performance. The VI score between dis-
tributional word categories and gold standard is
2.39. The NMI and V-measure score are 0.53
and 0.48, respectively.
The clustering performance of the HMM out-
puts are reported in Figure 2. The best VI
score achieved was 3.9524, while V-measure
was 62.09% and NMI reached 0.8051. Previous
40 60 80 100 120 140 160 180 200 220 2403.8
4
4.2
4.4
4.6
4.8
5
(a) VI score
40 60 80 100 120 140 160 180 200 220 2400
0.1
0.2
0.3
0.4
0.5
0.6
0.7
 
 
NMIhomogeneitycompletenessV?measure
(b) normalized scores
Figure 2: Clustering evaluation metrics against number of hidden states
work of Chinese tagging focuses on the tagging
accuracies, e.g. Wang (Wang and Schuurmans,
) and Huang et al (Huang et al, 2007). To
our knowledge, this is the first work to report
the distributional clustering similarity measures
based on informatics view for Chinese . Simi-
lar works can be found on English of WSJ cor-
pus (Van Gael et al, 2009). Their best results of
VI, V-measure, achieved with Pitman-Yor prior,
were 3.73 and 59%. We believe the Chinese re-
sults are not good as English correspondences
because of the rich unknown words in Chinese
(Tseng et al, 2005).
5.2 Dependency Parsing Evaluation
The next experiment is to test the goodness of the
outcome states of our model in the context of real
tasks. In this work, we consider unsupervised
dependency parsing for a fully unsupervised sys-
tem. The dependency parsing is to extract the
dependency graph whose nodes are the words of
the given sentence. The dependency graph is a
directed acyclic graph in which every edge links
from a head word to its dependent. Because we
work on unsupervised methods in this paper, we
choose a simple generative head-outward model
(DependencyModel with Valence, DMV) (Klein
and Manning, 2004; Headden III et al, 2009) for
parsing. The data through the experiment is re-
stricted to the sentences up to length 10 (exclud-
ing punctuation).
Because the main purpose is to test the HMM
output rather than to improve the parsing perfor-
mance, we select the original DMV model with-
out extensions or modifications. Starting from
the root, DMV generates the head, and then each
head recursively generates its left and right de-
pendents. In each direction, the possible depen-
dents are repeatedly chosen until a STOP marker
is seen. DMV use inside-outside algorithm for
re-estimation. We choose the ?harmonic? ini-
tializer proposed in (Klein and Manning, 2004)
for initialization. The valence information is the
simplest binary value indicating the adjacency.
For different HMM candidates with varied hid-
den state number, we directly use the outputs as
the input of the DMV and trained a set of models.
Performing test on these individual models, we
report the directed dependency accuracies (the
fraction of words assigned the correct parent) in
Figure 3.
40 60 80 100 120 140 160 180 200 220 24035
40
45
50
55
Figure 3: Directed accuracies for different hid-
den states
It is noted that the accuracy monotonically
increases when the number of states increases.
The most drastic increase happened when state
changes from 40 to 120. The accuracy increased
from 38.56% to 50.60%. If the state number is
larger than 180, the increase is not obvious. The
final best accuracy is 54.20%, which improve the
standard DMV model by 5.6%. Therefore we
can see that the introduction of more annotations
can help the parsing results. However, the im-
provement is limited and stable when the num-
ber of state number is large. To further improve
the parsing performance, one might turn to the
extension of DMV model, e.g. introducing more
knowledge (prior or lexical information) or more
sophistical smoothing techniques. However, the
development of parser is not the focus of this pa-
per.
6 Conclusion and Future Work
This paper works on the unsupervised Chinese
POS tagging based on the first-order HMM. Our
contributions are: 1). The number of hidden
states can be adjusted to fit the data. 2). For in-
ference, we use the variational inference, which
is faster and is guaranteed theoretically to con-
vergence. 3). To overcome the context limitation
in HMM, the words are clustered based on dis-
tributional similarities. It is a 1-to-many cluster-
ing, which means one word might be classified
into different clusters under different contexts.
Finally, experiments show the hidden states are
correlated to the latent annotations of the stan-
dard POS tags.
The future work includes to improve the per-
formance by incorporating a small amount of su-
pervision. The typical supervision used before
is dictionary extracted from a large corpus like
Chinese Gigaword. Another interesting idea is
to select some exemplars (Haghighi and Klein,
2006).
References
Beal, Matthew J., Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The infinite hidden
markov model. In NIPS, pages 577?584.
Beal, M. J. 2003. Variational algorithms for
approximate bayesian inference. Phd Thesis.
Gatsby Computational Neuroscience Unit, Uni-
versity College London.
Brand, Matthew. 1999. An entropic estimator for
structure discovery. In Proceedings of the 1998
conference on Advances in neural information pro-
cessing systems II, pages 723?729, Cambridge,
MA, USA. MIT Press.
Fox, Emily B., Erik B. Sudderth, Michael I. Jordan,
and Alan S. Willsky. 2008. An hdp-hmm for sys-
tems with state persistence. In ICML ?08: Pro-
ceedings of the 25th international conference on
Machine learning.
Frey, Brendan J. and Delbert Dueck. 2007. Clus-
tering by passing messages between data points.
Science, 315:972?976.
Goldwater, Sharon and Tom Griffiths. 2007. A
fully bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 744?751, Prague, Czech Republic,
June. Association for Computational Linguistics.
Haghighi, Aria and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Pro-
ceedings of the main conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 320?327.
Headden, III, William P., David McClosky, and Eu-
gene Charniak. 2008. Evaluating unsupervised
part-of-speech tagging for grammar induction. In
COLING ?08: Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics,
pages 329?336, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Headden III, William P., Mark Johnson, and David
McClosky. 2009. Improving unsupervised depen-
dency parsing with richer contexts and smoothing.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 101?109, Boulder, Colorado,
June. Association for Computational Linguistics.
Hernando, D., V. Crespi, and G. Cybenko. 2005. Ef-
ficient computation of the hidden markov model
entropy for a given observation sequence. vol-
ume 51, pages 2681?2685.
Huang, Zhongqiang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
1093?1102, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Johnson, Mark. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
296?305, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Kaji, Nobuhiro and Masaru Kitsuregawa. 2008. Us-
ing hidden markov random fields to combine dis-
tributional and pattern-based word clustering. In
COLING ?08: Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics,
pages 401?408, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Klein, Dan and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages
478?485, Barcelona, Spain, July.
Meila?, Marina. 2007. Comparing clusterings?an in-
formation based distance. volume 98, pages 873?
895.
Rabiner, Lawrence R. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of the IEEE, pages
257?286.
Rosenberg, Andrew and Julia Hirschberg. 2007.
V-measure: A conditional entropy-based exter-
nal cluster evaluation measure. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 410?420.
Siddiqi, Sajid, Geoffrey Gordon, and Andrew Moore.
2007. Fast state discovery for hmm model selec-
tion and learning. In Proceedings of the Eleventh
International Conference on Artificial Intelligence
and Statistics (AI-STATS).
Strehl, Alexander and Joydeep Ghosh. 2003. Clus-
ter ensembles ? a knowledge reuse framework
for combining multiple partitions. Journal of Ma-
chine Learning Research, 3:583?617.
Tseng, Huihsin, Daniel Jurafsky, and Christopher
Manning. 2005. Morphological features help pos
tagging of unknown words across language vari-
eties. pages 32?39.
Van Gael, Jurgen, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for
the infinite hidden markov model. In ICML ?08:
Proceedings of the 25th international conference
on Machine learning.
Van Gael, Jurgen, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 678?687, Singapore, Au-
gust. Association for Computational Linguistics.
Wang, Qin Iris and Dale Schuurmans. Improved es-
timation for unsupervised part-of-speech tagging.
page 2005, Wuhan, China.
Weeds, Julie and David Weir. 2003. A general
framework for distributional similarity. In Pro-
ceedings of the 2003 conference on Empirical
methods in natural language processing, pages
81?88, Morristown, NJ, USA. Association for
Computational Linguistics.
Zelling, Harris. 1968. Mathematical sturcture of lan-
guage. NewYork:Wiley.
APPENDIX
Pseudo-code of the extended Baum-Welch Al-
gorithm in our dynamic HMM
Input: Time step t:
State Candidate: k ? (k(1), k(2)) ;
Sate Number: Nt;
Model Parameter: ?(t) = (A(t), B(t), pi(t));
Initialize
u(l)[k(1), k(2)]? [ u
(l)[k]
2 ,
u(l)[k]
2 ], l ? {A, B, pi}
pik(1) ? 12pik; pik(2) ?
1
2pik
ak?k(i) ? 12ak?k(i) ; ak(i)k? ? ak(i)k?;
ak(i)k( j) ? 12ak(i)k( j) , here i, j ? 1, 2, k? , k
repeat
E step:
update forward: ?t(k(1)) and ?t(k(2))
backward: ?t(k(1)) and ?t(k(2))
update ?t(i, j) and ?t(i); if i, j ? {k(1), k(2)}
update E[ni j] =
?
t ?t(i, j)/
?
t ?t(i)
E[nik] =
?
t,wt=k ?t( j)/
?
t ?t( j)
M step:
update ?(t+1) using equation (7)
until (4F < ?)
Output: ?(t+1), F
