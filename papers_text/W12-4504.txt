Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 56?63,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Combining the Best of Two Worlds:
A Hybrid Approach to Multilingual Coreference Resolution
Chen Chen and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{yzcchen,vince}@hlt.utdallas.edu
Abstract
We describe our system for the CoNLL-2012
shared task, which seeks to model corefer-
ence in OntoNotes for English, Chinese, and
Arabic. We adopt a hybrid approach to
coreference resolution, which combines the
strengths of rule-based methods and learning-
based methods. Our official combined score
over all three languages is 56.35. In particu-
lar, our score on the Chinese test set is the best
among the participating teams.
1 Introduction
TheCoNLL-2012 shared task extends last year's task
on coreference resolution from a monolingual to a
multilingual setting (Pradhan et al, 2012). Unlike
the SemEval-2010 shared task on Coreference Reso-
lution inMultiple Languages (Recasens et al, 2010),
which focuses on coreference resolution in European
languages, the CoNLL shared task is arguably more
challenging: it focuses on three languages that come
from very different language families, namely En-
glish, Chinese, and Arabic.
We designed a system for resolving references in
all three languages. Specifically, we participated
in four tracks: the closed track for all three lan-
guages, and the open track for Chinese. In compari-
son to last year's participating systems, our resolver
has two distinguishing characteristics. First, unlike
last year's resolvers, which adopted either a rule-
based method or a learning-based method, we adopt
a hybrid approach to coreference resolution, attempt-
ing to combine the strengths of both methods. Sec-
ond, while last year's resolvers did not exploit genre-
specific information, we optimize our system's pa-
rameters with respect to each genre.
Our decision to adopt a hybrid approach is mo-
tivated by the observation that rule-based meth-
ods and learning-based methods each have their
unique strengths. As shown by the Stanford coref-
erence resolver (Lee et al, 2011), the winner of
last year's shared task, many coreference relations in
OntoNotes can be identified using a fairly small set
of simple hand-crafted rules. On the other hand, our
prior work on machine learning for coreference res-
olution suggests that coreference-annotated data can
be profitably exploited to (1) induce lexical features
(Rahman and Ng, 2011a, 2011b) and (2) optimize
system parameters with respect to the desired coref-
erence evaluation measure (Ng, 2004, 2009).
Our system employs a fairly standard architecture,
performing mention detection prior to coreference
resolution. As we will see, however, the parameters
of these two components are optimized jointly with
respect to the desired evaluation measure.
In the rest of this paper, we describe the men-
tion detection component (Section 2) and the coref-
erence resolution component (Section 3), show how
their parameters are jointly optimized (Section 4),
and present evaluation results on the development set
and the official test set (Section 5).
2 Mention Detection
To build a mention detector that strikes a relatively
good balance between precision and recall, we em-
ploy a two-step approach. First, in the extrac-
tion step, we identify named entities (NEs) and em-
ploy language-specific heuristics to extract mentions
56
from syntactic parse trees, aiming to increase our up-
per bound on recall as much as possible. Then, in
the pruning step, we aim to improve precision by
employing both language-specific heuristic pruning
and language-independent learning-based pruning.
Section 2.1 describes the language-specific heuris-
tics for extraction and pruning, and Section 2.2 de-
scribes our learning-based pruning method.
2.1 Heuristic Extraction and Pruning
English. During extraction, we create a candidate
mention from a contiguous text span s if (1) s is a
PRP or an NP in a syntactic parse tree; or (2) s cor-
responds to a NE that is not a PERCENT, MONEY,
QUANTITY or CARDINAL. During pruning, we
remove a candidate mentionmk if (1)mk is embed-
ded within a larger mentionmj such thatmj andmk
have the same head, where the head of a mention is
detected using Collins's (1999) rules; (2) mk has a
quantifier or a partitive modifier; or (3) mk is a sin-
gular common NP, with the exception that we retain
mentions related to time (e.g., "today").
Chinese. Similar to English mention extraction,
we create Chinese mentions from all NP and QP
nodes in syntactic parse trees. During pruning, we
remove a candidate mentionmk if (1)mk is embed-
ded within a larger mentionmj such thatmj andmk
have the same head, except if mj and mk appear
in a newswire document since, unlike other docu-
ment annotations, Chinese newswire document an-
notations do consider such pairs coreferent; (2) mk
is a NE that is a PERCENT, MONEY, QUANTITY
and CARDINAL; or (3) mk is an interrogative pro-
noun such as "?? [what]", "?? [where]".
Arabic. We employ as candidate mentions all the
NPs extracted from syntactic parse trees, removing
those that are PERCENT, MONEY, QUANTITY or
CARDINAL.
2.2 Learning-Based Pruning
While the heuristic pruning method identifies can-
didate mentions, it cannot determine which candi-
date mentions are likely to be coreferent. To improve
pruning (and hence the precision of mention detec-
tion), we employ learning-based pruning, where we
employ the training data to identify and subsequently
discard those candidate mentions that are not likely
to be coreferent with other mentions.
Language Recall Precision F-Score
English 88.59 40.56 55.64
Chinese 85.74 42.52 56.85
Arabic 81.49 21.29 33.76
Table 1: Mention detection results on the development set
obtained prior to coreference resolution.
Specifically, for each mention mk in the test set
that survives heuristic pruning, we compute its men-
tion coreference probability, which indicates the
likelihood that the head noun of mk is coreferent
with another mention. If this probability does not
exceed a certain threshold tC , we will remove mk
from the list of candidate mentions. Section 4 dis-
cusses how tC is jointly learned with the parameters
of the coreference resolution component to optimize
the coreference evaluation measure.
We estimate the mention coreference probability
ofmk from the training data. Specifically, since only
non-singleton mentions are annotated in OntoNotes,
we can compute this probability as the number of
times mk 's head noun is annotated (as a gold men-
tion) divided by the total number of timesmk 's head
noun appears. If mk 's head noun does not appear in
the training set, we set its coreference probability to
1, meaning that we let it pass through the filter. In
other words, we try to be conservative and do not
filter any mention for which we cannot compute the
coreference probability.
Table 1 shows the mention detection results of the
three languages on the development set after heuris-
tic extraction and pruning but prior to learning-based
pruning and coreference resolution.
3 Coreference Resolution
Like the mention detection component, our corefer-
ence resolution component employs heuristics and
machine learning. More specifically, we employ
Stanford's multi-pass sieve approach (Lee et al,
2011) for heuristic coreference resolution, but since
most of these sieves are unlexicalized, we seek to im-
prove the multi-pass sieve approach by incorporat-
ing lexical information using machine learning tech-
niques. As we will see below, while different sieves
are employed for different languages, the way we in-
corporate lexical information into the sieve approach
is the same for all languages.
57
3.1 The Multi-Pass Sieve Approach
A sieve is composed of one or more heuristic rules.
Each rule extracts a coreference relation between
two mentions based on one or more conditions. For
example, one rule in Stanford's discourse processing
sieve posits two mentions as coreferent if two con-
ditions are satisfied: (1) they are both pronouns; and
(2) they are produced by the same speaker.
Sieves are ordered by their precision, with the
most precise sieve appearing first. To resolve a set
of mentions in a document, the resolver makes mul-
tiple passes over them: in the i-th pass, it attempts
to use only the rules in the i-th sieve to find an an-
tecedent for each mention mk. Specifically, when
searching for an antecedent formk, its candidate an-
tecedents are visited in an order determined by their
positions in the associated parse tree (Haghighi and
Klein, 2009). The partial clustering of the mentions
created in the i-th pass is then passed to the i+1-th
pass. Hence, later passes can exploit the informa-
tion computed by previous passes, but a coreference
link established earlier cannot be overridden later.
3.2 The Sieves
3.2.1 Sieves for English
Our sieves for English are modeled after those em-
ployed by the Stanford resolver (Lee et al, 2011),
which is composed of 12 sieves.1 Since we partic-
ipated in the closed track, we re-implemented the
10 sieves that do not exploit external knowledge
sources. These 10 sieves are listed under the "En-
glish" column in Table 2. Specifically, we leave out
the Alias sieve and the Lexical Chain sieve, which
compute semantic similarity using information ex-
tracted from WordNet, Wikipedia, and Freebase.
3.2.2 Sieves for Chinese
Recall that for Chinese we participated in both the
closed track and the open track. The sieves we em-
ploy for both tracks are the same, except that we use
NE information to improve some of the sieves in the
system for the open track.2 To obtain automatic NE
annotations, we employ a NE model that we trained
on the gold NE annotations in the training data.
1Table 1 of Lee et al's (2011) paper listed 13 sieves, but one
of them was used for mention detection.
2Note that the use of NEs puts a Chinese resolver in the open
track.
English Chinese
Discourse Processing Chinese Head Match
Exact String Match Discourse Processing
Relaxed String Match Exacth String Match
Precise Constructs Precise Constructs
Strict Head Match A?C Strict Head Match A?C
Proper Head Match Proper Head Match
Relaxed Head Match Pronouns
Pronouns --
Table 2: Sieves for English and Chinese (listed in the or-
der in which they are applied).
The Chinese resolver is composed of 9 sieves,
as shown under the "Chinese" column of Table 2.
These sieves are implemented in essentially the same
way as their English counterparts except for a few
of them, which are modified in order to account for
some characteristics specific to Chinese or the Chi-
nese coreference annotations. As described in de-
tail below, we introduce a new sieve, the Chinese
Head Match sieve, and modify two existing sieves,
the Precise Constructs sieve, and the Pronoun sieve.
1. Chinese Head Match sieve: Recall from Sec-
tion 2 that the Chinese newswire articles were
coreference-annotated in such away that amen-
tion and its embedding mention can be coref-
erent if they have the same head. To iden-
tify these coreference relations, we employ the
Same Head sieve, which posits two mentions
mj and mk as coreferent if they have the same
head and mk is embedded within mj . There is
an exception to this rule, however: if mj is a
coordinated NP composed of two or more base
NPs, and mk is just one of these base NPs, the
two mentions will not be considered coreferent
(e.g., ??????? [Charles and Diana]
and??? [Diana]).
2. Precise Constructs sieve: Recall from Lee
et al (2011) that the Precise Constructs sieve
posits two mentions as coreferent based on in-
formation such as whether one is an acronym of
the other and whether they form an appositive
or copular construction. We incorporate addi-
tional rules to this sieve to handle specific cases
of abbreviations in Chinese: (a) Abbreviation
of foreign person names, e.g., ??????
? [Saddam Hussein] and ??? [Saddam].
(b) Abbreviation of Chinese person names, e.g.,
58
??? [Chen President] and ?????
[Chen Shui-bian President]. (c) Abbreviation
of country names, e.g, ?? [Do country] and
???? [Dominica].
3. Pronouns sieve: The Pronouns sieve resolves
pronouns by exploiting grammatical informa-
tion such as the gender and number of a men-
tion. While such grammatical information is
provided to the participants for English, the
same is not true for Chinese.
To obtain such grammatical information for
Chinese, we employ a simple method, which
consists of three steps.
First, we employ simple heuristics to extract
grammatical information from those Chinese
NPs for which such information can be easily
inferred. For example, we can heuristically de-
termine that the gender, number and animacy
for ? [she] is {Female, Single and Animate};
and for?? [they] is {Unknown, Plural, Inani-
mate}. In addition, we can determine the gram-
matical attributes of a mention by its named
entity information. For example, a PERSON
can be assigned the grammatical attributes {Un-
known, Single, Animate}.
Next, we bootstrap from these mentions with
heuristically determined grammatical attribute
values. This is done based on the observation
that all mentions in the same coreference chain
should agree in gender, number, and animacy.
Specifically, given a training text, if one of the
mentions in a coreference chain is heuristically
labeled with grammatical information, we au-
tomatically annotate all the remaining mentions
with the same grammatical attribute values.
Finally, we automatically create six word lists,
containing (1) animate words, (2) inanimate
words, (3) male words, (4) female words, (5)
singular words, and (6) plural words. Specif-
ically, we populate these word lists with the
grammatically annotated mentions from the
previous step, where each element of a word
list is composed of the head of a mention and a
count indicating the number of times the men-
tion is annotated with the corresponding gram-
matical attribute value.
We can then apply these word lists to determine
the grammatical attribute values of mentions in
a test text. Due to the small size of these word
lists, and with the goal of improving precision,
we consider two mentions to be grammatically
incompatible if for one of these three attributes,
onemention has anUnknown value whereas the
other has a known value.
As seen in Table 2, our Chinese resolver does
not have the Relaxed String Match sieve, unlike its
English counterpart. Recall that this sieve marks
two mentions as coreferent if the strings after drop-
ping the text following their head words are identical
(e.g.,MichaelWolf, andMichaelWolf, a contributing
editor for "New York"). Since person names in Chi-
nese are almost always composed of a single word
and that heads are seldom followed by other words
in Chinese, we believe that Relaxed HeadMatch will
not help identify Chinese coreference relations. As
noted before, cases of Chinese person name abbrevi-
ation will be handled by the Precise Constructs sieve.
3.2.3 Sieves for Arabic
We only employ one sieve for Arabic, the exact
match sieve. While we experimented with additional
sieves such as the Head Match sieve and the Pro-
nouns sieve, we ended up not employing them be-
cause they do not yield better results.
3.3 Incorporating Lexical Information
Asmentioned before, we improve the sieve approach
by incorporating lexical information.
To exploit lexical information, we first compute
lexical probabilities. Specifically, for each pair of
mentions mj and mk in a test text, we first com-
pute two probabilities: (1) the string-pair probability
(SP-Prob), which is the probability that the strings
of the two mentions, sj and sk, are coreferent; and
(2) the head-pair probability (HP-Prob), which is the
probability that the head nouns of the two mentions,
hj and hk, are coreferent. For better probability esti-
mation, we preprocess the training data and the two
mentions by (1) downcasing (but not stemming) each
English word, and (2) replacing each Arabic word w
by a string formed by concatenating w with its lem-
matized form, its Buckwalter form, and its vocalized
Buckwalter form. Note that SP-Prob(mj ,mk) (HP-
59
Prob(mj ,mk)) is undefined if one or both of sj (hj)
and sk (hk) do not appear in the training set.
Next, we exploit these lexical probabilities to im-
prove the resolution of mj and mk by presenting
two extensions to the sieve approach. The first ex-
tension aims to improve the precision of the sieve
approach. Specifically, before applying any sieve,
we check whether SP-Prob(mj ,mk) ? tSPL or HP-
Prob(mj ,mk)? tHPL for some thresholds tSPL and
tHPL. If so, our resolver will bypass all of the
sieves and simply posit mj and mk as not corefer-
ent. In essence, we use the lexical probabilities to
improve precision, specifically by positing twomen-
tions as not coreferent if there is "sufficient" infor-
mation in the training data for us to make this de-
cision. Note that if one of the lexical probabilities
(say SP-Prob(mj ,mk)) is undefined, we only check
whether the condition on the other probability (in this
case HP(mj ,mk) ? tHPL) is satisfied. If both of
them are undefined, this pair of mentions will sur-
vive this filter and be processed by the sieve pipeline.
The second extension, on the other hand, aims to
improve recall. Specifically, we create a new sieve,
the Lexical Pair sieve, which we add to the end of
the sieve pipeline and which posits two mentionsmj
and mk as coreferent if SP-Prob(mj ,mk) ? tSPU
or HP-Prob(mj ,mk) ? tHPU . In essence, we use
the lexical probabilities to improve recall, specifi-
cally by positing two mentions as coreferent if there
is "sufficient" information in the training data for
us to make this decision. Similar to the first ex-
tension, if one of the lexical probabilities (say SP-
Prob(mj ,mk)) is undefined, we only check whether
the condition on the other probability (in this case
HP(mj ,mk) ? tHPU ) is satisfied. If both of them
are undefined, the Lexical Pair sieve will not process
this pair of mentions.
The four thresholds, tSPL, tHPL, tSPU , and
tHPU , will be tuned to optimize coreference perfor-
mance on the development set.
4 Parameter Estimation
As discussed before, we learn the system parameters
to optimize coreference performance (which, for the
shared task, is Uavg, the unweighted average of the
three commonly-used evaluation measures, MUC,
B3, and CEAFe) on the development set. Our sys-
tem has two sets of tunable parameters. So far, we
have seen one set of parameters, namely the five lex-
ical probability thresholds, tC , tSPL, tHPL, tSPU ,
and tHPU . The second set of parameters contains the
rule relaxation parameters. Recall that each rule in
a sieve may be composed of one or more conditions.
We associate with condition i a parameter ?i, which
is a binary value that controls whether condition i
should be removed or not. In particular, if ?i=0, con-
dition iwill be dropped from the corresponding rule.
The motivation behind having the rule relaxation pa-
rameters should be clear: they allow us to optimize
the hand-crafted rules using machine learning. This
section presents two algorithms for tuning these two
sets of parameters on the development set.
Before discussing the parameter estimation algo-
rithms, recall from the introduction that one of the
distinguishing features of our approach is that we
build genre-specific resolvers. In other words, for
each genre of each language, we (1) learn the lexi-
cal probabilities from the corresponding training set;
(2) obtain optimal parameter values ?1 and ?2 for
the development set using parameter estimation al-
gorithms 1 and 2 respectively; and (3) among?1 and
?2, take the one that yields better performance on
the development set to be the final set of parameter
estimates for the resolver.
Parameter estimation algorithm 1. This algo-
rithm learns the two sets of parameters in a sequential
fashion. Specifically, it first tunes the lexical proba-
bility thresholds, assuming that all the rule relaxation
parameters are set to one. To tune the five probabil-
ity thresholds, we try all possible combinations of
the five probability thresholds and select the combi-
nation that yields the best performance on the devel-
opment set. To ensure computational tractability, we
allow each threshold to have the following possible
values. For tC , the possible values are?0.1, 0, 0.05,
0.1, . . ., 0.3; for tSPL and tHPL, the possible values
are ?0.1, 0, 0.05, 0.15, . . ., 0.45; and for tSPU and
tHPU , the possible values are 0.55, 0.65, . . ., 0.95,
1.0 and 1.1. Note that the two threshold values?0.1
and 1.1 render a probability threshold useless. For
example, if tC = ?0.1, that means all mentions will
survive learning-based pruning in the mention detec-
tion component. As another example, if tSPU and
tHPU are both 1.1, it means that the String Pair sieve
60
will be useless because it will not posit any pair of
mentions as coreferent.
Given the optimal set of probability thresholds, we
tune the rule relaxation parameters. To do so, we ap-
ply the backward elimination feature selection algo-
rithm, viewing each condition as a feature that can be
removed from the "feature set". Specifically, all the
parameters are initially set to one, meaning that all
the conditions are initially present. In each iteration
of backward elimination, we identify the condition
whose removal yields the highest score on the de-
velopment set and remove it from the feature set. We
repeat this process until all conditions are removed,
and identify the subset of the conditions that yields
the best score on the development set.
Parameter estimation algorithm 2. In this algo-
rithm, we estimate the two sets of parameters in an
interleaved, iterative fashion, where in each itera-
tion, we optimize exactly one parameter from one
of the two sets. More specifically, (1) in iteration
2n, we optimize the (n mod 5)-th lexical probabil-
ity threshold while keeping the remaining parame-
ters constant; and (2) in iteration 2n+1, we optimize
the (n mod m)-th rule relaxation parameter while
keeping the remaining parameters constant, where
n = 1, 2, . . ., and m is the number of rule relax-
ation parameters. When optimizing a parameter in a
given iteration, the algorithm selects the value that,
when used in combination with the current values of
the remaining parameters, optimizes theUavg value
on the development set. We begin the algorithm by
initializing all the rule relaxation parameters to one;
tC , tSPL and tHPL to ?0.1; and tSPU and tHPU
to 1.1. This parameter initialization is equivalent to
the configuration where we employ all and only the
hand-crafted rules as sieves and do not apply learn-
ing to perform any sort of optimization at all.
5 Results and Discussion
The results of our Full coreference resolver on the
development set with optimal parameter values are
shown in Table 3. As we can see, both the men-
tion detection results and the coreference results (ob-
tained via MUC, B3, and CEAFe) are expressed in
terms of recall (R), precision (P), and F-measure (F).
In addition, to better understand the role played by
the two sets of system parameters, we performed ab-
lation experiments, showing for each language-track
combination the results obtained without tuning (1)
the rule relaxation parameters (? ?i's); (2) the proba-
bility thresholds (? tj 's); and (3) any of these param-
eters (? ?i's & tj). Note that (1) we do not have any
rule relaxation parameters for the Arabic resolver
owing to its simplicity; and (2) for comparison pur-
poses, we show the results of the Stanford resolver
for English in the row labeled "Lee et al (2011)".
A few points regarding the results in Table 3 de-
serve mention. First, these mention detection re-
sults are different from those shown in Table 1: here,
the scores are computed over the mentions that ap-
pear in the non-singleton clusters in the coreference
partitions produced by a resolver. Second, our re-
implementation of the Stanford resolver is as good
as the original one. Third, parameter tuning is com-
paratively less effective for Chinese, presumably be-
cause we spent more time on engineering the sieves
for Chinese than for the other languages. Fourth,
our score on Arabic is the lowest among the three
languages, primarily because Arabic is highly inflec-
tional and we have little linguistic knowledge of the
language to design effective sieves. Finally, these
results and our official test set results (Table 4), as
well as our supplementary evaluation results on the
test set obtained using gold mention boundaries (Ta-
ble 5) and gold mentions (Table 6), exhibit similar
performance trends.
Table 7 shows the optimal parameter values ob-
tained for the Full resolver on the development set.
Since there are multiple genres for English and Chi-
nese, we show in the table the probability thresholds
averaged over all the genres and the corresponding
standard deviation values. For the rule relaxation
parameters, among the 36 conditions in the English
sieves and the 61 conditions in the Chinese sieves,
we show the number of conditions being removed
(when averaged over all the genres) and the corre-
sponding standard deviation values. Overall, differ-
ent conditions were removed for different genres.
To get a better sense of the usefulness of
the probability thresholds, we show in Tables 8
and 9 some development set examples of cor-
rectly and incorrectly identified/pruned mentions
and coreferent/non-coreferent pairs for English and
Chinese, respectively. Note that no Chinese exam-
ples for tC are shown, since its tuned value cor-
61
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 74.8 75.6 75.2 65.6 67.3 66.4 69.1 74.7 71.8 49.8 47.9 48.8 62.3
? ?i 's 75.2 73.4 74.3 64.6 65.8 65.2 68.5 74.1 71.2 48.8 47.6 48.2 61.5
? tj 's 76.4 73.0 74.7 65.1 65.3 65.2 68.6 73.8 71.1 48.6 48.3 48.4 61.6
? ?i 's & tj 's 75.2 72.8 74.0 64.2 64.8 64.5 68.0 73.4 70.6 47.8 47.1 47.5 60.8
Lee et al (2011) 74.1 72.5 73.3 64.3 64.9 64.6 68.2 73.1 70.6 47.0 46.3 46.7 60.6
Chinese Closed Full 72.2 72.7 72.4 62.4 65.8 64.1 70.8 77.7 74.1 52.3 48.9 50.5 62.9
? ?i 's 71.3 72.8 71.9 61.8 66.7 64.2 70.2 78.2 74.0 52.2 47.6 49.9 62.6
? tj 's 72.7 71.1 71.9 62.3 64.8 63.5 70.7 77.1 73.8 51.2 48.8 50.0 62.4
? ?i 's & tj 's 71.7 71.4 71.5 61.5 65.1 63.3 70.0 77.6 73.6 51.3 47.9 49.5 62.1
Chinese Open Full 73.1 72.6 72.9 63.5 67.2 65.3 71.6 78.2 74.8 52.5 48.9 50.7 63.6
? ?i 's 72.5 73.1 72.8 63.2 67.0 65.1 71.3 78.1 74.5 52.4 48.7 50.4 63.3
? tj 's 72.8 72.5 72.7 63.5 66.5 65.0 71.4 77.8 74.5 51.9 48.9 50.4 63.3
? ?i 's & tj 's 72.4 72.5 72.4 63.0 66.3 64.6 71.0 77.8 74.3 51.7 48.5 50.1 63.0
Arabic Closed Full 56.6 64.5 60.3 40.4 42.8 41.6 58.9 62.7 60.7 40.4 37.8 39.1 47.1
? tj 's 52.0 64.3 57.5 33.1 40.2 36.3 53.4 67.9 59.8 41.9 34.2 37.6 44.6
Table 3: Results on the development set with optimal parameter values.
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 75.1 72.6 73.8 63.5 64.0 63.7 66.6 71.5 69.0 46.7 46.2 46.4 59.7
Chinese Closed Full 71.1 72.1 71.6 59.9 64.7 62.2 69.7 77.8 73.6 53.4 48.7 51.0 62.2
Chinese Closed Full 71.5 73.5 72.4 62.5 67.1 64.7 71.2 78.4 74.6 53.6 49.1 51.3 63.5
Arabic Closed Full 56.2 64.0 59.8 38.1 40.0 39.0 60.6 62.5 61.5 41.9 39.8 40.8 47.1
Table 4: Official results on the test set.
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 74.8 75.7 75.2 63.3 66.8 65.0 65.4 73.6 69.2 48.8 44.9 46.8 60.3
Chinese Closed Full 82.0 79.0 80.5 70.8 72.1 71.4 74.4 79.9 77.0 58.0 56.4 57.2 68.6
Chinese Open Full 82.4 80.1 81.2 73.5 74.3 73.9 76.3 80.5 78.3 58.2 57.3 57.8 70.0
Arabic Closed Full 57.2 62.6 59.8 38.7 39.2 39.0 61.5 61.8 61.7 41.6 40.9 41.2 47.3
Table 5: Supplementary results on the test set obtained using gold mention boundaries and predicted parse trees.
Mention Detect. MUC B-CUBED CEAFe Avg
Language Track System R P F R P F R P F R P F F
English Closed Full 80.8 100 89.4 72.3 89.4 79.9 64.6 85.9 73.8 76.3 46.4 57.7 70.5
Chinese Closed Full 84.7 100 91.7 76.6 92.4 83.8 73.0 91.4 81.2 83.6 57.9 68.4 77.8
Chinese Open Full 84.8 100 91.8 78.1 93.2 85.0 75.0 91.6 82.5 84.0 59.2 69.4 79.0
Arabic Closed Full 58.3 100 73.7 41.7 63.2 50.3 50.0 75.3 60.1 64.6 36.2 46.4 52.3
Table 6: Supplementary results on the test set obtained using gold mentions and predicted parse trees.
tC tHPL tSPL tHPU tSPU Rule Relaxation
Language Track Avg. St.Dev. Avg. St.Dev. Avg. St.Dev. Avg. St.Dev. Avg. St.Dev. Avg. St.Dev.
English Closed ?0.06 0.11 ?0.04 0.08 ?0.06 0.12 0.90 0.23 0.60 0.05 6.13 1.55
Chinese Closed ?0.10 0.00 ?0.08 0.06 0.00 0.95 1.01 0.22 0.88 0.27 4.67 1.63
Chinese Open ?0.10 0.00 ?0.08 0.06 ?0.05 0.05 1.01 0.22 0.88 0.27 5.83 1.94
Arabic Closed 0.05 0.00 0.00 0.00 ?0.10 0.00 1.10 0.00 0.15 0.00 0.00 0.00
Table 7: Optimal parameter values.
responds to the case where no mentions should be
pruned.
6 Conclusion
We presented a multilingual coreference resolver de-
signed for the CoNLL-2012 shared task. We adopted
62
Parameter Correct Incorrect
tC no problem; the same that; that idea
tHPL (people,that); (both of you,that) (ours,they); (both of you,us)
tSPL (first,first); (the previous year,its) (China,its); (Taiwan,its)
tHPU (The movie's,the film); (Firestone,the company's) (himself,he); (My,I)
tSPU (Barak,the Israeli Prime Minister); (she,the woman); (Taiwan,the island)
(Kostunica,the new Yugoslav President)
Table 8: Examples of correctly & incorrectly identified/pruned English mentions and coreferent/non-coreferent pairs.
Parameter Correct Incorrect
tC --- ---
tHPL (????,??); (????,?) (?????,??); (??,?)
tSPL (??,??); (???,???) (??,??); (??,?)
tHPU (??,????); (??,???) (???,??); (??,?)
tSPU (??,????); (??,??); (??,?) ; (????,??)
Table 9: Examples of correctly & incorrectly identified/pruned Chinese mentions and coreferent/non-coreferent pairs.
a hybrid approach to coreference resolution, which
combined the advantages of rule-based methods and
learning-based methods. Specifically, we proposed
two extensions to Stanford's multi-pass sieve ap-
proach, which involved the incorporation of lexical
information using machine learning and the acqui-
sition of genre-specific resolvers. Experimental re-
sults demonstrated the effectiveness of these exten-
sions, whether or not they were applied in isolation
or in combination.
In future work, we plan to explore other ways
to combine rule-based methods and learning-based
methods for coreference resolution, as well as im-
prove the performance of our resolver on Arabic.
Acknowledgments
We thank the two anonymous reviewers for their
comments on the paper. This work was supported in
part by NSF Grants IIS-0812261 and IIS-1147644.
References
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 1152-
-1161.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford's multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28--34.
Vincent Ng. 2004. Learning noun phrase anaphoricity
to improve conference resolution: Issues in represen-
tation and optimization. In Proceedings of the 42nd
Meeting of the Association for Computational Linguis-
tics, pages 151--158.
Vincent Ng. 2009. Graph-cut-based anaphoricity deter-
mination for coreference resolution. In Proceedings of
the 2009 Conference of the North American Chapter
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 575--583.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes, In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning.
Altaf Rahman and Vincent Ng. 2011a. Coreference reso-
lution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 814--824.
Altaf Rahman and Vincent Ng. 2011b. Narrowing the
modeling gap: A cluster-ranking approach to corefer-
ence resolution. Journal of Artificial Intelligence Re-
search, 40:469--521.
Marta Recasens, Llu?s M?rquez, Emili Sapena,
M. Ant?nia Mart?, Mariona Taul?, V?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1--8.
63
