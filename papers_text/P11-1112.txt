Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1117?1126,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Semantic Role Induction via Split-Merge Clustering
Joel Lang and Mirella Lapata
Institute for Language, Cognition and Computation
School of Informatics, University of Edinburgh
10 Crichton Street, Edinburgh EH8 9AB, UK
J.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.uk
Abstract
In this paper we describe an unsupervised
method for semantic role induction which
holds promise for relieving the data acqui-
sition bottleneck associated with supervised
role labelers. We present an algorithm that it-
eratively splits and merges clusters represent-
ing semantic roles, thereby leading from an
initial clustering to a final clustering of bet-
ter quality. The method is simple, surpris-
ingly effective, and allows to integrate lin-
guistic knowledge transparently. By com-
bining role induction with a rule-based com-
ponent for argument identification we obtain
an unsupervised end-to-end semantic role la-
beling system. Evaluation on the CoNLL
2008 benchmark dataset demonstrates that
our method outperforms competitive unsuper-
vised approaches by a wide margin.
1 Introduction
Recent years have seen increased interest in the shal-
low semantic analysis of natural language text. The
term is most commonly used to describe the au-
tomatic identification and labeling of the seman-
tic roles conveyed by sentential constituents (Gildea
and Jurafsky, 2002). Semantic roles describe the re-
lations that hold between a predicate and its argu-
ments, abstracting over surface syntactic configura-
tions. In the example sentences below. window oc-
cupies different syntactic positions ? it is the object
of broke in sentences (1a,b), and the subject in (1c)
? while bearing the same semantic role, i.e., the
physical object affected by the breaking event. Anal-
ogously, rock is the instrument of break both when
realized as a prepositional phrase in (1a) and as a
subject in (1b).
(1) a. [Joe]A0 broke the [window]A1 with a
[rock]A2.
b. The [rock]A2 broke the [window]A1.
c. The [window]A1 broke.
The semantic roles in the examples are labeled
in the style of PropBank (Palmer et al, 2005), a
broad-coverage human-annotated corpus of seman-
tic roles and their syntactic realizations. Under the
PropBank annotation framework (which we will as-
sume throughout this paper) each predicate is as-
sociated with a set of core roles (named A0, A1,
A2, and so on) whose interpretations are specific to
that predicate1 and a set of adjunct roles (e.g., loca-
tion or time) whose interpretation is common across
predicates. This type of semantic analysis is admit-
tedly shallow but relatively straightforward to auto-
mate and useful for the development of broad cov-
erage, domain-independent language understanding
systems. Indeed, the analysis produced by existing
semantic role labelers has been shown to benefit a
wide spectrum of applications ranging from infor-
mation extraction (Surdeanu et al, 2003) and ques-
tion answering (Shen and Lapata, 2007), to machine
translation (Wu and Fung, 2009) and summarization
(Melli et al, 2005).
Since both argument identification and labeling
can be readily modeled as classification tasks, most
state-of-the-art systems to date conceptualize se-
1More precisely, A0 and A1 have a common interpretation
across predicates as proto-agent and proto-patient in the sense
of Dowty (1991).
1117
mantic role labeling as a supervised learning prob-
lem. Current approaches have high performance ?
a system will recall around 81% of the arguments
correctly and 95% of those will be assigned a cor-
rect semantic role (see Ma`rquez et al (2008) for
details), however only on languages and domains
for which large amounts of role-annotated training
data are available. For instance, systems trained on
PropBank demonstrate a marked decrease in per-
formance (approximately by 10%) when tested on
out-of-domain data (Pradhan et al, 2008).
Unfortunately, the reliance on role-annotated data
which is expensive and time-consuming to produce
for every language and domain, presents a major
bottleneck to the widespread application of semantic
role labeling. Given the data requirements for super-
vised systems and the current paucity of such data,
unsupervised methods offer a promising alternative.
They require no human effort for training thus lead-
ing to significant savings in time and resources re-
quired for annotating text. And their output can be
used in different ways, e.g., as a semantic prepro-
cessing step for applications that require broad cov-
erage understanding or as training material for su-
pervised algorithms.
In this paper we present a simple approach to un-
supervised semantic role labeling. Following com-
mon practice, our system proceeds in two stages.
It first identifies the semantic arguments of a pred-
icate and then assigns semantic roles to them. Both
stages operate over syntactically analyzed sentences
without access to any data annotated with semantic
roles. Argument identification is carried out through
a small set of linguistically-motivated rules, whereas
role induction is treated as a clustering problem. In
this setting, the goal is to assign argument instances
to clusters such that each cluster contains arguments
corresponding to a specific semantic role and each
role corresponds to exactly one cluster. We formu-
late a clustering algorithm that executes a series of
split and merge operations in order to transduce an
initial clustering into a final clustering of better qual-
ity. Split operations leverage syntactic cues so as to
create ?pure? clusters that contain arguments of the
same role whereas merge operations bring together
argument instances of a particular role located in
different clusters. We test the effectiveness of our
induction method on the CoNLL 2008 benchmark
dataset and demonstrate improvements over compet-
itive unsupervised methods by a wide margin.
2 Related Work
As mentioned earlier, much previous work has
focused on building supervised SRL systems
(Ma`rquez et al, 2008). A few semi-supervised ap-
proaches have been developed within a framework
known as annotation projection. The idea is to com-
bine labeled and unlabeled data by projecting an-
notations from a labeled source sentence onto an
unlabeled target sentence within the same language
(Fu?rstenau and Lapata, 2009) or across different lan-
guages (Pado? and Lapata, 2009). Outwith annota-
tion projection, Gordon and Swanson (2007) attempt
to increase the coverage of PropBank by leveraging
existing labeled data. Rather than annotating new
sentences that contain previously unseen verbs, they
find syntactically similar verbs and use their annota-
tions as surrogate training data.
Swier and Stevenson (2004) induce role labels
with a bootstrapping scheme where the set of la-
beled instances is iteratively expanded using a clas-
sifier trained on previously labeled instances. Their
method is unsupervised in that it starts with a dataset
containing no role annotations at all. However, it re-
quires significant human effort as it makes use of
VerbNet (Kipper et al, 2000) in order to identify the
arguments of predicates and make initial role assign-
ments. VerbNet is a broad coverage lexicon orga-
nized into verb classes each of which is explicitly
associated with argument realization and semantic
role specifications.
Abend et al (2009) propose an algorithm that
identifies the arguments of predicates by relying
only on part of speech annotations, without, how-
ever, assigning semantic roles. In contrast, Lang
and Lapata (2010) focus solely on the role induction
problem which they formulate as the process of de-
tecting alternations and finding a canonical syntactic
form for them. Verbal arguments are then assigned
roles, according to their position in this canonical
form, since each position references a specific role.
Their model extends the logistic classifier with hid-
den variables and is trained in a manner that makes
use of the close relationship between syntactic func-
tions and semantic roles. Grenager and Manning
1118
(2006) propose a directed graphical model which re-
lates a verb, its semantic roles, and their possible
syntactic realizations. Latent variables represent the
semantic roles of arguments and role induction cor-
responds to inferring the state of these latent vari-
ables.
Our own work also follows the unsupervised
learning paradigm. We formulate the induction of
semantic roles as a clustering problem and propose a
split-merge algorithm which iteratively manipulates
clusters representing semantic roles. The motiva-
tion behind our approach was to design a concep-
tually simple system, that allows for the incorpo-
ration of linguistic knowledge in a straightforward
and transparent manner. For example, arguments
occurring in similar syntactic positions are likely to
bear the same semantic role and should therefore
be grouped together. Analogously, arguments that
are lexically similar are likely to represent the same
semantic role. We operationalize these notions us-
ing a scoring function that quantifies the compatibil-
ity between arbitrary cluster pairs. Like Lang and
Lapata (2010) and Grenager and Manning (2006)
our method operates over syntactically parsed sen-
tences, without, however, making use of any infor-
mation pertaining to semantic roles (e.g., in form of
a lexical resource or manually annotated data). Per-
forming role-semantic analysis without a treebank-
trained parser is an interesting research direction,
however, we leave this to future work.
3 Learning Setting
We follow the general architecture of supervised se-
mantic role labeling systems. Given a sentence and
a designated verb, the SRL task consists of identify-
ing the arguments of the verbal predicate (argument
identification) and labeling them with semantic roles
(role induction).
In our case neither argument identification nor
role induction relies on role-annotated data or other
semantic resources although we assume that the in-
put sentences are syntactically analyzed. Our ap-
proach is not tied to a specific syntactic representa-
tion ? both constituent- and dependency-based rep-
resentations could be used. However, we opted for a
dependency-based representation, as it simplifies ar-
gument identification considerably and is consistent
with the CoNLL 2008 benchmark dataset used for
evaluation in our experiments.
Given a dependency parse of a sentence, our sys-
tem identifies argument instances and assigns them
to clusters. Thereafter, argument instances can be
labeled with an identifier corresponding to the clus-
ter they have been assigned to, similar to PropBank
core labels (e.g., A0, A1).
4 Argument Identification
In the supervised setting, a classifier is employed
in order to decide for each node in the parse tree
whether it represents a semantic argument or not.
Nodes classified as arguments are then assigned a se-
mantic role. In the unsupervised setting, we slightly
reformulate argument identification as the task of
discarding as many non-semantic arguments as pos-
sible. This means that the argument identification
component does not make a final positive decision
for any of the argument candidates; instead, this de-
cision is deferred to role induction. The rules given
in Table 1 are used to discard or select argument can-
didates. They primarily take into account the parts of
speech and the syntactic relations encountered when
traversing the dependency tree from predicate to ar-
gument. For each candidate, the first matching rule
is applied.
We will exemplify how the argument identifica-
tion component works for the predicate expect in the
sentence ?The company said it expects its sales to
remain steady? whose parse tree is shown in Fig-
ure 1. Initially, all words save the predicate itself
are treated as argument candidates. Then, the rules
from Table 1 are applied as follows. Firstly, words
the and to are discarded based on their part of speech
(rule (1)); then, remain is discarded because the path
ends with the relation IM and said is discarded as
the path ends with an upward-leading OBJ relation
(rule (2)). Rule (3) does not match and is therefore
not applied. Next, steady is discarded because there
is a downward-leading OPRD relation along the path
and the words company and its are discarded be-
cause of the OBJ relations along the path (rule (4)).
Rule (5) does not apply but words it and sales are
kept as likely arguments (rule (6)). Finally, rule (7)
does not apply, because there are no candidates left.
1119
1. Discard a candidate if it is a determiner, in-
finitival marker, coordinating conjunction, or
punctuation.
2. Discard a candidate if the path of relations
from predicate to candidate ends with coordi-
nation, subordination, etc. (see the Appendix
for the full list of relations).
3. Keep a candidate if it is the closest subject
(governed by the subject-relation) to the left
of a predicate and the relations from predi-
cate p to the governor g of the candidate are
all upward-leading (directed as g? p).
4. Discard a candidate if the path between the
predicate and the candidate, excluding the last
relation, contains a subject relation, adjectival
modifier relation, etc. (see the Appendix for
the full list of relations).
5. Discard a candidate if it is an auxiliary verb.
6. Keep a candidate if the predicate is its parent.
7. Keep a candidate if the path from predicate
to candidate leads along several verbal nodes
(verb chain) and ends with arbitrary relation.
8. Discard all remaining candidates.
Table 1: Argument identification rules.
5 Split-Merge Role Induction
We treat role induction as a clustering problem with
the goal of assigning argument instances (i.e., spe-
cific arguments occurring in an input sentence) to
clusters such that these represent semantic roles. In
accordance with PropBank, we induce a separate set
of clusters for each verb and each cluster thus repre-
sents a verb-specific role.
Our algorithm works by iteratively splitting and
merging clusters of argument instances in order to
arrive at increasingly accurate representations of se-
mantic roles. Although splits and merges could be
arbitrarily interleaved, our algorithm executes a sin-
gle split operation (split phase), followed by a se-
ries of merges (merge phase). The split phase par-
titions the seed cluster containing all argument in-
stances of a particular verb into more fine-grained
(sub-)clusters. This initial split results in a clustering
with high purity but low collocation, i.e., argument
instances in each cluster tend to belong to the same
role but argument instances of a particular role are
Figure 1: A sample dependency parse with depen-
dency labels SBJ (subject), OBJ (object), NMOD
(nominal modifier), OPRD (object predicative com-
plement), PRD (predicative complement), and IM
(infinitive marker). See Surdeanu et al (2008) for
more details on this variant of dependency syntax.
located in many clusters. The degree of dislocation
is reduced in the consecutive merge phase, in which
clusters that are likely to represent the same role are
merged.
5.1 Split Phase
Initially, all arguments of a particular verb are placed
in a single cluster. The goal then is to partition this
cluster in such a way that the split-off clusters have
high purity, i.e., contain argument instances of the
same role. Towards this end, we characterize each
argument instance by a key, formed by concatenat-
ing the following syntactic cues:
? verb voice (active/passive);
? argument linear position relative to predicate
(left/right);
? syntactic relation of argument to its governor;
? preposition used for argument realization.
A cluster is allocated for each key and all argument
instances with a matching key are assigned to that
cluster. Since each cluster encodes fine-grained syn-
tactic distinctions, we assume that arguments occur-
ring in the same position are likely to bear the same
semantic role. The assumption is largely supported
by our empirical results (see Section 7); the clusters
emerging from the initial split phase have a purity
of approximately 90%. While the incorporation of
additional cues (e.g., indicating the part of speech
of the subject or transitivity) would result in even
greater purity, it would also create problematically
small clusters, thereby negatively affecting the suc-
cessive merge phase.
1120
5.2 Merge Phase
The split phase creates clusters with high purity,
however, argument instances of a particular role are
often scattered amongst many clusters resulting in a
cluster assignment with low collocation. The goal
of the merge phase is to improve collocation by ex-
ecuting a series of merge steps. At each step, pairs
of clusters are considered for merging. Each pair is
scored by a function that reflects how likely the two
clusters are to contain arguments of the same role
and the best scoring pair is chosen for merging. In
the following, we will specify which pairs of clus-
ters are considered (candidate search), how they are
scored, and when the merge phase terminates.
5.2.1 Candidate Search
In principle, we could simply enumerate and score
all possible cluster pairs at each iteration. In practice
however, such a procedure has a number of draw-
backs. Besides being inefficient, it requires a scoring
function with comparable scores for arbitrary pairs
of clusters. For example, let a, b, c, and d denote
clusters. Then, score(a,b) and score(c,d) must be
comparable. This is a stronger requirement than de-
manding that only scores involving some common
cluster (e.g., score(a,b) and score(a,c)) be com-
parable. Moreover, it would be desirable to ex-
clude pairings involving small clusters (i.e., with
few instances) as scores for these tend to be unre-
liable. Rather than considering all cluster pairings,
we therefore select a specific cluster at each step and
score merges between this cluster and certain other
clusters. If a sufficiently good merge is found, it is
executed, otherwise the clustering does not change.
In addition, we prioritize merges between large clus-
ters and avoid merges between small clusters.
Algorithm 1 implements our merging procedure.
Each pass through the inner loop (lines 4?12) selects
a different cluster to consider at that step. Then,
merges between the selected cluster and all larger
clusters are considered. The highest-scoring merge
is executed, unless all merges are ruled out, i.e., have
a score below the threshold ?. After each comple-
tion of the inner loop, the thresholds contained in
the scoring function (discussed below) are adjusted
and this is repeated until some termination criterion
is met (discussed in Section 5.2.3).
Algorithm 1: Cluster merging procedure. Oper-
ation merge(Li,L j) merges cluster Li into cluster
L j and removes Li from the list L.
1 while not done do
2 L? a list of all clusters sorted by number
of instances in descending order
3 i? 1
4 while i < length(L) do
5 j? arg max
0? j?<i
score(Li,L j?)
6 if score(Li,L j)? ? then
7 merge(Li,L j)
8 end
9 else
10 i? i+1
11 end
12 end
13 adjust thresholds
14 end
5.2.2 Scoring Function
Our scoring function quantifies whether two clusters
are likely to contain arguments of the same role and
was designed to reflect the following criteria:
1. whether the arguments found in the two clus-
ters are lexically similar;
2. whether clause-level constraints are satisfied,
specifically the constraint that all arguments
of a particular clause have different semantic
roles, i.e., are assigned to different clusters;
3. whether the arguments present in the two clus-
ters have similar parts of speech.
Qualitatively speaking, criteria (2) and (3) provide
negative evidence in the sense that they can be used
to rule out incorrect merges but not to identify cor-
rect ones. For example, two clusters with drastically
different parts of speech are unlikely to represent
the same role. However, the converse is not neces-
sarily true as part of speech similarity does not im-
ply role-semantic similarity. Analogously, the fact
that clause-level constraints are not met provides ev-
idence against a merge, but the fact that these are
satisfied is not reliable evidence in favor of a merge.
In contrast, lexical similarity implies that the clus-
1121
ters are likely to represent the same semantic role.
It is reasonable to assume that due to selectional re-
strictions, verbs will be associated with lexical units
that are semantically related and assume similar syn-
tactic positions (e.g., eat prefers as an object edible
things such as apple, biscuit, meat), thus bearing the
same semantic role. Unavoidably, lexical similarity
will be more reliable for arguments with overt lex-
ical content as opposed to pronouns, however this
should not impact the scoring of sufficiently large
clusters.
Each of the criteria mentioned above is quantified
through a separate score and combined into an over-
all similarity function, which scores two clusters c
and c? as follows:
score(c,c?) =
?
??
??
0 if pos(c,c?) < ?,
0 if cons(c,c?) < ?,
lex(c,c?) otherwise.
(2)
The particular form of this function is motivated by
the distinction between positive and negative evi-
dence. When the part-of-speech similarity (pos) is
below a certain threshold ? or when clause-level
constraints (cons) are satisfied to a lesser extent than
threshold ?, the score takes value zero and the merge
is ruled out. If this is not the case, the lexical similar-
ity score (lex) determines the magnitude of the over-
all score. In the remainder of this section we will
explain how the individual scores (pos, cons, and
lex) are defined and then move on to discuss how
the thresholds ? and ? are adjusted.
Lexical Similarity We measure lexical similar-
ity between two clusters through cosine similarity.
Specifically, each cluster is represented as a vec-
tor whose components correspond to the occurrence
frequencies of the argument head words in the clus-
ter. The similarity on such vectors x and y is then
quantified as:
lex(x,y) = cossim(x,y) =
x?y
?x??y?
(3)
Clause-Level Constraints Arguments occurring
in the same clause cannot bear the same role. There-
fore, clusters should not merge if the resulting clus-
ter contains (many) arguments of the same clause.
For two clusters c and c? we assess how well they
satisfy this clause-level constraint by computing:
cons(c,c?) = 1?
2? viol(c,c?)
NC +NC?
(4)
where viol(c,c?) refers to the number of pairs of in-
stances (d,d?) ? c? c? for which d and d? occur in
the same clause (each instance can participate in at
most one pair) and NC and NC? are the number of
instances in clusters c and c?, respectively.
Part-of-speech Similarity Part-of-speech similar-
ity is also measured through cosine-similarity (equa-
tion (3)). Clusters are again represented as vectors x
and y whose components correspond to argument
part-of-speech tags and values to their occurrence
frequency.
5.2.3 Threshold Adaptation and Termination
As mentioned earlier the thresholds ? and ? which
parametrize the scoring function are adjusted at each
iteration. The idea is to start with a very restrictive
setting (high values) in which the negative evidence
rules out merges more strictly, and then to gradually
relax the requirement for a merge by lowering the
threshold values. This procedure prioritizes reliable
merges over less reliable ones.
More concretely, our threshold adaptation pro-
cedure starts with ? and ? both set to value 0.95.
Then ? is lowered by 0.05 at each step, leaving ?
unchanged. When ? becomes zero, ? is lowered
by 0.05 and ? is reset to 0.95. Then ? is iteratively
decreased again until it becomes zero, after which ?
is decreased by another 0.05. This is repeated until ?
becomes zero, at which point the algorithm termi-
nates. Note that the termination criterion is not tied
explicitly to the number of clusters, which is there-
fore determined automatically.
6 Experimental Setup
In this section we describe how we assessed the per-
formance of our system. We discuss the dataset
on which our experiments were carried out, explain
how our system?s output was evaluated and present
the methods used for comparison with our approach.
Data For evaluation purposes, the system?s out-
put was compared against the CoNLL 2008 shared
task dataset (Surdeanu et al, 2008) which provides
1122
Syntactic Function Lang and Lapata Split-Merge
PU CO F1 PU CO F1 PU CO F1
auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2
gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9
auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3
gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1
Table 2: Clustering results with our split-merge algorithm, the unsupervised model proposed in Lang and
Lapata (2010) and a baseline that assigns arguments to clusters based on their syntactic function.
PropBank-style gold standard annotations. The
dataset was taken from the Wall Street Journal por-
tion of the Penn Treebank corpus and converted into
a dependency format (Surdeanu et al, 2008). In
addition to gold standard dependency parses, the
dataset alo contains automatic parses obtained from
the MaltParser (Nivre et al, 2007). Although the
dataset provides annotations for verbal and nominal
predicate-argument constructions, we only consid-
ered the former, following previous work on seman-
tic role labeling (Ma`rquez et al, 2008).
Evaluation Metrics For each verb, we determine
the extent to which argument instances in a cluster
share the same gold standard role (purity) and the
extent to which a particular gold standard role is as-
signed to a single cluster (collocation).
More formally, for each group of verb-specific
clusters we measure the purity of the clusters as the
percentage of instances belonging to the majority
gold class in their respective cluster. Let N denote
the total number of instances, G j the set of instances
belonging to the j-th gold class and Ci the set of in-
stances belonging to the i-th cluster. Purity can then
be written as:
PU =
1
N ?i
max
j
|G j ?Ci| (5)
Collocation is defined as follows. For each gold role,
we determine the cluster with the largest number of
instances for that role (the role?s primary cluster)
and then compute the percentage of instances that
belong to the primary cluster for each gold role as:
CO =
1
N ?j
max
i
|G j ?Ci| (6)
The per-verb scores are aggregated into an overall
score by averaging over all verbs. We use the micro-
average obtained by weighting the scores for indi-
vidual verbs proportionately to the number of in-
stances for that verb.
Finally, we use the harmonic mean of purity and
collocation as a single measure of clustering quality:
F1 =
2?CO?PU
CO+PU
(7)
Comparison Models We compared our split-
merge algorithm against two competitive ap-
proaches. The first one assigns argument instances
to clusters according to their syntactic function
(e.g., subject, object) as determined by a parser. This
baseline has been previously used as point of com-
parison by other unsupervised semantic role label-
ing systems (Grenager and Manning, 2006; Lang
and Lapata, 2010) and shown difficult to outperform.
Our implementation allocates up to N = 21 clus-
ters2 for each verb, one for each of the 20 most fre-
quent functions in the CoNLL dataset and a default
cluster for all other functions. The second compar-
ison model is the one proposed in Lang and Lapata
(2010) (see Section 2). We used the same model set-
tings (with 10 latent variables) and feature set pro-
posed in that paper. Our method?s only parameter is
the threshold ? which we heuristically set to 0.1. On
average our method induces 10 clusters per verb.
7 Results
Our results are summarized in Table 2. We re-
port cluster purity (PU), collocation (CO) and their
harmonic mean (F1) for the baseline (Syntactic
Function), Lang and Lapata?s (2010) model and
our split-merge algorithm (Split-Merge) on four
2This is the number of gold standard roles.
1123
Syntactic Function Split-Merge
Verb Freq PU CO F1 PU CO F1
say 15238 91.4 91.3 91.4 93.6 81.7 87.2
make 4250 68.6 71.9 70.2 73.3 72.9 73.1
go 2109 45.1 56.0 49.9 52.7 51.9 52.3
increase 1392 59.7 68.4 63.7 68.8 71.4 70.1
know 983 62.4 72.7 67.1 63.7 65.9 64.8
tell 911 61.9 76.8 68.6 77.5 70.8 74.0
consider 753 63.5 65.6 64.5 79.2 61.6 69.3
acquire 704 75.9 79.7 77.7 80.1 76.6 78.3
meet 574 76.7 76.0 76.3 88.0 69.7 77.8
send 506 69.6 63.8 66.6 83.6 65.8 73.6
open 482 63.1 73.4 67.9 77.6 62.2 69.1
break 246 53.7 58.9 56.2 68.7 53.3 60.0
Table 3: Clustering results for individual verbs with
our split-merge algorithm and the syntactic function
baseline.
datasets. These result from the combination of au-
tomatic parses with automatically identified argu-
ments (auto/auto), gold parses with automatic argu-
ments (gold/auto), automatic parses with gold argu-
ments (auto/gold) and gold parses with gold argu-
ments (gold/gold). Bold-face is used to highlight the
best performing system under each measure on each
dataset (e.g., auto/auto, gold/auto and so on).
On all datasets, our method achieves the highest
purity and outperforms both comparison models by
a wide margin which in turn leads to a considerable
increase in F1. On the auto/auto dataset the split-
merge algorithm results in 9% higher purity than the
baseline and increases F1 by 2.8%. Lang and Lap-
ata?s (2010) logistic classifier achieves higher collo-
cation but lags behind our method on the other two
measures.
Not unexpectedly, we observe an increase in per-
formance for all models when using gold standard
parses. On the gold/auto dataset, F1 increases
by 2.7% for the split-merge algorithm, 2.7% for the
logistic classifier, and 5.5% for the syntactic func-
tion baseline. Split-Merge maintains the highest pu-
rity and levels the baseline in terms of F1. Perfor-
mance also increases if gold standard arguments are
used instead of automatically identified arguments.
Consequently, each model attains its best scores on
the gold/gold dataset.
We also assessed the argument identification com-
Syntactic Function Split-Merge
Role PU CO F1 PU CO F1
A0 74.5 87.0 80.3 79.0 88.7 83.6
A1 82.3 72.0 76.8 87.1 73.0 79.4
A2 65.0 67.3 66.1 82.8 66.2 73.6
A3 48.7 76.7 59.6 79.6 76.3 77.9
ADV 37.2 77.3 50.2 78.8 37.3 50.6
CAU 81.8 74.4 77.9 84.8 67.2 75.0
DIR 62.7 67.9 65.2 71.0 50.7 59.1
EXT 51.4 87.4 64.7 90.4 87.2 88.8
LOC 71.5 74.6 73.0 82.6 56.7 67.3
MNR 62.6 58.8 60.6 81.5 44.1 57.2
TMP 80.5 74.0 77.1 80.1 38.7 52.2
MOD 68.2 44.4 53.8 90.4 89.6 90.0
NEG 38.2 98.5 55.0 49.6 98.8 66.1
DIS 42.5 87.5 57.2 62.2 75.4 68.2
Table 4: Clustering results for individual semantic
roles with our split-merge algorithm and the syntac-
tic function baseline.
ponent on its own (settings auto/auto and gold/auto).
It obtained a precision of 88.1% (percentage of se-
mantic arguments out of those identified) and recall
of 87.9% (percentage of identified arguments out of
all gold arguments). However, note that these fig-
ures are not strictly comparable to those reported
for supervised systems, due to the fact that our ar-
gument identification component only discards non-
argument candidates.
Tables 3 and 4 shows how performance varies
across verbs and roles, respectively. We compare the
syntactic function baseline and the split-merge sys-
tem on the auto/auto dataset. Table 3 presents results
for 12 verbs which we selected so as to exhibit var-
ied occurrence frequencies and alternation patterns.
As can be seen, the macroscopic result ? increase
in F1 (shown in bold face) and purity ? also holds
across verbs. Some caution is needed in interpret-
ing the results in Table 43 since core roles A0?A3
are defined on a per-verb basis and do not necessar-
ily have a uniform corpus-wide interpretation. Thus,
conflating scores across verbs is only meaningful to
the extent that these labels actually signify the same
3Results are shown for four core roles (A0?A3) and all sub-
types of the ArgM role, i.e., adjuncts denoting general purpose
(ADV), cause (CAU), direction (DIR), extent (EXT), location
(LOC), manner (MNR), and time (TMP), modal verbs (MOD),
negative markers (NEG), and discourse connectives (DIS).
1124
role (which is mostly true for A0 and A1). Further-
more, the purity scores given here represent the av-
erage purity of those clusters for which the specified
role is the majority role. We observe that for most
roles shown in Table 4 the split-merge algorithm im-
proves upon the baseline with regard to F1, whereas
this is uniformly the case for purity.
What are the practical implications of these re-
sults, especially when considering the collocation-
purity tradeoff? If we were to annotate the clus-
ters induced by our system, low collocation would
result in higher annotation effort while low purity
would result in poorer data quality. Our system im-
proves purity substantially over the baselines, with-
out affecting collocation in a way that would mas-
sively increase the annotation effort. As an exam-
ple, consider how our system could support humans
in labeling an unannotated corpus. (The following
numbers are derived from the CoNLL dataset4 in the
auto/auto setting.) We might decide to annotate all
induced clusters with more than 10 instances. This
means we would assign labels to 74% of instances in
the dataset (excluding those discarded during argu-
ment identification) and attain a role classification
with 79.4% precision (purity).5 However, instead
of labeling all 165,662 instances contained in these
clusters individually we would only have to assign
labels to 2,869 clusters. Since annotating a cluster
takes roughly the same time as annotating a single
instance, the annotation effort is reduced by a factor
of about 50.
8 Conclusions
In this paper we presented a novel approach to un-
supervised role induction which we formulated as a
clustering problem. We proposed a split-merge al-
gorithm that iteratively manipulates clusters repre-
senting semantic roles whilst trading off cluster pu-
rity with collocation. The split phase creates ?pure?
clusters that contain arguments of the same role
whereas the merge phase attempts to increase col-
location by merging clusters which are likely to rep-
resent the same role. The approach is simple, intu-
4Of course, it makes no sense to label this dataset as it is
already labeled.
5Purity here is slightly lower than the score reported in Ta-
ble 2 (auto/auto setting), because it is computed over a different
number of clusters (only those with at least 10 instances).
itive and requires no manual effort for training. Cou-
pled with a rule-based component for automatically
identifying argument candidates our split-merge al-
gorithm forms an end-to-end system that is capable
of inducing role labels without any supervision.
Our approach holds promise for reducing the data
acquisition bottleneck for supervised systems. It
could be usefully employed in two ways: (a) to cre-
ate preliminary annotations, thus supporting the ?an-
notate automatically, correct manually? methodol-
ogy used for example to provide high volume anno-
tation in the Penn Treebank project; and (b) in com-
bination with supervised methods, e.g., by providing
useful out-of-domain data for training. An important
direction for future work lies in investigating how
the approach generalizes across languages as well as
reducing our system?s reliance on a treebank-trained
parser.
Acknowledgments We are grateful to Charles
Sutton for his valuable feedback on this work. The
authors acknowledge the support of EPSRC (grant
GR/T04540/01).
Appendix
The relations in Rule (2) from Table 1 are IM??,
PRT?, COORD??, P??, OBJ?, PMOD?, ADV?,
SUB??, ROOT?, TMP?, SBJ?, OPRD?. The sym-
bols ? and ? denote the direction of the dependency
arc (upward and downward, respectively).
The relations in Rule (3) are ADV??, AMOD??,
APPO??, BNF??-, CONJ??, COORD??, DIR??,
DTV??-, EXT??, EXTR??, HMOD??, IOBJ??,
LGS??, LOC??, MNR??, NMOD??, OBJ??,
OPRD??, POSTHON??, PRD??, PRN??, PRP??,
PRT??, PUT??, SBJ??, SUB??, SUFFIX??. De-
pendency labels are abbreviated here. A detailed
description is given in Surdeanu et al (2008), in
their Table 4.
References
O. Abend, R. Reichart, and A. Rappoport. 2009. Un-
supervised Argument Identification for Semantic Role
Labeling. In Proceedings of the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natu-
ral Language Processing, pages 28?36, Singapore.
1125
D. Dowty. 1991. Thematic Proto Roles and Argument
Selection. Language, 67(3):547?619.
H. Fu?rstenau and M. Lapata. 2009. Graph Aligment
for Semi-Supervised Semantic Role Labeling. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 11?20, Singa-
pore.
D. Gildea and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics,
28(3):245?288.
A. Gordon and R. Swanson. 2007. Generalizing Se-
mantic Role Annotations Across Syntactically Similar
Verbs. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, pages
192?199, Prague, Czech Republic.
T. Grenager and C. Manning. 2006. Unsupervised Dis-
covery of a Statistical Verb Lexicon. In Proceedings
of the Conference on Empirical Methods on Natural
Language Processing, pages 1?8, Sydney, Australia.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
Based Construction of a Verb Lexicon. In Proceedings
of the 17th AAAI Conference on Artificial Intelligence,
pages 691?696. AAAI Press / The MIT Press.
J. Lang and M. Lapata. 2010. Unsupervised Induction
of Semantic Roles. In Proceedings of the 11th Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 939?
947, Los Angeles, California.
L. Ma`rquez, X. Carreras, K. Litkowski, and S. Stevenson.
2008. Semantic Role Labeling: an Introduction to the
Special Issue. Computational Linguistics, 34(2):145?
159, June.
G. Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,
B. Gu, A. Sarkar, and F. Popowich. 2005. Description
of SQUASH, the SFU Question Answering Summary
Handler for the DUC-2005 Summarization Task. In
Proceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods in
Natural Language Processing Document Understand-
ing Workshop, Vancouver, Canada.
J. Nivre, J. Hall, J. Nilsson, G. Eryigit A. Chanev,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A Language-independent System for Data-
driven Dependency Parsing. Natural Language Engi-
neering, 13(2):95?135.
S. Pado? and M. Lapata. 2009. Cross-lingual Annotation
Projection of Semantic Roles. Journal of Artificial In-
telligence Research, 36:307?340.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. Computational Linguistics, 31(1):71?106.
S. Pradhan, W. Ward, and J. Martin. 2008. Towards Ro-
bust Semantic Role Labeling. Computational Linguis-
tics, 34(2):289?310.
D. Shen and M. Lapata. 2007. Using Semantic Roles
to Improve Question Answering. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing and the Conference on Com-
putational Natural Language Learning, pages 12?21,
Prague, Czech Republic.
M. Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 8?15, Sapporo, Japan.
M. Surdeanu, R. Johansson, A. Meyers, and L. Ma`rquez.
2008. The CoNLL-2008 Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies. In Proceed-
ings of the 12th CoNLL, pages 159?177, Manchester,
England.
R. Swier and S. Stevenson. 2004. Unsupervised Seman-
tic Role Labelling. In Proceedings of the Conference
on Empirical Methods on Natural Language Process-
ing, pages 95?102, Barcelona, Spain.
D. Wu and P. Fung. 2009. Semantic Roles for SMT:
A Hybrid Two-Pass Model. In Proceedings of North
American Annual Meeting of the Association for Com-
putational Linguistics HLT 2009: Short Papers, pages
13?16, Boulder, Colorado.
1126
