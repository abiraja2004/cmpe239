A reconfigurable stochastic tagger for languages with complex tag
structure
?ukasz De?bowski
Institute of Computer Science
Polish Academy of Sciences
ldebowsk@ipipan.waw.pl
Abstract
We present a case study of a complex
stochastic disambiguator of alternatives
of morphosyntactic tags which allows
for using incomplete disambiguation,
shorthand tag notation, external tagset
definition and external definition of mul-
tivalued context features. The tagger
bases on Naive Bayes modeling and al-
lows for using almost as general context
features as in classical trigram taggers as
well as more specific ones. Its prelimi-
nary results for Polish still do not meet
our expectations. Possible sources of the
tagger?s failures can be: inhomogene-
ity of the training corpus in preparation,
lack of the automatic search of probabil-
ity models, too general conditional in-
dependence assumptions in defining the
class of interpretable models.
Automatization of high-quality morphosyntac-
tic tagging for strongly inflective languages, such
as Slavic languages, seems to be a much harder
task than so called part-of-speech (POS) tagging
for weaker inflective languages. An important fac-
tor increasing the complexity is the very design of
the tagset. Usually, the tags assigned to word-
like segments in the former task are long lists
of subsequent attribute values, e.g. POS, num-
ber, case, gender, person etc. (Hajic? and Hladk?,
1998; Wolin?ski and Przepi?rkowski, 2001), so
they provide much more information than almost
atomic labels used for POS tagging (Manning and
Sch?tze, 1999). To make the matter harder, many
formal descriptions become easier when the tag
attribute values are allowed to form RSRL-like
type hierarchies (Przepi?rkowski et al, 2002). Al-
lowing the values to be partially ambiguous de-
pending on a context raises questions what is the
accurate level of disambiguation (Wolin?ski and
Przepi?rkowski, 2001) and how to model it prob-
abilistically in terms of random variables taking
disjunctive values (Brew, 1995).
Working for a project aiming at building a large
morphosyntactically tagged corpus of written Pol-
ish (information site http://dach.ipipan.
waw.pl/CORPUS/), we have tried implement-
ing a highly reconfigurable stochastic tagger ad-
dressing some of these problems. The main fea-
tures of our software are as follows:
? The tagger is a contextual disambiguator: It
only prunes the lists of tags admissible for
successive word-segments, given by a sepa-
rate morphological analyzer. Superiority of
this approach over simulating a stochastic
morphological analyzer by a tagger has been
discussed in Hajic? (2000).
? The tags processed by the tagger have form
of short human-readable lists of attribute val-
ues. Especially, non-applicable attributes are
omitted and multiple atomic values can be
given for the same attribute.
? The tagger?s internal representation of dis-
ambiguation alternatives (tagger?s decisions)
is different than the list of all admissible
atomic tags. In this approach, some kind
of contextually-dependent incompleteness of
disambiguation could be learned.
? Special configuration files inform the tag-
ger which tag attributes are to be disam-
biguated and what multivalued context at-
tribute are relevant for that. The tagger?s in-
ference uses a series of Naive-Bayes-like as-
sumptions founded on joint distributions of
disambiguation decisions for one tag attribute
and values of one context attribute.
? The values of the context attributes are au-
tomatically instantiated and smoothed strings
whose templates are given in a hand-made
configuration file. This approach allows to
combine strengths of generality of context at-
tributes as in n-gram models (Brants, 2000;
Megyesi, 2001) with their specificity as for
binary features in MaxEnt taggers (Ratna-
parkhi, 1996; Hajic? and Hladk?, 1998). Pos-
sibility of using alternative files defining the
templates of context attributes eases con-
structive critiques of the particular definition
of them.
? The tagger processes XML-formatted texts
where input and output files have the same
structure. Especially, it can be run on its own
output to disambiguate some attributes in cas-
cade rather than simultaneously.
? The tagger can be used for any other language
supplied with morphological analyzer, train-
ing data and tagset-dependent configuration
files.
In the following bulk of paper, we shall present
the features of our tagger in more detail, we shall
discuss its preliminary results for our Polish tag-
ging project, as well as, we shall share remarks on
possible extensions/improvements of the software
behavior.
Our general feeling is that the tagger as we have
it implemented and configured now for Polish is
not a very practical program: It works very slow
and it makes much more mistakes than state-of-
the-art taggers. In fact, the tagger gives its users so
much freedom of manual configuration and feed-
back information in the error reports that they get
lost. We hope that much accuracy can be earned
when some automatic search for the optimal con-
figuration files is implemented. On the hand, we
are still afraid if we have not underfitted with vari-
ous conditional independence assumptions, which
restrain the tagger from seeing sequences of very
specific tags as something systematic: Especially,
each tag attribute is disambiguated probabilisti-
cally independently and the program does not al-
low for treating tags as atomic entities in proba-
bility modeling. This cannot be overcome with-
out a major change in the program and, worse, in
the already complex structure of its configuration
files.
Despite all these drawbacks, we present some
study of how one can think creatively about tag-
ging and how one cannot find a quick break-even
when trying to implement too many good guide-
lines in one piece. We report on lots of apparently
technical details, hoping that their exposition can
help see the tagger as something more than a black
box and identify the sources of its errors.
1 Human-readable tags and tagger?s
decisions
Before we can define the probabilistic model of
our tagger and the scheme of its training, we need
to define some pretty abstract entities. These en-
tities are tagger?s disambiguating decisions whose
conditional probability is maximized given the ini-
tial state of the text annotation. The decisions dif-
fer from human-readable tags in the corpus. To
explain what they are and why they are there, we
need to clarify how the tagger interprets the struc-
ture of text annotation in a way which allows for
running the tagger on its earlier outputs.
Figure 1 presents a general scheme of using
the tagger. Any plain text (format 0?possibly
with some general XML tags) is first fed through
script analize.plain.text.plwhich iden-
tifies word-like segments and gives them their full
morphological analysis, i.e. the list of all possi-
ble tags (Morfeusz.pm is a library serving the
Polish morphological analyzer, written by our col-
league). After the analysis, the text obtains for-
mat 2 which is conserved through multiple calls of
the tagger (named Cypher.pl). The only thing
which happens during these calls is reducing the
Corpusio.pm
Morfeusz.pm
Morfeuszki.pm
attribute.counts.A
(attribute.weigths.A)
conx.attributes.A
survey.A
analize.plain.text.pl
test.corpus
(format 3)
(format 0)
tag.attributes.full
tag.vtcs.full
Corpusio.pm
(format 2)
Cypher.pl e
Cypher.pl l
test.corpus.A.v
(format 3)
training.corpus
(format 3)
Cypher.pl o
training.corpus.A.v
(format 3)
Cypher.pl o
CorpusioMorfeuszki.pm
(format 2)
Figure 1: Structure of tagger running.
number of alternative tags initially given by the
morphological analysis. Each tagger call can be
used to disambiguate different sets of tag attributes
depending on the given configuration files (such as
conx.attributes.*).
In fact, in format 2, each tagged segment
(roughly orthographic word) is given two identi-
cally formatted lists of potentially ambiguous lin-
guistic annotation: the full morphological analysis
and the reduced morphological analysis as pruned
by the tagger (initially equal to the full morpho-
logical analysis). In format 3, used for training
and test corpora, there is a third list containing the
morphological analysis as pruned by the human
annotators (it shares the same format and it can be
only partially disambiguated). During any kind of
the tagger call (training, testing, or tagging new
texts), it is only the tagger-pruned analyses that
are pruned against values of the call-dependent
tag attributes. Full morphological analyses and
human-pruned analyses remain untouched. Unfor-
tunately, tagger-pruned analyses must be pruned
also in the original training and test data to obtain
the relevant test and training data for the next tag-
ger call run in a pipe.
Each morphological analysis, full or reduced,
appearing in the annotated texts can be ambigu-
ous and it has a form of shorthand human-readable
tags grouped by lemmas. Here is an example of
the full morphological analysis for the Polish word
kurze (dust, cock, hen, hen:Adj):
<l>kurz<t>subst:pl:nom.acc:m3
<l>kur<t>subst:sg:loc.voc:m2
<l>kura<t>subst:sg:dat.loc:f
<l>kurzy<t>adj:sg:nom.acc:n1.n2:
pos<t>adj:pl:nom.acc:m2.m3.f.n1.
n2.p2.p3:pos\n
The tags are lists of values separated by colons (:)
and dots (.). Colons separate tag attributes and
dots separate alternative values for the same at-
tribute. When the dot is used for more than one
attribute, the tag means the full Cartesian product
of alternatives. Values of non-applicable attributes
are omitted.
The omission of attributes is not used in the tag-
ger?s internal representation of the reduced mor-
phological analysis (called tag list). Here, each
attribute has its fixed position, and our example of
full morphological analysis is transformed one-to-
one into list:
[[subst,pl,nom.acc,m3,-,-,...,kurz],
[subst,sg,loc.voc,m2,-,-,...,kur],
[subst,sg,dat.loc,f,-,-,...,kura],
[adj,sg,nom.acc,n1.n2,-,pos,...,
kurzy],
[adj,pl,nom.acc,m2.m3.f.n1.n2.p2.p3,
-,pos,...,kurzy]]
This transformation is controlled by file
tag.attributes.* which specifies names
of consequent tag attributes and enumerations of
their values
<POS> conj prep subst fin ...
<number> sg pl
<case> nom gen dat acc inst loc voc
...
(The last tag attribute is always lemma, which is
not enumerative but formally useful as a kind of
tag attribute.)
Human-reduced analyses in the training data are
allowed to be ambiguous, so the tagger could learn
not to disambiguate in some contexts. In this case,
one needs to differentiate tagger?s lists of disjunc-
tive disambiguation decisions (decision lists) from
the tagger-pruned tag lists. In fact, each deci-
sion list is a very simple functions of the tagger-
reduced tag list. E.g., if it is specified that only
number and case can be disambiguated in a given
tagger call, the list of decisions for our continued
example is:
[[LEAVE,LEAVE,LEAVE,LEAVE,...,LEAVE],
[LEAVE,sg,nom,LEAVE,...,LEAVE],
[LEAVE,sg,dat,LEAVE,...,LEAVE],
[LEAVE,sg,acc,LEAVE,...,LEAVE],
[LEAVE,sg,loc,LEAVE,...,LEAVE],
[LEAVE,sg,voc,LEAVE,...,LEAVE],
[LEAVE,pl,nom,LEAVE,...,LEAVE],
[LEAVE,pl,acc,LEAVE,...,LEAVE]]
Decision list contains all choices of atomic values
of disambiguated attributes plus special decision
[LEAVE,LEAVE,...,LEAVE].
When a decision is eventually selected from the
list, the new reduced tag list becomes former re-
duced tag list unified with the decision against all
tag attributes. LEAVE and any value X unifies
downto X. Atomic value Y and any value X uni-
fies downto Y if X contains Y and downto empty
set otherwise. For learning and testing purposes,
the right decision to be expected from the tagger
is computed as follows: When any manually dis-
ambiguated tag attribute has just one value, the
same attribute value is chosen for the right deci-
sion. When the attribute is ambiguous, same at-
tribute for the right decision equals to LEAVE.
The differentiation between all the tag attributes
and the disambiguated tag attributes was moti-
vated by the occurrence of tag lists close to large
Cartesian products: Some Polish participle forms
contain alternative values for so many tag at-
tributes, that one would get > 90 disjunctive de-
cisions for one word if disambiguating all at-
tributes simultaneously. Sequential disambigua-
tion reduces polynomial complexity downto linear
one and may enable better probability estimation.
Additionally, disambiguation of only 4 out of 14
identified morphosyntactic tag attributes usually
suffices in Polish for the full tag disambiguation.
2 Probability modeling
The general problem of probabilistic modeling of
decision processes is simple to phrase: If we want
a particular decision T to depend on all its con-
text, we find that pairs (decision, context state) are
unique events and we cannot generalize beyond
them. If we group the contexts into too few equiv-
alence classes and make the same decision for all
contexts in a class, then we lose sensitivity of the
decisions. Finding the optimal equivalence classes
can need large amounts of domain knowledge or
extensive search. Probability modeling gives addi-
tional hint: Define several not so sophisticated par-
titions of contexts into equivalence classes. Find
the joint probabilities of pairs (decision, equiva-
lence class of i-th context partition). From these,
compute the joint probability of tuples (decision,
equivalence classes for all context partitions) as-
suming some regularity of this distribution. De-
pending on the size of available training data, it
can give better results than direct estimation of the
tuple distribution.
In stochastic text tagging by our tagger, there is
a string of decisions for successive word segments
(text positions) rather than one decision. Let
Ti be the tagger?s decision at text position i,
i ? I . The admissible disjunctive choices for
Ti are tij , j ? Ji. Both Ti and tij are vectors
of values Tit and tijt for disambiguated tag
attributes t, t ? T (tijt equals to an atomic
tag attribute value or LEAVE). On the other
hand, there is a set of random variables, called
context attributes Citc, c ? Ct, which take values
identifying disjunctive equivalence classes of
successive context partitions. During tagging,
context attribute values are computed using
scheme: Citc = ftc(?, i, {Ti?t?}(i?,t?)?P(i,t)),
where ? represents the whole input text
(list of word-segments and their input tag
lists). P(i, t) = ({i ? k, ..., i ? 1} ? T ) ?
({i} ? ({0, ..., t ? 1} ? T )). ftc are fast-
computable functions taking repeatable values we
describe later.
For a formulated probability model
P
(
Ti = ? | {Citc = ? }t?T ,c?Ct
)
, the tagger
uses Viterbi search to find the optimal string of
decisions tiji , i ? I , maximizing product
?
i?I
P (Ti = tiji|{Citc = ftc(?, i,
{
ti?ji? t?
}
(i?,t?)?P(i,t))}t?T ,c?Ct).
(Viterbi search is done as for a non-stationary hid-
den Markov model of order k). Direct estimation
of P
(
Ti = ? | {Citc = ? }t?T ,c?Ct
)
is assumed un-
feasible. The tagger approximates it as
P
(
Ti = tij| {Citc = ctc}t?T ,c?Ct
)
=
?
t?T
P (T?t = tijt)
?
c?Ct
P (C?tc = ctc|T?t = tijt)
?
j??Ji
?
t?T
P (T?t = tij?t)
?
c?Ct
P (C?tc = ctc|T?t = tij?t)
,
(1)
where numerical values of P (T?t = ? ), P (C?tc =
? |T?t = ? ) do not depend explicitly on i.
If the decision lists were Cartesian products:
?
j?Ji ?t?T tijt = ?t?T
?
j?Ji tijt, then approx-
imation (1) would be equivalent to assuming that
(i) variables {Citc}c?C are independent given Tit
(naive Bayes), and (ii) given all {Citc}t?T ,c?C ,
variables {Tit}t?T are also independent (compare
Hajic? and Hladk? (1998)).
Estimation of probabilities P (T?t = ? ),
P (C?tc = ? |T?t = ? ) is done using formula
P (T?t = tijt|C?tc = ctc) =
# of positions i such that Tit = tijt i Citc = ctc
# of positions i such that Citc = ctc
,
where the counts of positions are faked by smooth-
ing procedure described in the next section.
3 Definition of context partitions
Functions ftc, defining the context partitions, are
specified in file conx.attributes.*. The file
is merely read by the tagger and it can be pre-
pared by hand. Table 1 shows an example of this
file. First column is the name of some tag at-
tribute, say t-th one. Then, the second column is
a string defining ftc. The string defining ftc re-
sembles definition of a decision tree with identi-
cal structure of all branches. It represents a suc-
cession of tests and variable instantiations evalu-
ated at each text position with decision ambigu-
ous for t-th tag attribute. Larger and larger pre-
fixes of the string are taken into account as long
as no false condition is demanded or no instanti-
ation results in disallowed prefixes. (Smoothing
procedure provides a list of allowed instantiated
prefixes, which is substantially smaller than the
list of all syntactically correct instantiations). The
longest achieved prefix with instantiated variables
is returned as ftc(?, i, {Ti?t?}(i?,t?)?P(i,t)).
The meaning of currently recognized com-
mands in the string defining ftc is such:
? rp:INTEGER_NUMBER: sets an auxiliary
text position as current text position plus
INTEGER_NUMBER.
? wd:SOME_STRING: checks if the
segment at auxiliary position equals to
SOME_STRING.
? ac:, ex:, al:, my: followed
by NAME_OF_A_TAG_ATTR: and
SOME_STRING: check if SOME_STRING
for tag attribute NAME_OF_A_TAG_ATTR
at auxiliary position is: the alternative of
available values (ac:), one of available
values (ex:), the only available value (al:),
the value of decision chosen by the tagger
(my:).
? <>: and <0>: are variables replaced for de-
manded kind of entity with its value at the
auxiliary text position (<>:) or at the current
position (<0>:).
For any definition of ftc, such as
rp:-1:wd:nie:rp:0:ac:<POS>:<>:ac:
<lemma>:<>, and each of its maximally long
values, such as C:rp:-1:wd:nie:rp:0:ac:
<POS>:inf.impt:ac:<lemma>:is?c?.
is?cic?, there is a list prefixes representing less
and less specific information on the context:
C:rp:-1:wd:nie:rp:0:ac:<POS>:inf.
impt:ac:<lemma>:is?c?.is?cic?, C:rp:
-1:wd:nie:rp:0:ac:<POS>:inf.impt,
C:rp:-1:wd:nie, C:. For each of these
prefixes, except empty one?C, there is exactly
one prefix immediately shorter. Basing on this
property, we have implemented the following
smoothing of ftc values:
1. During learning, the alphabet of allowed in-
stantiated prefixes is not closed, so for each
ftc, counts of all its maximally long values
are collected.
2. Each ftc value passes all its counts to its im-
mediate prefix if it was seen less than thresh-
old (3 times).
3. The alphabet of allowed values of ftc is fixed
as the set of all prefixes of ftc values currently
counted positively.
4. For each ftc value in the allowed alphabet, it
is faked it was also seen (once) with special
decision value Tit =SMOOTH.
5. During tagging, ftc yields the longest al-
lowed and matching instantiated value ctc.
If ctc has no positive count with particular
asked decision value Tit = tijt, tijt is treated
as it were SMOOTH.
In the adopted disambiguation scheme, the
order k of nonstationary Markov model to be
used in Viterbi search is the negative of min-
imal argument of rp: commands followed by
my: command. Thanks to that, given file
conx.attributes.*, our tagger identifies k
automatically and accepts any value of k.
4 First results
In this section, we would like to present sev-
eral scores of our tagger on processing Polish
texts. The scores should be considered prelimi-
nary due to several causes: (i) we believe that our
conx.attributes.* file is not optimal yet,
(ii) improving the tagger?s code continually, we
have had too little time to test its behavior on large
training data sufficiently, (iii) the training corpus
is still in a mix of two different degrees of dis-
ambiguation of gender values, (iv) morphologi-
cal analyzer contains no guesser and unrecognized
strings are not considered ambiguous by the tag-
ger.
In our current Polish annotation scheme, tags
consist of the following attributes: POS, num-
ber, case, gender, person, degree, aspect, nega-
tion, depreciation, accommodability, accentabil-
ity, post-prepositionality, vocalicity, punctuation,
lemma (Wolin?ski and Przepi?rkowski, 2001). We
have observed that for given morphological anal-
ysis, any atomic tag is almost always identified
given its values for just 4 attributes: POS, number,
case, gender, lemma, and decided to disambiguate
directly only these attributes.
Testing several simple versions of
conx.attributes.* file, we have ob-
served that smaller error on POS is obtained
when POS is disambiguated simultaneously
with number, case, and gender while lemma
is to be disambiguated afterwards.1 The best
conx.attributes.* file we have found so
far is presented in table 1. The general structure of
this model of ftcs resembles a product of trigram
models for each of the disambiguated attributes.
Some modification of conx.attributes.*
file might be adding POS values at positions
-1,0,+1 to ftc values for number, case, and gender.
Theoretically, this modification could be more
sensitive to a regular grammar syntax of simple
phrase structures. Strangely, this modification
yields twice as high error rate as the original
conx.attributes.* file.
Observed error rates for ftcs as in table 1 are:
? Training data 01k (784 word segments)
Error rate: (all tokens) (ambiguous)
Overall 0.25 0.45
on POS 0.06 0.10
on number 0.06 0.10
on case 0.16 0.28
on gender 0.13 0.22
? Training data 10k (8118 word segments)
1Some major ambiguity left for lemma disambiguation in
this case are homonymous present tense forms for musiec? ?
?to have? and music? ? ?to compel?.
<POS> ac:<POS>:<>:ac:<lemma>:<>
<POS> ac:<POS>:<>:rp:-1:my:<POS>:ac:<lemma>:<>
<POS> my:<POS>:<>:rp:+1:ac:<POS>:ac:<lemma>:<>
#
<number> ac:<number>:<>:rp:-1:my:<number>:<>
<number> ac:<number>:<>:rp:+1:ac:<number>:<>
#
<case> rp:-1:my:<case>:<>
<case> rp:+1:ac:<case>:<>
#
<gender> ac:<gender>:<>:rp:-1:my:<gender>:<>
<gender> ac:<gender>:<>:rp:+1:ac:<gender>:<>
Table 1: Preliminary conx.attributes.* file used for Polish.
Error rate: (all tokens) (ambiguous)
Overall 0.23 0.41
on POS 0.06 0.11
on number 0.06 0.10
on case 0.14 0.24
on gender 0.11 0.19
? Training data 50k (41208 word segments)
Error rate: (all tokens) (ambiguous)
Overall 0.20 0.34
on POS 0.05 0.08
on number 0.04 0.08
on case 0.12 0.20
on gender 0.07 0.13
? Full training data (558224 word segments)
Error rate: (all tokens) (ambiguous)
Overall 0.22 0.35
on POS 0.05 0.09
on number 0.05 0.08
on case 0.12 0.21
on gender 0.10 0.17
All trained models were tested on test data 5k
(4117 word segments).
The error rate for full training data is larger than
for 50k training data, especially on gender. This is
probably due to the inhomogenous manual anno-
tation of gender values we had already mentioned.
Comparing with publications on similar tasks, our
minimal overall error rate is twice as big as for
Slovene (D?eroski et al, 2000) and 3 times as big
as for Czech (Hajic? and Hladk?, 1998).
5 Comments and possible extensions
It has been remarked that HMM trigram tag-
gers using single multivalued context features can
perform better and run faster than MaxEnt tag-
gers trying to combine conditional probabilities
for a multitude of binary features (Brants, 2000;
Megyesi, 2001), even for large structured tagsets
and Slavonic free word-order (Hajic? et al, 2001).
Our intention was to try out a hybrid approach in
which a small number of multivalued context fea-
tures is used. We have thought it can help solv-
ing data sparseness problems. Now we do not
know exactly what is the most important cause
of our present high error rate: inhomogenous Pol-
ish corpus annotation, deficiencies of morpholog-
ical analysis, low efficiency of the manual model
search, or the assumption of probabilistic indepen-
dence of different tag attributes (used succesfully
by Hajic? and Hladk? (1998)). Due to the last thing,
we cannot either simulate a simple trigram tagger
interpreting tags as single entities.
So as to overcome the manual model search, we
have thought of writing another program which
could read the error reports of our tagger and
search for the optimal conx.attributes.*
file in a fixed space of context functions ftcs.
For binary context functions, Hajic? and Hladk?
(1998) truncated the length of their definitions and
searched extensively through a very large finite
space of them. We think that optimal ftc defini-
tions may be so long that genetic algorithm search
through the infinite space of ftcs may be a better
approach.
One might also search for more powerful se-
mantics of context functions ftc. As long as ftcs
do not depend on too distant previous tagger de-
cisions (increase the order k of Markov model),
or do not need too much computation on their
own, one can freely decide what functions of mor-
phologically analysed text ftcs are. Better disam-
biguation of Slavic nominative/accusative ambi-
guity may need testing if there is a possible ver-
bal form in the left/right context of a word (Hajic?
and Hladk?, 1998). Disambiguation of complex
gender ambiguities may need testing unifiability
of the attribute values.
Our idea of smoothing ftc values resembles
HMM reconstruction algorithm proposed by Shal-
izi, Shalizi and Crutchfield (2003), which uses ad-
ditionally Kolmogorov-Smirnov test to check if
extending the context depth is necessary for ad-
equate prediction. Similarly, we could replace ftc
value ctc by its prefix c?tc not only when ctc is
rare but also when P (T?t = ? |C?tc = ctc) does not
differ significantly from P (T?t = ? |C?tc = c?tc).
Such technique could decrease memory usage and
slightly speed up the tagger.
Gra?a, Alonso and Vilares (2002) proposed a
modification of Viterbi search for unknown text
segmentation as a common solution for disam-
biguation of segmentation and tagging. Our Pol-
ish tagset introduces some very rare ambiguities
of segmentation but we have consciously decided
not to touch the segmentation since extending the
modification of Viterbi search to multiattribute
tags with independent tag attributes needs a very
different structure of training corpus and proba-
bility estimation. Such tool would be formally
capable of performing functionality of a cascade
phrase parser and would be a good subject of an-
other project.
The research reported here was partly supported
by the KBN grant 8 T11C 043 20.
References
Thorsten Brants. 2000. TnT ? a statistical part-of-
speech tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference (ANLP-
2000). Seattle, WA, USA.
Chris Brew. 1995. Stochastic HPSG. In Proceedings
of the 7th Conference of the European Chapter of
the Association for Computational Linguistics.
Sa?o D?eroski, Toma? Erjavec, and Jakub Zavrel.
2000. Morphosyntactic Tagging of Slovene:
Evaluating PoS Taggers and Tagsets. In Sec-
ond International Conference on Language Re-
sources and Evaluation, LREC?00, pages 1099?
1104, Paris. ELRA. http://nl.ijs.si/et/
Bib/LREC00/lrec-tag-www/.
Jorge Gra?a, Miguel A. Alonso, and Manuel Vilares.
2002. A common solution for tokenization and part-
of-speech tagging. In Proceedings of Text, Speech
and Dialogue 2002. Lecture Notes in Artificial Intel-
ligence 2448. Springer Verlag.
Jan Hajic? and Barbora Hladk?. 1998. Tagging inflec-
tive languages: Prediction of morphological cate-
gories for a rich, structured tagset. In Proceedings
of COLING-ACL Conference. Montr?al.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva, and
Vladim?r Petkevic?. 2001. Serial combination of
rules and statistics: A case study in czech tagging.
In Proceedings of ACL?01. Toulouse, France.
Jan Hajic?. 2000. Morphological tagging: Data vs. dic-
tionaries. In Proceedings of ANLP-NAACL Confer-
ence. Seattle.
Christopher D. Manning and Hinrich Sch?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press.
Be?ta Megyesi. 2001. Comparing data-driven learning
algorithms for PoS tagging of Swedish. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP 2001). Carnegie
Mellon University, Pittsburgh, PA, USA.
Adam Przepi?rkowski, Anna Kups?c?, Ma?gorzata
Marciniak, and Agnieszka Mykowiecka. 2002. For-
malny opis je?zyka polskiego: Teoria i implemen-
tacja. Akademicka Oficyna Wydawnicza EXIT,
Warszawa.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings of
the First Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-1996). Univer-
sity of Pennsylvania, PA, USA.
Cosma Rohilla Shalizi, Kristina Lisa Shalizi, and
James P. Crutchfield. 2003. An algorithm for
pattern discovery in time series. http://www.
arxiv.org/abs/cs.LG/0210025.
Marcin Wolin?ski and Adam Przepi?rkowski. 2001.
Projekt anotacji morfosynktaktycznej korpusu
je?zyka polskiego. Technical Report 938, Institute of
Computer Science, Polish Academy of Sciences.
