Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 40?48,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Modeling Morphosyntactic Agreement
in Constituency-Based Parsing of Modern Hebrew
Reut Tsarfaty?and Khalil Sima?an
Institute for Logic, Language and Computation
University of Amsterdam
{r.tsarfaty,k.simaan}@uva.nl
Abstract
We show that na??ve modeling of morphosyn-
tactic agreement in a Constituency-Based
(CB) statistical parsing model is worse than
none, whereas a linguistically adequate way
of modeling inflectional morphology in CB
parsing leads to improved performance. In
particular, we show that an extension of the
Relational-Realizational (RR) model that in-
corporates agreement features is superior to
CB models that treat morphosyntax as state-
splits (SP), and that the RR model benefits
more from inflectional features. We focus on
parsing Hebrew and report the best result to
date, F184.13 for parsing off of gold-tagged
text, 5% error reduction from previous results.
1 Introduction
Agreement is defined by linguists as the system-
atic covariance of the grammatical properties of one
linguistic element to reflect the semantic or formal
properties of another (Corbett, 2001). Morpholog-
ically marked agreement features such as gender,
number and person are used to realize grammat-
ical relations between syntactic constituents, and
such patterns are abundantly found in (less- or) non-
configurational languages (Hale, 1983) where the
order of words is known to be (relatively) free.
Agreement features encompass information con-
cerning the functional relations between constituents
in the syntactic structure, but whether incorporat-
ing agreement features in a statistical parsing model
leads to improved performance has so far remained
an open question and saw contradictory results.
?The first author is currently a researcher at the department
of Linguistics and Philology at Uppsala University.
Taking Semitic languages as an example, it was
shown that an SVM-based shallow parser (Gold-
berg et al, 2006) does not benefit from includ-
ing agreement features for NP chunking in Hebrew.
Phrase-structure based parsers for Arabic system-
atically discard morphological features from their
label-set and never parametrize agreement explic-
itly (Maamouri et al, 2008). Models based on deep
grammars such as CCG (Hockenmaier and Steed-
man, 2003) and HPSG (Miyao and Tsujii, 2008)
could in principle use inflectional morphology, but
they currently rely on functional information mainly.
For formalisms that do incorporate morphology,
generative models are may leak probability due to
unification failures (Abney, 1997). Even results
from dependency parsing remain inconclusive. It
was shown for dependency parsing that case, defi-
niteness and animacy features are useful to enhance
parsing (e.g., (?vrelid and Nivre, 2007)), agreement
patterns are often excluded. When agreement fea-
tures were included as features in dependency parser
for Hebrew in (Goldberg and Elhadad, 2009) for He-
brew they obtained tiny-to-no improvement.
A question thus emerges whether there are any
benefits in explicitly incorporating morphosyntactic
agreement patterns into our models. This question is
a manifestation of a greater issue, namely, whether
it is beneficial to represent complex patterns of mor-
phology in the statistical parsing model, or whether
configurational information subsume the relevant
patterns, as it is commonly assumed in constituency-
based parsing. Here we claim that agreement fea-
tures are useful for statistical parsing provided that
they are represented and parametrized in a way that
reflects their linguistic substance; to express func-
tional information orthogonal to configuration.
40
We do so by extending the Relational-
Realizational (RR) model we presented in (Tsarfaty
and Sima?an, 2008) to explicitly encode agreement
features in its native representation (RR-AGR). In
the RR model, a joint distribution over grammatical
relations is firstly articulated in the projection phase.
The grammatical relations may be spelled out by
positioning them with respect to one another in the
configuration phase, through the use of morphology
in the realization phase, or both. This paper shows
that, for Hebrew, this RR-AGR strategy signifi-
cantly outperforms a constituency-based model that
treats agreement features as internally structured
non-terminal state-splits (SP-AGR). As we accumu-
late morphological features, the performance gap
between the RR and SP models becomes larger.
The best result we report for the RR-AGR model,
F184.13, is the best result reported for Hebrew to
date for parsing gold PoS-tagged segments, with
5% error reduction from previous results. This
result is also significantly higher than all parsing
results reported so far for Arabic, a Semitic lan-
guage with similar morphosyntactic phenomena.1
The RR approach is shown to be an adequate way
to model complex morphosyntactic patterns for im-
proving constituency-based parsing of a morpholog-
ically rich, free word order language. Because the
RR model is also proper and generative, it may also
embed as a language model to enhance more com-
plex NLP tasks, e.g., statistical Machine Translation.
2 The Data
The grammar of nonconfigurational languages al-
lows for freedom in word ordering and discontinu-
ities of syntactic constituents (Hale, 1983). Such
languages do not rely on configurational information
such as position and adjacency in marking grammat-
ical relations such as subject and object, but instead
they use word-level morphology. One way to encode
grammatical relations in the form of words is by us-
ing morphological case, that is, explicitly marking
an argument (e.g. nominative, accusative) with re-
spect to its grammatical function. In (Tsarfaty et
al., 2009) we showed that incorporating case indeed
leads to improved performance for constituency-
based, Relational-Realizational parsing of Hebrew.
1In (Maamouri et al, 2008), F178.1 for gold standard input.
A more involved way to morphologically encode
grammatical relations is by making explicit refer-
ence to the properties of multiple linguistic ele-
ments. This is the general pattern of agreement, i.e.,
?[A] systematic covariance between a se-
mantic or a formal property of one ele-
ment and a formal property of another.?
(Steele, adapted from (Corbett, 2001))
Describing agreement patterns involves explicit
reference to the following four components; the el-
ement which determines the agreement properties
is the Controller of the agreement, the element
whose properties are determined by agreement is
the Target, the syntactic environment in which the
agreement occurs is the Domain of agreement, and
the properties with respect to which they agree are
agreement Features (Corbett, 2001). Agreement is
an inherently asymmetrical relation. Combination
of features displayed by controllers has to be ac-
commodated by the inflectional features of the tar-
get, but there is no opposite requirement. Let us il-
lustrate the formal description of agreement through
Subject-Verb agreement familiar from English (1).
(1) a. Subject-Verb Agreement in English:
Controller: NP
Target: V
Domain: S
Features: number, person
b. Example:
i. They like the paper
ii. *They likes the paper
The agreement target (the verb) in English has a
rich enough inflectional paradigm that reflects the
person and number features inherent in controllers
? the nouns that realize subjects. (But nouns in En-
glish need not reflect, say, tense.) Had the subject
been an NP, e.g., the phrase ?the committee?, the
agreement pattern would have had to be determined
by the features of the entire NP, and in English the
features of the phrase would be determined by the
lexical head ?committee?. The controller of the
agreement (noun) does not coincide with the head of
the lexical dependency (the verb), which means that
the direction of morphological dependencies need
not coincide with that of lexical dependencies.
41
The Semitic LanguageModernHebrew Modern
Hebrew, (henceforth, Hebrew) is a Semitic language
with a flexible word order and rich morphological
structure. Hebrew nouns morphologically reflect
their inherent gender and number. Pronouns also
reflect person features. Hebrew verbs are inflected
to reflect gender, number, person and tense. Adjec-
tives are inflected to reflect the inherent properties of
nouns, and both nouns and adjectives are inflected
for definiteness. The Hebrew grammar uses this ar-
senal of properties to implement a wide variety of
agreement patterns realizing grammatical relations.
Agreement in Hebrew S Domains Hebrew man-
ifests different patterns of agreement in its S do-
main. Verbal predicates (the target) in matrix sen-
tences (the domain) agree with their nominal sub-
jects (the controller) on the agreement features gen-
der, number and person. This occurs regardless of
their configurational positions, as illustrated in (2b).
(2) a. Agreement in Verbal Sentences:
Controller: NP
Target: V
Domain: S
Features: number, person, gender
b. i. ????? ???? ??? ???
dani
Dani.3MS
natan
gave.3MS
matana
present
ledina
to-Dina
Dani gave a present to Dina (SVO)
ii. ????? ??? ??? ????
matana
present
natan
gave.3MS
dani
Dani.3MS
ledina
to-Dina
Dani gave a present to Dina (VI)
Subject-Predicate agreement relations are not
only independent of surface positions, but are also
orthogonal to the syntactic distributional type of
the constituent realizing the predicate. Semitic lan-
guages allow for predicates to be realized as an
NP, an ADJP or a PP clause (3b) lacking a verb
altogether. (In the Hebrew treebank, such pred-
icates are marked as PREDP). In all such cases,
agreement feature-bundles realized as pronominals,
which (Doron, 1986) calls Pron, are optionally
placed after the subject. The position of Pron el-
ement with respect to the subject and predicate is
fixed.2 The role of these Pron elements is to indicate
the argument-structure of a nominal sentence that is
not projected by a verb. In the Hebrew treebank they
are subsumed under predicative phrases (PREDPs).
If a PREDP head is of type NP or ADJP it must be
inflected to reflect the features of the subject con-
troller, as is illustrated in examples (3b-i)?(3b-ii).
(3) a. Agreement in Nominal Sentences:
Controller: NP
Target: Pron
Domain: S
Features: number, gender,person
b. i. ????? (???) ????
dina
Dina.FS
(hi)
(Pron.3FS)
cayeret
painter.FS
Dina is a painter
ii. ?????? (???) ????
Dina
Dina.FS
(hi)
(Pron.3FS)
muchsheret
talented.FS
Dina is talented
iii. ???? (???) ????
Dina
Dina.FS
(hi)
(Pron.3FS)
babayit
in-the-house
Dina is at home
c. i. ????? ???? *(???) (hi)* dina cayeret
(Pron.3FS)* Dina.FS painter.FS
The pronominal features gender, number, person
are also a part the inflectional paradigm of the verb
??? (be), which is extended to include tense features.
These inflected elements are used as AUX which
function as co-heads together with the main (nom-
inal or verbal) predicate. AUX elements that take a
nominal predicate as in (4b) agree with their subject,
and so do auxiliaries that take a verbal complement,
e.g., the modal verb in (4c). The nominal predicate
in (4b) also agrees with the subject ? and so does
the modal verb in (4c). Agreement of AUX with the
2Doron (1986) shows that these Pron elements can not be
considered the present tense supplements of AUX elements in
Hebrew since their position with respect to the subject and pred-
icate is fixed, whereas AUX can change position, see (4) below.
42
verbal or nominal predicates is again independent of
their surface positions.
(4) a. Subject-AUX Agreement in Hebrew:
Controller: NP
Target: AUX
Domain: S
Features: number, person, gender
b. i. ????? ??? ???? ???
hi
she.3FS
hayta
was.3FS
be?avar
in-past
cayeret
painter.FS
She was a painter in the past
ii. ????? ??? ???? ???
be?avar
in-past
hayta
was.3FS
hi
she.3FS
cayeret
painter.FS
She was a painter in the past?
c. i. ??? ????? ???? ???
hi
She.3FS
hayta
was.3FS
amura
supposed.FS
lehagi?a
to-arrive
She was supposed to arrive
ii. ??? ???? ????? ???
hi
She.3FS
amura
supposed.FS
hayta
was.3FS
lehagi?a
to-arrive
She was supposed to arrive
Agreement in Construct State Nouns Semitic
languages allow for the creation of noun compounds
by phonologically marking their lexical head and
adding a genitive complement. These constructions
are called Construct-State Nouns (CSN) (Danon,
2008) and an example of a CSN is provided in (5a).3
(5) a. ????? ??
bat
child.FS.CSN
ha-cayar
Def-painter.MS
The painter?s daughter
3Also known as iDaFa constructions in Arabic.
In such cases, all the agreement features are taken
from the head of the CSN, the noun ?daughter? in (5).
Since CSNs may be embedded in other CSNs, the
constructions may be arbitrarily long. When short
or long, CSNs themselves may be modified by ad-
jectives that agree with the CSN as a whole. This
gives rise to multiple patterns of agreement within
a single complex CSN. Consider, for instance, the
modified CSN in (6a).
(6) a. ??????? ????? ??
bat
child.FS.CSN
ha-cayar
Def-painter.MS
ha-muchsheret
Def-talented.FS
The talented daughter of the painter
The features Def, F, S of the adjective ?talented?
agree with the inherent properties of the CSN head
?child.FS? and with the definiteness status of the em-
bedded genitive Def-painter. This phenomenon is
called by Danon (2008) definiteness-spreading, and
what is important about such spreading is to observe
that it is not always the case that all agreement fea-
tures of a phrase are contributed by its lexical head.4
Interim Summary The challenges of model-
ing agreement inside constituency-based statistical
models can be summarized as follows. The models
are required to assign probability mass to alternating
sequences of constituents while retaining equivalent
feature distributions that capture agreement. Agree-
ment is (i) orthogonal to the position of constituents
(ii), orthogonal to their distributional types, and (iii)
orthogonal to features? distributions among domi-
nated subconstituents. Yet, from a functional point
of view their contribution is entirely systematic.
3 The Models
The strong version of the well-known Lexicalist
Hypothesis (LH) states that ?syntactic rules cannot
make reference to any aspect of word internal struc-
ture? (Chomsky, 1970). Anderson (1982) argues
that syntactic processes operating within configura-
tional structures can often manipulate, or have ac-
cess to, formal and inherent properties of individ-
ual words. Anderson (1982) argues that a model
4Examples for non-overlapping contribution of features by
multiple dependencies can be found in (Guthmann et al, 2009).
43
that is well-equipped to capture such phenomena is
one that retains a relaxed version of the LH, that is,
one in which syntactic processes do not make refer-
ence to aspects of word-internal structure other than
morphologically marked inflectional features. What
kind of parsing model would allow us to implement
this relaxed version of the Lexicalist Hypothesis?
The Morphosyntatctic State-Splits (SP) Model
One way to maintain a relaxed version of the LH
in syntax is to assume a constituency-based rep-
resentation in which the morphological features of
words are percolated to the level of constituency
in which they are syntactically relevant. This ap-
proach is characteristic of feature-based grammars
(e.g., GPSG (Gazdar et al, 1985) and follow-up
studies). These grammars assume a feature geom-
etry that defines the internal structure of node labels
in phrase-structure trees.5
Category-label state-splits can reflect the different
morphosyntactic behavior of different non-terminals
of the same type. Using such supervised, linguis-
tically motivated, state-splits, based on the phrase-
level marking of morphological information is one
may build an efficient implementation of a PCFG-
based parsing model that takes into account mor-
phological features. State-split models were shown
to obtain state-of-the-art performance with little
computational effort. Supervised state-splits for
constituency-based unlexicalized parsing in (Klein
and Manning, 2003) in an accurate English parser.
For the pair of Hebrew sentences (2b), the morpho-
logical state-split context-free representation of the
domain S is as described at the top of figure 1.6
The Relational-Realizational (RR) Model A dif-
ferent way to implement a syntactic model that con-
form to the relaxed LH is by separating the inflec-
tional features of surface words from their grammat-
ical functions in the syntactic representation and let-
5While agreement patterns in feature-rich grammars give
rise to re-entrancies that break context-freeness, GPSG shows
that using feature-percolation we can get quite far in modeling
morphosyntactic dependencies and retaining context-freeness.
6Horizontal markovization a` la (Klein and Manning, 2003)
would be self-defeating here. Markovization of constituents
conditions inflectional features on configurational positions,
which is inadequate for free word-order languages as Hebrew.
This is already conjectured in the PhD thesis of Collins, and it
is verified empirically for Hebrew in (Tsarfaty et al, 2009).
ting the model learn systematic form-function corre-
spondence patterns between them.
The Relational-Realizational (RR) model (Tsar-
faty and Sima?an, 2008) takes such a ?separational-
ist? approach which is constituent-based. Grammat-
ical relations are separated from their morphologi-
cal or syntactic means of realization, which are in
turn also distinguished. The easiest way to describe
the RR model is via a three-phase generative process
encompassing the projection, configuration and re-
alization phases. In the projection phase, a clause-
level syntactic category generates a Relational Net-
work (RN), i.e., a set of grammatical function-labels
representing the argument-structure of the clause. In
the configuration phase, linear ordering is generated
for the function-labels and optional realization slots
are reserved for elements such as punctuation, auxil-
iaries and adjuncts. The realization phase spells out
a rich morphosyntactic representation (MSR) ? a
syntactic label plus morphological features ? real-
izing each grammatical function and each of the re-
served slots. The process repeats as necessary until
MSRs of pre-terminals are mapped to lexical items.
In (Tsarfaty et al, 2009) we have shown that
the RR model makes beneficial use of morpholog-
ical patterns involving case marking, but did not
study the incorporation of inflectional agreement
features such as gender. Since agreement features
such as gender, number and case-related informa-
tion such accusativity, definiteness are determined
by non-overlapping subconstituents, it remains an
open question whether an addition of agreement fea-
tures into the model can be down in a linguistically
adequate and statistically sound way, and whether or
not they further improve performance.
We claim that the Relational-Realizational model
of (Tsarfaty et al, 2009) has all the necessary ingre-
dients to seamlessly migrate RR representations to
ones that encode agreement explicitely. In order to
explain how we do so let us recapitulate the empir-
ical facts. Agreement is an asymmetric relation de-
fined for a certain domain, in which the agreement
properties of a target co-vary with the inherent prop-
erties of the controller. Consider the two sentences
in (2b) in which the formal means to differentiate the
subject from the object is by the pattern of an agree-
ing predicate. The RR representations of the domain
S are given at the bottom of figure 1.
44
The agreement targets and agreement controllers
are easy to recognize; controllers are the syntac-
tic constituents that realize subjects, parametrized
as Prealization(V B|PRD@S), and targets are the
ones that realize predicates, parametrized as
Prealization(NP |SBJ@S). Now, if we take the
predicted labels of controllers and targets to in-
clude reference to inflectional features, we get
the following parameterization of the realization
parameters Prealization(V B?FEATSi?|PRD@S) and
Prealization(NP ?FEATSj?|SBJ@S) with ?FEATSi?,
?FEATSj? the inflectional features indicated in their
morphosyntactic representation. Now, we only need
to make sure that ?FEATSi?, ?FEATSj? indeed agree,
regardless of their position under S.
We do so by explicitly marking the domain
of agreement, the S category, with the features
of the syntactically most prominent participant in
the situation, the subject (this is where the non-
symmetrical nature of agreement comes into play).
The realization distributions take the following
forms Prealization(V B?FEATSj?|PRD@S?FEATSi?)
and Prealization(NP ?FEATSi?|SBJ@S?FEATSi?). In
the former, NP ?FEATSi? reflects the inherent fea-
tures of the SBJ and in the latter V B?FEATSj? re-
flects the agreement features of the PRD. Now, re-
gardless of word order, and regardless of the inter-
nal structure of NPs, the parameters capturing agree-
ment would be the same for examples (2b i-ii). The
only parameters that differ are the configuration pa-
rameters (boxed), reflecting word-order alternation.
For the sake of completeness we include here also
the SP vs. RR representation of S domains involv-
ing auxiliaries in figure 2. Here the sentences vary
only in the position of the AUX element relative to
the subject with which it agrees. Subjects, predi-
cates, and slots that have been reserved for AUX
elements, all reflect the same pattern of agreement
through their conditioning on the rich representa-
tion of the domain.7 More parameters that vary here
(boxed) are AUX placement and realization param-
eters. Since Pron elements endow PREDPs with
agreement features, agreement with verbless (nomi-
nal) predicates under S analogously follows.
7In Hebrew, even some adverbial modifiers reflect pat-
terns of agreement, e.g., ????? (literally, ?I am still?, glossed
?still.1S?). This solution caters for all such patterns in which
non-obligatory elements exhibit agreement.
4 Experiments
We aim to examine whether the explicit incorpora-
tion of agreement features helps Hebrew parsing,
and if so, which of the two modeling strategies is
better for utilizing the disambiguation cues provided
by morphosyntactic agreement.
Data We use the Hebrew treebank v2.0 with the
extended annotation of (Guthmann et al, 2009),
which adds inflectional properties to non-terminal
categories such as NP and VP. We head-annotate
the corpus and systematically add the agreement fea-
tures of Domains throughout the treebank. We fur-
ther distinguish finite from non-finite verb forms,
and cliticized from non-cliticized nouns, as in
(Goldberg and Tsarfaty, 2008; Tsarfaty et al, 2009).
On top of the treebank labels SBJ subject, OBJ ob-
ject, COM complement and CNJ conjunction we
add PRD predicates and IC infinitival complements.
Procedure We devised a procedure to read-off
treebank grammars based on (i) GPSG-like, states-
plit context-free parameters (SP-AGR), and (ii) RR-
AGR parameters in which context-free rules capture
the projection, configuration and realization phases.
In each model the multiplication provides the prob-
ability of the generation. We use relative frequency
estimates and exhaustively parse gold pos-tagged in-
put8 using a general-purpose CKY parser. We use
the same data split as in (Goldberg and Tsarfaty,
2008; Tsarfaty et al, 2009) (training on sentences
501-6000 and parsing sentences 1-500) and we con-
vert all trees to the flat, coarse-grained, original tree-
bank representation for the purpose of evaluation.
Setup We experiment with bare constituent labels,
grand-parent decorated labels (gp), and labels deco-
rated with grand-parent and head-tag labels (gp,hd).
We use increasingly richer subsets of the {gender,
definiteness, accusativity} set.9
8This choice to parse gold-tagged sentences is meant to alle-
viate the differences in the model?s morphological disambigua-
tion capacity. We want to evaluate the contribution of morpho-
logical features for syntactic disambiguation, and if the models
will disambiguate the morphological analyses differently, the
syntactic analysis will be assigned to different yields and the
accuracy results would be strictly incomparable. But see (Gold-
berg and Tsarfaty, 2008) for a way to combine the two.
9We deliberately choose features that have non-overlapping
behavior, to see whether their contribution is accumulative.
45
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
n tan
gave
NP-OBJ
m tana
present
P-COM
ledian
to-dina
SMS
NP-OBJ
m tana
present
VPMS-PRD
n tan
gave
NPMS-SBJ
dani
Dani
P-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ, P-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ, P-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
n tan
gave
OBJ@SMS
NP-OBJ
m tana
present
COM@SMS
P-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
m tana
present
PRD@SMS
VPMS
n tan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
P-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization( P | COM@ SMS ) Prealization( P | COM@ SMS )
Figure 1: The SP-AGR (top) and R-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPF -SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPF -SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, P, PREDPFS-PRD? | SFS ) P(? P, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and R-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
projection({SBJ,PRD,OBJ,COM} | SMS ) projection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P ,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR repres tations of s n ences (2b- (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) (?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@ FS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
mura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD S+FS
PP
mura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
projection({SBJ,PRD,COM} | SFS ) projection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUX | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of s n ences (4c- (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
atan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
atan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | S S ) Pprojection({SBJ,PRD,OBJ,COM} | S S )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
S S
NP S-SBJ
dani
Dani
VP S-PRD
natan
gave
NP-OBJ
matana
present
PP-CO
ledian
to-dina
S S
NP-OBJ
matana
present
VP S-PRD
natan
gave
NP S-SBJ
dani
Dani
PP-CO
ledian
to-dina
P( P S-SBJ, P S-PR , P- BJ,PP-C | S S ) P( P- BJ, P S-PR , P S-SBJ,PP-C | S S )
S S
{SBJ,PRD,OBJ,CO } S S
SBJ S S
NP S
dani
Dani
PRD S S
VP S
natan
gave
OBJ S S
NP-OBJ
matana
present
CO S S
PP-CO
ledian
to-dina
S S
{SBJ,PRD,OBJ,CO } S S
OBJ S S
NP-OBJ
matana
present
PRD S S
VP S
natan
gave
SBJ S S
NP S
dani
Dani
CO S S
PP-CO
ledian
to-dina
Pprojection({SBJ,PR , BJ,C } | S S ) Pprojection({SBJ,PR , BJ,C } | S S )
Pconfiguration(?S,P, ,C? | {SBJ,PR , BJ,C } S S) Pconfiguration(? ,P,S,C? | {SBJ,PR , BJ,C } S S)
Prealization( P S | SBJ S S ) Prealization( P S | SBJ S S )
Prealization( B S | PR S S ) Prealization( B S | PR S S )
Prealization( P | BJ S S ) Prealization( P | BJ S S )
Prealization(PP | C S S ) Prealization(PP | C S S )
igure 1: The SP- (top) and - representations of sentences (2b-i) (left) and (2b-ii).
FS
NPFS-SBJ
hi
she
AUXFS
hayta
was
DFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
FS
NPFS-SBJ
hi
she
DFS-PRD
amura
supposed
AUXFS
hayt
was
VPINF-COM
lehagia
to-arrive
P(? PFS-SBJ, FS, PP, PRE PFS-PR ? | SFS ) P(?PP, FS, PFS-SBJ, PRE PFS-PR ? | SFS )
FS
{SBJ,PRD,CO } FS
SBJ FS
NPFS
hi
she
SBJ:PRD FS
AUXFS
hayta
was
PRD S+FS
PP
amura
supposed
CO SINF
PREDPFS
lehagia
to-arrive
FS
{SBJ,PRD,CO } FS
SBJ FS
NPFS
hi
she
PRD S+FS
PP
amura
supposed
PRD:CO FS
AUXFS
hayta
was
CO SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PR ,C } | SFS ) Pprojection({SBJ,PR ,C } | SFS )
Pconfiguration( ?SBJ, SBJ:PR , PR , C ? | {SBJ,PR ,C } SFS) Pconfiguration( ?SBJ, PR , PR :C , C ? | {SBJ,PR ,C } SFS)
Prealization( PFS | SBJ SFS ) Prealization( PFS | SBJ SFS )
Prealization( FS | SBJ:PR SFS ) Prealization( FS | PR :C SFS )
Prealization( FS | PR SFS ) Prealization( FS | PR SFS )
Prealization( P | C SFS) Prealization( P | C SFS)
igure 2: The SP- (top) and - representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-C
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
P @SMS
VPMS
natan
gave
BJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-CO
ledian
to-dina
SM
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
p esent
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SM
PP-COM
ledian
to-dina
Pprojection({S J,PRD,OBJ,COM} | SMS ) Pproject on({SBJ,PRD, J,CO } | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfigur ti (?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealiz tion(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,P , }@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | BJ@SFS ) Prealizat on(NPFS | BJ@ FS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@ FS )
realization(VP | COM@SFS) Prealization(VP | COM@ FS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
V -PRD
n tan
gave
NP-OBJ
matana
pres nt
P COM
ledian
to-di a
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
P-COM
ledian
to-dina
P(NPMS-SBJ,V -PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS PRD,NP S-SBJ,PP-CO | SMS )
S S
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@ MS
VPMS
natan
gave
OBJ@SMS
N -OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ S S
NPMS
d i
Dani
COM@SMS
PP-COM
ledian
to-d na
Pprojection({SBJ,PRD,OBJ,CO } | S ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | { BJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii .
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
to-arrive
FS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
le gi
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | S ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | S )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
ayta
was
PRD@S+
PP
mura
supposed
COM@SINF
PREDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@ +FS
PP
amura
upposed
P :COM@SFS
AUXFS
hayta
was
SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfiguration( ?SBJ, J:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM COM? | {SBJ,PRD,COM}@SFS)
lization(NPFS | SBJ@S ) Prealization(N FS | SBJ@S )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii .
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
g ve
N -OBJ
matana
prese t
PP-COM
ledian
to- na
SMS
NP-OBJ
matana
prese t
VPMS- RD
tan
g ve
NPMS-SBJ
dani
Dani
PP-COM
ledian
to- na
P(NPMS-SBJ,VPMS-PRD,N OBJ,PP-COM | SMS ) P(N -OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,O COM}@SMS
BJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
g ve
OBJ@S S
NP-OBJ
matana
prese t
COM@SMS
PP-COM
ledian
to- na
SMS
{SBJ,PRD,O OM}@SMS
OBJ@SMS
NP-OBJ
matana
prese t
PRD@SMS
VPMS
tan
g ve
BJ@ S
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to- na
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfigurati (?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfigurati (?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@ MS ) Pre lization(NPMS | SBJ@ MS )
Prealization(VB | PRD S ) Prealization(VB | PR S )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM ) Prealization(PP | CO )
Figur 1: The SP-AGR (top) and R-AGR representatio of sentence (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayt
was
MD -PRD
amura
supposed
VPINF-COM
lehagia
t -arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amur
supposed
AUXFS
hayta
was
VPIN -COM
leh gia
to-arrive
P(?NPFS-SBJ, AUXFS, P , PREDPFS-PR ? | FS ) P(?PP, AUXFS, NPFS- BJ, PREDPFS-PR ? | FS )
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@ FS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
REDPFS
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@ INF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )
Pconfigurati ( ?SBJ, SBJ:PRD, PRD, COM? | {SBJ,PRD,COM}@SFS) Pconfigurati ( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
Prealization(NPFS | BJ@ FS ) Prealization(NPFS | BJ@ FS )
Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )
Prealization(MDFS | PR @SFS ) Prealization(MDFS | PR @SFS )
Prealization(VP | COM@SFS) realization(VP | COM@SFS)
Figur 2: The SP-AGR (top) and R-AGR representation of sentence (4c-i) (left) and (4c-ii).
SMS
NPMS-SBJ
dani
Dani
VPMS-PRD
natan
gave
NP-OBJ
matana
present
PP-COM
ledian
to-dina
SMS
NP-OBJ
matana
present
VPMS-PRD
natan
gave
NPMS-SBJ
dani
Dani
PP-COM
ledian
to-dina
P(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )
SMS
{SBJ,PRD,OBJ,COM}@SMS
SBJ@SMS
NPMS
dani
Dani
PRD@SMS
VPMS
natan
gave
OBJ@SMS
NP-OBJ
matana
present
COM@SMS
PP-COM
ledian
to-dina
SMS
{SBJ,PRD,OBJ,COM}@SMS
OBJ@SMS
NP-OBJ
matana
present
PRD@SMS
VPMS
natan
gave
SBJ@SMS
NPMS
dani
Dani
COM@SMS
PP-COM
ledian
to-dina
Pprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )
Pconfiguration(?S,P,O,C? | {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C? | {SBJ,PRD,OBJ,COM}@SMS)
Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )
Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )
Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )
Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )
Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).
SFS
NPFS-SBJ
hi
she
AUXFS
hayta
was
MDFS-PRD
amura
supposed
VPINF-COM
lehagia
o-arrive
SFS
NPFS-SBJ
hi
she
MDFS-PRD
amura
supposed
AUXFS
hayta
was
VPINF-COM
lehagia
to-arrive
P(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD? | SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD? | SFS )
SFS
{SBJ,PRD,COM}@ FS
SBJ@SFS
NPFS
hi
she
SBJ:PRD@SFS
AUXFS
hayta
was
PRD@S+FS
PP
amura
supposed
COM@SINF
PREDPF
lehagia
to-arrive
SFS
{SBJ,PRD,COM}@SFS
SBJ@SFS
NPFS
hi
she
PRD@S+FS
PP
amura
supposed
PRD:COM@SFS
AUXFS
hayta
was
COM@SINF
PREDPFS
lehagia
to-arrive
Pprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PR ,COM} | SFS )
Pc nfiguration( ?SBJ, S J:PRD, PRD, C M? | {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM? | {SBJ,PRD,COM}@SFS)
realization(NPFS | SBJ@SFS ) Pre lization(NPFS | SBJ@SFS )
Prealization(AUXFS | BJ:PRD SFS ) Prealization(AUXFS | PRD:C @SFS )
Prealization(MDFS | F ) Prealization(MDFS | PRD@SFS )
Prealization(VP | COM@SFS realization(VP | COM@ FS)
Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).
46
Model { gender def+acc gender+def+acc
SP-AGR 79.77 79.55 80.13 80.26
(3942) (7594) (4980) (8933)
RR-AGR 80.23 81.09 81.48 82.64
(3292) (5686) (3772) (6516)
SP-AGR (gp) 83.06 82.18 79.53 80.89
(5914) (10765) (12700) (11028)
RR-AGR (gp) 83.49 83.70 83.66 84.13
(6688) (10063) (12383) (12497)
SP-AGR (gp,hd) 76.61 64.07 75.12 61.69
(10081) (16721) (11681) (18428)
RR-AGR (gp,hd) 83.40 81.19 83.33 80.45
(12497) (22979) (13828) (24934)
Table 1: F-score (#params) measure for all models on
the Hebrew treebank dev-set for Sentences Length < 40
5 Results and Discussion
Table 1 shows the standard F1 scores (and #param-
eters) for all models. Throughout, the RR-AGR
model outperforms the SP-AGR models that use the
same category set and the same morphological fea-
tures as state splits. For RR-AGR and RR-AGR (gp)
models, adding agreement features to case features
improves performance. The accumulative contribu-
tion is significant. For SP-AGR and SP-AGR (gp)
models, adding more features either remains at the
same level of performance or becomes detrimental.
Since the SP/RR-AGR and SP/RR-AGR (gp)
models are of comparable size for each feature-set,
it is unlikely that the differences in performance are
due to the lack of training data. A more reason-
able explanation if that the RR parameters repre-
sent functional generalizations orthogonal to config-
uration for which statistical evidence is more easily
found in the data. The robust realization distribu-
tions which cut across ordering alternatives can steer
the disambiguation in the right direction.
The RR-AGR (gp) +gen+def+acc model yields
the best result for parsing Hebrew to date (F1 84.13),
improving upon our best model in (Tsarfaty et al,
2009) (F1 83.33, underlined) in a pos-tagged set-
ting. For this setting, Arabic parsing results are F1
78.1. Given the similar morphosyntactic phenomena
(agreement, MaSDaR, iDaFa) it would be interest-
ing to see if the model enhances parsing for Arabic.
For (gp,hd) models (a configuration which was
shown to give the best results in (Tsarfaty et al,
2009)) there is a significant decrease in accuracy
with the gender feature, but there is a lesson to be
learned. Firstly, while the RR-AGR (gp,hd) model
shows moderate decrease with gender, the decrease
in performance of SP-AGR (gp,hd) for the same
feature-set is rather dramatic, which is consistent
with the observation that the RR model is less vul-
nerable to sparseness and that it makes better use of
the statistics of functional relations in the data.
Consulting the size of the different grammars, the
combination of RR-AGR (gp, hd) with gender fea-
tures indeed results in substantially larger grammars,
and it is possible that at this point we indeed need to
incorporate smoothing. At the same time there may
be an alternative explanation for the decreased per-
formance. It might be that the head-tag does not add
informative cues beyond the contribution of the fea-
tures which are spread inside the constituent, and are
already specified. This is a reasonable hypothesis
since gender in Hebrew always percolates through
the head as opposed to def/acc that percolate from
other forms. Incorporating head-tag in (Tsarfaty et
al., 2009) might have led to improvement only due
to the lack of agreement features which subsume
the relevant pattern. This suggests that incorporat-
ing all co-heads and functional elements that con-
tribute morphological features spread inside the con-
stituent, is more adequate for modeling morphosyn-
tax than focusing on the features of a single head.
6 Conclusion
We show that morphologically marked agreement
features can significantly improve parsing perfor-
mance if they are represented and parametrized in
a way that reflects their linguistic substance: relat-
ing form-and-function in a non-linear fashion. We
have so far dealt with the adequacy of representa-
tion and we plan to test whether more sophisticated
estimation (e.g., split-merge-smooth estimation as in
(Petrov et al, 2006)) can obtain further improve-
ments from the explicit representation of agreement.
At the same time, the state-of-the-art results we
present render the RR model promising for further
exploration with morphologically rich languages.
Acknowledgements The work of the first author
has been funded by NWO, grant 017.001.271. We
wish to thank Joakim Nivre and three anonymous
reviewers for helpful comments on earlier drafts.
47
References
Steven Abney. 1997. Stochastic attribute-value gram-
mars. Computational Linguistics, 23(4):597?618.
Stephen R. Anderson. 1982. Where?s morphology? Lin-
guistic Inquiry.
Noam Chomsky. 1970. Remarks on nominalization. In
R. Jacobs and P. Rosenbaum, editors, Reading in En-
glish Transformational Grammar. Waltham: Ginn.
Greville G. Corbett. 2001. Agreement: Terms and
boundaries. In SMG conference papers.
Gabi Danon. 2008. Definiteness spreading in the hebrew
construct-state. Lingua, 118(7):872?906.
Edit Doron. 1986. The pronominal ?copula? as agree-
ment clitic. Syntax and Semantics, (19):313?332.
Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, and
Ivan A. Sag. 1985. Generalised phrase structure
grammar. Blackwell, Oxford, England.
Yoav Goldberg and Michael Elhadad. 2009. Hebrew de-
pendency parsing: Initial results. In Proceedings of
IWPT.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syn-
tactic parsing. In Proceedings of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2006.
Noun phrase chunking in hebrew: Influence of lex-
ical and morphological features. In Proceedings of
COLING-ACL.
Nomie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew treebank.
In Frank Van Eynde, Anette Frank, Koenraad De
Smedt, and Gertjan van Noord, editors, Proceedings
of TLT.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Julia Hockenmaier and Mark Steedman. 2003. Parsing
with generative models of predicate-argument struc-
ture. In Proceedings of ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the arabic tree-
bank. In Proceedings of INFOS.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature-forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80.
Lilja ?vrelid and Joakim Nivre. 2007. Swedish depen-
dency parsing with rich linguistic features. In Pro-
ceeding of RANLP.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proceedings of CoLing.
Reut Tsarfaty, Khalil Sima?an, and Remko Scha. 2009.
An alternative to head-driven approaches for parsing a
(relatively) free word order language. In Proceedings
of EMNLP.
48
