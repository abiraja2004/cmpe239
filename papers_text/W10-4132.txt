Adaptive Chinese Word Segmentation with
Online Passive-Aggressive Algorithm
Wenjun Gao
School of Computer Science
Fudan University
Shanghai, China
wjgao616@gmail.com
Xipeng Qiu
School of Computer Science
Fudan University
Shanghai, China
xpqiu@fudan.edu.cn
Xuanjing Huang
School of Computer Science
Fudan University
Shanghai, China
xjhuang@fudan.edu.cn
Abstract
In this paper, we describe our system1
for CIPS-SIGHAN-2010 bake-off task of
Chinese word segmentation, which fo-
cused on the cross-domain performance
of Chinese word segmentation algorithms.
We use the online passive-aggressive al-
gorithm with domain invariant informa-
tion for cross-domain Chinese word seg-
mentation.
1 Introduction
In recent years, Chinese word segmentation
(CWS) has undergone great development (Xue,
2003; Peng et al, 2004). The popular method is
to regard word segmentation as a sequence label-
ing problems. The goal of sequence labeling is to
assign labels to all elements of a sequence.
Due to the exponential size of the output
space, sequence labeling problems tend to be
more challenging than the conventional classifi-
cation problems. Many algorithms have been
proposed and the progress has been encourag-
ing, such as SVMstruct (Tsochantaridis et al,
2004), conditional random fields (CRF) (Lafferty
et al, 2001), maximum margin Markov networks
(M3N) (Taskar et al, 2003) and so on. After years
of intensive researches, Chinese word segmenta-
tion achieves a quite high precision. However, the
performance of segmentation is not so satisfying
for out-of-domain text.
There are two domains in domain adaption
problem, a source domain and a target domain.
When we use the machine learning methods for
1Available at http://code.google.com/p/
fudannlp/
Chinese word segmentation, we assume that train-
ing and test data are drawn from the same distri-
bution. This assumption underlies both theoreti-
cal analysis and experimental evaluations of learn-
ing algorithms. However, the assumption does
not hold for domain adaptation(Ben-David et al,
2007; Blitzer et al, 2006). The challenge is the
difference of distribution between the source and
target domains.
In this paper, we use online margin max-
imization algorithm and domain invariant fea-
tures for domain adaptive CWS. The online learn-
ing algorithm is Passive-Aggressive (PA) algo-
rithm(Crammer et al, 2006), which passively ac-
cepts a solution whose loss is zero, while it ag-
gressively forces the new prototype vector to stay
as close as possible to the one previously learned.
The rest of the paper is organized as follows.
Section 2 introduces the related works. Then we
describe our algorithm in section 3 and 4. The
feature templates are described in section 5. Sec-
tion 6 gives the experimental analysis. Section 7
concludes the paper.
2 Related Works
There are several approaches to deal with the do-
main adaption problem.
The first approach is to use semi-supervised
learning (Zhu, 2005).
The second approach is to incorporate super-
vised learning with domain invariant information.
The third approach is to improve the present
model with a few labeled domain data.
Altun et al (2006) investigated structured clas-
sification in a semi-supervised setting. They pre-
sented a discriminative approach that utilizes the
intrinsic geometry of inputs revealed by unlabeled
data points and we derive a maximum-margin for-
mulation of semi-supervised learning for struc-
tured variables.
Self-training (Zhu, 2005) is also a popular tech-
nology. In self-training a classifier is first trained
with the small amount of labeled data. The clas-
sifier is then used to classify the unlabeled data.
Typically the most confident unlabeled points, to-
gether with their predicted labels, are added to the
training set. The classifier is re-trained and the
procedure repeated. Note the classifier uses its
own predictions to teach itself. Yarowsky (1995)
uses self-training for word sense disambiguation,
e.g. deciding whether the word plant means a liv-
ing organism or a factory in a given context.
Zhao and Kit (2008) integrated unsupervised
segmentation and CRF learning for Chinese word
segmentation and named entity recognition. They
found word accessory variance (Feng et al, 2004)
is useful to CWS.
3 Online Passive-Aggressive Algorithm
Sequence labeling, the task of assigning labels
y = y1, . . . , yL to an input sequence x =
x1, . . . , xL.
Give a sample (x,y), we define the feature is
?(x,y). Thus, we can label x with a score func-
tion,
y? = argmax
z
F (w,?(x, z)), (1)
where w is the parameter of function F (?).
The score function of our algorithm is linear
function.
Given an example (x,y), y? is denoted as the
incorrect label with the highest score,
y? = argmax
z 6=y
wT?(x, z). (2)
The margin ?(w; (x,y)) is defined as
?(w; (x,y)) = wT?(x,y)?wT?(x, y?). (3)
Thus, we calculate the hinge loss.
`(w; (x,y) =
{
0, ?(w; (x,y)) > 1
1? ?(w; (x,y)), otherwise
(4)
We use the online PA learning algorithm to
learn the weights of features. In round t, we find
new weight vector wt+1 by
wt+1 = arg min
w?Rn
1
2
||w ?wt||2 + C ? ?,
s.t. `(w; (xt,yt)) <= ? and ? >= 0 (5)
where C is a positive parameter which controls
the influence of the slack term on the objective
function.
The algorithms goal is to achieve a margin at
least 1 as often as possible, thus the Hamming loss
is also reduced indirectly. On rounds where the
algorithm attains a margin less than 1 it suffers an
instantaneous loss.
We abbreviate `(wt; (x, y)) to `t. If `t = 0
then wt itself satisfies the constraint in Eq. (5)
and is clearly the optimal solution. We therefore
concentrate on the case where `t > 0.
First, we define the Lagrangian of the optimiza-
tion problem in Eq. (5) to be
L(w, ?, ?, ?) = 1
2
||w ?wt||2 + C ? ?
+ ?(`t ? ?)? ??
s.t. ? >= 0, ? >= 0. (6)
where ?, ? is a Lagrange multiplier.
Setting the partial derivatives of L with respect
to the elements of ? to zero gives
? + ? = C. (7)
The gradient of w should be zero,
w ? wt ? ?(?(x,y) ? ?(x, y?)) = 0, (8)
we get
w = wt + ?(?(x,y) ? ?(x, y?)). (9)
Substitute Eq. (7) and Eq. (9) to dual objective
function Eq. (6), we get
L(?) = ?1
2
||?(?(x,y)? ?(x, y?))||2
? ?(wtT (?(x,y)? ?(x, y?)) + ? (10)
Differentiate with ?, and set it to zero, we get
?||?(x,y)? ?(x, y?)||2
+wtT (?(x,y)? ?(x, y?))? 1 = 0. (11)
So,
?? = 1?wt
T (?(x,y)? ?(x, y?))
||?(x,y)? ?(x, y?)||2
. (12)
From ? + ? = C, we know that ? < C, so
??? = min(C, ??). (13)
Finally, we get update strategy,
wt+1 = wt + ???(?(x,y)? ?(x, y?)). (14)
Our final algorithm is shown in Algorithm 1. In
order to avoiding overfitting, the averaging tech-
nology is employed.
input : training data set:
(xn,yn), n = 1, ? ? ? , N , and
parameters: C,K
output: w
Initialize: cw? 0,;
for k = 0 ? ? ?K ? 1 do
w0 ? 0 ;
for t = 0 ? ? ?T ? 1 do
receive an example (xt,yt);
predict:
y?t = argmax
z 6=yt
?wt,?(xt, z)?;
calculate `(w; (x,y));
update wt+1 with Eq.(14);
end
cw = cw +wT ;
end
w = cw/K ;
Algorithm 1: Labelwise Margin Maxi-
mization Algorithm
4 Inference
The PA algorithm is used to learn the weights of
features in training procedure. In inference pro-
cedure, we use Viterbi algorithm to calculate the
maximum score label.
Let ?(n) be the best score of the partial label
sequence ending with yn. The idea of the Viterbi
algorithm is to use dynamic programming to com-
pute ?(n):
?(n) = max
n?1
(
?(n? 1) +wT?(x, yn, yn?1)
)
(15)
+wt?(x, yn)
Using this recursive definition, we can evalu-
ate ?(N) for all yN , where N is the input length.
This results in the identification of the best label
sequence.
The computational cost of the Viterbi algorithm
is O(NL2), where L is the number of labels.
5 Feature Templates
All feature templates used in this paper are shown
in Table 1. C represents a Chinese character while
the subscript of C indicates its position in the sen-
tence relative to the current character, whose sub-
script is 0. T represents the character-based tag:
?B?, ?B2?, ?B3?, ?M?, ?E? and ?S?, which repre-
sent the beginning, second, third, middle, end or
single character of a word respectively.
The type of character includes: digital, letter,
punctuation and other.
We also use the word accessor variance for do-
main adaption. Word accessor variance (AV) was
proposed by (Feng et al, 2004) and was used to
evaluate how independently a string is used, and
thus how likely it is that the string can be a word.
The accessor variety of a string s of more than one
character is defined as
AV (s) = min{Lav(s), Rav(s)} (16)
Lav(s) is called the left accessor variety and is
defined as the number of distinct characters (pre-
decessors) except ?S? that precede s plus the num-
ber of distinct sentences of which s appears at
the beginning. Similarly, the right accessor va-
riety Rav(s) is defined as the number of distinct
characters (successors) except ?E? that succeed s
plus the number of distinct sentences in which s
appears at the end. The characters ?S? and ?E?
are defined as the begin and end of a sentence.
The word accessor variance was found effective
for CWS with unsegmented text (Zhao and Kit,
2008).
Table 1: Feature templates
Ci, T0, (i = ?1, 0, 1, 2)
Ci, Ci+1, T0, (i = ?2,?1, 0, 1)
T?1,0
Tc: Type of Character
AV : word accessor variance
6 CIPS-SIGHAN-2010 Bakeoff
CIPS-SIGHAN-2010 bake-off task of Chinese
word segmentation focused on the cross-domain
performance of Chinese word segmentation algo-
rithms. There are two subtasks for this evaluation:
(1) Word Segmentation for Simplified Chinese
Text;
(2) Word Segmentation for Traditional Chinese
Text.
The test corpus of each subtask covers four do-
mains: literature, computer science, medicine and
finance.
We participate in closed training evaluation of
both subtasks.
Firstly, we calculate the word accessor variance
AVL(s)of the continuous string s from labeled
corpus. Here, we set the largest length of string
s to be 4.
Secondly, we train our model with feature tem-
ples and AVL(s).
Thirdly, when we process the different domain
unlabeled corpus, we recalculate the word ac-
cessory variance AVU (s) from the corresponding
corpus.
Fourthly, we segment the domain corpus with
new word accessory variance AVU (s) instead of
AVL(s).
The results are shown in Table 2 and 3. The
results show our method has a poor performance
in OOV ( Out-Of-Vocabulary) word.
The running environment is shown in Table 4.
Table 4: Experimental environment
OS Win 2003
CPU Intel Xeon 2.0G
Memory 4G
We set the max iterative number is 20. Our run-
ning time is shown in Table 5. ?s? represents sec-
ond, ?chars? is the number of Chinese character,
and ?MB? is the megabyte. In practice, we found
the system can achieve the same performance af-
ter 7 loops. Therefore, we just need less half the
time in Table 5 actually.
7 Conclusion
In this paper, we describe our system in CIPS-
SIGHAN-2010 bake-off task of Chinese word
segmentation. Although our method just achieve
a consequence of being average and not outstand-
ing, it has an advantage of faster training than
other batch learning algorithm, such as CRF and
M3N.
In the future, we wish to improve our method
in the following aspects. Firstly, we will investi-
gate more effective domain invariant feature rep-
resentation. Secondly, we will integrate our algo-
rithm with self-training and other semi-supervised
learning methods.
Acknowledgments
This work was (partially) funded by 863 Pro-
gram (No. 2009AA01A346), 973 Program (No.
2010CB327906), and Shanghai Science and Tech-
nology Development Funds (No. 08511500302).
References
Altun, Y., D. McAllester, and M. Belkin. 2006. Max-
imum margin semi-supervised learning for struc-
tured variables. Advances in neural information
processing systems, 18:33.
Ben-David, S., J. Blitzer, K. Crammer, and F. Pereira.
2007. Analysis of representations for domain adap-
tation. Advances in Neural Information Processing
Systems, 19:137.
Blitzer, J., R. McDonald, and F. Pereira. 2006.
Domain adaptation with structural correspondence
learning. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 120?128. Association for Computational
Linguistics.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Table 2: Evaluation results on simplified corpus
R P F1 OOV RR IV RR
Best 0.945 0.946 0.946 0.816 0.954
Literature Our 0.915 0.925 0.92 0.577 0.94
Best 0.953 0.95 0.951 0.827 0.975
Computer Our 0.934 0.919 0.926 0.739 0.969
Best 0.942 0.936 0.939 0.75 0.965
Medicine Our 0.927 0.924 0.925 0.714 0.953
Best 0.959 0.96 0.959 0.827 0.972
Finance Our 0.94 0.942 0.941 0.719 0.961
Table 3: Evaluation results on traditional corpus
R P F1 OOV RR IV RR
Best 0.942 0.942 0.942 0.788 0.958
Literature Our 0.869 0.91 0.889 0.698 0.887
Best 0.948 0.957 0.952 0.666 0.977
Computer Our 0.933 0.949 0.941 0.791 0.948
Best 0.953 0.957 0.955 0.798 0.966
Medicine Our 0.908 0.932 0.92 0.771 0.919
Best 0.964 0.962 0.963 0.812 0.975
Finance Our 00.925 0.939 0.932 0.793 0.935
Table 5: Execution time of training and test phase.
Task A B C D
Training Simp 817.2s 795.6s 774.0s 792.0s
Trad 903.6s 889.2s 885.6s 874.8s
Test 20327 chars/s, or 17.97 s/MB
Feng, H., K. Chen, X. Deng, and W. Zheng. 2004. Ac-
cessor variety criteria for chinese word extraction.
Computational Linguistics, 30(1):75?93.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In ICML ?01: Proceedings of the
Eighteenth International Conference on Machine
Learning.
Peng, F., F. Feng, and A. McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. Proceedings of the 20th inter-
national conference on Computational Linguistics.
Taskar, Ben, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In Proceed-
ings of Neural Information Processing Systems.
Tsochantaridis, I., T. Hofmann, T. Joachims, and Y Al-
tun. 2004. Support vector machine learning for in-
terdependent and structured output spaces. In Pro-
ceedings of the International Conference on Ma-
chine Learning(ICML).
Xue, N. 2003. Chinese word segmentation as charac-
ter tagging. Computational Linguistics and Chinese
Language Processing, 8(1):29?48.
Yarowsky, D. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, pages 189?196. Associ-
ation for Computational Linguistics.
Zhao, H. and C. Kit. 2008. Unsupervised segmenta-
tion helps supervised learning of character tagging
for word segmentation and named entity recogni-
tion. In The Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111. Citeseer.
Zhu, Xiaojin. 2005. Semi-supervised learning
literature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf.
