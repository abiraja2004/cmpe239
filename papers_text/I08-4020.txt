Two Step Chinese Named Entity Recognition Based on Conditional 
Random Fields Models 
Yuanyong Feng*          Ruihong Huang*         Le Sun? 
*Institute of Software, Graduate University 
Chinese Academy of Sciences 
Beijing, China, 100080 
{comerfeng,ruihong2}@iscas.cn 
?Institute of Software 
Chinese Academy of Sciences 
Beijing, China, 100080 
sunle@iscas.cn 
 
 
Abstract 
This paper mainly describes a Chinese 
named entity recognition (NER) system 
NER@ISCAS, which integrates text, part-
of-speech and a small-vocabulary-
character-lists feature and heristic post-
process rules for MSRA NER open track 
under the framework of Conditional Ran-
dom Fields (CRFs) model. 
1  Introduction 
The system NER@ISCAS is designed under the 
Conditional Random Fields (CRFs. Lafferty et al, 
2001) framework. It integrates multiple features 
based on single Chinese character or space sepa-
rated ASCII words. The early designed system 
(Feng et al, 2006) is used for the MSRA NER 
open track this year. The output of an external 
part-of-speech tagging tool and some carefully 
collected small-scale-character-lists are used as 
open knowledge. Some post process steps are also 
applied to complement the local limitation in 
model?s feature engineering. 
The remaining of this paper is organized as fol-
lows. Section 2 introduces Conditional Random 
Fields model. Section 3 presents the details of our 
system on Chinese NER integrating multiple fea-
tures. Section 4 describes the post-processings 
based on some heuristic rules. Section 5 gives the 
evaluation results. We end our paper with some 
conclusions and future works. 
2  Conditional Random Fields Model 
Conditional random fields are undirected graphical 
models for calculating the conditional probability 
for output vertices based on input ones. While 
sharing the same exponential form with maximum 
entropy models, they have more efficient proce-
dures for complete, non-greedy finite-state infer-
ence and training.  
Given an observation sequence o=<o1, o2, ..., 
oT>, linear-chain CRFs model based on the as-
sumption of first order Markov chains defines the 
corresponding state sequence s? probability as fol-
lows (Lafferty et al, 2001): 
1
1
1
( | ) exp( ( , , , ))
T
k k t t
t k
p f
Z
?? ?
=
= ??
o
s o os s t
 
(1)
Where ? is the model parameter set, Zo is the nor-
malization factor over all state sequences, fk is an 
arbitrary feature function, and ?k is the learned fea-
ture weight. A feature function defines its value to 
be 0 in most cases, and to be 1 in some designated 
cases. For example, the value of a feature named 
?MAYBE-SURNAME? is 1 if and only if st-1 is 
OTHER, st is PER, and the t-th character in o is a 
common-surname. 
The inference and training procedures of CRFs 
can be derived directly from those equivalences in 
HMM. For instance, the forward variable ?t(si) 
defines the probability that state at time t being si 
at time t given the observation sequence o. As-
sumed that we know the probabilities of each 
possible value si for the beginning state  ?0(si), 
then we have 
1( ) ( )exp( ( , , , )t i t k k i
s k
s s f s s t? ? ?+
?
? ?=? ? o (2)
120
Sixth SIGHAN Workshop on Chinese Language Processing
In similar ways, we can obtain the backward 
variables and Baum-Welch algorithm. 
3  Chinese NER Using CRFs Model Inte-
grating Multiple Features 
Besides the text feature(TXT), simplified part-of-
speech (POS) feature, and small-vocabulary-
character-lists (SVCL) feature, which use in the 
early system (Feng et al, 2006), some new fea-
tures such as word boundary, adjoining state bi-
gram ? observation and early NE output are also 
combined under the unified CRFs framework. 
The text feature includes single Chinese charac-
ter, some continuous digits or letters. 
POS feature is an important feature which car-
ries some syntactic information. Unlike those in 
the earyly system, the POS tag set are merged into 
9 categories from the criterion of modern Chinese 
corpora construction (Yu, 1999), which contains 
39 tags.  
The third type of features are derived from the 
small-vocabulary-character lists which are essen-
tially same as the ones used in last year except 
with some additional items. Some examples of this 
list are given in Table 1. 
Value Description Examples 
digit Arabic digit(s) 1,2,3 
letter Letter(s) A,B,C,...,a, b, c 
Continuous digits and/or letters (The sequence is 
regarded as a single token) 
chseq Chinese order 1 ? ? ? ?, , ,  
chdigit Chinese digit ? ? ?, ,  
tianseq Chinese order 2 ? ? ? ?, , ,  
chsurn Surname ? ? ? ?, , ,  
notname Not name ? ? ? ? ? ?, , , , , 
loctch LOC tail charac-ter 
? ? ? ? ?, , , , , 
? ?,  
orgtch ORG tail charac- ? ? ? ? ?, , , , , 
ter ?, ? 
other Other case ? ?, ?, ,   ?, ? 
Table 1.  Some Examples of SVCL Feature 
The fourth type of feature is word boundary. We 
use the B, I, E, U, and O to indicate Begining, In-
ner, Ending, and Uniq part of, or outside of a word 
given a word segmentation. The O case occurs 
when a token, for example the charactor ?&?, is 
ignored by the segmentator. We do not combine 
the boundary information with other features be-
cause we argue it is very limited and may cause 
errors. 
The last type of features is bigram state com-
bined with observations. We argue that observa-
toin (mainly is of named entity derived by early 
system or character text itself) and state transition 
are not conditionally independent and entails dedi-
cate considerings. 
Each token is presented by its feature vector, 
which is combined by these features we just dis-
cussed. Once all token feature (Maybe including 
context features) values are determined, an obser-
vation sequence is feed into the model. 
Each token state is a combination of the type of 
the named entity it belongs to and the boundary 
type it locates within. The entity types are person 
name (PER), location name (LOC), organization 
name (ORG), date expression (DAT), time expres-
sion (TIM), numeric expression (NUM), and not 
named entity (OTH). The boundary types are sim-
ply Beginning, Inside, and Outside (BIO).  
All above types of features are extracted from a 
varying length window. The main criteria is that 
wider window with smaller feature space and nar-
row window when the observation features are in a 
large range. 
The main feature set is shown the following. 
Character Texts(TXT): 
TXT-2, TXT-1, TXT0, TXT1, TXT2, 
TXT-1TXT0,  TXT1TXT0, TXT1TXT2 
simplified part-of-speech (POS): 
unigram: POS-4 ~ POS4 
121
Sixth SIGHAN Workshop on Chinese Language Processing
small-vocabulary-character-lists (SVCL): 
unigram: SVCL-2 ~ SVCL7 
bigram:SVCL0SVCL1, SVCL1SVCL2 
Word Boundary (WB): 
WB-1,WB0,WB1 
Named Entity (NE): 
unigram: NE-4 ~ NE4 
bigram:NE-2NE-1, NE-1NE0, NE0NE1, NE1NE2 
State Bigram (B) ? Observation: 
B, B-TXT0, B-NE-1, B-NE0, B-NE1 
Table 2.  The Main Feature Set 
4  Post Processing on Heuristic Rules 
Observing from the evaluation, our model has 
worse performance on ORG and PER than LOC. 
Furthermore, the analysis of the errors tells us that 
they are hard to be tackled with the improvement 
of the model itself. Therefore, we decided to do 
some post-process to correct certain types of tag-
ging errors of the unified model mainly concerning 
the two kinds of entities, ORG and PER.  
At the training phrase, we compare the tagging 
output of the model with the correct tags and col-
lect the falsely tagged instances. To identify the 
rules used in the post-process, we categorize the 
errors into several types, discriminate the types 
and encode them into the rules according to two 
principles: 
1) the rules are applied on the tagged sequences 
output by the unified model.  
2) The rules applied shouldn?t introduce more 
other errors. 
As a result, we have extracted eight rules, seven 
for ORG, one for PER. Generally, the rules work 
only on the local context of the examined tags, 
they correct some type of error by changing some 
tags when seeing certain pattern of context before 
or after the current tags in a limited distance. We 
want to give one rule as one example to explain 
the way they function. 
Example: {<LOC>}+<ORG> ==> <ORG>,  
After this rule is applied, one or more locations 
followed by a organization name will be tagged 
ORG. This is the case where there are a location 
name in a organization name. Besides, we can see 
since the location and latter part of the organiza-
tion name are tagged separately in the unified 
model, we may only resort to the post-process to 
get the right government boundary.  
5  Evaluation 
5.1  Results 
The evaluations in training phrase tell us the 
post-process can improve the performance by one 
percent. We are satisfied since we just applied 
eight rules. 
The formal eveluation results of our system are 
shown in Table 3. 
 R P F 
Overall 86.74 90.03 88.36 
PER 90.83 92.16 91.49 
LOC 89.89 91.66 90.77 
ORG 77.99 85.16 81.41 
Table 3. Formal Results on MSRA NER Open 
5.2  Errors  from NER Track 
The NER errors in our system are mainly of as 
follows: 
z Abbreviations 
Abbreviations are very common among the er-
rors. Among them, a significant part of abbrevia-
tions are mentioned before their corresponding full 
names. Some common abbreviations has no corre-
sponding full names appeared in document. Here 
are some examples: 
R1: ??[???? LOC]????? 
K: [?????? LOC]????? 
R: [? ? LOC]?? 
K: [? LOC][? LOC]?? 
In current system, the recognition is fully de-
pended on the linear-chain CRFs model, which is 
heavily based on local window observation fea-
tures; no abbreviation list or special abbreviation 
                                                 
1 R stands for system response, K for key. 
122
Sixth SIGHAN Workshop on Chinese Language Processing
recognition involved. Because lack of constraint 
checking on distant entity mentions, the system 
fails to catch the interaction among similar text 
fragments cross sentences. 
z Concatenated Names 
For many reasons, Chinese names in titles and 
some sentences, especially in news, are not sepa-
rated. The system often fails to judge the right 
boundaries and the reasonable type classification. 
For example: 
R:?[???? LOC]?[???? PER]
?? 
K:?[???? PER]?[???? PER]
?? 
z Hints 
Though it helps to recognize an entity at most 
cases, the small-vocabulary-list hint feature may 
recommend a wrong decision sometimes. For in-
stance, common surname character ??? in the 
following sentence is wrongly labeled when no 
word segmentation information given: 
R:[?? LOC]?[? ???????
? PER] 
K:[?? LOC]? ?[???????
? PER] 
Other errors of this type may result from failing 
to identify verbs and prepositions, such as: 
R:??????????? ????? 
K:[??????????? ORG]????
? 
6  Conclusions and Future Work 
We mainly described a Chinese named entity rec-
ognition system NER@ISCAS, which integrates 
text, part-of-speech and a small-vocabulary-
character-lists feature for MSRA NER open track 
under the framework of Conditional Random 
Fields (CRFs) model. Although it provides a uni-
fied framework to integrate multiple flexible fea-
tures, and to achieve global optimization on input 
text sequence, the popular linear chained Condi-
tional Random Fields model often fails to catch 
semantic relations among reoccurred mentions and 
adjoining entities in a catenation structure. 
The situations containing exact reoccurrence 
and shortened occurrence enlighten us to take 
more effort on feature engineering or post process-
ing on abbreviations / recurrence recognition. 
Another effort may be poured on the common 
patterns, such as paraphrase, counting, and con-
straints on Chinese person name lengths. 
From current point of view, enriching the hint 
lists is also desirable. 
Acknowledgements 
This work is partially supported by National Natural Science 
Foundation of China under grant #60773027, #60736044 and 
by ?863? Key Projects #2006AA010108.  
References 
Chinese 863 program. 2005. Results on Named 
Entity Recognition. The 2004HTRDP Chinese 
Information Processing and Intelligent Human-
Machine Interface Technology Evaluation. 
Yuanyong Feng, Le Sun, Yuanhua Lv. 2006. 
Chinese Word Segmentation and Named Entity 
Recognition Based on Conditional Random 
Fields Models. Proceedings of SIGHAN-2006, 
Fifth SIGHAN Workshop on Chinese Language 
Processing, Sydney, Australia,. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and Label-
ing Sequence Data. ICML. 
Shiwen Yu. 1999. Manual on Modern Chinese 
Corpora Construction. Institute of Computa-
tional Language, Peking Unversity. Beijing.  
123
Sixth SIGHAN Workshop on Chinese Language Processing
