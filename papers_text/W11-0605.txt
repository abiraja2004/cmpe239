Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 39?48,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Top-down recognizers for MCFGs and MGs
Edward P. Stabler
stabler@ucla.edu
Abstract
This paper defines a normal form for MCFGs
that includes strongly equivalent representa-
tions of many MG variants, and presents
an incremental priority-queue-based TD rec-
ognizer for these MCFGs. After introduc-
ing MGs with overt phrasal movement, head
movement and simple adjunction are added
without change in the recognizer. The MG
representation can be used directly, so that
even rather sophisticated analyses of properly
non-CF languages can be defined very suc-
cinctly. As with the similar stack-based CF-
methods, finite memory suffices for the recog-
nition of infinite languages, and a fully con-
nected left context for probabilistic analysis is
available at every point.
1 Introduction
In the years after Joshi (1985) proposed that human
languages are weakly and strongly ?mildly context
sensitive? (MCS), it was discovered that many in-
dependently proposed grammar formalisms define
exactly the same MCS languages. The languages
defined by Joshi?s tree adjoining grammars (TAGs)
are exactly the same as those defined by a ver-
sion of Steedman?s combinatory categorial gram-
mars, and the same as those defined by head wrap-
ping grammars (Vijay-Shanker and Weir, 1994). A
slightly larger class of languages is defined by an-
other variant of TAGs (set-local multicomponent),
by a version of Pollard?s generalized phrase struc-
ture grammars called multiple context free gram-
mars (MCFGs), and by a wide range of minimalist
grammar (MG) formalizations of Chomskian syn-
tax (Seki et al, 1991; Michaelis, 1998; Michaelis,
2001b; Harkema, 2001a; Stabler, 2011). These
remarkable convergences provide evidence from
across grammatical traditions that something like
these MCS proposals may be approximately right,
and so it is natural to consider psychological models
that fit with these proposals. With a range of per-
formance models for a range of MCS grammars, it
becomes possible to explore how grammatical de-
pendencies interact with other factors in the condi-
tioning of human linguistic performance.
For context free grammars (CFGs), perhaps the
simplest parsing model is top-down: beginning with
the prediction of a sentence, rules are applied to the
leftmost predicted category until a terminal element
is reached, which is then checked against the in-
put. This parsing method is of interest in psycho-
logical modeling not only because it uses the gram-
mar in a very transparent way, but also it is because
it is predictive in a way that may be similar to hu-
man parsing. At every point in analyzing a sentence
from left to right, the structure that has been con-
structed is fully connected: grammatical relation-
ships among the elements that have been heard have
been guessed, and there are no pieces of structure
which have not been integrated. Consequently, this
structure can be interpreted by a standard composi-
tional semantics and may be appropriate for ?incre-
mental? models of sentence interpretation (cf. Had-
dock, 1989; Chambers et al, 2004; Shen and Joshi,
2005; Altmann and Mirkovic?, 2009; Demberg and
Keller, 2009, Kato and Matsubara, 2009; Schuler,
2010). And like human parsing, when used with
39
backtracking or a beam search, TD memory de-
mands need not continually increase with sentence
length: a fixed bound on stack depth and on back-
track or beam depth suffices for infinitely many sen-
tences. Furthermore, TD parsing provides explicit,
relevant ?left contexts? for probabilistic condition-
ing (Roark and Johnson, 1999; Roark, 2001; Roark,
2004). But it has not been clear until recently how
to apply this method to Chomskian syntax or any
of the other MCS grammar formalisms. There have
been some proposals along these lines, but they have
either been unnecessarily complex or applicable to
only a restricted to range of grammatical proposals
(Chesi, 2007; Mainguy, 2010).
This paper extends TD parsing to minimalist con-
text free grammars (MCFGs) in a certain normal
form and presents minimalist grammars (MGs) as a
succinct representation for some of those MCFGs.
With this extension, the TD parsing method han-
dles an infinite range of MCFGs that encompasses,
strongly and weakly, an infinite range of (many vari-
ants of) MGs in a very transparent and direct way.
The parsing method can be defined in complete de-
tail very easily, and, abstracting away from limita-
tions of time and memory, it is provably sound and
complete for all those grammars.
The TD recognizer for MCFGs is presented in ?4,
generalizing and adapting ideas from earlier work
(Mainguy, 2010; Villemonte de la Clergerie, 2002).
Instead of using a stack memory, this recognizer
uses a ?priority queue,? which just means that we
can access all the elements in memory, sorting them
into left-to-right order. Then it is easy to observe:
(?3.2) while the reference to MCFG is useful for
understanding the recognizer, an MG representation
can be used directly without explicitly computing
out its MCFG equivalent; (?5.1) the extensions for
head movement and simple adjunction allow the rec-
ognizer of ?4 to apply without change; (?5.2) like its
stack-based CF counterpart, the MG recognizer re-
quires only finite memory to recognize certain infi-
nite subsets of languages ? that is, memory demands
do not always strictly increase with sentence length;
and (?5.3) the TD recognizer provides, at every point
in processing the input, a fully connected left con-
text for interpretation and probabilistic condition-
ing, unlike LC and other familiar methods. Since
a very wide range of grammatical proposals can be
expressed in this formalism and parsed transparently
by this method, it is straightforward to compute fully
explicit and syntactically sophisticated parses of the
sorts of sentences used in psycholinguistic studies.
2 MCFGs
MCFGs are first defined by Seki et al (1991),
but here it will be convenient to represent MCFGs
in a Prolog-like Horn clause notation, as in
Kanazawa (2009). In this notation, the familiar con-
text free rule for sentences would be written
S(x01x11) :- NP (x01),
V P (x11).
Reading :- as ?if?, this formula says that a string
formed by concatenating any string x01 with string
x11 is an S, if x01 is an NP, and x11 is a VP. We
number the variables on the right side in such a way
as to indicate that each variable that appears on the
right side of any rule appears exactly once on the
right and once on the left. Lexical rules like
NP (Mary)
V P (sings),
have empty ?right sides? and no variables in this no-
tation.
MCFGs allow categories to have multiple string
arguments, so that, for example, a VP with a wh-
phrase that is moving to another position could be
represented with two string arguments, one of which
holds the moving element. In general, each MCFG
rule for building an instance of category A from cat-
egories B0 . . . Bn (n ? 0) has the form,
A(t1, . . . , td(A)) :- B0(x01 , . . . , x0d(B0)),
. . . ,
Bn(xn1 , . . . , xnd(Bn)),
where each ti is an term (i.e. a sequence) over the (fi-
nite nonempty) vocabulary ? and the variables that
appear on the right; no variable on the right occurs
more than once on the left (no copying); and the des-
ignated ?start? category S has ?arity? or ?dimension?
d(S) = 1. For any such grammar, the language
L(G) is the set of strings s ? ?? such that we can
derive S(s).
40
Here, we restrict attention to a normal form in
which (i) each MCFG rule is nondeleting in the
sense that every variable xij on the right occurs ex-
actly once on the left, and (ii) each rule is either lexi-
cal or nonlexical, where a lexical rule is one in which
n = 0 and d(A) = 1 and t1 ? ??{?}, and a nonlex-
ical rule is one in which n > 0 and each ti ? V ar?.
Clearly these additional restrictions do not affect the
expressive power of the grammars.
2.1 Example 1
Consider this MCFG for {aibjcidj | i, j > 0}, with 5
non-lexical rules, 4 lexical rules, and start category
S. We letter the rules for later reference:
a. S(x0x1x2x3) :- AC(x0, x2), BD(x1, x3)
b. AC(x0x2, x1x3) :- A(x0), C(x1), AC(x2, x3)
c. AC(x0, x1) :- A(x0), C(x1)
d. BD(x0x2, x1x3) :- B(x0),D(x1), BD(x2, x3)
e. BD(x0, x1) :- B(x0),D(x1)
f. A(a)
g. B(b)
h. C(c)
i. D(d)
With this grammar we can show that abbcdd has cat-
egory S with a derivation tree like this:
S(abbcdd)
AC(a, c)
A(a) C(c)
BD(bb, dd)
B(b) D(d) BD(b, d)
B(b) D(d)
See, for example, Kanazawa (2009) for a more de-
tailed discussion of MCFGs in this format.
3 MGs as MCFGs
Michaelis (1998; 2001a) shows that every MG has
a ?strongly equivalent? MCFG, in the sense that the
MG derivation trees are a relabeling of the MCFG
derivation trees. Here we present MGs as finite sets
of lexical rules that define MCFGs. MG categories
contain finite tuples of feature sequences, where the
features include categories like N,V,A,P,. . . , selec-
tors for those categories =N,=V,=A,=P,. . . , licensors
+case,+wh,. . . , and licensees -case,-wh,. . . . In our
MCFG representation, a category is a tuple
?x, ?0, ?1, . . . , ?j ?
where (i) j ? 0, (ii) x = 1 if the element is lexical
and 0 otherwise, (iii) each ?i is a nonempty feature
sequence, and (iv) the category has dimension j+1.
An MG is then given by a specified start category
and a finite set of lexical rules
? 1, ?0 ?(a).
for some a ? ?. The MG defines the language gen-
erated by its lexicon together with MCFG rules de-
termined by the lexicon, as follows. Let ?2(Lex)
be the set of feature sequences ?0 contained in the
lexical rules, and let k be the number of differ-
ent types of licensees f that occur in the lexical
rules. For all 0 ? i, j ? k, all x, y ? {0, 1}, all
?, ?, ?i, ?i ? suffix(?2(Lex)), and ? 6= ?, we have
these ?merge? rules, broken as usual into the cases
where (i) we are merging into complement position
on the right, (ii) merging into specifier position on
the left, or (iii) merging with something that is mov-
ing:
? 0, ?, ?1, . . . , ?j ?(s0t0, t1, . . . , tj) :-
? 1,=f? ?(s0),
?x, f, ?1, . . . , ?j ?(t0, . . . , tj)
? 0, ?, ?1, . . . , ?i, ?1, . . . , ?j ?(t0s0, s1, . . . , si, t1, . . . , tj) :-
? 0,=f?, ?1, . . . , ?i, ?(s0, . . . , si),
?x, f, ?1, . . . , ?j ?(t0, . . . , tj)
? 0, ?, ?1, . . . , ?i, ?, ?1, . . . , ?j ?(s0, . . . , si, t0, . . . , tj) :-
?x,=f?, ?1, . . . , ?i, ?(s0, . . . , si),
? y, f?, ?1, . . . , ?j ?(t0, . . . , tj)
And we have these ?move? rules, broken as usual
into the cases where the moving element is landing,
when ?i = -f ,
? 0, ?, ?1, . . . , ?i?1, ?i+1, . . . , ?j ?
(sis0, s1, . . . , si?1, si+1, . . . , sj) :-
? 0,+f?, ?1, . . . , ?j ?(s0, . . . , sj),
and cases where the moving element must move
again, when ?i = -f?,
? 0, ?, ?1, . . . , ?i?1, ?, ?i+1, . . . , ?j ?(s0, . . . , si) :-
? 0,+f?, ?1, . . . , ?j ?(s0, . . . , si),
where none of ?1, . . . , ?i?1, ?i+1, . . . , ?j begin with
-f . The language of the MG is the MCFL defined by
the lexicon and all instances of these 5 rule schemes
(always a finite set).
By varying the lexicon, MGs can define all the
MCFLs (Michaelis, 2001b; Harkema, 2001b), i.e.,
41
the set-local multi-component tree adjoining lan-
guages (MCTALs) (Weir, 1988; Seki et al, 1991).
TALs are a proper subset, defined by ?well-nested
2-MCFGs? (Seki et al, 1991; Kanazawa, 2009).
3.1 Example 2
Consider the following lexicon containing 7 items,
with the ?complementizer? start category C,
?1,=D =D V?(likes) ?1,D?(Mary)
?1,=C =D V?(knows) ?1,D?(John)
?1,=V C?(?) ?1,D -wh?(who)
?1,=V +wh C?(?)
Using the definition given just above, this deter-
mines an MG. This is a derivation tree for one of
the infinitely many expressions of category C:
?0,C?(Mary knows who John likes)
?1,=V C?(?) ?0,V?(Mary knows who John likes)
?0,=D V?(knows who John likes)
?1,=C =D V?(knows) ?0,C?(who John likes)
?0,+wh C,-wh?(John likes,who)
?1,=V +wh C?(?) ?0,V,-wh?(John likes,who)
?0,=D V,-wh?(likes,who)
?1,=D =D V?(likes) ?1,D -wh?(who)
?1,D?(John)
?1,D?(Mary)
If we relabel this tree so that each instance of merge
is labeled Merge or ?, and each instance of move is
labeled Move or ?, the result is the corresponding
MG derivation tree, usually depicted like this:
?
?::=V C ?
?
knows::=C =D V ?
?
?:=V +wh C ?
?
likes::=D =D V who::D -wh
John::D
Mary::D
In fact, the latter tree fully specifies the MCFG
derivation above, because, in every MG derivation,
for every internal node, the categories of the children
determine which rule applies. This is easily verified
by checking the 5 schemes for non-lexical rules on
the previous page; the left side of each rule is a func-
tion of the right. Consequently the MCFG categories
at the internal nodes can be regarded as specifying
the states of a deterministic finite state bottom-up
tree recognizer for the MG derivation trees (Kobele
et al, 2007; Graf, 2011; Kobele, 2011).
3.2 MCFGs need not be computed
We did not explicitly present the nonlexical MCFG
rules used in the previous section ?3.1, since they are
determined by the lexical rules. The first rule used
at the root of the derivation tree is, for example, an
instance of the first rule scheme in ?3, namely:
? 0, C ?(s0t0) :- ? 1,=V C ?(s0), ? 0, V ?(t0).
Generating these non-lexical MCFG rules from the
MG lexicon is straightforward, and has been im-
plemented in (freely available) software by Guillau-
min (2004). But the definition given in ?3 requires
that all feature sequences in all rules be suffixes
of lexical feature sequences, and notice that in any
derivation tree, like the one shown in ?3.1, for exam-
ple, feature sequences increase along the left branch
from any node to the leaf which is its ?head.? Along
any such path, the feature sequences increase one
feature at a time until they reach the lexical leaf. So
in effect, if we are building the derivation top-down,
each step adds or ?unchecks? features in lexical se-
quences one at a time, and obviously the options for
doing this can be seen without compiling out all the
MCFG nonlexical rules.
4 The top-down recognizer
For any sequence s of elements of S, let |s|=the
length of s and nth(i, s) = a iff a ? S, and for
some u, v ? S?, s = uav and |u| = i. Adapt-
ing basic ideas from earlier work (Mainguy, 2010;
Villemonte de la Clergerie, 2002) for TD recogni-
tion, we will instantiate variables not with strings but
with indices i ? N? to represent linear order of con-
stituents, to obtain indexed atoms A(i1, . . . , id(A)).
Consider any nonlexical rule ? :- ? and any in-
dexed atom ? where
?=A(t1, . . . , td(A))
?=A(i1, . . . , id(A))
?=B0(x01 , . . . , x0d(B0)), . . . , Bn(xn1 , . . . , xnd(Bn)).
For each variable xij in ?, define
index?,?(xij ) =
{
ik if tk = xij
ikp if |tk| > 1, xij = nth(p, tk).
42
Let index?,?(?) be the result of replacing each vari-
able xij in ? by index?,?(xij ). Finally, let trim(?)
map ? to itself except in the case when when every
index in ? begins with the same integer n, in which
case that initial n is deleted from every index.
Define a total order on the indices N? as follows.
For any ?, ? ? N?,
? < ? iff
?
?
?
?
?
? = ? 6= ?, or
? = i??, ? = j??, i < j, or
? = i??, ? = i??, ?? < ??.
For any atom ?, let ?(?) be the least index in ?. So,
for example, ?(AB(31, 240)) = 240. And for any
indexed atoms ?, ?, let ? < ? iff ?(?) < ?(?). We
use this order it to sort categories into left-to-right
order in the ?expand? rule below.
We now define TD recognition in a deductive for-
mat. The state of the recognition sequence is given
by a (remaining input,priority queue) pair, where the
queue represents the memory of predicted elements,
sorted according to < so that they can be processed
from left to right. We have 1 initial axiom, which
predicts that input s will have start category S, where
S initially has index ?:
(s, S(?))
The main work is done by the expand rule, which
pops atom ? off the queue, leaving sequence ? un-
derneath. Then, for any rule ? :- ? with ? of the
same category as ?, we compute index?,?(?), ap-
pend the result and ?, then sort and trim:
(s, ??)
(s, sort(trim(index?,?(?)?)))
? :- ?
(We could use ordered insertion instead of sorting,
and we could trim the indices much more aggres-
sively, but we stick to simple formulations here.) Fi-
nally, we have a scan rule, which scans input a if we
have predicted an A and our grammar tells us that
A(a). For all a ? (? ? ?), s ? ??, n ? N?:
(as, A(n) ?)
(s,?)
A(a)
A string s is accepted if we can use these rules to get
from the start axiom to (?,?). This represents the fact
that we have consumed the whole input and there are
no outstanding predictions in memory.
4.1 Example 1, continued.
Here is the sequence of recognizer states that accepts
abbcdd, using the grammar presented in ?2.1:
initial axiom:
init. (abbcdd, S(?))
expand with rule a:
1. (abbcdd, AC(0,2),BD(1,3))
expand with rule c (note sort):
2. (abbcdd, A(0),BD(1,3),C(2))
scan with rule f:
3. (bbcdd, BD(1,3),C(2))
expand with rule d:
4. (bbcdd, B(10),BD(11,31),C(2),D(30))
scan with rule g:
5. (bcdd, BD(11,31),C(2),D(30))
expand with rule e:
6. (bcdd, B(11),C(2),D(30),D(31))
scan with rule g:
7. (cdd, C(2),D(30),D(31))
scan with rule h (note trim removes 3):
8. (dd, D(0),D(1))
scan with rule i:
9. (d, D(1))
scan with rule i:
10. (?, ?)
The number of recognizer steps is always exactly
the number of nodes in the corresponding derivation
tree; compare this accepting sequence to the deriva-
tion tree shown in ?2.1, for example.
5 Properties and extensions
5.1 Adding adjunction, head movement
Frey and Ga?rtner (2002) propose that adjunction be
added to MGs by (i) allowing another kind of select-
ing feature ?f , which selects but does not ?check
and delete? the feature f of a phrase that it modi-
fies, where (ii) the head of the result is the selected,
?modified? phrase that it combines with, and (iii)
the selecting ?modifier? cannot have any constituents
moving out of it. We can implement these ideas by
adding a rule scheme like the following (compare
the first rule scheme in ?3):
? 0, f?, ?1, . . . , ?j ?(t0s0, t1, . . . , tj) :-
? y, f?, ?1, . . . , ?j ?(t0, . . . , tj),
?x,?f ?(s0).
Note this rule ?attaches? the modifier on the right.
We could also allow left modifiers, but in the exam-
ples below will only use this one.
43
Some analyses of simple tensed sentences say that
tense affixes ?hop? onto the verb after the verb has
combined with its object. Affix hopping and head
movement are more challenging that adjunction, but
previous approaches can be adapted to the present
perspective by making two changes: (i) we keep the
head separate from other material in its phrase until
that phrase is merged with another phrase, so now
every non-lexical category A has d(A) ? 3 and (ii)
we add diacritics to the selection features to indi-
cate whether hopping or head movement should ap-
ply in the merge step. To indicate that a head A
selects category f we give A the feature =f , but
to indicate the the head of A should hop onto the
head of the selected constituent, we give A the fea-
ture f=>. Essentially this representation of MGs
with head movement and affix hopping as MCFGs is
immediate from the formalization in Stabler (2001)
and the automated translation by Guillaumin (2004).
The examples in this paper below will use only affix
hopping which is defined by the following modified
version of the first rule in ?3:
? 0, ?, ?1, . . . , ?j ?(?, ?, tsthshtc, t1, . . . , tj) :-
? 1, f=>? ?(sh),
?x, f, ?1, . . . , ?j ?(ts, th, tc, t1, . . . , tj)
The first atom on the right side of this rule, the ?se-
lector?, is a lexical head with string sh. The sec-
ond atom on the right of the rule has string com-
ponents ts, th, tc (these are the specifier, head, and
complement strings) together with j ? 0 moving el-
ements t1, . . . , tj . In the result on the left, we see
that the lexical selector sh is ?hopped? to the right of
the selected head th, where it is sandwiched between
the other concatenated parts of the selected phrase,
leaving ? in the head position. Since the usual start
category C now has 3 components, like every other
head, we begin with a special category S that serves
only to concatenate the 3 components of the matrix
complementizer phrase, by providing the recognizer
with this additional initializing rule:
S(ssshsc) :- ?x,C ?(ss, sh, sc).
The nature of adjunction is not quite clear, and
there is even less consensus about whether head
movement or affix hopping or both are needed in
grammars of human languages, but these illustrate
how easily the MCFG approach to MGs can be ex-
tended. Like many of the other MG variants, these
extensions do not change the class of languages that
can be defined (Stabler, 2011), and the recognizer
defined in ?4 can handle them without change.
With head movement and adjunction we can,
for example, provide a roughly traditional analy-
sis of the famous example sentence from King and
Just (1991) shown in Figure 1. Note again that
the derivation tree in that figure has lexical items at
the leaves, and these completely determine the non-
lexical rules and the structure of the derivation. Var-
ious representations of the ?derived trees?, like the
X-bar tree shown in this figure, are easily computed
from the derivation tree (Kobele et al, 2007). And
Figure 2 shows the recognizer steps accepting that
sentence. Plotting queue size versus recognizer step,
and simply overlaying the King and Just self-paced
reading times to see if they are roughly similar, we
see that, at least in sentences like these, readers go
more slowly when the queue gets large:
 0
 1
 2
 3
 4
 5
 6
 7
 0  5  10  15  20  25  30
TD queue size
King and Just reading times
Recent work has challenged the claim that reading
times are a function of the number of predictions in
memory, (e.g., Nakatani and Gibson, 2008, p.81) but
preliminary studies suggest that other performance
measures may correlate (Bachrach, 2008; Brennan
et al, 2010; VanWagenen et al, 2011). Exploring
these possibilities is beyond the scope this paper.
The present point is that any analysis expressible
in the MG formalism can be parsed transparently
with this approach, assessing its memory demands;
partially parallel beam search models for ambigu-
ity, used in natural language engineering, can also
be straightforwardly assessed.
44
?
?::=T C ?
?
-ed::V=> +ep T ?
?
admit::=D =D V ?
the::=N D error::N
?
the::=N D -ep ?
reporter::N ?
?
that::=T +wh ?N ?
?
-ed::V=> +ep T ?
?
attack::=D =D V ?::D -wh
?
the::=N D -ep senator::N
CP
C?
C TP
DP(2)
D?
D
the NP
NP
N
reporter
CP
DP(1)
?
C?
C
that
TP
DP(0)
D?
D
the
NP
N?
N
senator
T?
T VP
DP
t(0)
V?
V
V
attack
T
-ed
DP
t(1)
T?
T VP
DP
t(2)
V?
V
V
admit
T
-ed
DP
D?
D
the
NP
N?
N
error
Figure 1: 28 node derivation tree and corresponding X-bar tree for King and Just (1991) example
5.2 Infinite languages with finite memory
Although memory use is not the main concern of
this paper, it is worth noting that, as in stack-based
CF models, memory demands do not necessarily in-
crease without bound as sentence length increases.
So for example, we can extend the naive grammar
of Figure 2 to accept this is the man that kiss -ed
the maid that milk -ed the cow that toss -ed the dog
that worry -ed the cat that chase -ed the rat, a sen-
tence with 6 clauses, and use no more memory at
any time than is needed for the 2 clause King and
Just example. Dynamic, chart-based parsing meth-
ods usually require more memory without bound as
sentence length grows, even when there is little or
no indeterminacy.
5.3 Connectedness
More directly relevant to incremental models is the
fact that the portions of the derivation traversed at
any point in TD recognition are all connected to each
other, their syntactic relations are established. As we
see in all our examples, the TD recognizer is always
traversing the derivation tree on paths connected to
the root; while the indexing and sorting ensures that
the leaves are scanned in the order of their appear-
ance in the derived X-bar tree. Left corner traver-
sals do not have this property. Consider a sentence
like the reporter poured the egg in the bowl over the
flour. In a syntax in the spirit of the one we see in
Figure 1, for example, in the bowl could be right ad-
joined to the direct object, and over the flour right
adjoined to VP. Let VP1 be the parent of over the
flour, and VP2 its sister. With LC, VP1 will be pre-
dicted right after the subject is completed. But the
verb is the left corner of VP2, and VP2 will not be
attached to VP1 ? and so the subject and verb will
not be connected ? until VP1 is completed. This de-
lay in the LC attachment of the subject to the verb
can be extended by adding additional right modifiers
to the direct object or the verb phrase, but the evi-
dence suggests that listeners make such connections
immediately upon hearing the words, as the TD rec-
ognizer does.
6 Future work
Standard methods for handling indeterminacy in
top-down CF parsers, when there are multiple ways
to expand a derivation top down, are easily adapted
to the MCFG and MG parsers proposed here. With
backtracking search, left recursion can cause non-
45
termination, but a probabilistic beam search can do
better. For ? = (i,?) any recognizer state, let
step(?) be the (possibly empty) sequence of all the
next states that are licensed by the rules in ?3 (al-
ways finitely many). A probabilistic beam search
uses the rules,
?(s, S(?))? init
??
prune(sortC(step(?)?))
search,
popping a recognizer state ? off the top of the queue
??, appending step(?) and ?, then sorting and
pruning the result. The sort in the search steps
is done according to the probability of each parser
state in context C , where the context may include a
history of previous recognizer steps ? i.e. of each
derivation up to this point ? but also possibly ex-
trasentential information of any sort. The pruning
rule acts to remove highly improbable analyses, and
success is achieved if a step puts (?, ?) on top of
the queue. Roark shows that this ability to condi-
tion on material not in parser memory ? indeed on
anything in the left context ? can allow better esti-
mates of parse probability. On small experimental
grammars, we are finding that TD beam search per-
formance can be better than our chart parsers using
the same grammar. Further feasibility studies are in
1 init. (trttsa-a-te, S(?))
1 init. (trttsa-a-te, ?0,C?(0,1,2))
2 1. (trttsa-a-te, ?1,=TC?(1),?0,T?(20,21,22))
1 2. (trttsa-a-te, ?0,T?(0,1,2))
1 3. (trttsa-a-te, ?0,+epT,-ep?(01,1,2,00))
2 4. (trttsa-a-te, ?0,V,-ep?(20,21,23,00),?1,V=>+epT?(22))
3 5. (trttsa-a-te, ?0,D-ep?(000,001,002),?0,=DV?(20,21,23),?1,V=>+epT?(22))
4 6. (trttsa-a-te, ?1,=ND-ep?(001),?0,N?(0020,0021,0022),?0,=DV?(20,21,23),?1,V=>+epT?(22))
3 7. (rttsa-a-te, ?0,N?(0020,0021,0022),?0,=DV?(20,21,23),?1,V=>+epT?(22))
4 8. (rttsa-a-te, ?1,N?(0021),?0,?N?(00220,00221,00222),?0,=DV?(20,21,23),?1,V=>+epT?(22))
3 9. (ttsa-a-te, ?0,?N?(00220,00221,00222),?0,=DV?(20,21,23),?1,V=>+epT?(22))
3 10. (ttsa-a-te, ?0,+wh?N ,-wh?(002201,00221,00222,002200),?0,=DV?(20,21,23),?1,V=>+epT?(22))
4 11. (ttsa-a-te, ?0,T,-wh?(002220,002221,002222,002200),?1,=T+wh?N?(00221),?0,=DV?(20,21,23),?1,V=>+epT?(22))
4 12. (ttsa-a-te, ?0,+epT,-ep,-wh?(0022201,002221,002222,0022200,002200),?1,=T+wh?N?(00221),?0,=DV?(20,21,23),
?1,V=>+epT?(22))
5 13. (ttsa-a-te, ?0,V,-ep,-wh?(0022220,0022221,0022223,0022200,002200),?1,=T+wh?N?(00221),?1,V=>+epT?(0022222),
?0,=DV?(20,21,23),?1,V=>+epT?(22))
6 14. (ttsa-a-te, ?0,=DV,-wh?(0022220,0022221,0022223,002200),?1,=T+wh?N?(00221),
?0,D-ep?(00222000,00222001,00222002),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?(22))
7 15. (ttsa-a-te, ?1,D-wh?(002200),?1,=T+wh?N?(00221),?0,D-ep?(00222000,00222001,00222002),?1,=D=DV?(0022221),
?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?(22))
6 16. (ttsa-a-te, ?1,=T+wh?N?(00221),?0,D-ep?(00222000,00222001,00222002),?1,=D=DV?(0022221),
?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?(22))
5 17. (tsa-a-te, ?0,D-ep?(00222000,00222001,00222002),?1,=D=DV?(0022221),?1,V=>+epT?(0022222),
?0,=DV?(20,21,23),?1,V=>+epT?(22))
6 18. (tsa-a-te, ?1,=ND-ep?(00222001),?1,N?(00222002),?1,=D=DV?(0022221),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),
?1,V=>+epT?(22))
5 19. (sa-a-te, ?1,N?(00222002),?1,=D=DV?(0022221),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?(22))
4 20. (a-a-te, ?1,=D=DV?(0022221),?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?(22))
3 21. (-a-te, ?1,V=>+epT?(0022222),?0,=DV?(20,21,23),?1,V=>+epT?(22))
2 22. (a-te, ?0,=DV?(20,21,23),?1,V=>+epT?(22))
3 23. (a-te, ?1,=D=DV?(1),?1,V=>+epT?(2),?0,D?(30,31,32))
2 24. (-te, ?1,V=>+epT?(2),?0,D?(30,31,32))
1 25. (te, ?0,D?(30,31,32))
2 26. (te, ?1,=ND?(1),?1,N?(2))
1 27. (e, ?1,N?(2))
0 28. (?, ?)
Figure 2: 28 step TD recognition of derivation in Figure 1, abbreviating input words by their initial characters. The
left column indicates queue size, plotted in ?5.1.
46
progress.
The recognizer presented here simplifies Main-
guy?s (2010) top-down MG recognizer by generaliz-
ing it handle an MCFG normal form, so that a wide
range of MG extensions are immediately accommo-
dated. This is made easy when we adopt Kanazawa?s
Horn clause formulation of MCFGs where the order
of variables on the left side of the rules so visibly in-
dicates the surface order of string components. With
the Horn clause notation, the indexing can be string-
based and general rather than tree-based and tied to
particular assumptions about how the MGs work.
Transparently generalizing the operations of CF TD
recognizers, the indexing and operations here are
also slightly simpler than ?thread automata? (Ville-
monte de la Clergerie, 2002). Compare also the
indexing, sometimes more or less similar, in chart-
based recognizers of MCF and closely related sys-
tems (Burden and Ljunglo?f, 2005; Harkema, 2001c;
Boullier, 1998; Kallmeyer, 2010).
Mainguy shows that when the probability of a
derivation is the product of the rule probabilities, as
usual, and when those rule probabilities are given by
a consistent probability assignment, a beam search
without pruning will always find a derivation if there
is one. When there is no derivation, though, an
unpruned search can fail to terminate; a pruning
rule can guarantee termination in such cases. Those
results extend to the MCFG recognizers proposed
here. Various applications have found it better to
use a beam search with top-down recognition of left-
or right-corner transforms of CF grammars (Roark,
2001; Roark, 2004; Schuler, 2010; Wu et al, 2010);
those transforms can (but need not always) disrupt
grammatical connectedness as noted in ?5.3. Work
in progress explores the possibilities for such strate-
gies in incremental MCFG parsing. It would also
be interesting to generalize Hale?s (2011) ?rational
parser? to these grammars.
Acknowledgments
Thanks to Thomas Mainguy, Sarah VanWagenen
and ?Eric Villemonte de la Clergerie for helpful dis-
cussions of this material.
References
Gerry T. M. Altmann and Jelena Mirkovic?. 2009. Incre-
mentality and prediction in human sentence process-
ing. Cognitive Science, 33:583?809.
Asaf Bachrach. 2008. Imaging Neural Correlates of Syn-
tactic Complexity in a Naturalistic Context. Ph.D. the-
sis, Massachusetts Institute of Technology.
Pierre Boullier. 1998. Proposal for a natural lan-
guage processing syntactic backbone. Technical Re-
port 3242, Projet Atoll, INRIA, Rocquencourt.
Jonathan Brennan, Yuval Nir, Uri Hasson, Rafael
Malach, David J. Heeger, and Liinay Pylkka?nen.
2010. Syntactic structure building in the anterior tem-
poral lobe during natural story listening. Forthcoming.
Ha?kan Burden and Peter Ljunglo?f. 2005. Parsing linear
context-free rewriting systems. In Ninth International
Workshop on Parsing Technologies, IWPT?05.
Craig G. Chambers, Michael K. Tanenhaus, Kathleen M.
Eberhard, Hana Filip, and Greg N. Carlson. 2004.
Actions and affordances in syntactic ambiguity resolu-
tion. Journal of Experimental Psychology: Learning,
Memory and Cognition, 30(3):687?696.
Cristiano Chesi. 2007. An introduction to phase-based
minimalist grammars: Why move is top-down from
left-to-right. Technical report, Centro Interdepartmen-
tale di Studi Cognitivi sul Linguaggio.
Vera Demberg and Frank Keller. 2009. A computational
model of prediction in human parsing: Unifying local-
ity and surprisal effects. In Proceedings of the 29th
meeting of the Cognitive Science Society (CogSci-09),
Amsterdam.
Werner Frey and Hans-Martin Ga?rtner. 2002. On the
treatment of scrambling and adjunction in minimal-
ist grammars. In Proceedings, Formal Grammar?02,
Trento.
Thomas Graf. 2011. Closure properties of minimalist
derivation tree languages. In Logical Aspects of Com-
putational Linguistics, LACL?11, Forthcoming.
Matthieu Guillaumin. 2004. Conversions be-
tween mildly sensitive grammars. UCLA and
?Ecole Normale Supe?rieure. http://www.linguistics.
ucla.edu/people/stabler/epssw.htm.
Nicholas J. Haddock. 1989. Computational models of
incremental semantic interpretation. Language and
Cognitive Processes, 4((3/4)):337?368.
John T. Hale. 2011. What a rational parser would do.
Cognitive Science, 35(3):399?443.
Henk Harkema. 2001a. A characterization of minimalist
languages. In Proceedings, Logical Aspects of Com-
putational Linguistics, LACL?01, Port-aux-Rocs, Le
Croisic, France.
Henk Harkema. 2001b. A characterization of minimalist
languages. In Philippe de Groote, Glyn Morrill, and
47
Christian Retore?, editors, Logical Aspects of Compu-
tational Linguistics, Lecture Notes in Artificial Intelli-
gence, No. 2099, pages 193?211, NY. Springer.
Henk Harkema. 2001c. Parsing Minimalist Languages.
Ph.D. thesis, University of California, Los Angeles.
Aravind Joshi. 1985. How much context-sensitivity is
necessary for characterizing structural descriptions. In
D. Dowty, L. Karttunen, and A. Zwicky, editors, Natu-
ral Language Processing: Theoretical, Computational
and Psychological Perspectives, pages 206?250. Cam-
bridge University Press, NY.
Laura Kallmeyer. 2010. Parsing beyond context-free
grammars. Springer, NY.
Makoto Kanazawa. 2009. A pumping lemma for well-
nested multiple context free grammars. In 13th In-
ternational Conference on Developments in Language
Theory, DLT 2009.
Yoshihide Kato and Shigeki Matsubara. 2009. In-
cremental parsing with adjoining operation. IE-
ICE Transactions on Information and Systems,
E92.D(12):2306?2312.
Jonathan King and Marcel Adam Just. 1991. Individual
differences in syntactic processing: the role of working
memory. Journal of Memory and Language, 30:580?
602.
Gregory M. Kobele, Christian Retore?, and Sylvain Sal-
vati. 2007. An automata-theoretic approach to min-
imalism. In J. Rogers and S. Kepser, editors, Model
Theoretic Syntax at 10, ESSLLI?07.
GregoryM. Kobele. 2011. Minimalist tree languages are
closed under intersection with recognizable tree lan-
guages. In Logical Aspects of Computational Linguis-
tics, LACL?11, Forthcoming.
Thomas Mainguy. 2010. A probabilistic
top-down parser for minimalist grammars.
http://arxiv.org/abs/1010.1826v1.
Jens Michaelis. 1998. Derivational minimalism is mildly
context-sensitive. In Proceedings, Logical Aspects of
Computational Linguistics, LACL?98, pages 179?198,
NY. Springer.
Jens Michaelis. 2001a. On Formal Properties of Mini-
malist Grammars. Ph.D. thesis, Universita?t Potsdam.
Linguistics in Potsdam 13, Universita?tsbibliothek,
Potsdam, Germany.
Jens Michaelis. 2001b. Transforming linear context
free rewriting systems into minimalist grammars. In
P. de Groote, G. Morrill, and C. Retore?, editors, Logi-
cal Aspects of Computational Linguistics, LNCS 2099,
pages 228?244, NY. Springer.
Kentaro Nakatani and Edward Gibson. 2008. Distin-
guishing theories of syntactic expectation cost in sen-
tence comprehension: Evidence from Japanese. Lin-
guistics, 46(1):63?86.
Brian Roark and Mark Johnson. 1999. Efficient proba-
bilistic top-down and left-corner parsing. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics, pages 421?428.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Brian Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
William Schuler. 2010. Incremental parsing in bounded
memory. In Proceedings of the 10th International
Workshop on Tree Adjoining Grammars and Related
Frameworks, TAG+10.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science, 88:191?229.
Libin Shen and Aravind Joshi. 2005. Incremental LTAG
parsing. In Proceedings, Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Human Language Processing.
Edward P. Stabler. 2001. Recognizing head movement.
In Philippe de Groote, Glyn Morrill, and Christian Re-
tore?, editors, Logical Aspects of Computational Lin-
guistics, Lecture Notes in Artificial Intelligence, No.
2099, pages 254?260. Springer, NY.
Edward P. Stabler. 2011. Computational perspectives on
minimalism. In Cedric Boeckx, editor, Oxford Hand-
book of Linguistic Minimalism, pages 617?641. Ox-
ford University Press, Oxford.
Sarah VanWagenen, Jonathan Brennan, and Edward P.
Stabler. 2011. Evaluating parsing strategies in sen-
tence processing. In Proceedings of the CUNY Sen-
tence Processing Conference.
K. Vijay-Shanker and David Weir. 1994. The equiva-
lence of four extensions of context free grammar for-
malisms. Mathematical Systems Theory, 27:511?545.
?Eric Villemonte de la Clergerie. 2002. Parsing MCS lan-
guages with thread automata. In Proceedings of the
6th International Workshop on Tree Adjoining Gram-
mars and Related Frameworks, TAG+6.
David Weir. 1988. Characterizing Mildly Context-
Sensitive Grammar Formalisms. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Stephen Wu, Asaf Bachrach, Carlos Cardenas, and
William Schuler. 2010. Complexity metrics in an in-
cremental right-corner parser. In Proceedings of the
48th Annual Meeting of the Association for Computer
Linguistics, pages 1189?1198.
48
