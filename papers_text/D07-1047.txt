Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 448?457, Prague, June 2007. c?2007 Association for Computational Linguistics
Enhancing Single-document Summarization by Combining RankNet and
Third-party Sources
Krysta M. Svore
Microsoft Research
1 Microsoft Way
Redmond, WA 98052
ksvore@microsoft.com
Lucy Vanderwende
Microsoft Research
1 Microsoft Way
Redmond, WA 98052
Christopher J.C. Burges
Microsoft Research
1 Microsoft Way
Redmond, WA 98052
Abstract
We present a new approach to automatic
summarization based on neural nets, called
NetSum. We extract a set of features from
each sentence that helps identify its impor-
tance in the document. We apply novel
features based on news search query logs
and Wikipedia entities. Using the RankNet
learning algorithm, we train a pair-based
sentence ranker to score every sentence in
the document and identify the most impor-
tant sentences. We apply our system to
documents gathered from CNN.com, where
each document includes highlights and an
article. Our system significantly outper-
forms the standard baseline in the ROUGE-1
measure on over 70% of our document set.
1 Introduction
Automatic summarization was first studied almost
50 years ago by Luhn (Luhn, 1958) and has contin-
ued to be a steady subject of research. Automatic
summarization refers to the creation of a shortened
version of a document or cluster of documents by
a machine, see (Mani, 2001) for details. The sum-
mary can be an abstraction or extraction. In an ab-
stract summary, content from the original document
may be paraphrased or generated, whereas in an ex-
tract summary, the content is preserved in its original
form, i.e., sentences. Both summary types can in-
volve sentence compression, but abstracts tend to be
more condensed. In this paper, we focus on produc-
ing fully automated single-document extract sum-
maries of newswire articles.
To create an extract, most automatic systems use
linguistic and/or statistical methods to identify key
words, phrases, and concepts in a sentence or across
single or multiple documents. Each sentence is then
assigned a score indicating the strength of presence
of key words, phrases, and so on. Sentence scoring
methods utilize both purely statistical and purely se-
mantic features, for example as in (Vanderwende et
al., 2006; Nenkova et al, 2006; Yih et al, 2007).
Recently, machine learning techniques have been
successfully applied to summarization. The meth-
ods include binary classifiers (Kupiec et al, 1995),
Markov models (Conroy et al, 2004), Bayesian
methods (Daume? III and Marcu, 2005; Aone et al,
1998), and heuristic methods to determine feature
weights (Schiffman, 2002; Lin and Hovy, 2002).
Graph-based methods have also been employed
(Erkan and Radev, 2004a; Erkan and Radev, 2004b;
Mihalcea, 2005; Mihalcea and Tarau, 2005; Mihal-
cea and Radev, 2006).
In 2001?02, the Document Understanding Con-
ference (DUC, 2001), issued the task of creat-
ing a 100-word summary of a single news article.
The best performing systems (Hirao et al, 2002;
Lal and Ruger, 2002) used various learning and
semantic-based methods, although no system could
outperform the baseline with statistical significance
(Nenkova, 2005). After 2002, the single-document
summarization task was dropped.
In recent years, there has been a decline in stud-
ies on automatic single-document summarization,
in part because the DUC task was dropped, and in
part because the task of single-document extracts
may be counterintuitively more difficult than multi-
448
document summarization (Nenkova, 2005). How-
ever, with the ever-growing internet and increased
information access, we believe single-document
summarization is essential to improve quick ac-
cess to large quantities of information. Recently,
CNN.com (CNN.com, 2007a) added ?Story High-
lights? to many news articles on its site to allow
readers to quickly gather information on stories.
These highlights give a brief overview of the arti-
cle and appear as 3?4 related sentences in the form
of bullet points rather than a summary paragraph,
making them even easier to quickly scan.
Our work is motivated by both the addition of
highlights to an extremely visible and reputable on-
line news source, as well as the inability of past
single-document summarization systems to outper-
form the extremely strong baseline of choosing the
first n sentences of a newswire article as the sum-
mary (Nenkova, 2005). Although some recent sys-
tems indicate an improvement over the baseline (Mi-
halcea, 2005; Mihalcea and Tarau, 2005), statistical
significance has not been shown. We show that by
using a neural network ranking algorithm and third-
party datasets to enhance sentence features, our sys-
tem, NetSum, can outperform the baseline with sta-
tistical significance.
Our paper is organized as follows. Section 2 de-
scribes our two studies: summarization and high-
light extraction. We describe our dataset in detail in
Section 3. Our ranking system and feature vectors
are outlined in Section 4. We present our evaluation
measure in Section 5. Sections 6 and 7 report on our
results on summarization and highlight extraction,
respectively. We conclude in Section 8 and discuss
future work in Section 9.
2 Our Task
In this paper, we focus on single-document summa-
rization of newswire documents. Each document
consists of three highlight sentences and the article
text. Each highlight sentence is human-generated,
but is based on the article. In Section 4 we discuss
the process of matching a highlight to an article sen-
tence. The output of our system consists of purely
extracted sentences, where we do not perform any
sentence compression or sentence generation. We
leave such extensions for future work.
We develop two separate problems based on our
document set. First, can we extract three sentences
that best ?match? the highlights as a whole? In
this task, we concatenate the three sentences pro-
duced by our system into a single summary or block,
and similarly concatenate the three highlight sen-
tences into a single summary or block. We then
compare our system?s block against the highlight
block. Second, can we extract three sentences that
best ?match? the three highlights, such that order-
ing is preserved? In this task, we produce three sen-
tences, where the first sentence is compared against
the first highlight, the second sentence is compared
against the second highlight, and the third sentence
is compared against the third highlight. Credit is
not given for producing three sentences that match
the highlights, but are out of order. The second task
considers ordering and compares sentences on an in-
dividual level, whereas the first task considers the
three chosen sentences as a summary or block and
disregards sentence order. In both tasks, we assume
the title has been seen by the reader and will be listed
above the highlights.
3 Evaluation Corpus
Our data consists of 1365 news documents gathered
from CNN.com (CNN.com, 2007a). Each document
was extracted by hand, where a maximum of 50
documents per day were collected. The documents
were hand-collected on consecutive days during the
month of February.
Each document includes the title, timestamp,
story highlights, and article text. The timestamp
on articles ranges from December 2006 to Febru-
ary 2007, since articles remain posted on CNN.com
for up to several months. The story highlights are
human-generated from the article text. The number
of story highlights is between 3?4. Since all articles
include at least 3 story highlights, we consider only
the task of extracting three highlights from each ar-
ticle.
4 Description of Our System
Our goal is to extract three sentences from a single
news document that best match various characteris-
tics of the three document highlights. One way to
identify the best sentences is to rank the sentences
449
TIMESTAMP: 1:59 p.m. EST, January 31, 2007
TITLE: Nigeria reports first human death from bird flu
HIGHLIGHT 1: Government boosts surveillance after woman dies
HIGHLIGHT 2: Egypt, Djibouti also have reported bird flu in humans
HIGHLIGHT 3: H5N1 bird flu virus has killed 164 worldwide since 2003
ARTICLE: 1. Health officials reported Nigeria?s first cases of bird flu in humans on Wednesday,
saying one woman had died and a family member had been infected but was responding to
treatment. 2. The victim, a 22-year old woman in Lagos, died January 17, Information Minister
Frank Nweke said in a statement. 3. He added that the government was boosting surveillance
across Africa?s most-populous nation after the infections in Lagos, Nigeria?s biggest city. 4.
The World Health Organization had no immediate confirmation. 5. Nigerian health officials
earlier said 14 human samples were being tested. 6. Nweke made no mention of those cases on
Wednesday. 7. An outbreak of H5N1 bird flu hit Nigeria last year, but no human infections had
been reported until Wednesday. 8. Until the Nigerian report, Egypt and Djibouti were the only
African countries that had confirmed infections among people. 9. Eleven people have died in
Egypt. 10. The bird flu virus remains hard for humans to catch, but health experts fear H5N1
may mutate into a form that could spread easily among humans and possibly kill millions in
a flu pandemic. 11. Amid a new H5N1 outbreak reported in recent weeks in Nigeria?s north,
hundreds of miles from Lagos, health workers have begun a cull of poultry. 12. Bird flu is
generally not harmful to humans, but the H5N1 virus has claimed at least 164 lives worldwide
since it began ravaging Asian poultry in late 2003, according to the WHO. 13. The H5N1 strain
had been confirmed in 15 of Nigeria?s 36 states. 14. By September, when the last known case
of the virus was found in poultry in a farm near Nigeria?s biggest city of Lagos, 915,650 birds
had been slaughtered nationwide by government veterinary teams under a plan in which the
owners were promised compensation. 15. However, many Nigerian farmers have yet to receive
compensation in the north of the country, and health officials fear that chicken deaths may be
covered up by owners reluctant to slaughter their animals. 16. Since bird flu cases were first
discovered in Nigeria last year, Cameroon, Djibouti, Niger, Ivory Coast, Sudan and Burkina
Faso have also reported the H5N1 strain of bird flu in birds. 17. There are fears that it has
spread even further than is known in Africa because monitoring is difficult on a poor continent
with weak infrastructure. 18. With sub-Saharan Africa bearing the brunt of the AIDS epidemic,
there is concern that millions of people with suppressed immune systems will be particularly
vulnerable, especially in rural areas with little access to health facilities. 19. Many people keep
chickens for food, even in densely populated urban areas.
Figure 1: Example document containing highlights
and article text. Sentences are numbered by their
position. Article is from (CNN.com, 2007b).
using a machine learning approach, for example as
in (Hirao et al, 2002). A train set is labeled such
that the labels identify the best sentences. Then a
set of features is extracted from each sentence in the
train and test sets, and the train set is used to train
the system. The system is then evaluated on the test
set. The system learns from the train set the distri-
bution of features for the best sentences and outputs
a ranked list of sentences for each document. In this
paper, we rank sentences using a neural network al-
gorithm called RankNet (Burges et al, 2005).
4.1 RankNet
From the labels and features for each sentence, we
train a model that, when run on a test set of sen-
tences, can infer the proper ranking of sentences
in a document based on information gathered dur-
ing training about sentence characteristics. To ac-
complish the ranking, we use RankNet (Burges et
al., 2005), a ranking algorithm based on neural net-
works.
RankNet is a pair-based neural network algorithm
used to rank a set of inputs, in this case, the set
of sentences in a given document. The system is
trained on pairs of sentences (Si, Sj), such that Si
should be ranked higher or equal to Sj . Pairs are
generated between sentences in a single document,
not across documents. Each pair is determined from
the input labels. Since our sentences are labeled us-
ing ROUGE (see Section 4.3), if the ROUGE score
of Si is greater than the ROUGE score of Sj , then
(Si, Sj) is one input pair. The cost function for
RankNet is the probabilistic cross-entropy cost func-
tion. Training is performed using a modified version
of the back propagation algorithm for two layer nets
(Le Cun et al, 1998), which is based on optimiz-
ing the cost function by gradient descent. A simi-
lar method of training on sentence pairs in the con-
text of multi-document summarization was recently
shown in (Toutanova et al, 2007).
Our system, NetSum, is a two-layer neural net
trained using RankNet. To speed up the performance
of RankNet, we implement RankNet in the frame-
work of LambdaRank (Burges et al, 2006). For de-
tails, see (Burges et al, 2006; Burges et al, 2005).
We experiment with between 5 and 15 hidden nodes
and with an error rate between 10?2 and 10?7.
We implement 4 versions of NetSum. The first
450
version, NetSum(b), is trained for our first sum-
marization problem (b indicates block). The pairs
are generated using the maximum ROUGE scores
l1 (see Section 4.3). The other three rankers are
trained to identify the sentence in the document
that best matches highlight n. We train one ranker,
NetSum(n), for each highlight n, for n = 1, 2, 3,
resulting in three rankers. NetSum(n) is trained us-
ing pairs generated from the l1,n ROUGE scores be-
tween sentence Si and highlight Hn (see Section
4.3).
4.2 Matching Extracted to Generated
Sentences
In this section, we describe how to determine which
sentence in the document best matches a given high-
light. Choosing three sentences most similar to the
three highlights is very challenging since the high-
lights include content that has been gathered across
sentences and even paragraphs, and furthermore in-
clude vocabulary that may not be present in the
text. Jing showed, for 300 news articles, that 19%
of human-generated summary sentences contain no
matching article sentence (Jing, 2002). In addition,
only 42% of the summary sentences match the con-
tent of a single article sentence, where there are still
semantic and syntactic transformations between the
summary sentence and article sentence.. Since each
highlight is human generated and does not exactly
match any one sentence in the document, we must
develop a method to identify how closely related a
highlight is to a sentence. We use the ROUGE (Lin,
2004b) measure to score the similarity between an
article sentence and a highlight sentence. We antic-
ipate low ROUGE scores for both the baseline and
NetSum due to the difficulty of finding a single sen-
tence to match a highlight.
4.3 ROUGE
Recall-Oriented Understudy for Gisting Evaluation
(Lin, 2004b), known as ROUGE, measures the qual-
ity of a model-generated summary or sentence by
comparing it to a ?gold-standard?, typically human-
generated, summary or sentence. It has been shown
that ROUGE is very effective for measuring both
single-document summaries and single-document
headlines (Lin, 2004a).
ROUGE-N is a N -gram recall between a model-
generated summary and a reference summary. We
use ROUGE-N , for N = 1, for labeling and evalua-
tion of our model-generated highlights.1 ROUGE-
1 and ROUGE-2 have been shown to be statisti-
cally similar to human evaluations and can be used
with a single reference summary (Lin, 2004a). We
have only one reference summary, the set of human-
generated highlights, per document. In our work,
the reference summary can be a single highlight sen-
tence or the highlights as a block. We calculate
ROUGE-N as
?
gramj?R?Si Count(gramj)
?
gramj?R Count(gramj)
, (1)
where R is the reference summary, Si is the model-
generated summary, and N is the length of the N -
gram gramj .2 The numerator cannot excede the
number of N -grams (non-unique) in R.
We label each sentence Si by its ROUGE-1 score.
For the first problem of matching the highlights
as a block, we label each Si by l1, the maximum
ROUGE-1 score between Si and each highlight Hn,
for n = 1, 2, 3, given by l1 = maxn(R(Si,Hn)).
For the second problem of matching three sen-
tences to the three highlights individually, we label
each sentence Si by l1,n, the ROUGE-1 score be-
tween Si and Hn, given by l1,n = R(Si,Hn). The
ranker for highlight n, NetSum(n), is passed sam-
ples labeled using l1,n.
4.4 Features
RankNet takes as input a set of samples, where each
sample contains a label and feature vector. The la-
bels were previously described in Section 4.3. In this
section, we describe each feature in detail and moti-
vate in part why each feature is chosen. We generate
10 features for each sentence Si in each document,
listed in Table 1. Each feature is chosen to identify
characteristics of an article sentence that may match
those of a highlight sentence. Some of the features
such as position and N -gram frequencies are com-
monly used for scoring. Sentence scoring based on
1We use an implementation of ROUGE that does not per-
form stemming or stopword removal.
2ROUGE is typically used when the length of the reference
summary is equal to length of the model-generated summary.
Our reference summary and model-generated summary are dif-
ferent lengths, so there is a slight bias toward longer sentences.
451
Symbol Feature Name
F (Si) Is First Sentence
Pos(Si) Sentence Position
SB(Si) SumBasic Score
SBb(Si) SumBasic Bigram Score
Sim(Si) Title Similarity Score
NT (Si) Average News Query Term Score
NT+(Si) News Query Term Sum Score
NTr(Si) Relative News Query Term Score
WE(Si) Average Wikipedia Entity Score
WE+(Si) Wikipedia Entity Sum Score
Table 1: Features used in our model.
sentence position, terms common with the title, ap-
pearance of keyword terms, and other cue phrases
is known as the Edmundsonian Paradigm (Edmund-
son, 1969; Alfonesca and Rodriguez, 2003; Mani,
2001). We use variations on these features as well
as a novel set of features based on third-party data.
Typically, news articles are written such that the
first sentence summarizes the article. Thus, we in-
clude a binary feature F (Si) that equals 1 if Si is
the first sentence of the document: F (Si) = ?i,1,
where ? is the Kronecker delta function. This fea-
ture is used only for NetSum(b) and NetSum(1).
We include sentence position since we found in
empirical studies that the sentence to best match
highlight H1 is on average 10% down the article, the
sentence to best match H2 is on average 20% down
the article, and the sentence to best match H3 is 31%
down the article.3 We calculate the position of Si in
document D as
Pos(Si) =
i
? , (2)
where i = {1, . . . , ?} is the sentence number and ?
is the number of sentences in D.
We include the SumBasic score (Nenkova et al,
2006) of a sentence to estimate the importance of a
sentence based on word frequency. We calculate the
SumBasic score of Si in document D as
SB(Si) =
?
w?Si p(w)
|Si|
, (3)
3Though this is not always the case, as the sentence to match
H2 precedes that to match H1 in 22.03% of documents, and the
sentence to match H3 precedes that to match H2 in 29.32% of
and precedes that to match H1 in 28.81% of documents.
where p(w) is the probability of word w and |Si| is
the number of words in sentence Si. We calculate
p(w) as p(w) = Count(w)|D| , where Count(w) is the
number of times word w appears in document D and
|D| is the number of words in document D. Note
that the score of a sentence is the average probability
of a word in the sentence.
We also include the SumBasic score over bi-
grams, where w in Eq 3 is replaced by bigrams and
we normalize by the number of bigrams in Si.
We compute the similarity of a sentence Si in doc-
ument D with the title T of D as the relative proba-
bility of title terms t ? T in Si as
Sim(Si) =
?
t?Si p(t)
|Si|
, (4)
where p(t) = Count(t)|T | is the number of times term t
appears in T over the number of terms in T .
The remaining features we use are based on third-
party data sources. Previously, third-party sources
such as WordNet (Fellbaum, 1998), the web (Ja-
galamudi et al, 2006), or click-through data (Sun
et al, 2005) have been used as features. We pro-
pose using news query logs and Wikipedia entities
to enhance features. We base several features on
query terms frequently issued to Microsoft?s news
search engine http://search.live.com/news, and enti-
ties4 found in the online open-source encyclopedia
Wikipedia (Wikipedia.org, 2007). If a query term or
Wikipedia entity appears frequently in a CNN docu-
ment, then we assume highlights should include that
term or entity since it is important on both the doc-
ument and global level. Sentences containing query
terms or Wikipedia entities therefore contain impor-
tant content. We confirm the importance of these
third-party features in Section 7.
We collected several hundred of the most fre-
quently queried terms in February 2007 from the
news query logs. We took the daily top 200 terms
for 10 days. Our hypothesis is that a sentence with
a higher number of news query terms should be a
better candidate highlight. We calculate the average
probability of news query terms q in Si as
NT (Si) =
?
q?Si p(q)
|q ? Si|
, (5)
4We define an entity as a title of a Wikipedia page.
452
where p(q) is the probability of a news term q and
|q ? Si| is the number of news terms in Si. p(q) =
Count(q)
|q?D| , where Count(q) is the number of times
term q appears in D and |q ? D| is the number of
news query terms in D.
We also include the sum of news query terms in
Si, given by NT+(Si) =
?
q?Si p(q), and the rela-
tive probability of news query terms in Si, given by
NTr(Si) =
?
q?Si
p(q)
|Si| .
We perform term disambiguation on each doc-
ument using an entity extractor (Cucerzan, 2007).
Terms are disambiguated to a Wikipedia entity
only if they match a surface form in Wikipedia.
Wikipedia surface forms are terms that disambiguate
to a Wikipedia entity and link to a Wikipedia page
with the entity as its title. For example, ?WHO? and
?World Health Org.? both refer to the World Health
Organization, and should disambiguate to the entity
?World Health Organization?. Sentences in CNN
document D that contain Wikipedia entities that fre-
quently appear in CNN document D are considered
important. We calculate the average Wikipedia en-
tity score for Si as
WE(Si) =
?
e?Si p(e)
|e ? Si|
, (6)
where p(e) is the probability of entity e, given by
p(e) = Count(e)|e?D| , where Count(e) is the number of
times entity e appears in CNN document D and |e ?
D| is the total number of entities in CNN document
D.
We also include the sum of Wikipedia entities,
given by WE+(Si) =
?
e?Si p(e).
Note that all features except position features are
a variant of SumBasic over different term sets. All
features are computed over sentences where every
word has been lowercased and punctuation has been
removed after sentence breaking. We examined us-
ing stemming, but found stemming to be ineffective.
5 Evaluation
We evaluate the performance of NetSum using
ROUGE and by comparing against a baseline sys-
tem. For the first summarization task, we compare
against the baseline of choosing the first three sen-
tences as the block summary. For the second high-
lights task, we compare NetSum(n) against the base-
line of choosing sentence n (to match highlight n).
Both tasks are novel in attempting to match high-
lights rather than a human-generated summary.
We consider ROUGE-1 to be the measure of im-
portance and thus train our model on ROUGE-1 (to
optimize ROUGE-1 scores) and likewise evaluate
our system on ROUGE-1. We list ROUGE-2 scores
for completeness, but do not expect them to be sub-
stantially better than the baseline since we did not
directly optimize for ROUGE-2.5
For every document in our corpus, we compare
NetSum?s output with the baseline output by com-
puting ROUGE-1 and ROUGE-2 between the high-
light block and NetSum and between the highlight
block and the block of sentences. Similarly, for each
highlight, we compute ROUGE-1 and ROUGE-2
between highlight n and NetSum(n) and between
highlight n and sentence n, for n = 1, 2, 3. For
each task, we calculate the average ROUGE-1 and
ROUGE-2 scores of NetSum and of the baseline.
We also report the percent of documents where the
ROUGE-1 score of NetSum is equal to or better than
the ROUGE-1 score of the baseline.
We perform all experiments using five-fold cross-
validation on our dataset of 1365 documents. We
divide our corpus into five random sets and train on
three combined sets, validate on one set, and test on
the remaining set. We repeat this procedure for ev-
ery combination of train, validation, and test sets.
Our results are the micro-averaged results on the five
test sets. For all experiments, Table 3 lists the statis-
tical tests performed and the significance of perfor-
mance differences between NetSum and the baseline
at 95% confidence.
6 Results: Summarization
We first find three sentences that, as a block, best
match the three highlights as a block. NetSum(b)
produces a ranked list of sentences for each docu-
ment. We create a block from the top 3 ranked sen-
tences. The baseline is the block of the first 3 sen-
tences of the document. A similar baseline outper-
5NetSum can directly optimize for any measure by training
on it, such as training on ROUGE-2 or on a weighted sum of
ROUGE-1 and ROUGE-2 to optimize both. Thus, ROUGE-2
scores could be further improved. We leave such studies for
future work.
453
System Av. ROUGE-1 Av. ROUGE-2
Baseline 0.4642 ? 0.0084 0.1726 ? 0.0064
NetSum(b) 0.4956 ? 0.0075 0.1775 ? 0.0066
Table 2: Results on summarization task with stan-
dard error at 95% confidence. Bold indicates signif-
icance under paired tests.
ROUGE-1 ROUGE-2
System 1 2 3 1 2 3
NetSum(b) x x x x o o
NetSum(1) x x x o o o
NetSum(2) x x x x o x
NetSum(3) x x x x x x
Table 3: Paired tests for statistical significance
at 95% confidence between baseline and NetSum
performance; 1: McNemar, 2: Paired t-test, 3:
Wilcoxon signed-rank. ?x? indicates pass, ?o? in-
dicates fail. Since our studies are pair-wise, tests
listed here are more accurate than error bars reported
in Tables 2?5.
forms all previous systems for news article summa-
rization (Nenkova, 2005) and has been used in the
DUC workshops (DUC, 2001).
For each block produced by NetSum(b) and the
baseline, we compute the ROUGE-1 and ROUGE-2
scores of the block against the set of highlights as a
block. For 73.26% of documents, NetSum(b) pro-
duces a block with a ROUGE-1 score that is equal
to or better than the baseline score. The two systems
produce blocks of equal ROUGE-1 score for 24.69%
of documents. Under ROUGE-2, NetSum(b) per-
forms equal to or better than the baseline on 73.19%
of documents and equal to the baseline on 40.51%
of documents.
Table 2 shows the average ROUGE-1 and
ROUGE-2 scores obtained with NetSum(b) and the
baseline. NetSum(b) produces a higher quality
block on average for ROUGE-1.
Table 4 lists the sentences in the block produced
by NetSum(b) and the baseline block, for the arti-
cles shown in Figure 1. The NetSum(b) summary
achieves a ROUGE-1 score of 0.52, while the base-
line summary scores only 0.36.
System Sent. # ROUGE-1
Baseline S1, S2, S3 0.36
NetSum(b) S1, S7, S15 0.52
Table 4: Block results for the block produced by
NetSum(b) and the baseline block for the exam-
ple article. ROUGE-1 scores computed against the
highlights as a block are listed.
7 Results: Highlights
Our second task is to extract three sentences from
a document that best match the three highlights in
order. To accomplish this, we train NetSum(n) for
each highlight n = 1, 2, 3. We compare NetSum(n)
with the baseline of picking the nth sentence of the
document. We perform five-fold cross-validation
across our 1365 documents. Our results are reported
for the micro-average of the test results. For each
highlight n produced by both NetSum(n) and the
baseline, we compute the ROUGE-1 and ROUGE-
2 scores against the nth highlight.
We expect that beating the baseline for n = 1 is a
more difficult task than for n = 2 or 3 since the first
sentence of a news article typically acts as a sum-
mary of the article and since we expect the first high-
light to summarize the article. NetSum(1), however,
produces a sentence with a ROUGE-1 score that is
equal to or better than the baseline score for 93.26%
of documents. The two systems produce sentences
of equal ROUGE-1 scores for 82.84% of documents.
Under ROUGE-2, NetSum(1) performs equal to or
better than the baseline on 94.21% of documents.
Table 5 shows the average ROUGE-1 and
ROUGE-2 scores obtained with NetSum(1) and the
baseline. NetSum(1) produces a higher quality sen-
tence on average under ROUGE-1.
The content of highlights 2 and 3 is typically from
later in the document, so we expect the baseline to
not perform as well in these tasks. NetSum(2) out-
performs the baseline since it is able to identify sen-
tences from further down the document as impor-
tant. For 77.73% of documents, NetSum(2) pro-
duces a sentence with a ROUGE-1 score that is equal
to or better than the score for the baseline. The two
systems produce sentences of equal ROUGE-1 score
for 33.92% of documents. Under ROUGE-2, Net-
Sum(2) performs equal to or better than the baseline
454
System Av. ROUGE-1 Av. ROUGE-2
Baseline(1) 0.4343 ? 0.0138 0.1833 ? 0.0095
NetSum(1) 0.4478 ? 0.0133 0.1857 ? 0.0085
Baseline(2) 0.2451 ? 0.0128 0.0814 ? 0.0106
NetSum(2) 0.3036 ? 0.0117 0.0877 ? 0.0107
Baseline(3) 0.1707 ? 0.0103 0.0412 ? 0.0069
NetSum(3) 0.2603 ? 0.0133 0.0615 ? 0.0075
Table 5: Results on ordered highlights task with
standard error at 95% confidence. Bold indicates
significance under paired tests.
System Sent. # ROUGE-1
Baseline S1 0.167
NetSum(1) S1 0.167
Baseline S2 0.111
NetSum(2) S1 0.556
Baseline S3 0.000
NetSum(3) S15 0.400
Table 6: Highlight results for highlight n produced
by NetSum(n) and highlight n produced by the base-
line for the example article. ROUGE-1 scores com-
puted against highlight n are listed.
84.84% of the time. For 81.09% of documents, Net-
Sum(3) produces a sentence with a ROUGE-1 score
that is equal to or better than the score for the base-
line. The two systems produce sentences of equal
ROUGE-1 score for 28.45% of documents. Under
ROUGE-2, NetSum(3) performs equal to or better
than the baseline 89.91% of the time.
Table 5 shows the average ROUGE-1 and
ROUGE-2 scores obtained for NetSum(2), Net-
Sum(3), and the baseline. Both NetSum(2) and Net-
Sum(3) produce a higher quality sentence on aver-
age under both measures.
Table 6 gives highlights produced by NetSum(n)
and the highlights produced by the baseline, for the
article shown in Figure 1. The NetSum(n) highlights
produce ROUGE-1 scores equal to or higher than the
baseline ROUGE-1 scores.
In feature ablation studies, we confirmed that the
inclusion of news-based and Wikipedia-based fea-
tures improves NetSum?s peformance. For example,
we removed all news-based and Wikipedia-based
features in NetSum(3). The resulting performance
moderately declined. Under ROUGE-1, the base-
line produced a better highlight on 22.34% of docu-
ments, versus only 18.91% when using third-party
features. Similarly, NetSum(3) produced a sum-
mary of equal or better ROUGE-1 score on only
77.66% of documents, compared to 81.09% of doc-
uments when using third-party features. In addi-
tion, the average ROUGE-1 score dropped to 0.2182
and the average ROUGE-2 score dropped to 0.0448.
The performance of NetSum with third-party fea-
tures over NetSum without third-party features is
statistically significant at 95% confidence. However,
NetSum still outperforms the baseline without third-
party features, leading us to conclude that RankNet
and simple position and term frequency features
contribute the maximum performance gains, but in-
creased ROUGE-1 and ROUGE-2 scores are a clear
benefit of third-party features.
8 Conclusions
We have presented a novel approach to automatic
single-document summarization based on neural
networks, called NetSum. Our work is the first
to use both neural networks for summarization and
third-party datasets for features, using Wikipedia
and news query logs. We have evaluated our sys-
tem on two novel tasks: 1) producing a block of
highlights and 2) producing three ordered highlight
sentences. Our experiments were run on previously
unstudied data gathered from CNN.com. Our sys-
tem shows remarkable performance over the base-
line of choosing the first n sentences of the docu-
ment, where the performance difference is statisti-
cally significant under ROUGE-1.
9 Future Work
An immediate future direction is to further explore
feature selection. We found third-party features
beneficial to the performance of NetSum and such
sources can be mined further. In addition, feature se-
lection for each NetSum system could be performed
separately since, for example, highlight 1 has differ-
ent characteristics than highlight 2.
In our experiments, ROUGE scores are fairly low
because a highlight rarely matches the content of a
single sentence. To improve NetSum?s performance,
we must consider extracting content across sentence
455
boundaries. Such work requires a system to produce
abstract summaries. We hope to incorporate sen-
tence simplification and sentence splicing and merg-
ing in a future version of NetSum.
Another future direction is the identification of
?hard? and ?easy? inputs. Although we report av-
erage ROUGE scores, such measures can be mis-
leading since some highlights are simple to match
and some are much more difficult. A better system
evaluation measure would incorporate the difficulty
of the input and weight reported results accordingly.
References
E. Alfonesca and P. Rodriguez. 2003. Description of
the uam system for generating very short summaries
at DUC?2003. In DUC 2003: Document Under-
standing Conference, May 31?June 1, 2003, Edmon-
ton, Canada.
C. Aone, M. Okurowski, and J. Gorlinsky. 1998. Train-
able scalable summarization using robust nlp and ma-
chine learning. In Proceedings of the 17th COLING
and 36th ACL.
C.J.C. Burges, T. Shaked, E. Renshaw, A. Lazier,
M. Deeds, N. Hamilton, and G. Hullender. 2005.
Learning to Rank using Gradient Descent. In Luc De
Raedt and Stefan Wrobel, editors, ICML, pages 89?96.
ACM.
C.J.C. Burges, R. Ragno, and Q. Le. 2006. Learning to
rank with nonsmooth cost functions. In NIPS 2006:
Neural Information Processing Systems, December 4-
7, 2006, Vancouver, CA.
CNN.com. 2007a. Cable news network.
http://www.cnn.com/.
CNN.com. 2007b. Nigeria reports
first human death from bird flu.
http://edition.cnn.com/2007/WORLD/africa/01/31/
nigeria.bird.flu.ap/index.html?eref=edition world.
J. Conroy, J. Schlesinger, J. Goldstein, and D. O?Leary.
2004. Left-brain/right-brain multi-document summa-
rization. In DUC 2004: Document Understanding
Workshop, May 6?7, 2004, Boston, MA, USA.
S. Cucerzan. 2007. Large scale named entity disam-
biguation based on wikipedia data. In EMNLP 2007:
Empirical Methods in Natural Language Processing,
June 28-30, 2007, Prague, Czech Republic.
H. Daume? III and D. Marcu. 2005. Bayesian multi-
document summarization at mse. In Proceedings of
MSE.
DUC. 2001. Document understanding conferences.
http://www-nlpir.nist.gov/projects/duc/index.html.
H.P. Edmundson. 1969. New methods in automatic ex-
tracting. Journal for the Association of Computing
Machinery, 16:159?165.
G. Erkan and D. R. Radev. 2004a. Lexpagerank:
Prestige in multi-document text summarization. In
EMNLP 2004: Empirical Methods in Natural Lan-
guage Processing, 2004, Barcelona, Spain.
G. Erkan and D. R. Radev. 2004b. Lexrank: Graph-
based centrality as salience in text summarization.
Journal of Artificial Intelligence Research (JAIR), 22.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
T. Hirao, Y. Sasaki, H. Isozaki, and E. Maeda. 2002.
Ntt?s text summarization system for DUC?2002. In
DUC 2002: Workshop on Text Summarization, July
11?12, 2002, Philadelphia, PA, USA.
J. Jagalamudi, P. Pingali, and V. Varma. 2006. Query
independent sentence scoring approach to DUC 2006.
In DUC 2006: Document Understanding Conference,
June 8?9, 2006, Brooklyn, NY, USA.
H. Jing. 2002. Using hidden markov modeling to de-
compose human-written summaries. Computational
Linguistics, 4(28):527?543.
J. Kupiec, J. Pererson, and F. Chen. 1995. A trainable
document summarizer. Research and Development in
Information Retrieval, pages 68?73.
P. Lal and S. Ruger. 2002. Extract-based summarization
with simplification. In DUC 2002: Workshop on Text
Summarization, July 11?12, 2002, Philadelphia, PA,
USA.
Y. Le Cun, L. Bottou, G.B. Orr, and K.R. Mu?ller.
1998. Efficient backprop. In Neural Networks, Tricks
of the Trade, Lecture Notes in Computer Science
LNCS 1524. Springer Verlag.
C.Y. Lin and E. Hovy. 2002. Automated multi-document
summarization in neats. In Proceedings of the Human
Language Technology Conference (HLT2002).
C.Y. Lin. 2004a. Looking for a few good metrics: Auto-
matic summarization evaluation ? how many samples
are enough? In Proceedings of the NTCIR Workshop
4, June 2?4, 2004, Tokyo, Japan.
C.Y. Lin. 2004b. Rouge: A package for automatic evalu-
ation of summaries. In WAS 2004: Proceedings of the
Workshop on Text Summarization Branches Out, July
25?26, 2004, Barcelona, Spain.
456
H. Luhn. 1958. The automatic creation of literature ab-
stracts. IBM Journal of Research and Development,
2(2):159?165.
I. Mani. 2001. Automatic Summarization. John Ben-
jamins Pub. Co.
R. Mihalcea and D. R. Radev, editors. 2006. Textgraphs:
Graph-based methods for NLP. New York City, NY.
R. Mihalcea and P. Tarau. 2005. An algorithm for lan-
guage independent single and multiple document sum-
marization. In Proceedings of the International Joint
Conference on Natural Language Processing (IJC-
NLP), October, 2005, Korea.
R. Mihalcea. 2005. Language independent extractive
summarization. In ACL 2005: Proceedings of the 43rd
Annual Meeting of the Association for Computational
Linguistics, June, 2005, Ann Arbor, MI, USA.
A. Nenkova, L. Vanderwende, and K. McKeown. 2006.
A compositional context sensitive multi-document
summarizer: exploring the factors that influence sum-
marization. In E. N. Efthimiadis, S. T. Dumais,
D. Hawking, and K. Ja?rvelin, editors, SIGIR, pages
573?580. ACM.
A. Nenkova. 2005. Automatic text summarization of
newswire: Lessons learned from the document un-
derstanding conference. In Proceedings of the 20th
National Conference on Artificial Intelligence (AAAI
2005), Pittsburgh, PA.
B. Schiffman. 2002. Building a resource for evaluat-
ing the importance of sentences. In Proceedings of
the Third International Conference on Language Re-
sources and Evaluation (LREC).
J.T. Sun, D. Shen, H.J. Zeng, Q. Yang, Y. Lu, and
Z. Chen. 2005. Web-page summarization using
click-through data. In R. A. Baeza-Yates, N. Ziviani,
G. Marchionini, A. Moffat, and J. Tait, editors, SIGIR.
ACM.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarla-
mudi, H. Suzuki, and L. Vanderwende. 2007. The
pythy summarization system: Microsoft research at
DUC2007. In DUC 2007: Document Understanding
Conference, April 26?27, 2007, Rochester, NY, USA.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Mi-
crosoft research at DUC2006: Task-focused summa-
rization with sentence simplification. In DUC 2006:
Document Understanding Workshop, June 8?9, 2006,
Brooklyn, NY, USA.
Wikipedia.org. 2007. Wikipedia org.
http://www.wikipedia.org.
W.T. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007. Multi-document summarization by maximizing
informative content words. In IJCAI 2007: 20th In-
ternational Joint Conference on Artificial Intelligence,
January, 2007.
457
