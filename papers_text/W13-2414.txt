Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 94?99,
Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational Linguistics
Recognition of Named Entities Boundaries in Polish Texts
Micha? Marcin?czuk and Jan Kocon?
Institute of Informatics, Wroc?aw University of Technology
Wybrzez?e Wyspian?skiego 27
Wroc?aw, Poland
{michal.marcinczuk,jan.kocon}@pwr.wroc.pl
Abstract
In the paper we discuss the problem of low
recall for the named entity (NE) recogni-
tion task for Polish. We discuss to what
extent the recall of NE recognition can be
improved by reducing the space of NE cat-
egories. We also present several exten-
sions to the binary model which give an
improvement of the recall. The extensions
include: new features, application of ex-
ternal knowledge and post-processing. For
the partial evaluation the final model ob-
tained 90.02% recall with 91.30% preci-
sion on the corpus of economic news.
1 Introduction
Named entity recognition (NER) aims at identi-
fying text fragments which refer to some objects
and assigning a category of that object from a pre-
defined set (for example: person, location, orga-
nization, artifact, other). According to the ACE
(Automatic Content Extraction) English Annota-
tion Guidelines for Entities (LDC, 2008) there are
several types of named entities, including: proper
names, definite descriptions and noun phrases.
In this paper we focus on recognition of proper
names (PNs) in Polish texts.
For Polish there are only a few accessible mod-
els for PN recognition. Marcin?czuk and Jan-
icki (2012) presented a hybrid model (a statisti-
cal model combined with some heuristics) which
obtained 70.53% recall with 91.44% precision for
a limited set of PN categories (first names, last
names, names of countries, cities and roads) tested
on the CEN corpus1 (Marcin?czuk et al, 2013).
A model for an extended set of PN categories
(56 categories) presented by Marcin?czuk et al
(2013) obtained much lower recall of 54% with
93% precision tested on the same corpus. Savary
1Home page: http://nlp.pwr.wroc.pl/cen.
and Waszczuk (2012) presented a statistical model
which obtained 76% recall with 83% precision for
names of people, places, organizations, time ex-
pressions and name derivations tested on the Na-
tional Corpus of Polish2 (Przepi?rkowski et al,
2012).
There are also several other works on PN recog-
nition for Polish where a rule-based approach was
used. Piskorski et al (2004) constructed a set of
rules and tested them on 100 news from the Rzecz-
pospolita newspaper. The rules obtained 90.6%
precision and 85.3% recall for person names and
87.9% precision and 56.6% recall for company
names. Urban?ska and Mykowiecka (2005) also
constructed a set of rules for recognition of person
and organization names. The rules were tested on
100 short texts from the Internet. The rules ob-
tained 98% precision and 89% recall for person
names and 85% precision and 73% recall for orga-
nization names. Another rule-based approach for
an extended set of proper names was presented by
Abramowicz et al (2006). The rules were tested
on 156 news from the Rzeczpospolita newspaper,
the Tygodnik Powszechny newspaper and the news
web portals. The rules obtained 91% precision and
93% recall for country names, 55% precision and
73% recall for city names, 87% precision and 70%
recall for road names and 82% precision and 66%
recall for person names.
The accessible models for PN recognition for
Polish obtain relatively good performance in terms
of precision. However, in some NLP tasks like
recognition of semantic relations between PNs
(Marcin?czuk and Ptak, 2012), coreference reso-
lution (Kopec? and Ogrodniczuk, 2012; Broda et
al., 2012a), machine translation (Gralin?ski et al,
2009a) or sensitive data anonymization (Gralin?ski
et al, 2009b) the recall is much more impor-
tant than the fine-grained categorization of PNs.
2Home page: http://nkjp.pl
94
Unfortunately, the only model recognising wide
range of PN categories obtains only 54% recall.
Therefore, our goal is to evaluate to what extent
the recall for this model can be improved.
2 Evaluation methodology
In the evaluation we used two corpora anno-
tated with 56 categories of proper names: KPWr3
(Broda et al, 2012b) and CEN (already men-
tioned in Section 1). The KPWr corpus consists of
747 documents containing near 200K tokens and
16.5K NEs. The CEN corpus consists of 797 doc-
uments containing 148K tokens and 13.6K NEs.
Both corpora were tagged using the morphologi-
cal tagger WCRFT (Radziszewski, 2013).
We used a 10-fold cross validation on the KPWr
corpus to select the optimal model. The CEN cor-
pus was used for a cross-corpus evaluation of the
selected model. In this case the model was trained
on the KPWr corpus and evaluated on the CEN
corpus. We presented results for strict and partial
matching evaluation (Chinchor, 1992). The ex-
periments were conducted using an open-source
framework for named entity recognition called
Liner24 (Marcin?czuk et al, 2013).
3 Reduction of NE categories
In this section we investigate to what extent the re-
call of NE recognition can be improved by reduc-
ing the number of NE categories. As a reference
model we used the statistical model presented by
Marcin?czuk and Janicki (2012). The model uses
the Conditional Random Fields method and uti-
lize four types of features, i.e. orthographic (18
features), morphological (6 features), wordnet (4
features) and lexicon (10 features) ? 38 features
in total. The model uses only local features from
a window of two preceding and two following to-
kens. The detailed description of the features is
presented in Marcin?czuk et al (2013). We did
not used any post-processing methods described
by Marcin?czuk and Janicki (2012) (unambiguous
gazetteer chunker, heuristic chunker) because they
were tuned for the specific set of NE categories.
We have evaluated two schemas with a limited
number of the NE categories. In the first more
common (Finkel et al, 2005) schema, all PNs
are divided into four MUC categories, i.e. per-
son, organization, location and other. In the other
3Home page: http://nlp.pwr.wroc.pl/kpwr.
4http://nlp.pwr.wroc.pl/liner2
schema, assuming a separate phases for PN recog-
nition and classification (Al-Rfou? and Skiena,
2012), we mapped all the PN categories to a single
category, namely NAM.
For the MUC schema we have tested two ap-
proaches. In the first approach we trained a sin-
gle classifier for all the NE categories and in the
second approach we trained four classifiers ? one
for each category. This way we have evaluated
three models: Multi-MUC ? a cascade of four
classifiers, one classifier for every NE category;
One-MUC ? a single classifier for all MUC cat-
egories; One-NAM ? a single classifier for NAM
category.
Model P R F
Multi-MUC 76.09% 57.41% 65.44%
One-MUC 70.66% 65.39% 67.92%
One-NAM 80.46% 78.59% 79.52%
Table 1: Strict evaluation of the three NE models
For each model we performed the 10-fold cross-
validation on the KPWr corpus and the results are
presented in Table 1. As we expected the high-
est performance was obtained for the One-NAM
model where the problem of PN classification was
ignored. The model obtained recall of 78% with
80% precision. The results also show that the lo-
cal features used in the model are insufficient to
predict the PN category.
4 Improving the binary model
In this section we present and evaluate several ex-
tensions which were introduced to the One-NAM
model in order to increase its recall. The exten-
sions include: new features, application of exter-
nal resources and post processing.
4.1 Extensions
4.1.1 Extended gazetteer features
The reference model (Marcin?czuk and Janicki,
2012) uses only five gazetteers of PNs (first names,
last names, names of countries, cities and roads).
To include the other categories of PNs we used two
existing resources: a gazetteer of proper names
called NELexicon5 containing ca. 1.37 million
of forms and a gazetteer of PNs extracted from
the National Corpus of Polish6 containing 153,477
5http://nlp.pwr.wroc.pl/nelexicon.
6http://clip.ipipan.waw.pl/Gazetteer
95
forms. The categories of PNs were mapped into
four MUC categories: person, location, organi-
zation and other. The numbers of PNs for each
category are presented in Table 2.
Category Symbol Form count
person per 455,376
location loc 156,886
organization org 832,339
other oth 13,612
TOTAL 1,441,634
Table 2: The statistics of the gazetteers.
We added four features, one for every category.
The features were defined as following:
gaz(n, c) =
?
?
?
?
??
?
?
?
??
B if n-th token starts a sequence of words
found in gazetteer c
I if n-th token is part of a sequence of
words found in gazetteer c excluding
the first token
0 otherwise
where c ? {per, loc, org, oth} and n is the token
index in a sentence. If two or more PNs from the
same gazetteer overlap, then the first and longest
PN is taken into account.
4.1.2 Trigger features
A trigger is a word which can indicate presence
of a proper name. Triggers can be divided into
two groups: external (appear before or after PNs)
and internal (are part of PNs). We used a lexi-
con of triggers called PNET (Polish Named En-
tity Triggers)7. The lexicon contains 28,000 in-
flected forms divided into 8 semantic categories
(bloc, country, district, geogName, orgName, per-
sName, region and settlement) semi-automatically
extracted from Polish Wikipedia8. We divided the
lexicon into 16 sets ? two for every semantic cat-
egory (with internal and external triggers). We de-
fined one feature for every lexicon what gives 16
features in total. The feature were defined as fol-
lowing:
trigger(n, s) =
?
??
??
1 if n-th token base is found
in set s
0 otherwise
7http://zil.ipipan.waw.pl/PNET.
8http://pl.wikipedia.org
4.1.3 Agreement feature
An agreement of the morphological attributes be-
tween two consecutive words can be an indicator
of phrase continuity. This observation was used by
Radziszewski and Pawlaczek (2012) to recognize
noun phrases. This information can be also help-
ful in PN boundaries recognition. The feature was
defined as following:
agr(n) =
?
?
??
?
??
1 if number[n] = number[n? 1]
and case[n] = case[n? 1]
and gender[n] = gender[n? 1]
0 otherwise
The agr(n) feature for a token n has value 1
when the n-th and n ? 1-th words have the same
case, gender and number. In other cases the value
is 0. If one of the attributes is not set, the value is
also 0.
4.1.4 Unambiguous gazetteer look-up
There are many proper names which are well
known and can be easily recognized using
gazetteers. However, some of the proper names
present in the gazetteers can be also common
words. In order to avoid this problem we used an
unambiguous gazetteer look-up (Marcin?czuk and
Janicki, 2012). We created one gazetteer contain-
ing all categories of PNs (see Section 4.1.1) and
discarded all entries which were found in the SJP
dictionary9 in a lower case form.
4.1.5 Heuristics
We created several simple rules to recognize PNs
on the basis of the orthographic features. The fol-
lowing phrases are recognized as proper names re-
gardless the context:
? a camel case word ? a single word contain-
ing one or more internal upper case letters
and at least one lower case letter, for exam-
ple RoboRally ? a name of board game,
? a sequence of words in the quotation
marks ? the first word must be capitalised
and shorter than 5 characters to avoid match-
ing ironic or apologetic words and citations,
? a sequence of all-uppercase words ? we
discard words which are roman numbers and
ignore all-uppercase sentences.
9http://www.sjp.pl/slownik/ort.
96
4.1.6 Names propagation
The reference model does not contain any
document-based features. This can be a prob-
lem for documents where the proper names oc-
cur several times but only a few of its occur-
rences are recognised by the statistical model. The
other may not be recognized because of the un-
seen or unambiguous contexts. In such cases the
global information about the recognized occur-
rences could be used to recognize the other unrec-
ognized names. However, a simple propagation of
all recognized names might cause loss in the preci-
sion because of the common words which are also
proper names. To handle this problem we defined
a set of patterns and propagate only those proper
names which match one of the following pattern:
(1) a sequence of two or more capitalised words;
(2) all-uppercase word ended with a number; or
(3) all-uppercase word ended with hyphen and in-
flectional suffix.
4.2 Evaluation
Table 3 contains results of the 10-fold cross valida-
tion on the KPWr corpus for the One-NAM model,
One-NAM with every single extension and a com-
plete model with all extensions. The bold values
indicate an improvement comparing to the base
One-NAM model. To check the statistical signif-
icance of precision, recall and F-measure differ-
ence we used Student?s t-test with a significance
level ? = 0.01 (Dietterich, 1998). The asterisk
indicates the statistically significant improvement.
Model P R F
One-NAM 80.46% 78.59% 79.52%
Gazetteers 80.60% 78.71% 79.64%
Triggers 80.60% 78.58% 79.58%
Agreement 80.73% 78.90% 79.80%
Look-up 80.18% 79.56%* 79.87%
Heuristics 79.98% 79.20%* 79.59%
Propagate 80.46% 78.59% 79.52%
Complete 80.33% 80.61%* 80.47%*
Table 3: The 10-fold cross validation on the KPWr
corpus for One-NAM model with different exten-
sions.
Five out of six extensions improved the perfor-
mance. Only for the name propagation we did
not observe any improvement because the KPWr
corpus contains only short documents (up to 300
words) and it is uncommon that a name will appear
more than one time in the same fragment. How-
ever, tests on random documents from the Internet
showed the usefulness of this extension.
For the unambiguous gazetteer look-up and the
heuristics we obtained a statistically significant
improvement of the recall. In the final model we
included all the presented extensions. The final
model achieved a statistically significant improve-
ment of the recall and the F-measure.
To check the generality of the extensions, we
performed the cross-domain evaluation on the
CEN corpus (see Section 2). The results for the
56nam, the One-NAM and the Improved One-
NAM models are presented in Table 4. For the
strict evaluation, the recall was improved by al-
most 4 percentage points with a small precision
improvement by almost 2 percentage points.
Evaluation P R F
56nam model (Marcin?czuk et al, 2013)
Strict 93% 54% 68%
One-NAM model
Strict 85.98% 81.31% 83.58%
Partial 91.12% 86.65% 88.83%
Improved One-NAM model
Strict 86.61% 85.05% 85.82%
Partial 91.30% 90.02% 90.65%
Table 4: The cross-domain evaluation of the basic
and improved One-NAM models on CEN.
5 Conclusions
In the paper we discussed the problem of low re-
call of models for recognition of a wide range of
PNs for Polish. We tested to what extent the reduc-
tion of the PN categories can improve the recall.
As we expected the model without PN classifica-
tion obtained the best results in terms of precision
and recall.
Then we presented a set of extensions to the
One-NAM model, including new features (mor-
phological agreement, triggers, gazetteers), ap-
plication of external knowledge (a set of heuris-
tics and a gazetteer-based recogniser) and post-
processing (proper names propagation). The final
model obtained 90.02% recall with 91.30% preci-
sion on the CEN corpus for the partial evaluation
what is a good start of further NE categorization
phase.
97
Acknowledgments
This work was financed by Innovative Economy
Programme project POIG.01.01.02-14-013/09.
References
Witold Abramowicz, Agata Filipowska, Jakub Pisko-
rski, Krzysztof We?cel, and Karol Wieloch. 2006.
Linguistic Suite for Polish Cadastral System. In
Proceedings of the LREC?06, pages 53?58, Genoa,
Italy.
Rami Al-Rfou? and Steven Skiena. 2012. SpeedRead:
A fast named entity recognition pipeline. In Pro-
ceedings of COLING 2012, pages 51?66, Mumbai,
India, December. The COLING 2012 Organizing
Committee.
Bartosz Broda, Lukasz Burdka, and Marek Maziarz.
2012a. Ikar: An improved kit for anaphora resolu-
tion for polish. In Martin Kay and Christian Boitet,
editors, COLING (Demos), pages 25?32. Indian In-
stitute of Technology Bombay.
Bartosz Broda, Micha? Marcin?czuk, Marek Maziarz,
Adam Radziszewski, and Adam Wardyn?ski. 2012b.
KPWr: Towards a Free Corpus of Polish. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet Ug?ur Dog?an, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of LREC?12. ELRA.
Nancy Chinchor. 1992. MUC-4 Evaluation Metrics.
In Proceedings of the Fourth Message Understand-
ing Conference, pages 22?29.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learn-
ing algorithms. Neural Computation, 10(7):1895?
1924.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2005. Incorporating Non-local
Information into Information Extraction Systems by
Gibbs Sampling. In The Association for Com-
puter Linguistics, editor, Proceedings of the 43nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2005), pages 363?370.
Filip Gralin?ski, Krzysztof Jassem, and Micha? Mar-
cin?czuk. 2009a. An Environment for Named En-
tity Recognition and Translation. In L M?rquez and
H Somers, editors, Proceedings of the 13th Annual
Conference of the European Association for Ma-
chine Translation, pages 88?95, Barcelona, Spain.
Filip Gralin?ski, Krzysztof Jassem, Micha? Marcin?czuk,
and Pawe? Wawrzyniak. 2009b. Named Entity
Recognition in Machine Anonymization. In M A
K?opotek, A Przepi?rkowski, A T Wierzchon?, and
K Trojanowski, editors, Recent Advances in Intel-
ligent Information Systems., pages 247?260. Aca-
demic Pub. House Exit.
Mateusz Kopec? and Maciej Ogrodniczuk. 2012.
Creating a coreference resolution system for pol-
ish. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Ug?ur
Dog?an, Bente Maegaard, Joseph Mariani, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
LDC. 2008. ACE (Automatic Content Extraction) En-
glish Annotation Guidelines for Relations (Version
6.2).
Micha? Marcin?czuk and Maciej Janicki. 2012. Opti-
mizing CRF-Based Model for Proper Name Recog-
nition in Polish Texts. In Alexander F. Gelbukh, ed-
itor, CICLing (1), volume 7181 of Lecture Notes in
Computer Science, pages 258?269. Springer.
Micha? Marcin?czuk, Jan Kocon?, and Maciej Janicki.
2013. Liner2 - A Customizable Framework for
Proper Names Recognition for Polish. In Robert
Bembenik, ?ukasz Skonieczny, Henryk Rybin?ski,
Marzena Kryszkiewicz, and Marek Niezg?dka, ed-
itors, Intelligent Tools for Building a Scientific In-
formation Platform, volume 467 of Studies in Com-
putational Intelligence, pages 231?253. Springer.
Micha? Marcin?czuk and Marcin Ptak. 2012. Prelimi-
nary study on automatic induction of rules for recog-
nition of semantic relations between proper names
in polish texts. In Petr Sojka, Ales Hor?k, Ivan
Kopecek, and Karel Pala, editors, Text, Speech and
Dialogue ? 15th International Conference, TSD
2012, Brno, Czech Republic, September 3-7, 2012.
Proceedings, volume 7499 of Lecture Notes in Arti-
ficial Intelligence (LNAI). Springer-Verlag, Septem-
ber.
Jakub Piskorski, Peter Homola, Ma?gorzata Marciniak,
Agnieszka Mykowiecka, Adam Przepi?rkowski,
and Marcin Wolin?ski. 2004. Information Extrac-
tion for Polish Using the SProUT Platform. In
Mieczyslaw A. K?opotek, Slawomir T. Wierzchon?,
and Krzysztof Trojanowski, editors, Intelligent In-
formation Processing and Web Mining, Proceed-
ings of the International IIS: IIPWM?04 Conference,
Advances in Soft Computing, Zakopane. Springer-
Verlag.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L.
G?rski, and Barbara Lewandowska-Tomaszczyk,
editors. 2012. Narodowy Korpus Je?zyka Polskiego
[Eng.: National Corpus of Polish]. Wydawnictwo
Naukowe PWN, Warsaw.
Adam Radziszewski and Adam Pawlaczek. 2012.
Large-Scale Experiments with NP Chunking of Pol-
ish. In Petr Sojka, Ale? Hor?k, Ivan Kopec?ek,
and Karel Pala, editors, TSD, volume 7499 of Lec-
ture Notes in Computer Science, pages 143?149.
Springer Berlin Heidelberg.
98
Adam Radziszewski. 2013. A Tiered CRF Tagger for
Polish. In Robert Bembenik, ?ukasz Skonieczny,
Henryk Rybin?ski, Marzena Kryszkiewicz, and
Marek Niezg?dka, editors, Intelligent Tools for
Building a Scientific Information Platform, volume
467 of Studies in Computational Intelligence, pages
215?230. Springer Berlin Heidelberg.
Agata Savary and Jakub Waszczuk. 2012. Narze?dzia
do anotacji jednostek nazewniczych. In Adam
Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors,
Narodowy Korpus Je?zyka Polskiego. Wydawnictwo
Naukowe PWN. Creative Commons Uznanie Au-
torstwa 3.0 Polska.
Dominika Urban?ska and Agnieszka Mykowiecka.
2005. Multi-words Named Entity Recognition in
Polish texts. In Radovan Grab?k, editor, SLOVKO
2005 ? Third International Seminar Computer
Treatment of Slavic and East European Languages,
Bratislava, Slovakia, pages 208?215. VEDA.
99
