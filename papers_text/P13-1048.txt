Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 486?496,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Combining Intra- and Multi-sentential Rhetorical Parsing for
Document-level Discourse Analysis
Shafiq Joty?
sjoty@qf.org.qa
Qatar Computing Research Institute
Qatar Foundation
Doha, Qatar
Giuseppe Carenini, Raymond Ng, Yashar Mehdad
{carenini, rng, mehdad}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, Canada
Abstract
We propose a novel approach for develop-
ing a two-stage document-level discourse
parser. Our parser builds a discourse tree
by applying an optimal parsing algorithm
to probabilities inferred from two Con-
ditional Random Fields: one for intra-
sentential parsing and the other for multi-
sentential parsing. We present two ap-
proaches to combine these two stages of
discourse parsing effectively. A set of
empirical evaluations over two different
datasets demonstrates that our discourse
parser significantly outperforms the state-
of-the-art, often by a wide margin.
1 Introduction
Discourse of any kind is not formed by inde-
pendent and isolated textual units, but by related
and structured units. Discourse analysis seeks
to uncover such structures underneath the surface
of the text, and has been shown to be benefi-
cial for text summarization (Louis et al, 2010;
Marcu, 2000b), sentence compression (Sporleder
and Lapata, 2005), text generation (Prasad et al,
2005), sentiment analysis (Somasundaran, 2010)
and question answering (Verberne et al, 2007).
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988), one of the most influential the-
ories of discourse, represents texts by labeled hier-
archical structures, called Discourse Trees (DTs),
as exemplified by a sample DT in Figure 1. The
leaves of a DT correspond to contiguous Elemen-
tary Discourse Units (EDUs) (six in the exam-
ple). Adjacent EDUs are connected by rhetori-
cal relations (e.g., Elaboration, Contrast), form-
ing larger discourse units (represented by internal
?This work was conducted at the University of British
Columbia, Vancouver, Canada.
nodes), which in turn are also subject to this re-
lation linking. Discourse units linked by a rhetori-
cal relation are further distinguished based on their
relative importance in the text: nucleus being the
central part, whereas satellite being the peripheral
one. Discourse analysis in RST involves two sub-
tasks: discourse segmentation is the task of identi-
fying the EDUs, and discourse parsing is the task
of linking the discourse units into a labeled tree.
While recent advances in automatic discourse
segmentation and sentence-level discourse parsing
have attained accuracies close to human perfor-
mance (Fisher and Roark, 2007; Joty et al, 2012),
discourse parsing at the document-level still poses
significant challenges (Feng and Hirst, 2012) and
the performance of the existing document-level
parsers (Hernault et al, 2010; Subba and Di-
Eugenio, 2009) is still considerably inferior com-
pared to human gold-standard. This paper aims
to reduce this performance gap and take discourse
parsing one step further. To this end, we address
three key limitations of existing parsers as follows.
First, existing discourse parsers typically model
the structure and the labels of a DT separately
in a pipeline fashion, and also do not consider
the sequential dependencies between the DT con-
stituents, which has been recently shown to be crit-
ical (Feng and Hirst, 2012). To address this limi-
tation, as the first contribution, we propose a novel
document-level discourse parser based on proba-
bilistic discriminative parsing models, represented
as Conditional Random Fields (CRFs) (Sutton et
al., 2007), to infer the probability of all possible
DT constituents. The CRF models effectively rep-
resent the structure and the label of a DT con-
stituent jointly, and whenever possible, capture the
sequential dependencies between the constituents.
Second, existing parsers apply greedy and sub-
optimal parsing algorithms to build the DT for a
document. To cope with this limitation, our CRF
models support a probabilistic bottom-up parsing
486
But he added:
"Some people use the purchasers?
 index as a leading indicator, some use it as a coincident indicator. But the thing it?s supposed to measure -- manufacturing strength --
it missed altogether last month." <P>Elaboration
Same-UnitContrast
Contrast
Attribution
(1)
(2) (3)
(4) (5)
(6)
Figure 1: Discourse tree for two sentences in RST-DT. Each of the sentences contains three EDUs. The
second sentence has a well-formed discourse tree, but the first sentence does not have one.
algorithm which is non-greedy and optimal.
Third, existing discourse parsers do not dis-
criminate between intra-sentential (i.e., building
the DTs for the individual sentences) and multi-
sentential parsing (i.e., building the DT for the
document). However, we argue that distinguish-
ing between these two conditions can result in
more effective parsing. Two separate parsing
models could exploit the fact that rhetorical re-
lations are distributed differently intra-sententially
vs. multi-sententially. Also, they could indepen-
dently choose their own informative features. As
another key contribution of our work, we devise
two different parsing components: one for intra-
sentential parsing, the other for multi-sentential
parsing. This provides for scalable, modular and
flexible solutions, that can exploit the strong cor-
relation observed between the text structure (sen-
tence boundaries) and the structure of the DT.
In order to develop a complete and robust dis-
course parser, we combine our intra-sentential
and multi-sentential parsers in two different ways.
Since most sentences have a well-formed dis-
course sub-tree in the full document-level DT (for
example, the second sentence in Figure 1), our first
approach constructs a DT for every sentence us-
ing our intra-sentential parser, and then runs the
multi-sentential parser on the resulting sentence-
level DTs. However, this approach would disre-
gard those cases where rhetorical structures vio-
late sentence boundaries. For example, consider
the first sentence in Figure 1. It does not have a
well-formed sub-tree because the unit containing
EDUs 2 and 3 merges with the next sentence and
only then is the resulting unit merged with EDU
1. Our second approach, in an attempt of dealing
with these cases, builds sentence-level sub-trees
by applying the intra-sentential parser on a sliding
window covering two adjacent sentences and by
then consolidating the results produced by over-
lapping windows. After that, the multi-sentential
parser takes all these sentence-level sub-trees and
builds a full rhetorical parse for the document.
While previous approaches have been tested on
only one corpus, we evaluate our approach on
texts from two very different genres: news articles
and instructional how-to-do manuals. The results
demonstrate that our contributions provide con-
sistent and statistically significant improvements
over previous approaches. Our final result com-
pares very favorably to the result of state-of-the-art
models in document-level discourse parsing.
In the rest of the paper, after discussing related
work in Section 2, we present our discourse pars-
ing framework in Section 3. In Section 4, we de-
scribe the intra- and multi-sentential parsing com-
ponents. Section 5 presents the two approaches
to combine the two stages of parsing. The exper-
iments and error analysis, followed by future di-
rections are discussed in Section 6. Finally, we
summarize our contributions in Section 7.
2 Related work
The idea of staging document-level discourse
parsing on top of sentence-level discourse parsing
was investigated in (Marcu, 2000a; LeThanh et al,
2004). These approaches mainly rely on discourse
markers (or cues), and use hand-coded rules to
build DTs for sentences first, then for paragraphs,
and so on. However, often rhetorical relations
are not explicitly signaled by discourse markers
(Marcu and Echihabi, 2002), and discourse struc-
tures do not always correspond to paragraph struc-
tures (Sporleder and Lascarides, 2004). Therefore,
rather than relying on hand-coded rules based on
discourse markers, recent approaches employ su-
pervised machine learning techniques with a large
set of informative features.
Hernault et al, (2010) presents the publicly
available HILDA parser. Given the EDUs in a doc-
487
Elaboration Joint AttributionSame-Unit Contrast Explanation
0
5
10
15
20
25
30 Multi-sentential
Intra-sentential
Figure 2: Distributions of six most frequent relations in
intra-sentential and multi-sentential parsing scenarios.
ument, HILDA iteratively employs two Support
Vector Machine (SVM) classifiers in pipeline to
build the DT. In each iteration, a binary classifier
first decides which of the adjacent units to merge,
then a multi-class classifier connects the selected
units with an appropriate relation label. They eval-
uate their approach on the RST-DT corpus (Carl-
son et al, 2002) of news articles. On a different
genre of instructional texts, Subba and Di-Eugenio
(2009) propose a shift-reduce parser that relies on
a classifier for relation labeling. Their classifier
uses Inductive Logic Programming (ILP) to learn
first-order logic rules from a set of features includ-
ing compositional semantics. In this work, we ad-
dress the limitations of these models (described in
Section 1) introducing our novel discourse parser.
3 Our Discourse Parsing Framework
Given a document with sentences already seg-
mented into EDUs, the discourse parsing prob-
lem is determining which discourse units (EDUs
or larger units) to relate (i.e., the structure), and
how to relate them (i.e., the labels or the discourse
relations) in the resulting DT. Since we already
have an accurate sentence-level discourse parser
(Joty et al, 2012), a straightforward approach to
document-level parsing could be to simply apply
this parser to the whole document. However this
strategy would be problematic because of scalabil-
ity and modeling issues. Note that the number of
valid trees grows exponentially with the number
of EDUs in a document.1 Therefore, an exhaus-
tive search over the valid trees is often unfeasible,
even for relatively small documents.
For modeling, the problem is two-fold. On the
one hand, it appears that rhetorical relations are
distributed differently intra-sententially vs. multi-
sententially. For example, Figure 2 shows a com-
parison between the two distributions of six most
1For n + 1 EDUs, the number of valid discourse trees is
actually the Catalan number Cn.
model
AlgorithmSentences segmented into EDUs
Document-level
 discourse tree
Intra-sententialparser Multi-sententialparser
model
Algorithm
Figure 3: Discourse parsing framework.
frequent relations on a development set containing
20 randomly selected documents from RST-DT.
Notice that relations Attribution and Same-Unit
are more frequent than Joint in intra-sentential
case, whereas Joint is more frequent than the other
two in multi-sentential case. On the other hand,
different kinds of features are applicable and in-
formative for intra-sentential vs. multi-sentential
parsing. For example, syntactic features like dom-
inance sets (Soricut and Marcu, 2003) are ex-
tremely useful for sentence-level parsing, but are
not even applicable in multi-sentential case. Like-
wise, lexical chain features (Sporleder and Las-
carides, 2004), that are useful for multi-sentential
parsing, are not applicable at the sentence level.
Based on these observations, our discourse
parsing framework comprises two separate mod-
ules: an intra-sentential parser and a multi-
sentential parser (Figure 3). First, the intra-
sentential parser produces one or more discourse
sub-trees for each sentence. Then, the multi-
sentential parser generates a full DT for the doc-
ument from these sub-trees. Both of our parsers
have the same two components: a parsing model
assigns a probability to every possible DT, and
a parsing algorithm identifies the most probable
DT among the candidate DTs in that scenario.
While the two models are rather different, the
same parsing algorithm is shared by the two mod-
ules. Staging multi-sentential parsing on top of
intra-sentential parsing in this way allows us to ex-
ploit the strong correlation between the text struc-
ture and the DT structure as explained in detail in
Section 5. Before describing our parsing models
and the parsing algorithm, we introduce some ter-
minology that we will use throughout the paper.
Following (Joty et al, 2012), a DT can be for-
mally represented as a set of constituents of the
form R[i,m, j], referring to a rhetorical relation
R between the discourse unit containing EDUs i
through m and the unit containing EDUs m+1
through j. For example, the DT for the sec-
ond sentence in Figure 1 can be represented as
488
{Elaboration-NS[4,4,5], Same-Unit-NN[4,5,6]}.
Notice that a relation R also specifies the nuclear-
ity statuses of the discourse units involved, which
can be one of Nucleus-Satellite (NS), Satellite-
Nucleus (SN) and Nucleus-Nucleus (NN).
4 Parsing Models and Parsing Algorithm
The job of our intra-sentential and multi-sentential
parsing models is to assign a probability to each
of the constituents of all possible DTs at the sen-
tence level and at the document level, respectively.
Formally, given the model parameters ?, for each
possible constituent R[i,m, j] in a candidate DT
at the sentence or document level, the parsing
model estimates P (R[i,m, j]|?), which specifies
a joint distribution over the label R and the struc-
ture [i,m, j] of the constituent.
4.1 Intra-Sentential Parsing Model
Recently, we proposed a novel parsing model
for sentence-level discourse parsing (Joty et
al., 2012), that outperforms previous approaches
by effectively modeling sequential dependencies
along with structure and labels jointly. Below we
briefly describe the parsing model, and show how
it is applied to obtain the probabilities of all possi-
ble DT constituents at the sentence level.
Figure 4 shows the intra-sentential parsing
model expressed as a Dynamic Conditional Ran-
dom Field (DCRF) (Sutton et al, 2007). The ob-
served nodes Uj in a sequence represent the dis-
course units (EDUs or larger units). The first layer
of hidden nodes are the structure nodes, where
Sj?{0, 1} denotes whether two adjacent discourse
units Uj?1 and Uj should be connected or not.
The second layer of hidden nodes are the relation
nodes, with Rj?{1 . . .M} denoting the relation
between two adjacent unitsUj?1 andUj , whereM
is the total number of relations in the relation set.
The connections between adjacent nodes in a hid-
den layer encode sequential dependencies between
the respective hidden nodes, and can enforce con-
straints such as the fact that a Sj= 1 must not fol-
low a Sj?1= 1. The connections between the two
hidden layers model the structure and the relation
of a DT (sentence-level) constituent jointly.
To obtain the probability of the constituents
of all candidate DTs for a sentence, we apply
the parsing model recursively at different levels
of the DT and compute the posterior marginals
over the relation-structure pairs. To illustrate the
U UU U U
2
2
2
3 j t-1 t
SS S S S
R R R R R
3
3 j
j t-1
t-1 t
Unit sequenceat level i
Structure sequence
Relationsequence
U1
t
Figure 4: A chain-structured DCRF as our intra-
sentential parsing model.
process, let us assume that the sentence contains
four EDUs. At the first (bottom) level, when all
the units are the EDUs, there is only one possible
unit sequence to which we apply our DCRF
model (Figure 5(a)). We compute the posterior
marginals P (R2, S2=1|e1, e2, e3, e4,?), P (R3,
S3=1|e1, e2, e3, e4,?) and P (R4, S4=1|e1, e2, e3,
e4,?) to obtain the probability of the con-
stituents R[1, 1, 2], R[2, 2, 3] and R[3, 3, 4],
respectively. At the second level, there are
three possible unit sequences (e1:2, e3, e4),
(e1,e2:3, e4) and (e1,e2,e3:4). Figure 5(b) shows
their corresponding DCRFs. The posterior
marginals P (R3, S3=1|e1:2,e3,e4,?), P (R2:3
S2:3=1|e1,e2:3,e4,?), P (R4, S4=1|e1,e2:3,e4,?)
and P (R3:4, S3:4=1|e1,e2,e3:4,?) computed from
the three sequences correspond to the probability
of the constituents R[1, 2, 3], R[1, 1, 3], R[2, 3, 4]
and R[2, 2, 4], respectively. Similarly, we attain
the probability of the constituents R[1, 1, 4],
R[1, 2, 4] and R[1, 3, 4] by computing their
respective posterior marginals from the three
possible sequences at the third (top) level.
e 1 e e2
2
2
3
S S3
R R3
(a)
e 1e
S
R
1:2 3
3
3
e e
S
R
2:3
2:3
(b)
2:3e4
S4
R4
e4
S4
R4
e4
S4
R4
1e e
S
R
2
2
2 e3:4
S3:4
R3:4
1 e
S
R
1:3 4
4
4
e e
S
R
2:4
2:4
(c)
2:4 ee
S
R
1:2e
3:4
3:4
3:4
(i) (ii)
(iii)(i) (ii) (iii)
Figure 5: Our parsing model applied to the sequences at
different levels of a sentence-level DT. (a) Only possible se-
quence at the first level, (b) Three possible sequences at the
second level, (c) Three possible sequences at the third level.
At this point what is left to be explained is
how we generate all possible sequences for a
given number of EDUs in a sentence. Algorithm
1 demonstrates how we do that. More specifi-
cally, to compute the probabilities of each DT con-
489
stituent R[i, k, j], we need to generate sequences
like (e1, ? ? ? , ei?1, ei:k, ek+1:j , ej+1, ? ? ? , en) for
1 ? i ? k < j ? n. In doing so, we may
generate some duplicate sequences. Clearly, the
sequence (e1, ? ? ? , ei?1, ei:i, ei+1:j , ej+1, ? ? ? , en)
for 1 ? i ? k < j < n is already considered
for computing the probability of R[i+ 1, j, j+ 1].
Therefore, it is a duplicate sequence that we ex-
clude from our list of all possible sequences.
Input: Sequence of EDUs: (e1, e2, ? ? ? , en)
Output: List of sequences: L
for i = 1? n? 1 do
for j = i+ 1? n do
if j == n then
for k = i? j ? 1 do
L.append
((e1, .., ei?1, ei:k, ek+1:j , ej+1, .., en))
end
else
for k = i+ 1? j ? 1 do
L.append
((e1, .., ei?1, ei:k, ek+1:j , ej+1, .., en))
end
end
end
end
Algorithm 1: Generating all possible sequences
for a sentence with n EDUs.
Once we obtain the probability of all possible
DT constituents, the discourse sub-trees for the
sentences are built by applying an optimal prob-
abilistic parsing algorithm (Section 4.4) using one
of the methods described in Section 5.
4.2 Multi-Sentential Parsing Model
Given the discourse units (sub-trees) for all the
sentences of a document, a simple approach to
build the rhetorical tree of the document would be
to apply a new DCRF model, similar to the one
in Figure 4 (with different parameters), to all the
possible sequences generated from these units to
infer the probability of all possible higher-order
constituents. However, the number of possible se-
quences and their length increase with the number
of sentences in a document. For example, assum-
ing that each sentence has a well-formed DT, for
a document with n sentences, Algorithm 1 gener-
ates O(n3) sequences, where the sequence at the
bottom level has n units, each of the sequences at
the second level has n-1 units, and so on. Since
the model in Figure 4 has a ?fat? chain structure,
U Ut-1 t
S
R t
Adjacent Unitsat level i
Structure
Relation
t
Figure 6: A CRF as a multi-sentential parsing model.
we could use forwards-backwards algorithm for
exact inference in this model (Sutton and McCal-
lum, 2012). However, forwards-backwards on a
sequence containing T units costs O(TM2) time,
where M is the number of relations in our rela-
tion set. This makes the chain-structured DCRF
model impractical for multi-sentential parsing of
long documents, since learning requires to run in-
ference on every training sequence with an overall
time complexity of O(TM2n3) per document.
Our model for multi-sentential parsing is shown
in Figure 6. The two observed nodes Ut?1 and
Ut are two adjacent discourse units. The (hidden)
structure node S?{0, 1} denotes whether the two
units should be connected or not. The hidden node
R?{1 . . .M} represents the relation between the
two units. Notice that like the previous model, this
is also an undirected graphical model. It becomes
a CRF if we directly model the hidden (output)
variables by conditioning its clique potential (or
factor) ? on the observed (input) variables:
P (Rt, St|x,?) = 1Z(x,?)?(Rt, St|x,?) (1)
where x represents input features extracted from
the observed variables Ut?1 and Ut, and Z(x,?)
is the partition function. We use a log-linear rep-
resentation of the factor:
?(Rt, St|x,?) = exp(?T f(Rt, St, x)) (2)
where f(Rt, St, x) is a feature vector derived from
the input features x and the labels Rt and St, and
? is the corresponding weight vector. Although,
this model is similar in spirit to the model in Fig-
ure 4, we now break the chain structure, which
makes the inference much faster (i.e., complex-
ity of O(M2)). Breaking the chain structure also
allows us to balance the data for training (equal
number instances with S=1 and S=0), which dra-
matically reduces the learning time of the model.
We apply our model to all possible adjacent
units at all levels for the multi-sentential case, and
490
compute the posterior marginals of the relation-
structure pairs P (Rt, St=1|Ut?1, Ut,?) to obtain
the probability of all possible DT constituents.
4.3 Features Used in our Parsing Models
Table 1 summarizes the features used in our pars-
ing models, which are extracted from two adjacent
unitsUt?1 andUt. Since most of these features are
adopted from previous studies (Joty et al, 2012;
Hernault et al, 2010), we briefly describe them.
Organizational features include the length of
the units as the number of EDUs and tokens.
It also includes the distances of the units from
the beginning and end of the sentence (or text in
the multi-sentential case). Text structural fea-
tures indirectly capture the correlation between
text structure and rhetorical structure by counting
the number of sentence and paragraph boundaries
in the units. Discourse markers (e.g., because, al-
though) carry informative clues for rhetorical re-
lations (Marcu, 2000a). Rather than using a fixed
list of discourse markers, we use an empirically
learned lexical N-gram dictionary following (Joty
et al, 2012). This approach has been shown to
be more robust and flexible across domains (Bi-
ran and Rambow, 2011; Hernault et al, 2010). We
also include part-of-speech (POS) tags for the be-
ginning and end N tokens in a unit.
8 Organizational features Intra & Multi-Sentential
Number of EDUs in unit 1 (or unit 2).
Number of tokens in unit 1 (or unit 2).
Distance of unit 1 in EDUs to the beginning (or to the end).
Distance of unit 2 in EDUs to the beginning (or to the end).
4 Text structural features Multi-Sentential
Number of sentences in unit 1 (or unit 2).
Number of paragraphs in unit 1 (or unit 2).
8 N-gram features N?{1, 2, 3} Intra & Multi-Sentential
Beginning (or end) lexical N-grams in unit 1.
Beginning (or end) lexical N-grams in unit 2.
Beginning (or end) POS N-grams in unit 1.
Beginning (or end) POS N-grams in unit 2.
5 Dominance set features Intra-Sentential
Syntactic labels of the head node and the attachment node.
Lexical heads of the head node and the attachment node.
Dominance relationship between the two units.
8 Lexical chain features Multi-Sentential
Number of chains start in unit 1 and end in unit 2.
Number of chains start (or end) in unit 1 (or in unit 2).
Number of chains skipping both unit 1 and unit 2.
Number of chains skipping unit 1 (or unit 2).
2 Contextual features Intra & Multi-Sentential
Previous and next feature vectors.
2 Substructure features Intra & Multi-Sentential
Root nodes of the left and right rhetorical sub-trees.
Table 1: Features used in our parsing models.
Lexico-syntactic features dominance sets
(Soricut and Marcu, 2003) are very effective for
intra-sentential parsing. We include syntactic
labels and lexical heads of head and attachment
nodes along with their dominance relationship
as features. Lexical chains (Morris and Hirst,
1991) are sequences of semantically related words
that can indicate topic shifts. Features extracted
from lexical chains have been shown to be useful
for finding paragraph-level discourse structure
(Sporleder and Lascarides, 2004). We compute
lexical chains for a document following the ap-
proach proposed in (Galley and McKeown, 2003),
that extracts lexical chains after performing word
sense disambiguation. Following (Joty et al,
2012), we also encode contextual and rhetorical
sub-structure features in our models. The rhetori-
cal sub-structure features incorporate hierarchical
dependencies between DT constituents.
4.4 Parsing Algorithm
Given the probability of all possible DT con-
stituents in the intra-sentential and multi-sentential
scenarios, the job of the parsing algorithm is to
find the most probable DT for that scenario. Fol-
lowing (Joty et al, 2012), we implement a prob-
abilistic CKY-like bottom-up algorithm for com-
puting the most likely parse using dynamic pro-
gramming. Specifically, with n discourse units,
we use the upper-triangular portion of the n?n
dynamic programming table D. Given Ux(0) and
Ux(1) are the start and end EDU Ids of unit Ux:
D[i, j] = P (R[Ui(0), Uk(1), Uj(1)]) (3)
where, k = argmax
i?p?j
P (R[Ui(0), Up(1), Uj(1)]).
Note that, in contrast to previous studies on
document-level parsing (Hernault et al, 2010;
Subba and Di-Eugenio, 2009; Marcu, 2000b),
which use a greedy algorithm, our approach finds
a discourse tree that is globally optimal.
5 Document-level Parsing Approaches
Now that we have presented our intra-sentential
and our multi-sentential parsers, we are ready to
describe how they can be effectively combined to
perform document-level discourse analysis. Re-
call that a key motivation for a two-stage parsing is
that it allows us to capture the correlation between
text structure and discourse structure in a scalable,
modular and flexible way. Below we describe two
different approaches to model this correlation.
491
5.1 1S-1S (1 Sentence-1 Sub-tree)
A key finding from several previous studies on
sentence-level discourse analysis is that most sen-
tences have a well-formed discourse sub-tree in
the full document-level DT (Joty et al, 2012;
Fisher and Roark, 2007). For example, Figure 7(a)
shows 10 EDUs in 3 sentences (see boxes), where
the DTs for the sentences obey their respective
sentence boundaries. The 1S-1S approach aims to
maximally exploit this finding. It first constructs
a DT for every sentence using our intra-sentential
parser, and then it provides our multi-sentential
parser with the sentence-level DTs to build the
rhetorical parse for the whole document.
1     2  3S 1
8  9   10S 34   5      6   7S 2
1    2   3S 1
8   9    10S 34   5    6    7S 2(a) (b)
???
Figure 7: Two possible DTs for three sentences.
5.2 Sliding Window
While the assumption made by 1S-1S clearly sim-
plifies the parsing process, it totally ignores the
cases where discourse structures violate sentence
boundaries. For example, in the DT shown in Fig-
ure 7(b), sentence S2 does not have a well-formed
sub-tree because some of its units attach to the
left (4-5, 6) and some to the right (7). Vliet and
Redeker (2011) call these cases as ?leaky? bound-
aries. Even though less than 5% of the sentences
have leaky boundaries in RST-DT, in other corpora
this can be true for a larger portion of the sen-
tences. For example, we observe over 12% sen-
tences with leaky boundaries in the Instructional
corpus of (Subba and Di-Eugenio, 2009). How-
ever, we notice that in most cases where discourse
structures violate sentence boundaries, its units are
merged with the units of its adjacent sentences, as
in Figure 7(b). For example, this is true for 75%
cases in our development set containing 20 news
articles from RST-DT and for 79% cases in our
development set containing 20 how-to-do manuals
from the Instructional corpus. Based on this obser-
vation, we propose a sliding window approach.
In this approach, our intra-sentential parser
works with a window of two consecutive sen-
tences, and builds a DT for the two sentences. For
example, given the three sentences in Figure 7, our
intra-sentential parser constructs a DT for S1-S2
and a DT for S2-S3. In this process, each sentence
in a document except the first and the last will be
associated with two DTs: one with the previous
sentence (say DTp) and one with the next (say
DTn). In other words, for each non-boundary sen-
tence, we will have two decisions: one from DTp
and one from DTn. Our parser consolidates the
two decisions and generates one or more sub-trees
for each sentence by checking the following three
mutually exclusive conditions one after another:
? Same in both: If the sentence has the same (in
terms of both structure and labels) well-formed
sub-tree in both DTp and DTn, we take this sub-
tree for the sentence. For example, in Figure 8(a),
S2 has the same sub-tree in the two DTs, i.e. a DT
for S1-S2 and a DT for S2-S3. The two decisions
agree on the DT for the sentence.
? Different but no cross: If the sentence has a
well-formed sub-tree in both DTp and DTn, but
the two sub-trees vary either in structure or in la-
bels, we pick the most probable one. For example,
consider the DT for S1-S2 in Figure 8(a) and the
DT for S2-S3 in Figure 8(b). In both cases S2 has
a well-formed sub-tree, but they differ in structure.
We pick the sub-tree which has the higher proba-
bility in the two dynamic programming tables.
1     2  3S1 8  9   10S34   5      6   7S2
1    2   3S1 8   9    10S3
4   5    6    7S2
(a)
(c)
8  9   10S 34   5    6     7S2 (b)
4   5    6    7S2(i) (ii)
4   5      6   7S2
Figure 8: Extracting sub-trees for S2.
? Cross: If either or both of DTp and DTn seg-
ment the sentence into multiple sub-trees, we pick
the one with more sub-trees. For example, con-
sider the two DTs in Figure 8(c). In the DT for
S1-S2, S2 has three sub-trees (4-5,6,7), whereas
in the DT for S2-S3, it has two (4-6,7). So, we ex-
tract the three sub-trees for S2 from the first DT. If
the sentence has the same number of sub-trees in
both DTp and DTn, we pick the one with higher
probability in the dynamic programming tables.
At the end, the multi-sentential parser takes all
these sentence-level sub-trees for a document, and
builds a full rhetorical parse for the document.
492
6 Experiments
6.1 Corpora
While previous studies on document-level parsing
only report their results on a particular corpus, to
show the generality of our method, we experiment
with texts from two very different genres. Our
first corpus is the standard RST-DT (Carlson et
al., 2002), which consists of 385 Wall Street Jour-
nal articles, and is partitioned into a training set
of 347 documents and a test set of 38 documents.
53 documents, selected from both sets were anno-
tated by two annotators, based on which we mea-
sure human agreement. In RST-DT, the original 25
rhetorical relations defined by (Mann and Thomp-
son, 1988) are further divided into a set of 18
coarser relation classes with 78 finer-grained rela-
tions. Our second corpus is the Instructional cor-
pus prepared by (Subba and Di-Eugenio, 2009),
which contains 176 how-to-do manuals on home-
repair. The corpus was annotated with 26 informa-
tional relations (e.g., Preparation-Act, Act-Goal).
6.2 Experimental Setup
We experiment with our discourse parser on the
two datasets using our two different parsing ap-
proaches, namely 1S-1S and the sliding window.
We compare our approach with HILDA (Hernault
et al, 2010) on RST-DT, and with the ILP-based
approach of (Subba and Di-Eugenio, 2009) on the
Instructional corpus, since they are the state-of-
the-art on the respective genres. On RST-DT, the
standard split was used for training and testing
purposes. The results for HILDA were obtained
by running the system with default settings on the
same inputs we provided to our system. Since we
could not run the ILP-based system of (Subba and
Di-Eugenio, 2009) (not publicly available) on the
Instructional corpus, we report the performances
presented in their paper. They used 151 documents
for training and 25 documents for testing. Since
we did not have access to their particular split,
we took 5 random samples of 151 documents for
training and 25 documents for testing, and report
the average performance over the 5 test sets.
To evaluate the parsing performance, we use
the standard unlabeled (i.e., hierarchical spans)
and labeled (i.e., nuclearity and relation) preci-
sion, recall and F-score as described in (Marcu,
2000b). To compare with previous studies, our
experiments on RST-DT use the 18 coarser rela-
tions. After attaching the nuclearity statuses (NS,
SN, NN) to these relations, we get 41 distinct re-
lations. Following (Subba and Di-Eugenio, 2009)
on the Instructional corpus, we use 26 relations,
and treat the reversals of non-commutative rela-
tions as separate relations. That is, Goal-Act and
Act-Goal are considered as two different relations.
Attaching the nuclearity statuses to these relations
gives 76 distinct relations. Analogous to previous
studies, we map the n-ary relations (e.g., Joint)
into nested right-branching binary relations.
6.3 Results and Error Analysis
Table 2 presents F-score parsing results for our
parsers and the existing systems on the two cor-
pora.2 On both corpora, our parser, namely, 1S-1S
(TSP 1-1) and sliding window (TSP SW), outper-
form existing systems by a wide margin (p<7.1e-
05).3 On RST-DT, our parsers achieve absolute
F-score improvements of 8%, 9.4% and 11.4%
in span, nuclearity and relation, respectively, over
HILDA. This represents relative error reductions
of 32%, 23% and 21% in span, nuclearity and rela-
tion, respectively. Our results are also close to the
upper bound, i.e. human agreement on this corpus.
On the Instructional genre, our parsers deliver
absolute F-score improvements of 10.5%, 13.6%
and 8.14% in span, nuclearity and relations, re-
spectively, over the ILP-based approach. Our
parsers, therefore, reduce errors by 36%, 27% and
13% in span, nuclearity and relations, respectively.
If we compare the performance of our parsers
on the two corpora, we observe higher results
on RST-DT. This can be explained in at least
two ways. First, the Instructional corpus has a
smaller amount of data with a larger set of rela-
tions (76 when nuclearity attached). Second, some
frequent relations are (semantically) very similar
(e.g., Preparation-Act, Step1-Step2), which makes
it difficult even for the human annotators to distin-
guish them (Subba and Di-Eugenio, 2009).
Comparison between our two models reveals
that TSP SW significantly outperforms TSP 1-1
only in finding the right structure on both corpora
(p<0.01). Not surprisingly, the improvement is
higher on the Instructional corpus. A likely ex-
planation is that the Instructional corpus contains
more leaky boundaries (12%), allowing the sliding
2Precision, Recall and F-score are the same when manual
segmentation is used (see Marcu, (2000b), page 143).
3Since we did not have access to the output or to the sys-
tem of (Subba and Di-Eugenio, 2009), we were not able to
perform a significance test on the Instructional corpus.
493
RST-DT Instructional
Metrics HILDA TSP 1-1 TSP SW Human ILP TSP 1-1 TSP SW
Span 74.68 82.47* 82.74*? 88.70 70.35 79.67 80.88?
Nuclearity 58.99 68.43* 68.40* 77.72 49.47 63.03 63.10
Relation 44.32 55.73* 55.71* 65.75 35.44 43.52 43.58
Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA
(with p<7.1e-05) are denoted by *. Significant differences between TSP 1-1 and TSP SW (with p<0.01) are denoted by ?.
T-CT-OT-CMM-MCMPEVSUCNDENCATEEXBACOJOS-UATEL
T-C  T-O  T-CM      M-M     CMP  EV  SU     CND  EN     CA  TE    EX     BA   CO      JO   S-U   AT     EL10732111227114121291309359
00010203133350012722
0000100000001018532
 0020121007936757108
0000000302112332000
0001300102901921001
0001320004112121008
000000000070431001
000010001305111006
00000000242210000014
000000800000001000
0000100221010122010
0001010000011110000
000040000000020000
000000000000000000
001000000000000000
020000000000000001
000000000000000000
Figure 9: Confusion matrix for relation labels on the
RST-DT test set. Y-axis represents true and X-axis repre-
sents predicted relations. The relations are Topic-Change
(T-C), Topic-Comment (T-CM), Textual Organization (T-
O), Manner-Means (M-M), Comparison (CMP), Evaluation
(EV), Summary (SU), Condition (CND), Enablement (EN),
Cause (CA), Temporal (TE), Explanation (EX), Background
(BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribu-
tion (AT) and Elaboration (EL).
window approach to be more effective in finding
those, without inducing much noise for the labels.
This clearly demonstrates the potential of TSP SW
for datasets with even more leaky boundaries e.g.,
the Dutch (Vliet and Redeker, 2011) and the Ger-
man Potsdam (Stede, 2004) corpora.
Error analysis reveals that although TSP SW
finds more correct structures, a corresponding im-
provement in labeling relations is not present be-
cause in a few cases, it tends to induce noise from
the neighboring sentences for the labels. For ex-
ample, when parsing was performed on the first
sentence in Figure 1 in isolation using 1S-1S, our
parser rightly identifies the Contrast relation be-
tween EDUs 2 and 3. But, when it is considered
with its neighboring sentences by the sliding win-
dow, the parser labels it as Elaboration. A promis-
ing strategy to deal with this and similar problems
that we plan to explore in future, is to apply both
approaches to each sentence and combine them by
consolidating three probabilistic decisions, i.e. the
one from 1S-1S and the two from sliding window.
To further analyze the errors made by our parser
on the hardest task of relation labeling, Figure 9
presents the confusion matrix for TSP 1-1 on the
RST-DT test set. The relation labels are ordered
according to their frequency in the RST-DT train-
ing set. In general, the errors are produced by two
different causes acting together: (i) imbalanced
distribution of the relations, and (ii) semantic sim-
ilarity between the relations. The most frequent
relation Elaboration tends to mislead others es-
pecially, the ones which are semantically similar
(e.g., Explanation, Background) and less frequent
(e.g., Summary, Evaluation). The relations which
are semantically similar mislead each other (e.g.,
Temporal:Background, Cause:Explanation).
These observations suggest two ways to im-
prove our parser. We would like to employ a more
robust method (e.g., ensemble methods with bag-
ging) to deal with the imbalanced distribution of
relations, along with taking advantage of a richer
semantic knowledge (e.g., compositional seman-
tics) to cope with the errors caused by semantic
similarity between the rhetorical relations.
7 Conclusion
In this paper, we have presented a novel discourse
parser that applies an optimal parsing algorithm
to probabilities inferred from two CRF models:
one for intra-sentential parsing and the other for
multi-sentential parsing. The two models exploit
their own informative feature sets and the distribu-
tional variations of the relations in the two parsing
conditions. We have also presented two novel ap-
proaches to combine them effectively. Empirical
evaluations on two different genres demonstrate
that our approach yields substantial improvement
over existing methods in discourse parsing.
Acknowledgments
We are grateful to Frank Tompa and the anony-
mous reviewers for their comments, and the
NSERC BIN and CGS-D for financial support.
494
References
O. Biran and O. Rambow. 2011. Identifying Justi-
fications in Written Dialogs by Classifying Text as
Argumentative. International Journal of Semantic
Computing, 5(4):363?381.
L. Carlson, D. Marcu, and M. Okurowski. 2002. RST
Discourse Treebank (RST-DT) LDC2002T07. Lin-
guistic Data Consortium, Philadelphia.
V. Feng and G. Hirst. 2012. Text-level Discourse Pars-
ing with Rich Linguistic Features. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, ACL ?12, pages 60?68,
Jeju Island, Korea. Association for Computational
Linguistics.
S. Fisher and B. Roark. 2007. The Utility of Parse-
derived Features for Automatic Discourse Segmen-
tation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, ACL
?07, pages 488?495, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
M. Galley and K. McKeown. 2003. Improving Word
Sense Disambiguation in Lexical Chaining. In Pro-
ceedings of the 18th International Joint Conference
on Artificial Intelligence, IJCAI ?07, pages 1486?
1488, Acapulco, Mexico.
H. Hernault, H. Prendinger, D. duVerle, and
M. Ishizuka. 2010. HILDA: A Discourse Parser
Using Support Vector Machine Classification. Dia-
logue and Discourse, 1(3):1?33.
S. Joty, G. Carenini, and R. T. Ng. 2012. A Novel
Discriminative Framework for Sentence-Level Dis-
course Analysis. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ?12, pages 904?
915, Jeju Island, Korea. Association for Computa-
tional Linguistics.
H. LeThanh, G. Abeysinghe, and C. Huyck. 2004.
Generating Discourse Structures for Written Texts.
In Proceedings of the 20th international confer-
ence on Computational Linguistics, COLING ?04,
Geneva, Switzerland. Association for Computa-
tional Linguistics.
A. Louis, A. Joshi, and A. Nenkova. 2010. Discourse
Indicators for Content Selection in Summarization.
In Proceedings of the 11th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
SIGDIAL ?10, pages 147?156, Tokyo, Japan. Asso-
ciation for Computational Linguistics.
W. Mann and S. Thompson. 1988. Rhetorical Struc-
ture Theory: Toward a Functional Theory of Text
Organization. Text, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. In
Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?02, pages
368?375. Association for Computational Linguis-
tics.
D. Marcu. 2000a. The Rhetorical Parsing of Unre-
stricted Texts: A Surface-based Approach. Compu-
tational Linguistics, 26:395?448.
D. Marcu. 2000b. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press,
Cambridge, MA, USA.
J. Morris and G. Hirst. 1991. Lexical Cohesion
Computed by Thesaural Relations as an Indicator
of Structure of Text. Computational Linguistics,
17(1):21?48.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki,
and B. Webber. 2005. The Penn Discourse Tree-
Bank as a Resource for Natural Language Gener-
ation. In Proceedings of the Corpus Linguistics
Workshop on Using Corpora for Natural Language
Generation, pages 25?32, Birmingham, U.K.
S. Somasundaran, 2010. Discourse-Level Relations for
Opinion Analysis. PhD thesis, University of Pitts-
burgh.
R. Soricut and D. Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. In Proceedings of the 2003 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology, NAACL-HLT ?03, pages 149?
156, Edmonton, Canada. Association for Computa-
tional Linguistics.
C. Sporleder and M. Lapata. 2005. Discourse Chunk-
ing and its Application to Sentence Compression.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Nat-
ural Language Processing, pages 257?264, Van-
couver, British Columbia, Canada. Association for
Computational Linguistics.
C. Sporleder and A. Lascarides. 2004. Combining Hi-
erarchical Clustering and Machine Learning to Pre-
dict High-Level Discourse Structure. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04, Geneva, Switzer-
land. Association for Computational Linguistics.
M. Stede. 2004. The Potsdam Commentary Corpus.
In Proceedings of the ACL-04 Workshop on Dis-
course Annotation, Barcelona. Association for Com-
putational Linguistics.
R. Subba and B. Di-Eugenio. 2009. An Effective Dis-
course Parser that Uses Rich Linguistic Information.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT-NAACL ?09, pages 566?574, Boul-
der, Colorado. Association for Computational Lin-
guistics.
495
C. Sutton and A. McCallum. 2012. An Introduction
to Conditional Random Fields. Foundations and
Trends in Machine Learning, 4(4):267?373.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting
Sequence Data. Journal of Machine Learning Re-
search (JMLR), 8:693?723.
S. Verberne, L. Boves, N. Oostdijk, and P. Coppen.
2007. Evaluating Discourse-based Answer Extrac-
tion for Why-question Answering. In Proceedings
of the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 735?736, Amsterdam, The Nether-
lands. ACM.
N. Vliet and G. Redeker. 2011. Complex Sentences as
Leaky Units in Discourse Parsing. In Proceedings
of Constraints in Discourse, Agay-Saint Raphael,
September.
496
