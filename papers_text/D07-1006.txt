Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 51?60, Prague, June 2007. c?2007 Association for Computational Linguistics
Getting the structure right for word alignment: LEAF
Alexander Fraser
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
fraser@isi.edu
Daniel Marcu
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
marcu@isi.edu
Abstract
Word alignment is the problem of annotating
parallel text with translational correspon-
dence. Previous generative word alignment
models have made structural assumptions
such as the 1-to-1, 1-to-N, or phrase-based
consecutive word assumptions, while previ-
ous discriminative models have either made
such an assumption directly or used features
derived from a generative model making one
of these assumptions. We present a new gen-
erative alignment model which avoids these
structural limitations, and show that it is
effective when trained using both unsuper-
vised and semi-supervised training methods.
1 Introduction
Several generative models and a large number of
discriminatively trained models have been proposed
in the literature to solve the problem of automatic
word alignment of bitexts. The generative propos-
als have required unrealistic assumptions about the
structure of the word alignments. Two assumptions
are particularly common. The first is the 1-to-N as-
sumption, meaning that each source word generates
zero or more target words, which requires heuristic
techniques in order to obtain alignments suitable for
training a SMT system. The second is the consec-
utive word-based ?phrasal SMT? assumption. This
does not allow gaps, which can be used to particular
advantage by SMT models which model hierarchi-
cal structure. Previous discriminative models have
either made such assumptions directly or used fea-
tures from a generative model making such an as-
sumption. Our objective is to automatically produce
alignments which can be used to build high quality
machine translation systems. These are presumably
close to the alignments that trained bilingual speak-
ers produce. Human annotated alignments often
contain M-to-N alignments, where several source
words are aligned to several target words and the re-
sulting unit can not be further decomposed. Source
or target words in a single unit are sometimes non-
consecutive.
In this paper, we describe a new generative model
which directly models M-to-N non-consecutive
word alignments. The rest of the paper is organized
as follows. The generative story is presented, fol-
lowed by the mathematical formulation. Details of
the unsupervised training procedure are described.
The generative model is then decomposed into fea-
ture functions used in a log-linear model which is
trained using a semi-supervised algorithm. Experi-
ments show improvements in word alignment accu-
racy and usage of the generated alignments in hier-
archical and phrasal SMT systems results in an in-
creased BLEU score. Previous work is discussed
and this is followed by the conclusion.
2 LEAF: a generative word alignment
model
2.1 Generative story
We introduce a new generative story which enables
the capture of non-consecutive M-to-N alignment
structure. We have attempted to use the same la-
bels as the generative story for Model 4 (Brown et
51
al., 1993), which we are extending.
Our generative story describes the stochastic gen-
eration of a target string f (sometimes referred to
as the French string, or foreign string) from a source
string e (sometimes referred to as the English string),
consisting of l words. The variable m is the length
of f . We generally use the index i to refer to source
words (ei is the English word at position i), and j to
refer to target words.
Our generative story makes the distinction be-
tween different types of source words. There are
head words, non-head words, and deleted words.
Similarly, for target words, there are head words,
non-head words, and spurious words. A head word
is linked to zero or more non-head words; each non-
head word is linked to from exactly one head word.
The purpose of head words is to try to provide a ro-
bust representation of the semantic features neces-
sary to determine translational correspondence. This
is similar to the use of syntactic head words in sta-
tistical parsers to provide a robust representation of
the syntactic features of a parse sub-tree.
A minimal translational correspondence consists
of a linkage between a source head word and a target
head word (and by implication, the non-head words
linked to them). Deleted source words are not in-
volved in a minimal translational correspondence, as
they were ?deleted? by the translation process. Spu-
rious target words are also not involved in a min-
imal translational correspondence, as they sponta-
neously appeared during the generation of other tar-
get words.
Figure 1 shows a simple example of the stochas-
tic generation of a French sentence from an English
sentence, annotated with the step number in the gen-
erative story.
1. Choose the source word type.
for each i = 1, 2, ..., l choose a word type
?i = ?1 (non-head word), ?i = 0 (deleted
word) or ?i = 1 (head word) according to the
distribution g(?i|ei)
let ?0 = 1
2. Choose the identity of the head word for each
non-head word.
for each i = 1, 2, ..., l if ?i = ?1 choose a
?linked from head word? value ?i (the position
of the head word which ei is linked to) accord-
ing to the distribution w?1(?i ? i|classe(ei))
for each i = 1, 2, ..., l if ?i = 1 let ?i = i
for each i = 1, 2, ..., l if ?i = 0 let ?i = 0
for each i = 1, 2, ..., l if ??i 6= 1 return ?fail-
ure?
3. Choose the identity of the generated target head
word for each source head word.
for each i = 1, 2, ..., l if ?i = 1 choose ?i1
according to the distribution t1(?i1|ei)
4. Choose the number of words in a target cept
conditioned on the identity of the source head
word and the source cept size (?i is 1 if the cept
size is 1, and 2 if the cept size is greater).
for each i = 1, 2, ..., l if ?i = 1 choose a For-
eign cept size ?i according to the distribution
s(?i|ei, ?i)
for each i = 1, 2, ..., l if ?i < 1 let ?i = 0
5. Choose the number of spurious words.
choose ?0 according to the distribution
s0(?0|
?
i ?i)
let m = ?0 +
?l
i=1 ?i
6. Choose the identity of the spurious words.
for each k = 1, 2, ..., ?0 choose ?0k according
to the distribution t0(?0k)
7. Choose the identity of the target non-head
words linked to each target head word.
for each i = 1, 2, ..., l and for each k =
2, 3, ..., ?i choose ?ik according to the distribu-
tion t>1(?ik|ei, classh(?i1))
8. Choose the position of the target head and non-
head words.
for each i = 1, 2, ..., l and for each k =
1, 2, ..., ?i choose a position piik as follows:
? if k = 1 choose pii1 accord-
ing to the distribution d1(pii1 ?
c?i |classe(e?i), classf (?i1))
? if k = 2 choose pii2 according to the dis-
tribution d2(pii2 ? pii1|classf (?i1))
52
source absolutely [comma] they do not want to spend that money
word type (1) DEL. DEL. HEAD non-head HEAD HEAD non-head HEAD HEAD HEAD
linked from (2) THEY do NOT|| WANT to SPEND{{ THAT MONEY
head(3) ILS PAS DESIRENT DEPENSER CET ARGENT
cept size(4) 1 2 1 1 1 1
num spurious(5) 1
spurious(6) aujourd?hui
non-head(7) ILS PAS "" ne DESIRENT DEPENSER CET ARGENT
placement(8) aujourd?hui ILS ne DESIRENT PASww DEPENSER CET ARGENT
spur. placement(9) ILS ne DESIRENT PASww DEPENSER CET ARGENT aujourd?hui
Figure 1: Generative story example, (number) indicates step number
? if k > 2 choose piik according to the dis-
tribution d>2(piik ? piik?1|classf (?i1))
if any position was chosen twice, return ?fail-
ure?
9. Choose the position of the spuriously generated
words.
for each k = 1, 2, ..., ?0 choose a position pi0k
from ?0 ? k + 1 remaining vacant positions in
1, 2, ...,m according to the uniform distribution
let f be the string fpiik = ?ik
We note that the steps which return ?failure? are
required because the model is deficient. Deficiency
means that a portion of the probability mass in the
model is allocated towards generative stories which
would result in infeasible alignment structures. Our
model has deficiency in the non-spurious target word
placement, just as Model 4 does. It has addi-
tional deficiency in the source word linking deci-
sions. (Och and Ney, 2003) presented results sug-
gesting that the additional parameters required to en-
sure that a model is not deficient result in inferior
performance, but we plan to study whether this is
the case for our generative model in future work.
Given e, f and a candidate alignment a, which
represents both the links between source and tar-
get head-words and the head-word connections of
the non-head words, we would like to calculate
p(f, a|e). The formula for this is:
p(f, a|e) =[
l?
i=1
g(?i|ei)]
[
l?
i=1
?(?i,?1)w?1(?i ? i|classe(ei))]
[
l?
i=1
?(?i, 1)t1(?i1|ei)]
[
l?
i=1
?(?i, 1)s(?i|ei, ?i)]
[s0(?0|
l?
i=1
?i)]
[
?0?
k=1
t0(?0k)]
[
l?
i=1
?i?
k=2
t>1(?ik|ei, classh(?i1))]
[
l?
i=1
?i?
k=1
Dik(piik)]
where:
?(i, i?) is the Kronecker delta function which is
equal to 1 if i = i? and 0 otherwise.
?i is the position of the closest English head word
to the left of the word at i or 0 if there is no such
word.
53
classe(ei) is the word class of the English word at
position i, classf (fj) is the word class of the French
word at position j, classh(fj) is the word class of
the French head word at position j.
p0 and p1 are parameters describing the proba-
bility of not generating and of generating a target
spurious word from each non-spurious target word,
p0 + p1 = 1.
m? =
l?
i=1
?i (1)
s0(?0|m?) =
(m?
?0
)
pm???00 p?01 (2)
Dik(j) =
?
???????
???????
d1(j ? c?i |classe(e?i), classf (?ik))
if k = 1
d2(j ? pii1|classf (?ik))
if k = 2
d>2(j ? piik?1|classf (?ik))
if k > 2
(3)
?i = min(2,
l?
i?=1
?(?i? , i)) (4)
ci =
{ ceiling(??ik=1 piik/?i) if ?i 6= 0
0 if ?i = 0 (5)
The alignment structure used in many other mod-
els can be modeled using special cases of this frame-
work. We can express the 1-to-N structure of mod-
els like Model 4 by disallowing ?i = ?1, while for
1-to-1 structure we both disallow ?i = ?1 and de-
terministically set ?i = ?i. We can also specialize
our generative story to the consecutive word M-to-N
alignments used in ?phrase-based? models, though
in this case the conditioning of the generation deci-
sions would be quite different. This involves adding
checks on source and target connection geometry to
the generative story which, if violated, would return
?failure?; naturally this is at the cost of additional
deficiency.
2.2 Unsupervised Parameter Estimation
We can perform maximum likelihood estimation of
the parameters of this model in a similar fashion
to that of Model 4 (Brown et al, 1993), described
thoroughly in (Och and Ney, 2003). We use Viterbi
training (Brown et al, 1993) but neighborhood es-
timation (Al-Onaizan et al, 1999; Och and Ney,
2003) or ?pegging? (Brown et al, 1993) could also
be used.
To initialize the parameters of the generative
model for the first iteration, we use bootstrapping
from a 1-to-N and a M-to-1 alignment. We use the
intersection of the 1-to-N and M-to-1 alignments
to establish the head word relationship, the 1-to-N
alignment to delineate the target word cepts, and the
M-to-1 alignment to delineate the source word cepts.
In bootstrapping, a problem arises when we en-
counter infeasible alignment structure where, for in-
stance, a source word generates target words but no
link between any of the target words and the source
word appears in the intersection, so it is not clear
which target word is the target head word. To ad-
dress this, we consider each of the N generated tar-
get words as the target head word in turn and assign
this configuration 1/N of the counts.
For each iteration of training we search for the
Viterbi solution for millions of sentences. Evidence
that inference over the space of all possible align-
ments is intractable has been presented, for a sim-
ilar problem, in (Knight, 1999). Unlike phrase-
based SMT, left-to-right hypothesis extension using
a beam decoder is unlikely to be effective because in
word alignment reordering is not limited to a small
local window and so the necessary beam would be
very large. We are not aware of admissible or inad-
missible search heuristics which have been shown to
be effective when used in conjunction with a search
algorithm similar to A* search for a model predict-
ing over a structure like ours. Therefore we use
a simple local search algorithm which operates on
complete hypotheses.
(Brown et al, 1993) defined two local search op-
erations for their 1-to-N alignment models 3, 4 and
5. All alignments which are reachable via these
operations from the starting alignment are consid-
ered. One operation is to change the generation de-
cision for a French word to a different English word
(move), and the other is to swap the generation de-
cision for two French words (swap). All possible
operations are tried and the best is chosen. This is
repeated. The search is terminated when no opera-
54
tion results in an improvement. (Och and Ney, 2003)
discussed efficient implementation.
In our model, because the alignment structure is
richer, we define the following operations: move
French non-head word to new head, move English
non-head word to new head, swap heads of two
French non-head words, swap heads of two English
non-head words, swap English head word links of
two French head words, link English word to French
word making new head words, unlink English and
French head words. We use multiple restarts to try to
reduce search errors. (Germann et al, 2004; Marcu
and Wong, 2002) have some similar operations with-
out the head word distinction.
3 Semi-supervised parameter estimation
Equation 6 defines a log-linear model. Each feature
function hm has an associated weight ?m. Given
a vector of these weights ?, the alignment search
problem, i.e. the search to return the best alignment
a? of the sentences e and f according to the model, is
specified by Equation 7.
p?(f, a|e) = exp(
?
m ?mhm(a, e, f))?
a?,f ? exp(
?
m ?mhm(a?, e, f ?))
(6)
a? = argmax
a
?
m
?mhm(f, a, e) (7)
We decompose the new generative model pre-
sented in Section 2 in both translation directions
to provide the initial feature functions for our log-
linear model, features 1 to 10 and 16 to 25 in Table
1.
We use backoffs for the translation decisions (fea-
tures 11 and 26 and the HMM translation tables
which are features 12 and 27) and the target cept size
distributions (features 13, 14, 28 and 29 in Table 1),
as well as heuristics which directly control the num-
ber of unaligned words we generate (features 15 and
30 in Table 1).
We use the semi-supervised EMD algorithm
(Fraser and Marcu, 2006b) to train the model. The
initial M-step bootstraps parameters as described in
Section 2.2 from a M-to-1 and a 1-to-N alignment.
We then perform the D-step following (Fraser and
A B C
D
nnnnnnnnnnnnnn E
@@@@@@@
~~~~~~~
A B C
D
nnnnnnnnnnnnnn E
@@@@@@@
~~~~~~~
Figure 2: Two alignments with the same transla-
tional correspondence
Marcu, 2006b). Given the feature function param-
eters estimated in the M-step and the feature func-
tion weights ? determined in the D-step, the E-step
searches for the Viterbi alignment for the full train-
ing corpus.
We use 1 ? F-Measure as our error criterion.
(Fraser and Marcu, 2006a) established that it is im-
portant to tune ? (the trade-off between Precision
and Recall) to maximize performance. In working
with LEAF, we discovered a methodological prob-
lem with our baseline systems, which is that two
alignments which have the same translational cor-
respondence can have different F-Measures. An ex-
ample is shown in Figure 2.
To overcome this problem we fully interlinked the
transitive closure of the undirected bigraph formed
by each alignment hypothesized by our baseline
alignment systems1. This operation maps the align-
ment shown to the left in Figure 2 to the alignment
shown to the right. This operation does not change
the collection of phrases or rules extracted from a
hypothesized alignment, see, for instance, (Koehn et
al., 2003). Working with this fully interlinked rep-
resentation we found that the best settings of ? were
? = 0.1 for the Arabic/English task and ? = 0.4 for
the French/English task.
4 Experiments
4.1 Data Sets
We perform experiments on two large alignments
tasks, for Arabic/English and French/English data
sets. Statistics for these sets are shown in Table 2.
All of the data used is available from the Linguis-
tic Data Consortium except for the French/English
1All of the gold standard alignments were fully interlinked
as distributed. We did not modify the gold standard alignments.
55
1 chi(?i|ei) source word type 9 d2(4j|classf (fj)) movement for left-most target
non-head word
2 ?(4i|classe(ei)) choosing a head word 10 d>2(4j|classf (fj)) movement for subsequent target
non-head words
3 t1(fj |ei) head word translation 11 t(fj |ei) translation without dependency on word-type
4 s(?i|ei, ?i) ?i is number of words in target cept 12 t(fj |ei) translation table from final HMM iteration
5 s0(?0|
P
i ?i) number of unaligned target words 13 s(?i|?i) target cept size without dependency onsource head word e
6 t0(fj) identity of unaligned target words 14 s(?i|ei) target cept size without dependency on ?i
7 t>1(fj |ei, classh(?i1)) non-head word translation 15 target spurious word penalty
8 d1(4j|classe(e?), classf (fj)) movement for target
head words
16-30 (same features, other direction)
Table 1: Feature functions
gold standard alignments which are available from
the authors.
4.2 Experiments
To build all alignment systems, we start with 5 iter-
ations of Model 1 followed by 4 iterations of HMM
(Vogel et al, 1996), as implemented in GIZA++
(Och and Ney, 2003).
For all non-LEAF systems, we take the best per-
forming of the ?union?, ?refined? and ?intersection?
symmetrization heuristics (Och and Ney, 2003) to
combine the 1-to-N and M-to-1 directions resulting
in a M-to-N alignment. Because these systems do
not output fully linked alignments, we fully link the
resulting alignments as described at the end of Sec-
tion 3. The reader should recall that this does not
change the set of rules or phrases that can be ex-
tracted using the alignment.
We perform one main comparison, which is of
semi-supervised systems, which is what we will use
to produce alignments for SMT. We compare semi-
supervised LEAF with a previous state of the art
semi-supervised system (Fraser and Marcu, 2006b).
We performed translation experiments on the align-
ments generated using semi-supervised training to
verify that the improvements in F-Measure result in
increases in BLEU.
We also compare the unsupervised LEAF sys-
tem with GIZA++ Model 4 to give some idea of
the performance of the unsupervised model. We
made an effort to optimize the free parameters of
GIZA++, while for unsupervised LEAF there are
no free parameters to optimize. A single iteration
of unsupervised LEAF2 is compared with heuristic
2Unsupervised LEAF is equivalent to using the log-linear
model and setting ?m = 1 for m = 1 to 10 and m = 16 to 25,
symmetrization of GIZA++?s extension of Model 4
(which was run for four iterations). LEAF was boot-
strapped as described in Section 2.2 from the HMM
Viterbi alignments.
Results for the experiments on the French/English
data set are shown in Table 3. We ran GIZA++
for four iterations of Model 4 and used the ?re-
fined? heuristic (line 1). We ran the baseline semi-
supervised system for two iterations (line 2), and in
contrast with (Fraser and Marcu, 2006b) we found
that the best symmetrization heuristic for this sys-
tem was ?union?, which is most likely due to our
use of fully linked alignments which was discussed
at the end of Section 3. We observe that LEAF
unsupervised (line 3) is competitive with GIZA++
(line 1), and is in fact competitive with the baseline
semi-supervised result (line 2). We ran the LEAF
semi-supervised system for two iterations (line 4).
The best result is the LEAF semi-supervised system,
with a gain of 1.8 F-Measure over the LEAF unsu-
pervised system.
For French/English translation we use a state of
the art phrase-based MT system similar to (Och and
Ney, 2004; Koehn et al, 2003). The translation test
data is described in Table 2. We use two trigram lan-
guage models, one built using the English portion of
the training data and the other built using additional
English news data. The BLEU scores reported in
this work are calculated using lowercased and tok-
enized data. For semi-supervised LEAF the gain of
0.46 BLEU over the semi-supervised baseline is not
statistically significant (a gain of 0.78 BLEU would
be required), but LEAF semi-supervised compared
with GIZA++ is significant, with a gain of 1.23
BLEU. We note that this shows a large gain in trans-
while setting ?m = 0 for other values of m.
56
ARABIC/ENGLISH FRENCH/ENGLISH
A E F E
TRAINING
SENTS 6,609,162 2,842,184
WORDS 147,165,003 168,301,299 75,794,254 67,366,819
VOCAB 642,518 352,357 149,568 114,907
SINGLETONS 256,778 158,544 60,651 47,765
ALIGN DISCR.
SENTS 1,000 110
WORDS 26,882 37,635 1,888 1,726
LINKS 39,931 2,292
ALIGN TEST
SENTS 83 110
WORDS 1,510 2,030 1,899 1,716
LINKS 2,131 2,176
TRANS. DEV SENTS 728 (4 REFERENCES) 833 (1 REFERENCE)WORDS 18,255 22.0K TO 24.6K 20,562 17,454
TRANS. TEST SENTS 1,056 (4 REFERENCES) 2,380 (1 REFERENCE)WORDS 28,505 35.8K TO 38.1K 58,990 49,182
Table 2: Data sets
lation quality over that obtained using GIZA++ be-
cause BLEU is calculated using only a single refer-
ence for the French/English task.
Results for the Arabic/English data set are also
shown in Table 3. We used a large gold standard
word alignment set available from the LDC. We ran
GIZA++ for four iterations of Model 4 and used the
?union? heuristic. We compare GIZA++ (line 1)
with one iteration of the unsupervised LEAF model
(line 2). The unsupervised LEAF system is worse
than four iterations of GIZA++ Model 4. We be-
lieve that the features in LEAF are too high dimen-
sional to use for the Arabic/English task without the
backoffs available in the semi-supervised models.
The baseline semi-supervised system (line 3) was
run for three iterations and the resulting alignments
were combined with the ?union? heuristic. We ran
the LEAF semi-supervised system for two iterations.
The best result is the LEAF semi-supervised system
(line 4), with a gain of 5.4 F-Measure over the base-
line semi-supervised system.
For Arabic/English translation we train a state of
the art hierarchical model similar to (Chiang, 2005)
using our Viterbi alignments. The translation test
data used is described in Table 2. We use two tri-
gram language models, one built using the English
portion of the training data and the other built using
additional English news data. The test set is from the
NIST 2005 translation task. LEAF had the best per-
formance scoring 1.43 BLEU better than the base-
line semi-supervised system, which is statistically
significant.
5 Previous Work
The LEAF model is inspired by the literature on gen-
erative modeling for statistical word alignment and
particularly by Model 4 (Brown et al, 1993). Much
of the additional work on generative modeling of 1-
to-N word alignments is based on the HMM model
(Vogel et al, 1996). (Toutanova et al, 2002) and
(Lopez and Resnik, 2005) presented a variety of re-
finements of the HMM model particularly effective
for low data conditions. (Deng and Byrne, 2005)
described work on extending the HMM model us-
ing a bigram formulation to generate 1-to-N align-
ment structure. The common thread connecting
these works is their reliance on the 1-to-N approx-
imation, while we have defined a generative model
which does not require use of this approximation, at
the cost of having to rely on local search.
There has also been work on generative models
for other alignment structures. (Wang and Waibel,
1998) introduced a generative story based on ex-
tension of the generative story of Model 4. The
alignment structure modeled was ?consecutive M
to non-consecutive N?. (Marcu and Wong, 2002)
defined the Joint model, which modeled consec-
utive word M-to-N alignments. (Matusov et al,
2004) presented a model capable of modeling 1-to-
N and M-to-1 alignments (but not arbitrary M-to-
N alignments) which was bootstrapped from Model
4. LEAF directly models non-consecutive M-to-N
alignments.
One important aspect of LEAF is its symmetry.
(Och and Ney, 2003) invented heuristic symmetriza-
57
FRENCH/ENGLISH ARABIC/ENGLISH
SYSTEM F-MEASURE (? = 0.4) BLEU F-MEASURE (? = 0.1) BLEU
GIZA++ 73.5 30.63 75.8 51.55
(FRASER AND MARCU, 2006B) 74.1 31.40 79.1 52.89
LEAF UNSUPERVISED 74.5 72.3
LEAF SEMI-SUPERVISED 76.3 31.86 84.5 54.34
Table 3: Experimental Results
tion of the output of a 1-to-N model and a M-to-1
model resulting in a M-to-N alignment, this was ex-
tended in (Koehn et al, 2003). We have used in-
sights from these works to help determine the struc-
ture of our generative model. (Zens et al, 2004)
introduced a model featuring a symmetrized lexi-
con. (Liang et al, 2006) showed how to train two
HMM models, a 1-to-N model and a M-to-1 model,
to agree in predicting all of the links generated, re-
sulting in a 1-to-1 alignment with occasional rare 1-
to-N or M-to-1 links. We improve on these works by
choosing a new structure for our generative model,
the head word link structure, which is both sym-
metric and a robust structure for modeling of non-
consecutive M-to-N alignments.
In designing LEAF, we were also inspired by
dependency-based alignment models (Wu, 1997;
Alshawi et al, 2000; Yamada and Knight, 2001;
Cherry and Lin, 2003; Zhang and Gildea, 2004). In
contrast with their approaches, we have a very flat,
one-level notion of dependency, which is bilingually
motivated and learned automatically from the paral-
lel corpus. This idea of dependency has some sim-
ilarity with hierarchical SMT models such as (Chi-
ang, 2005).
The discriminative component of our work is
based on a plethora of recent literature. This lit-
erature generally views the discriminative modeling
problem as a supervised problem involving the com-
bination of heuristically derived feature functions.
These feature functions generally include the predic-
tion of some type of generative model, such as the
HMM model or Model 4. A discriminatively trained
1-to-N model with feature functions specifically de-
signed for Arabic was presented in (Ittycheriah and
Roukos, 2005). (Lacoste-Julien et al, 2006) created
a discriminative model able to model 1-to-1, 1-to-
2 and 2-to-1 alignments for which the best results
were obtained using features based on symmetric
HMMs trained to agree, (Liang et al, 2006), and
intersected Model 4. (Ayan and Dorr, 2006) de-
fined a discriminative model which learns how to
combine the predictions of several alignment algo-
rithms. The experiments performed included Model
4 and the HMM extensions of (Lopez and Resnik,
2005). (Moore et al, 2006) introduced a discrimi-
native model of 1-to-N and M-to-1 alignments, and
similarly to (Lacoste-Julien et al, 2006) the best re-
sults were obtained using HMMs trained to agree
and intersected Model 4. LEAF is not bound by
the structural restrictions present either directly in
these models, or in the features derived from the
generative models used. We also iterate the gener-
ative/discriminative process, which allows the dis-
criminative predictions to influence the generative
model.
Our work is most similar to work using discrim-
inative log-linear models for alignment, which is
similar to discriminative log-linear models used for
the SMT decoding (translation) problem (Och and
Ney, 2002; Och, 2003). (Liu et al, 2005) presented
a log-linear model combining IBM Model 3 trained
in both directions with heuristic features which re-
sulted in a 1-to-1 alignment. (Fraser and Marcu,
2006b) described symmetrized training of a 1-to-
N log-linear model and a M-to-1 log-linear model.
These models took advantage of features derived
from both training directions, similar to the sym-
metrized lexicons of (Zens et al, 2004), including
features derived from the HMM model and Model
4. However, despite the symmetric lexicons, these
models were only able to optimize the performance
of the 1-to-N model and the M-to-1 model sepa-
rately, and the predictions of the two models re-
quired combination with symmetrization heuristics.
We have overcome the limitations of that work by
defining new feature functions, based on the LEAF
generative model, which score non-consecutive M-
to-N alignments so that the final performance crite-
rion can be optimized directly.
58
6 Conclusion
We have found a new structure over which we can
robustly predict which directly models translational
correspondence commensurate with how it is used
in hierarchical SMT systems. Our new generative
model, LEAF, is able to model alignments which
consist of M-to-N non-consecutive translational cor-
respondences. Unsupervised LEAF is comparable
with a strong baseline. When coupled with a dis-
criminative training procedure, the model leads to
increases between 3 and 9 F-score points in align-
ment accuracy and 1.2 and 2.8 BLEU points in trans-
lation accuracy over strong French/English and Ara-
bic/English baselines.
7 Acknowledgments
This work was partially supported under the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-C-0022. We
would like to thank the USC Center for High Per-
formance Computing and Communications.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John D. Lafferty, I. Dan Melamed, David
Purdy, Franz J. Och, Noah A. Smith, and David
Yarowsky. 1999. Statistical machine translation, final
report, JHU workshop.
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45?60.
Necip Fazil Ayan and Bonnie J. Dorr. 2006. A maxi-
mum entropy approach to combining word alignments.
In Proceedings of HLT-NAACL, pages 96?103, New
York.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
ACL, pages 88?95, Sapporo, Japan.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, MI.
Yonggang Deng and William Byrne. 2005. Hmm word
and phrase alignment for statistical machine trans-
lation. In Proceedings of HLT-EMNLP, Vancouver,
Canada.
Alexander Fraser and Daniel Marcu. 2006a. Measuring
word alignment quality for statistical machine transla-
tion. In Technical Report ISI-TR-616, ISI/University
of Southern California.
Alexander Fraser and Daniel Marcu. 2006b. Semi-
supervised training for statistical word alignment. In
Proceedings of COLING-ACL, pages 769?776, Syd-
ney, Australia.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2004. Fast decoding and
optimal decoding for machine translation. Artificial
Intelligence, 154(1-2):127?143.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT-EMNLP,
pages 89?96, Vancouver, Canada.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT-NAACL, pages 127?133, Edmonton, Canada.
Simon Lacoste-Julien, Dan Klein, Ben Taskar, and
Michael Jordan. 2006. Word alignment via quadratic
assignment. In Proceedings of HLT-NAACL, pages
112?119, New York, NY.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL,
New York.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-linear
models for word alignment. In Proceedings of ACL,
pages 459?466, Ann Arbor, MI.
Adam Lopez and Philip Resnik. 2005. Improved hmm
alignment models for languages with scarce resources.
In Proceedings of the ACL Workshop on Building and
Using Parallel Texts, pages 83?86, Ann Arbor, MI.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of EMNLP, pages 133?139,
Philadelphia, PA.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In Proceedings of COLING,
Geneva, Switzerland.
59
Robert C. Moore, Wen-Tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word align-
ment. In Proceedings of COLING-ACL, pages 513?
520, Sydney, Australia.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL, pages
295?302, Philadelphia, PA.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(1):417?449.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160?167, Sapporo, Japan.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In Proceedings of EMNLP,
Philadelphia, PA.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING, pages 836?841,
Copenhagen, Denmark.
Ye-Yi Wang and Alex Waibel. 1998. Modeling with
structures in statistical machine translation. In Pro-
ceedings of COLING-ACL, volume 2, pages 1357?
1363, Montreal, Canada.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL,
pages 523?530, Toulouse, France.
Richard Zens, Evgeny Matusov, and Hermann Ney.
2004. Improved word alignment using a symmetric
lexicon model. In Proceedings of COLING, Geneva,
Switzerland.
Hao Zhang and Daniel Gildea. 2004. Syntax-based
alignment: Supervised or unsupervised? In Proceed-
ings of COLING, Geneva, Switzerland.
60
