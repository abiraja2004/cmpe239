Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 29?32,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Shared task system description:
Frustratingly hard compositionality prediction
Anders Johannsen, Hector Martinez Alonso, Christian Rish?j and Anders S?gaard
Center for Language Technology
University of Copenhagen
{ajohannsen|alonso|crjensen|soegaard}@hum.ku.dk
Abstract
We considered a wide range of features for
the DiSCo 2011 shared task about composi-
tionality prediction for word pairs, including
COALS-based endocentricity scores, compo-
sitionality scores based on distributional clus-
ters, statistics about wordnet-induced para-
phrases, hyphenation, and the likelihood of
long translation equivalents in other lan-
guages. Many of the features we considered
correlated significantly with human compo-
sitionality scores, but in support vector re-
gression experiments we obtained the best re-
sults using only COALS-based endocentric-
ity scores. Our system was nevertheless the
best performing system in the shared task, and
average error reductions over a simple base-
line in cross-validation were 13.7% for En-
glish and 50.1% for German.
1 Introduction
The challenge in the DiSCo 2011 shared task is to
estimate and predict the semantic compositionality
of word pairs. Specifically, the data set consists of
adjective-noun, subject-verb and object-verb pairs
in English and German. The organizers also pro-
vided the Wacky corpora for English and German
with lowercased lemmas.1 In addition, we also ex-
perimented with wordnets and using Europarl cor-
pora for the two languages (Koehn, 2005), but none
of the features based on these resources were used
in the final submission.
Semantic compositionality is an ambiguous term
in the linguistics litterature. It may refer to the po-
sition that the meaning of sentences is built from
1http://wacky.sslmit.unibo.it/
the meaning of its parts through very general prin-
ciples of application, as for example in type-logical
grammars. It may also just refer to a typically not
very well defined measure of semantic transparency
of expressions or syntactic constructions, best illus-
trated by examples:
(1) pull the plug
(2) educate people
The verb-object word pair in example (1) is in
the training data rated as much less compositional
than example (2). The intuition is that the mean-
ing of the whole is less related to the meaning of
the parts. The compositionality relation is not de-
fined more precisely, however, and this may in part
explain why compositionality prediction seems frus-
tratingly hard.
2 Features
Many of our features were evaluated with different
amounts of slop. The slop parameter permits non-
exact matches without resorting to language-specific
shallow patterns. The words in the compounds are
allowed to move around in the sentence one position
at a time. The value of the parameter is the maxi-
mum number of steps. Set to zero, it is equivalent
to an exact match. Below are a couple of example
configurations. Note that in order for w1 and w2 to
swap positions, we must have slop > 1 since slop=1
would place them on top of each other.
x x w1 w2 x x (slop=0)
x x w1 x w2 x (slop=1)
x x w1 x x w2 (slop=2)
x x w2 w1 x x (slop=2)
29
2.1 LEFT-ENDOC, RIGHT-ENDOC and
DISTR-DIFF
These features measure the endocentricity of a word
pair w1 w2. The distribution of w1 is likely to be
similar to the distribution of ?w1 w2? if w1 is the
syntactic head of ?w1 w2?. The same is to be ex-
pected for w2, when w2 is the head.
Syntactic endocentricity is related to composi-
tionality, but the implication is one-way only. A
highly compositional compound is endocentric, but
an endocentric compound need not be highly com-
positional. For example, the distribution of ?olive
oil?, which is endocentric and highly compositional,
is very similar to the distribution of ?oil?, the head
word. On the other hand, ?golden age? which is
ranked as highly non-compositional in the training
data, is certainly endocentric. The distribution of
?golden age? is not very different from that of ?age?.
We used COALS (Rohde et al, 2009) to cal-
culate word distributions. The COALS algorithm
builds a word-to-word semantic space from a cor-
pus. We used the implementation by Jurgens and
Stevens (2010), generating the semantic space from
the Wacky corpora for English and German with du-
plicate sentences removed and low-frequency words
substituted by dummy symbols. The word pairs
have been fed to COALS as compounds that have to
be treated as single tokens, and the semantic space
has been generated and reduced using singular value
decompositon. The vectors for w1, w2 and ?w1 w2?
are calculated, and we compute the cosine distance
between the semantic space vectors for the word
pair and its parts, and between the parts themselves,
namely for ?w1 w2? and w1, for ?w1 w2? and w2,
and for w1 and w2, say for ?olive oil? and ?olive?,
for ?olive oil? and ?oil?, and for ?olive? and ?oil?.
LEFT-ENDOC is the cosine distance between the left
word and the compound. RIGHT-ENDOC is the co-
sine distance between the right word and the com-
pound. Finally, DISTR-DIFF is the cosine distance
between the two words, w1 and w2.
2.2 BR-COMP
To accommodate for the weaknesses of syntactic en-
docentricity features, we also tried introducing com-
positionality scores based on hierarchical distribu-
tional clusters that would model semantic composi-
tionality more directly. The scores are referred to
below as BR-COMP (compositionality scores based
on Brown clusters), and the intuition behind these
scores is that a word pair ?w1 w2?, e.g. ?hot dog?, is
non-compositional if w1 and w2 have high colloca-
tional strength, but if w1 is replaced with a different
word w?1 with similar distribution, e.g. ?warm?, then
?w?1 w2? is less collocational. Similarly, if w2 is re-
placed with a different word w?2 with similar distri-
bution, e.g. ?terrier?, then ?w1 w?2? is also much less
collocational than ?w1 w2?.
We first induce a hierarchical clustering of the
words in the Wacky corpora cl : W ? 2W with
W the set of words in our corpora, using publicly
available software.2 Let the collocational strength of
the two words w1 and w2 be G2(w1, w2). We then
compute the average collocational strength of distri-
butional clusters, BR-CS (collocational strength of
Brown clusters):
BR-CS(w1, w2) =
?Nx?cl(w1),x??cl(w2)G
2(x, x?)
N
with N = |cl(w1)| ? |cl(w2)|. We now let
BR-COMP(w1, w2) =
BR-CS(w1,w2)
G2(w1,w2)
.
The Brown clusters were built with C = 1000
and a cut-off frequency of 1000. With these settings
the number of word types per cluster is quite high,
which of course has a detrimental effect on the se-
mantic coherence of the cluster. To counter this we
choose to restrict cl(w) and cl(w?) to include only
the 50 most frequently occurring terms.
2.3 PARAPHR
These features have to do with alternative phrasings
using synonyms from Princeton WordNet 3 and Ger-
maNet4. One word in the compound is held con-
stant while the other is replaced with its synonyms.
The intuition is again that non-compositional com-
pounds are much more frequent than any compound
that results from replacing one of the constituent
words with one of its synonyms. For ?hot dog? we
thus generate ?hot terrier? and ?warm dog?, but not
?warm terrier?. Specifically, PARAPHR?100 means
2http://www.cs.berkeley.edu/?pliang/software/
3http://wordnet.princeton.edu/
4GermaNet Copyright c? 1996, 2008 by University of
Tu?bingen.
30
that at least one of the alternative compounds has
a document count of more than 100 in the cor-
pus. PARAPHRav is the average count for all para-
phrases, PARAPHRsum is the sum of these counts,
and PARAPHRrel is the average count for all para-
phrases over the count of the word pair in question.
2.4 HYPH
The HYPH features were inspired by Bergsma et
al. (2010). It was only used for English. Specif-
ically, we used the relative frequency of hyphen-
ated forms as features. For adjective-noun pairs
we counted the number of hyphenated occurrences,
e.g. ?front-page?, and divided that number by the
number of non-hyphenated occurrences, e.g. ?front
page?. For subject-verb and object-verb pairs, we
add -ing to the verb, e.g. ?information-collecting?,
and divided the number of such forms with non-
hyphenated equivalents, e.g. ?information collect-
ing?.
2.5 TRANS-LEN
The intuition behind our bilingual features is that
non-compositional words typically translate into a
single word or must be paraphrased using multiple
words (circumlocution or periphrasis). TRANS-LEN
is the probability that the phrase?s translation, possi-
bly with intervening articles and markers, is longer
than lmin and shorter than lmax , i.e.:
TRANS-LEN(w1, w2, lmin , lmax ) =
???trans(w1 w2),l1?|? |?l2P (?|w1 w2)
???trans(w1 w2)P (?|w1 w2)
We use English and German Europarl (Koehn,
2005) to train our translation models. In particular,
we use the phrase tables of the Moses PB-SMT sys-
tem5 trained on a lemmatized version of the WMT11
parallel corpora for English and German. Below
TRANS-LEN-n will be the probability of the trans-
lation of a word pair being n or more words. We
also experimented with average translation length as
a feature, but this did not correlate well with seman-
tic compositionality.
5http://statmt.org
feat ?
English German
rel-type = ADJ NN 0.0750 *0.1711
rel-type = V SUBJ 0.0151 **0.2883
rel-type = V OBJ 0.0880 0.0825
LEFT-ENDOC **0.3257 *0.1637
RIGHT-ENDOC **0.3896 0.1379
DISTR-DIFF *0.1885 0.1128
HYPH (5) 0.1367 -
HYPH (5) reversed *0.1829 -
G2 0.1155 0.0535
BR-CS *0.1592 0.0242
BR-COMP 0.0292 0.0024
Count (5) 0.0795 *0.1523
PARAPHR?|w1 w?2| 0.1123 0.1242
PARAPHRrel (5) 0.0906 0.0013
PARAPHRav (1) 0.1080 0.0743
PARAPHRav (5) 0.1313 0.0707
PARAPHRsum (1) 0.0496 0.0225
PARAPHR?100 (1) **0.2434 0.0050
PARAPHR?100 (5) **0.2277 0.0198
TRANS-LEN-1 0.0797 0.0509
TRANS-LEN-2 0.1109 0.0158
TRANS-LEN-3 0.0935 0.0489
TRANS-LEN-5 0.0240 0.0632
Figure 1: Correlations. Coefficients marked with * are
significant (p < 0.05), and coefficients marked with **
are highly significant (p < 0.01). We omit features with
different slop values if they perform significantly worse
than similar features.
3 Correlations
We have introduced five different kinds of features,
four of which are supposed to model semantic com-
positionality directly. For feature selection, we
therefore compute the correlation of features with
compositionality scores and select features that cor-
relate significantly with compositionality. The fea-
tures are then used for regression experiments.
4 Regression experiments
For our regression experiments, we use support vec-
tor regression with a high (7) degree kernel. Other-
wise we use default parameters of publicly available
software.6 In our experiments, however, we were
not able to produce substantially better results than
what can be obtained using only the features LEFT-
ENDOC and RIGHT-ENDOC. In fact, for German
using only LEFT-ENDOC gave slightly better results
than using both. These features are also those that
correlate best with human compositionality scores
according to Figure 1. Consequently, we only use
6http://www.csie.ntu.edu.tw/?cjlin/libsvm/
31
these features in our official runs. Our evaluations
below are cross-validation results on training and de-
velopment data using leave-one-out. We compare
using only LEFT-ENDOC and RIGHT-ENDOC (for
English) with using all significant features that seem
relatively independent. For English, we used LEFT-
ENDOC, RIGHT-ENDOC, DISTR-DIFF, HYPH (5)
reversed, BR-CS, PARAPHR?100 (1). For German,
we used rel-type = ADJ NN, rel-type=V SUBJ and
RIGHT-ENDOC. We only optimized on numeric
scores. The submitted coarse-grained scores were
obtained using average +/- average deviation.7
English German
dev test dev test
BL 18.395 47.123
all sign. indep. 19.22 23.02
L-END+R-END 15.89 16.19 23.51 24.03
err.red (L+R) 0.137 0.501
5 Discussion
Our experiments have shown that the DiSCo 2011
shared task about compositionality prediction was a
tough challenge. This may be because of the fine-
grained compositionality metric or because of in-
consistencies in annotation, but note also that the
syntactically oriented features seem to perform a
lot better than those trying to single out semantic
compositionality from syntactic endocentricity and
collocational strength. For example, LEFT-ENDOC,
RIGHT-ENDOC and BR-CS correlate with compo-
sitionality scores, whereas BR-COMP does not, al-
though it is supposed to model compositionality
more directly. Could it perhaps be that annotations
reflect syntactic endocentricity or distributional sim-
ilarity to a high degree, rather than what is typically
thought of as semantic compositionality?
Consider a couple of examples of adjective-noun
pairs in English in Figure 2 for illustration. These
examples are taken from the training data, but we
have added our subjective judgments about semantic
and syntactic markedness and collocational strength
(peaking at G2 scores). It seems that semantic
markedness is less important for scores than syntac-
7These thresholds were poorly chosen, by the way. Had we
chosen less balanced cut-offs, say 0 and 72, our improved accu-
racy on coarse-grained scores (59.4) would have been compara-
ble to and slightly better than the best submitted coarse-grained
scores (58.5).
sem syn coll score
floppy disk X 61
free kick X 77
happy birthday X X 47
large scale X X 55
old school X X X 37
open source X X 49
real life X 69
small group 91
Figure 2: Subjective judgments about semantic and syn-
tactic markedness and collocational strength.
tic markedness and collocational strength. In partic-
ular, the combination of syntactic markedness and
collocational strength makes annotators rank word
pairs such as happy birthday and open source as
non-compositional, although they seem to be fully
compositional from a semantic perspective. This
may explain why our COALS-features are so predic-
tive of human compositionality scores, and why G2
correlates better with these scores than BR-COMP.
6 Conclusions
In our experiments for the DiSCo 2011 shared task
we have considered a wide range of features and
showed that some of them correlate significantly and
sometimes highly significantly with human compo-
sitionality scores. In our regression experiments,
however, our best results were obtained with only
one or two COALS-based endocentricity features.
We report error reductions of 13.7% for English and
50.1% for German.
References
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz
Kondrak. 2010. Predicting the semantic composition-
ality of prefix verbs. In EMNLP.
David Jurgens and Keith Stevens. 2010. The S-Space
package: an open source package for word space mod-
els. In ACL.
Philipp Koehn. 2005. Europarl: a parallel corpus for
statistical machine translation. In MT-Summit.
Douglas Rohde, Laura Gonnerman, and David Plaut.
2009. An improved model of semantic similarity
based on lexical co-occurrence. In Cognitive Science.
32
