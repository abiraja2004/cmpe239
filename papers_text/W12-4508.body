Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 83?87,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Simple Maximum Entropy Models for Multilingual Coreference Resolution
Xinxin Li, Xuan Wang, Xingwei Liao
Computer Application Research Center
Harbin Institute of Technology Shenzhen Graduate School
Shenzhen, China
lixxin2@gmail.com
Abstract
This paper describes our system participat-
ing in the CoNLL-2012 shared task: Mod-
eling Multilingual Unrestricted Coreference
in Ontonotes. Maximum entropy models are
used for our system as classifiers to deter-
mine the coreference relationship between ev-
ery two mentions (usually noun phrases and
pronouns) in each document. We exploit rich
lexical, syntactic and semantic features for the
system, and the final features are selected us-
ing a greedy forward and backward strategy
from an initial feature set. Our system partici-
pated in the closed track for both English and
Chinese languages.
1 Introduction
In this paper, we present our system for the CoNLL-
2012 shared task which aims to model coreference
resolution for multiple languages. The task of coref-
erence resolution is to group different mentions in a
document into coreference equivalent classes (Prad-
han et al., 2012). Plenty of machine learning al-
gorithms such as Decision tree (Ng and Cardie,
2002), maximum entropy model, logistic regres-
sion (Bjo?rkelund and Nugues, 2011), Support Vec-
tor Machines, have been used to solve this problem.
Meanwhile, the CoNLL-2011 shared task on En-
glish language show that a well-designed rule-based
approach can achieve a comparable performance as
a statistical one (Pradhan et al., 2011).
Our system treats coreference resolution problem
as classification problem by determining whether
every two mentions in a document has a corefer-
ence relationship or not. We use maximum entropy
(ME) models to train the classifiers. Previous work
reveal that features play an important role on coref-
erence resolution problem, and many different kinds
of features has been exploited. In this paper, we use
many different lexical, syntactic and semantic fea-
tures as candidate features, and use a greedy forward
and backward approach for feature selection for ME
models.
2 System Description
The framework of our system is shown in figure 1. It
includes four components: candidate mention selec-
tion, training example generation, model generation,
and decoding algorithm for test data. The details of
each component as described below.
2.1 Candidate Mention Selection
In both training and test sets, our system only con-
sider all noun phrases (NP) and pronouns (PRP,
PRP$) as candidate mentions for both English and
Chinese. The mentions in each sentence are ob-
tained from given syntactic tree by their syntactic
label. Other phrases in the syntactic tree are omit-
ted due to their small proportion. For example, in
the English training dataset, our candidate mentions
includes about 91% of golden mentions.
2.2 Training Example Generation
There are many different training example gen-
eration algorithms, e.g., McCarthy and Lehnert?s
method, Soon et al.s method, Ng and Cardies
method (Ng, 2005). For our baseline system, we
choose Soon et al.?s method because it is easily un-
derstandable, implemented and popularly used. It
83
Figure 1: The framework of our coreference resolution
system
selects pairs of two coreferent mentions as positive
examples, and pairs between mentions among the
two mentions and the last mention as negative ex-
amples.
2.3 Feature Selection
Rich and meaningful features are important for
coreference resolution. Our system starts with
Soon?s 12 features as baseline features (Soon et al.,
2001), and exploits many lexical, syntactic, and se-
mantic features as candidate features. Totally 71 fea-
tures are considered in our system, and summarized
below:
 Distance features: sentence distance, distance
in phrases, whether it?s a first mention (Strube
et al., 2002)
 Lexical features: string match, partial match,
apposition, proper name match, head word
match, partial head word match, minimum edit
distance (Daume? III and Marcu, 2005)
 Grammatical features: pronoun, demonstrative
noun phrase, embedded noun, gender agree-
ment, number agreement (Soon et al., 2001)
 Syntactic features: same head, maximal NP,
syntactic path (Yang et al., 2006)
 Semantic features: semantic class agreement,
governing verb and its grammatical role, predi-
cate (Ponzetto and Strube, 2006)
For English, the number agreement and gender
agreement features can be obtained through the gen-
der corpus provided. However, there is no corpus
for Chinese. Our system obtains this information
by collecting dictionaries for number and gender in-
formation from training dataset. For example, the
Algorithm 1 Greedy forward and backward feature
selection
Initialization: all candidate features in set C
Choose initial feature set 
Compute F1 with features c
while forward jj backward:
while forward:
for each feature f in C-c
Compute F1 with features c+f
if best(F1) increases:
backward = true, c=c+f, continue forward
else forward = false
while backward:
for each feature f in in c
Compute F1 with features c-f
if best(F1) increases:
forward = true, c=c-f continue backward
else backward = false
pronoun ??? (he) denotes a male mention, and the
noun phrase ?s?? (girlfriend) represents a female
mention. Similarly for number information, e.g., the
mentions containing ??? (and), ??? (group) are
plural. We use these words to build number and
gender dictionaries, and determine the number and
gender information of a new mention by checking
whether one of the words in the dictionaries is in the
mention.
For semantic class agreement feature in English,
the relation between two mentions is extracted from
WordNet 3.0 (Ng, 2007),(Miller, 1995). There is no
corresponding dictionary for Chinese, so we keep
it blank. The head word for each mention is se-
lected by its dependency head, which can be ex-
tracted throught the conversion head rules ( English
1 and Chinese 2).
Maximum Entropy modeling is used to train the
classifier for our system 3. We employ a greedy for-
ward and backward procedure for feature selection.
The procedure is shown in Algorithm 1.
The algorithm will iterate forward and backward
procedures until the performance does not improve.
We use two initial feature sets: a blank set and
Soon?s baseline feature set. Both feature sets start
1http://w3.msi.vxu.se/ nivre/research/headrules.txt
2http://w3.msi.vxu.se/ nivre/research/chn headrules.txt
3http://homepages.inf.ed.ac.uk/lzhang10/maxent.html
84
with a forward procedure.
2.4 Decoding
For every candidate mention pair, to determine their
coreference relationship is simple because the prob-
ability whether they are coreferent can be obtained
by our maximum entropy model. We can just set a
threshold  = 0:5 and select the pairs with probabil-
ity larger than . But usually it is hard for multiple
mentions. Suppose there are three mentions A, B, C
where the probability between A and B, A and C is
larger than , but B and C is small. Thus choosing
an appropriate decoding algorithm is necessary.
We use best-first clustering method for our system
which for each candidate mention in a document,
chooses the mention before it with best probability
larger than threshold . The difference between En-
glish and Chinese is that we consider the coreference
relationship of two mentions nested in Chinese, but
not in English.
3 Experiments
3.1 Setting
Our system participates in the English and Chinese
closed tracks with auto mentions. For both the En-
glish and Chinese datasets, we use gold annotated
training data for training, and a portion of auto an-
notated development data for feature selection. Only
part of development data is chosen because the eval-
uation procedure takes lot of time. To simplify, We
only select one or two file in each directory as our
development data.
The performance of the system is evaluated on
MUC, B-CUBED, CEAF(M), CEAF(E), BLANC
metrics. The official metric is calculated as
(MUC+B
3
+CEAF )
=
3
.
3.2 Development set
Figures 2 and 3 show the performance on the En-
glish and Chinese development datasets using fea-
ture selection starting from a empty feature set and
Soon?s baseline feature set. The x-axis means the
number of iterations with either forward or back-
ward selection. The performance on Soon?s baseline
feature set for both languages are shown on 1st itera-
tion. The performance from empty feature set starts
on 2nd iteration. From these figures, we can see that
Figure 2: Performance of English development data with
Feature selection
Figure 3: Performance of Chinese development data with
Feature selection
using feature selection in both initial feature sets, the
performance improves.
However the performance of our system is im-
proved only on a few iteration. The best system for
English stops at the 4th iteration with total 10 fea-
tures left, which starts from Soon?s baseline feature
set. Similarly, the system for Chinese achieves its
best performance at the 4th iteration with only 8 fea-
tures. The phenomenon reveals that most of the fea-
tures left for our system are still from Soon?s base-
line features, and our newly exploited lexical, syn-
tactic, and semantic features are not well utilized.
Then we evaluate our model on the entire devel-
opment data. The results are shown on Table 1.
Comparing Figures 2, 3 and Table 1, we can observe
that the performance on entire development data is
lower than part one, about 1% decrease.
3.3 Test
For test data, we retrain our model on both gold
training data and development data using the se-
lected features. The final results for English and
Chinese are shown in Table 2.
85
Model English Chinese
MUC 49.28 48.31
B
3 62.79 67.97
CEAF(M) 46.77 49.49
CEAF(E) 38.19 38.9
BLANC 66.31 68.91
Average 50.09 51.73
Table 1: Results on entire development data
Model English Chinese
MUC 48.27 48.09
B
3 61.37 68.31
CEAF(M) 44.83 49.92
CEAF(E) 36.68 38.89
BLANC 65.42 71.44
Official 48.77 51.76
Table 2: Results on test data
Comparing tables 2 and 1, we can observe that
the performance for the Chinese test data is similar
as the development data. The result seems reason-
able because the model for testing use additional de-
velopment data which is much smaller than training
data. However, the result on English test data seem a
little odd. The performance is about 1.4% less than
that on the development data. The result needs fur-
ther analysis.
4 Conclusion
In this paper, we presented our coreference resolu-
tion system which uses maximum entropy model to
determine the coreference relationship between two
mentions. Our system exploits many lexical, syn-
tactic and semantic features. However, using greedy
forward and backward feature selection strategy for
ME model, these rich features are not well utilized.
In future work we will analyze the reason for this
phenomenon and extend these features to other ma-
chine learning algorithms.
References
