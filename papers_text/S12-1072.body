First Joint Conference on Lexical and Computational Semantics (*SEM), pages 506?513,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Zhou qiaoli: A divide-and-conquer strategy for  
semantic dependency parsing 
 
 
Qiaoli Zhou Ling Zhang Fei Liu Dongfeng 
Cai 
Guiping 
Zhang 
Knowledge Engineering  
Research Center Shenyang Aerospace University 
No.37 Daoyi South Avenue 
Shenyang, Liaoning, China 
Zhou_qiao_li@
hotmail.com 
710138892@qq.
com 
fei_l2011@
163.com 
caidf@vip.16
3.com 
zgp@ge-
soft.com 
 
 
 
 
 
 
Abstract 
We describe our SemEval2012 shared Task 5 
system in this paper. The system includes 
three cascaded components: the tagging se-
mantic role phrase, the identification of se-
mantic role phrase, phrase and frame semantic 
dependency parsing. In this paper, semantic 
role phrase is tagged automatically based on 
rules, and takes Conditional Random Fields 
(CRFs) as the statistical identification model 
of semantic role phrase. A projective graph-
based parser is used as our semantic depend-
ency parser. Finally, we gain Labeled At-
tachment Score (LAS) of 61.84%, which 
ranked the first position. At present, we gain 
the LAS of 62.08%, which is 0.24% higher 
than that ranked the first position in the task 5. 
1 System Architecture  
To solve the problem of low accuracy of long dis-
tance dependency parsing, this paper proposes a 
divide-and-conquer strategy for semantic depend-
ency parsing. Firstly, Semantic Role (SR) phrase in 
a sentence are identified; next, SR phrase can be 
replaced by their head or SR of head. Therefore, 
the original sentence is divided into two kinds of 
parts, which can be parsed separately. The first 
kind is SR phrase parsing; the second kind is  
parsing the sentence in which the SR phrases are 
replaced by their head or SR of head. Finally, the 
paper takes graph-based parser as the semantic de-
pendency parser for all parts. They are described in 
Section 2 and Section 4. Their experimental results 
are shown in Section5. Section 6 gives our conclu-
sion and future work. 
2 SR Phrase Tagging and Frame  
To identify SR phrase, SR phrase of train corpus 
are tagged. SR phrase is tagged automatically 
based on rules in this paper. A phrase of the sen-
tence is called Semantic Role phrase (SR phrase) 
when the parent of only one word of this phrase is 
out of this phrase. The word with the parent out of 
the phrase is called Head of Phrase (HP). The 
shortest SR phrase is one word, while the longest 
SR phrase is a part of the sentence. In this paper, 
the new sequence in which phrases are replaced by 
their head or SR of head is defined as the frame. In 
this paper, firstly, SR phrases of the sentence are 
identified; secondly, the whole sentence is divided 
into SR phrases and frame; thirdly, SR phrase and 
frame semantic dependency are parsed; finally, the 
dependency parsing results of all components are 
combined into the dependency parsing result of the 
whole sentence. 
SR of HP is used as the type of this phrase. Only 
parts of types of SR phrases are tagged. In this pa-
per, the tagged SR phrases are divided into two 
506
types: Main Semantic Role (MSR) phrase and 
Preposition Semantic Role (PSR) phrase. 
2.1 MSR Phrase Tagging  
In this paper, MSR phrase includes: OfPart, agent, 
basis, concerning, content, contrast, cost, existent, 
experiencer, isa, partner, patient, possession, pos-
sessor, relevant, scope and whole. MSR phrase 
tagging rules are shown in figure1&2. 
  
Figure1: Tagging Rule of the Last Word of MSR Phrase 
Figure 1 shows the rule for identification of the 
last word of MSR phrase. If the SR of the current 
word is MSR and its POS is not VV, VE, VC or 
VA, it is the last word of phrase. 
As shown in the figure 2, the first word of 
phrase is found based on the last word of phrase. 
The child with the longest distance from the last 
word of phrase is used as the current word, and if 
the current word has no child, it is the first word of 
phrase; otherwise, the child of the current word is 
found recursively. If the first word of phrase POS 
is preposition and punctuation, and its parent is the 
last word, the word following the first word serves 
as the first word of phrase. 
 
Figure2: Tagging Rule of the First Word of MSR Phrase 
 
 
Figure3: Example of the Tagging MSR Phrase 
As shown in the figure 3, the first column is 
word ID and the seventh column is parent ID of 
word. SR of ID40 is content, so ID40 is the last 
word of phrase. Its children include ID39 and ID37, 
thus ID37 with the longest distance from ID40 is 
the current word. The child of ID37 is ID33, the 
child of ID33 is ID32, ID32 has no child, and ID32 
is the first word of SR phrase. 
The tagged result in the above figure 3 is as fol-
lows: ?/CC ?/VC ??/VV content[ ??/JJ ?
? /NN ? /CC ?? /NR ? /ETC ?? /NN ?
/DEG ??/NN ??/NN ]  
Input: wi: word index (ID) in a given sentence. 
           N: the number of words. 
          Mi: MSR list. 
          Vi: POS tags list 
Output: the last word ID of MSR phrase 
Function: Findmainsemanticword(wi): return word 
ID when wi of semantic belongs to Mi. 
Otherwise return 0. 
Function: FindPOSword(wi): return true when wi 
of POS tagging not belongs to Vi. Oth-
erwise return 0. 
Function Findlastword(wi) 
For i?1 to N do begin 
             If (Findmainsemanticword(wi)&& 
FindPOSword(wi)) 
               { 
                   return wi; 
} 
else { 
                          i++; 
} 
       end 
return 0; 
29  ?  ?  CC  CC  _  30  aux-depend  _  _ 
30  ?  ?  VC  VC  _  58 s-succession  _  _ 
31  ?? ??  VV  VV  _  54  s-succession _  _ 
32  ??  ??  JJ   JJ  _  33  d-attribute  _  _ 
33  ??  ??  NN  NN  _  37  s-coordinate  _  _ 
34  ?  ?  CC  CC  _  37  aux-depend  _  _ 
35  ??  ??  NR  NR  _  37  d-member  _  _ 
36  ?  ?  ETC  ETC  _  35  aux-depend  _  _ 
37  ??  ??  NN  NN  _  40  d-genetive  _  _ 
38  ?  ?  DEG  DEG  _  37  aux-depend  _  _ 
39  ??  ??  NN  NN  _  40  s-coordinate  _ _ 
40  ?? ??  NN  NN  _  31  content  _  _ 
Input: Lword: the last word ID of MSR phrase. 
Output: Fword: the first word ID of MSR phrase. 
Function: Findmaxlenchild (w): return child ID 
with the longest distance from w when w 
has child. Otherwise returns 0. 
Fuction: FindPOSword(w): return POS of w. 
Fuction:Findparent(w): return parent ID of w. 
Function Findfirstword(Lword) 
     If(Findmaxlenchild (Lword)= =0) 
      { 
         return Lword; 
} 
Else { 
Fword=Findmaxlenchildword(Lword); 
If(findPOSword(Fword)==P||  
findPOSword(Fword)= =PU) 
{ 
    If (findparent(Fword)= =Lword) 
        Return Fword +1; 
} 
Findfirstword(Fword); 
} 
507
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with HP is called 
MSR frame. 
MSR frame: ?/CC ?/VC ??/VV ??/NN  
Example of sentences with nested phrases: 
?/P ??/JJ ??/NN ?/PU ??/NT exis-
tent[ ? /P ?? /NR ?? /NN ?? /VV con-
tent[ ??/NN ] ?/DEC ??/NN ???/NN ] 
?/AD ?/VE ?????/CD ?/M  
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with HP is called 
MSR frame. 
MSR frame: ?/P ??/JJ ??/NN ?/PU ??
/NT???/NN ?/AD ?/VE ?????/CD ?
/M 
2.2 PSR Phrase Tagging  
In this paper, SR phrase containing preposition is 
defined as PSR phrase. If the POS tags of the cur-
rent word is Preposition (P), the first word and the 
last word of PSR phrase are found based on the 
current word. PSR phrase tagging rule as figure 4 
& 5. 
 
Figure 4: Tagging Rule of the First Word of PSR Phrase 
As shown in the figure 4, the child with the 
longest distance from the current word is the first 
word of phrase. If the prep has no child, then it is 
PSR phrase. 
As shown in the figure 5, firstly, the parent of 
the prep is found; next, the parent is taken as the 
current word, and the child with the longest dis-
tance from the current word is found recursively. If 
no child is found, the current word is the last word 
of PSR phrase. If preposition of SR is root or par-
ent of preposition is root, and proposition is PSR. 
If ID of preposition is larger than ID of parent of 
preposition, and preposition is PSR. 
 
Figure5: Tagging Rule of the Last Word of PSR Phrase 
 
 
Figure6: Example of the Tagging PSR Phrase 
As shown in the figure6, ID4 is prep, and it has 
no child, so the first word is ID4. The parent of 
Input: Pword: the word ID that word POS tags is P. 
Output: Fword: the first word ID of PSR phrase. 
Function: Findmaxlenchildword(w): return word ID 
with the longest distance from w when w 
has child. Otherwise returns 0. 
Function Findfirstword(Pword) 
        If(Findmaxlenchildword(Pword)= =0) 
          { 
             return Pword; 
} 
Else { 
return Fwrod= 
 Findmaxlenchildword(Pword); 
} 
Input: Pword: the word ID that word POS tags is P. 
Output: Lword: the last word ID of PSR phrase. 
Function: Findmaxchild (w): return word ID that 
length is max with w when w has child. 
Otherwise return 0. 
Function: Findparent (w): return word ID when w of 
parent is not root. Otherwise return 0.  
Function: Findroot(w): return 1 when w of semantic 
role is root. Other wise return 0. 
Function Findlastword(Pword) 
Var cword: parent ID 
     If(Findparentsword(Pword)= =0|| 
 findroot(Pword)= =1)  { 
             return Pword; 
} 
else { cword=Findparent (Pword) ) 
 If(Pword>cword){ 
return Pword; 
} 
else { 
                   if(Findmaxchild (cword)= =0) { 
                               return cword; 
} 
else{  
Lword= 
Findmaxchild (cword); 
Findlastword(Lword); 
} 
                           } 
}
1  ??  ??  NN  NN  _  2  j-agent  _  _ 
2  ??  ??  NN  NN  _  3  r-patient  _  _ 
3  ??  ??  NN  NN  _  11  agent  _  _ 
4  ?  ?  P P  _ 5  prep-depend  _ first word 
5  ??  ??  VV  VV  _  11 duration _ head_ 
6  ?? ?? NR  NR _ 8  d-genetive  _ _ 
7  ?? ?? NN  NN _  8 r-patient _ _ 
8  ??  ?? NN  NN _ 9 d-host _  _ 
9  ??  ?? NN  NN _ 5 patient  _  _ 
10  ?  ? LC  LC  _ 5  aux-depend _ last word_ 
11  ??  ?? VV VV  _  0  ROOT _  _ 
12  ?  ?  AS  AS  _ 11 aspect  _  _ 
13 ??  ?? JJ  JJ  _ 14 d-attribute  _  _ 
14  ?? ??  NN NN  _  11 content  _  _ 
15  ?  ?  PU  PU  _ 11  PU  _  _ 
508
ID4 is ID5, the child with the longest distance from 
ID5 is ID10, and ID10 with no child is the last 
word of phrase. 
The tagged result in the above figure 6 is as fol-
lows: ??/NN ??/NN ??/NN duration[?/P 
??/VV ??/NR ??/NN ??/NN ?/LC] ?
?/VV ?/AS ??/JJ ??/NN ?/PU 
The position of HP in PSR phrase is not fixed. 
After phrases are tagged, a new sequence gener-
ated by replacing the phrase with SR of HP is 
called PSR frame. 
PSR frame: ??/NN ??/NN ??/NN dura-
tion/duration ?? /VV ? /AS ?? /JJ ??
/NN ?/PU 
Examples of sentences with nested phrases: 
s-cause[ ??/P ??/NR s-purpose[ ?/P ?
?/VV ???/NT ]  ?/MSP ??/VV ??/VV 
?/DT ?/M ??/NN ??/NN ],/PU ??/AD 
?? /NN ?? /NN ? /VV ? /VV ????
/VV ?/PU 
PSR frame: s-cause/s-cause ,/PU ??/AD ??
/NN ??/NN ?/VV ?/VV ????/VV ?/PU 
2.3 SR Phrase Tagging Performance 
If the parent of only one word of the tagged phrase 
is out of this phrase, this phrase is tagged correctly. 
If each word in the generated frame has one parent 
(i.e. words out of the phrase are dependent on HP 
instead of other words of the phrase), the frame is 
correct. 
 Phrase Frame 
MSR 99.99% 100% 
PSR 99.98% 99.70% 
Table 1. Tagging Performance (P-score) 
 
As shown in the table 1, tagging results were of 
very high accuracy. The wrong results were not 
contained in phrase and frame train corpus of de-
pendency parsing. 
3 SR Phrase Identification  
In this paper, we divide SR phrase into two classes: 
Max SR phrase and Base SR phrase. Max SR 
phrase refers to SR phrase is not included in any 
other SR phrase in a sentence. Base SR phrase re-
fers to SR phrase does not include any other SR 
phrase in a SR phrase. Therefore, MSR phrase is 
divided into two classes: Max MSR (MMSR) 
phrase and Base MSR (BMSR) phrase. PSR phrase 
was divided into two classes: Max PSR (MPSR) 
phrase and Base PSR (BPSR) phrase. 
3.1 MMSR Phrase Identification based on 
Cascaded Conditional Random Fields 
Reference (Qiaoli Zhou, 2010) is selected as our 
approach of MMSR phrase identification. The 
MMSR identifying process is conceptually very 
simple. The MMSR identification first performs 
identifying BMSR phrase, and converts the identi-
fied phrase to head. It then performs identifying for 
the updated sequence and converts the newly rec-
ognized phrases into head. The identification re-
peats this process until the whole sequence has no 
phrase, and the top-level phrase are the MMSR 
phrases. A common approach to the phrase identi-
fication problem is to convert the problem into a 
sequence tagging task by using the ?BIEO? (B for 
beginning, I for inside, E for ending, and O for 
outside) representation. If the phrase has one word, 
the tag is E. This representation enables us to use 
the linear chain CRF model to perform identifying, 
since the task is simply assigning appropriate la-
bels to sequence. 
There are two differences between our feature 
set and Qiaoli (2010)?s: 
1) We use dependency direction of word as iden-
tification feature, while Qiaoli (2010) did not 
use. 
2) We do not use scoring algorithm which is used 
by Qiaoli (2010). 
Direction Unigrams D-3,D-2 ,D-1 , D0 , D+1 ,D+2 ,D+3
Direction Bigrams D-2D-1, D-1D0, D0D+1, D+1D+2,  
Word & Direction W0D0
Table 2. Feature Templates of MMSR Phrase 
 
Table 2 is additional new feature templates 
based on Qiaoli (2010). W represents a word, and 
D represents dependency direction of the word. 
With this approach, nested MSR phrases are identi-
fied, and the top-level MSR phrase is the MMSR 
that we obtained. 
corpus P R F 
dev 81.41% 75.40% 78.29% 
test 81.23% 73.04% 76.92% 
Table 3.  MMSR Identification Performance 
509
3.2 BMSR Phrase Identification based on 
CRFs  
We use the tag set ?BIEO? the same as that used 
for MMSR identification. 
Word Unigrams W-3, W-2, W-1, W0, W+1, W+2, W+3
Word Bigrams 
W-3W-2, W-2W-1, W-1W0, W0W+1, 
W+1W+2, W+2W+3
POS Unigrams P-3 , P-2, P-1, P0, P+1, P+2, P+3
POS Bigrams 
P-3P-2, P-2P-1, P-1P0, P0P+1,  
P+1P+2, P+2P+3
Word_X X0
Word_Y Y0
Word_D D0
Word_S S-3, S-2 , S-1 , S0, S+1, S+2, S+3
Word & POS W-1P-1, W0P0, W+1P+1
Word & Word_X W-3X0
Word & Word_D 
W0D0, W-3W-2D0, W-2W-1D0,  
W-1W0D0, W0W+1D0, W+1W+2D0, 
W+2W+3D0
Word & Word_S W-1S-1, W0S0, W+1S+1, W+2S+2
Word_X & Word_Y X0Y0
POS & Word_D 
P0D0, P-3P-2D0, P-2P-1D0, P-1P0D0, 
P0P+1D0, P+1P+2D0, P+2P+3D0
POS & Word_S 
P-1S-1, P-2S-2, P-3S-3, P0S0, 
 P+1S+1, P+2S+2, P+3S+3
Word_D & Word_S 
D-1S-1, D-2S-2, D-3S-3, D0S0, 
 D+1S+1, D+2S+2, D+3S+3
Word & POS & 
Word_D 
W-1P-1D0, W0P0D0, W+1P+1D0
Word & POS & 
Word_D & Word_S 
W-3P-3D-3S-3, W-2P-2D-2S-2,  
W-1P-1D-1S-1, W0P0D0S0, W1P1D1S1, 
W2P2D2S2, W3P3D3S3
Table 4. Feature Templates of BMSR Phrase 
 
In table 4, ?W? represents a word, ?P? repre-
sents the part-of-speech of the word, ?X? repre-
sents the fourth word following the current word, 
?Y? represents the fifth word following the current 
word, ?D? represents the dependency direction of 
the current word, and ?S? represents the paired 
punctuation feature. ?S? consists of ?RLIO? (R for 
the right punctuation, L for the left punctuation, I 
for the part between the paired punctuation and O 
for outside). 
 
corpus P R F 
dev 79.32% 80.65% 79.98% 
test 79.22% 79.96% 79.59% 
Table 5.  BMSR Identification Performance (F-score) 
3.3 MPSR Phrase Identification Based on 
Collection  
Reference (Dongfeng, 2011) is selected as our ap-
proach of MPSR phrase identification. The posi-
tion of HP in PSR phrase is not fixed. Not only 
PSR phrase is identified, but also PSR phrase type 
is identified.  
There are two major differences between our 
feature set and Dongfeng (2011)?s: 
1) We take the PSR phrase type (the SR of HP) 
as tag.  
2)  We use ?S-type? represents that the PSR 
phrase is the single preposition. ?Type? represents 
SR of the preposition. 
For example: ???/NN location [?/P ??
/NR ??/NR] ??/VV 
O|W POS
Dongfeng 
(2011) Tag 
Our Tag 
*|??? NN O O 
*|? P O O 
?|?? NR I I 
?|?? NR E Location-E
?|?? VV N N 
Table 6. Example of PSR Phrase Tag Set  
 
In table 6, Dongfeng(2011) takes ?E? as the tag 
of last word of PSR phrase, but we take ?Location-
E? as the tag of last word of PSR phrase  (Location 
is type of  PSR phrase). 
With this approach, nested PSR phrases are 
identified, and the top-level PSR phrase is the 
MPSR that we obtained. 
corpus MPSR phrase MPSR phrase & type
dev 84.00% 54.23% 
test 83.78% 51.60% 
Table 7. MPSR Identification Performance (F-score) 
3.4 Combined Identification of MSR Phrase 
and PSR Phrase 
Identification process: MSR phrase and PSR 
phrase are respectively identified in one sentence, 
and the results are combined in accordance with 
this rule: if phrases are nested, only the top-level 
phrase is tagged; if phrases are same, only the PSR 
510
phrase is tagged; if phrases are overlapped, only 
PSR phrase is tagged. 
There are two combinations in this paper:  
1) MMSR phrase and MPSR phrase combined 
result is defined as MMMP phrase. For exam-
ple as follow (?[ ]?represents MMSR, 
?{}?represents MPSR): 
Example A: [ ??/NN ] ?/VC [ ??/VV ?
?/NR ?/DEC ?/CD ?/M ??/JJ ??/NN ?
?/NN ] ?/PU ??/DT ?/M ?/VE [ ??/CD 
?/M ??/NN ??/NN ?/PU ???/CD ?/M 
??/NN ??/NN ] ??/VV location{ ?/P ?
/DT ?/M ??/NN ?/LC } ?/PU  
MMMP  frame: [ ??/NN ] ?/VC ??/NN ?
/PU ??/DT ?/M ?/VE ??/NN ??/VV 
location/location ?/PU 
2) BMSR phrase and MPSR phrase combined 
result is defined as BMMP phrase. 
Example B: [ ??/NN ] ?/VC ??/VV [ ??
/NR ] ?/DEC ?/CD ?/M ??/JJ ??/NN ?
?/NN ?/PU ??/DT ?/M ?/VE [ ??/CD ?
/M ??/NN ??/NN ?/PU ???/CD ?/M ?
?/NN ??/NN ] ??/VV location{ ?/P ?/DT 
?/M ??/NN ?/LC } ?/PU 
BMMP  frame: ??/NN ?/VC ??/VV ??
/NR ?/DEC ?/CD ?/M ??/JJ ??/NN ??
/NN ?/PU ??/DT ?/M ?/VE ??/NN ??
/VV location/location ?/PU 
corpus phrase P R F 
BMMP 79.48% 81.60% 80.53%
dev 
MMMP 80.00% 76.79% 78.36%
BMMP 80.14% 82.48% 81.30%
test 
MMMP 80.19% 78.53% 79.35%
Table 8.  Combination Phrase Identification 
Performance 
3.5 Phrase and Frame Length Distribution   
We count phrases, frame and Original Sentence 
(OS) length distribution in training set and dev set. 
 BMMP MMMP MMSR BMSR OS 
[0,5) 80.07% 71.36% 75.36% 85.74% 9.07%
[5,10) 16.15% 21.63% 18.93% 12.33% 8.30%
[10,20) 3.35% 6.13% 5.05% 1.80% 17.23%
20? 0.43% 0.88% 0.66% 0.13% 65.40%
Table 9.  Length Distribution of Phrases and OS 
 
Table 9 shows, about 95% of phrases have less 
than 10 words, but about 65% of OS has more than 
20 words. 
 BMMP MMMP MMSR BMSR OS 
[0,5) 16.00% 18.70% 16.43% 14.36% 9.07%
[5,10) 18.87% 24.91% 19.41% 14.11% 8.30%
[10,20) 34.26% 35.42% 33.94% 30.68% 17.23%
20? 30.87% 20.97% 30.22% 40.85% 65.40%
Table 10.  Length Distribution of Frames and OS 
 
Table 10 shows, about 70% of frames have less 
than 20 words, especially 80% of MMMP frame 
has less than 20 words, but about 65% of OS has 
more than 20 words. 
 BMMP MMMP BMSR MMSR OS 
phrase 3.07 3.83 2.53 3.44 30.07
frame 16.00 13.21 19.16 15.79 30.07
Table 11. Average Length 
 
We count phrases, frame and Original Sentence 
(OS) Average Length (AL) in training set and dev 
set. Table 11 shows phrase of AL accounted for 
10% of OS of AL, and frame of AL accounted for 
50% of OS of AL. The AL shows that the semantic 
dependency paring unit length of OS is greatly re-
duced after dividing an original sentence into SR 
phrases and frame.  
As shown in tables 9, 10 and 11, the length dis-
tribution indicates that the divide-and-conquer 
strategy reduces the complexity of sentences sig-
nificantly. 
4 Semantic Dependency Parsing  
Graph-based parser is selected as our basic seman-
tic dependency parser. It views the semantic de-
pendency parsing as problem of finding maximum 
spanning trees (McDonald, 2006) in directed 
graphs. In this paper, phrase and frame semantic 
dependency parsing result was obtained by Graph-
based parser. Training set of phrase comes from 
phrases, and training set of frame comes from 
frames. 
5 Experiments  
5.1 Direction of Identification  
511
Dependency direction serves as feature of SR 
phrase identification, so we need to identify de-
pendency direction of word. We use tag set is {B, 
F}, B represents backward dependence, F repre-
sents forward dependence. The root?s dependency 
direction in sentence is B. Dependency direction 
identification p-score has reached 94.87%. 
Word Unigrams W-4, W-3, W-2, W-1, W0, W+1,  
W+ 2, W+ 3, W+ 4
Word Bigrams W-3W-2, W-2W-1, W-1W0, W0W+1, 
W+1W+2, W+2W+3
Word Trigrams W-1W 0W+1
Word Four-grams W-2W-1W0 W +1, W0W+1W+2W+3
Word Five-grams W- 4W-3W-2W-1W0,  
W0W+1W+2W+3W+ 4
POS Unigrams P-4, P-3, P-2, P-1, P0, P+1, P+2, P+3, P+ 4
POS Bigrams P-3P-2, P-2P-1, P-1P0, P0P+1, 
 P+1P+2, P +2P+3
POS Trigrams P-1P0P+1
POS Four-grams P-2P-1P0P+1, P0P+1P+2P+3
POS Five-grams P-4P-3P-2P-1P0, P0P+1P+2P+3P+4
Word & POS W-2 P-2, W-1P-1, W0P0, W+1P+1, 
W+2P+2
Table 12.  Feature Templates of Dependency Direction 
In table12, w represents word, p represents POS. 
5.2 System and Model  
For a sentence for which phrases has been identi-
fied, if phrases can be identified, then the whole 
sentence semantic dependency parsing result is 
obtained by phrase parsing model and frame pars-
ing model. Therefore, in this paper, the sentence is 
divided into the following types based on the 
phrase identification results: (1) SentMMMP indi-
cates MMSR phrase and MPSR phrase identified 
in a sentence; (2) SentBMMP indicates BMSR 
phrase and MPSR phrase identified in a sentence; 
(3) SentMMSR indicates only MMSR phrase iden-
tified in a sentence; (4) SentMPSR indicates only 
MPSR phrase identified in a sentence; (5) 
SentBMSR indicates only BMSR phrase identified 
in a sentence; (6) SentNone indicates no phrase 
identified in a sentence. 
Sentence type Phrase parsing Model 
Frame parsing
Model 
SentMMMP MMMP phrase MMMP frame
SentBMMP BMMP phrase BMMP frame
SentMMSR MMSR phrase MMSR frame
SentMPSR MPSR phrase MPSR frame 
SentBMSR BMSR phrase BMSR frame
SentNone Sentence model 
Table 13.  Type of Sentence and Parsing Model 
Table 13 shows types of sentence, and parsing 
models for every type of sentence. For example, 
parsing SentMMMP needs MMMP phrase parsing 
model and MMMP frame paring model 
The corpus contains the sentence type deter-
mined by the phrase identification strategy. 
Strategy of phrase 
identification Sentence type in the corpus
Strategy MMMP SentMMMP, SentMMSR, SentMPSR, SentNone 
Strategy BMMP SentBMMP, SentMPSR, SentBMSR, SentNone 
Strategy BMSR SentBMSR, SentNone 
Table 14.  Sentence Types in the Corpus 
 
As shown in table 14, Strategy MMMP indicates 
that MMMP phrase in the corpus was identified, 
and sentences in the corpus were divided into 
SentMMMP, SentMMSR, SentMPSR and Sent-
None. Strategy BMMP indicates that BMMP 
phrase in the corpus was identified, and sentences 
in the corpus were divided into SentBMMP, 
SentBMSR, SentMPSR and SentNone. Strategy 
BMSR indicates that BMSR phrase in the corpus 
was identified, and sentences in the corpus were 
divided into SentBMSR and SentNone. 
5.3 Comparative Experiments  
In this paper, we carry out comparative experi-
ments of parsing for the test set by 3 systems. 
1) System1 represents strategy MMMP in the 
table 14. 
2) System2 represents strategy BMMP in the ta-
ble 14. 
3) System3 represents strategy BMSR in the table 
14. 
 Dev Test 
G-parser 62.31% 61.68% 
System1(MMMP) 61.98% 61.84% 
System2(BMMP) 62.7% 62.08% 
System3(BMSR) 62.22% 61.15% 
Table 15.  Comparative Experiments 
 
As shown in the table 15, system2 result is more 
accurate than system1, because BMMP phrase 
identification is more accurate than MMMP as 
shown in the table 8. Although, BMSR phrase 
identification is more accurate than MMMP phrase 
as shown in the table 5 & 8, system 3 result is less 
accurate than systm1. Compared with BMSR iden-
512
tification, MMMP identification reduces the com-
plexity of sentences significantly, because the table 
11 shows that the AL of MMMP frame is about 
30% less than that of BMSR frame. G-parser is 
graph-based parser (Wangxiang Che, 2008). 
6 Conclusion and Future Work  
To solve the problem of low accuracy of long dis-
tance dependency parsing, this paper proposes a 
divide-and-conquer strategy for semantic depend-
ency parsing. We present our SemEval2012 shared 
Task 5 system which is composed of three cas-
caded components: the tagging of SR phrase, the 
identification of Semantic-role- phrase and seman-
tic dependency parsing.  
Divide-and-conquer strategy is influenced by 
two factors: one is identifying the type of phrase 
will greatly reduce the sentence complexity; the 
other is phrase identifying precision results in cas-
caded errors. The topic of this evaluation is seman-
tic dependency parsing, and word and POS contain 
less semantic information. If we can make seman-
tic label on words, then it will be more helpful for 
semantic dependency parsing. In the future, we 
will study how to solve the long distance depend-
ency parsing problem. 
Acknowledgments 
The authors would like to thank the reviewers for 
their helpful comments. This work was supported 
by National Natural Science Foundation of China 
(NSFC) via grant 61073123 and Natural Science 
Foundation of Liaoning province via grant 
20102174. 
References  
