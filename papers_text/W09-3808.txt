Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 61?64,
Paris, October 2009. c?2009 Association for Computational Linguistics
An Incremental Earley Parser for Simple Range Concatenation Grammar
Laura Kallmeyer and Wolfgang Maier
Collaborative Research Center 833
University of Tu?bingen
Tu?bingen, Germany
{lk,wmaier}@sfs.uni-tuebingen.de
Abstract
We present an Earley-style parser for
simple range concatenation grammar, a
formalism strongly equivalent to linear
context-free rewriting systems. Further-
more, we present different filters which
reduce the number of items in the pars-
ing chart. An implementation shows that
parses can be obtained in a reasonable
time.
1 Introduction
Linear context-free rewriting systems (LCFRS)
(Vijay-Shanker et al, 1987), the equivalent mul-
tiple context-free grammars (MCFG) (Seki et al,
1991) and simple range concatenation grammars
(sRCG) (Boullier, 1998) have recently attracted
an increasing interest in the context of natu-
ral language processing. For example, Maier
and S?gaard (2008) propose to extract simple
RCGs from constituency treebanks with crossing
branches while Kuhlmann and Satta (2009) pro-
pose to extract LCFRS from non-projective depen-
dency treebanks. Another application area of this
class of formalisms is biological computing (Kato
et al, 2006).
This paper addresses the symbolic parsing of
sRCG/LCFRS. Starting from the parsing algo-
rithms presented in Burden and Ljunglo?f (2005)
and Villemonte de la Clergerie (2002), we pro-
pose an incremental Earley algorithm for simple
RCG. The strategy is roughly like the one pur-
sued in Villemonte de la Clergerie (2002). How-
ever, instead of the automaton-based formalization
in Villemonte de la Clergerie?s work, we give a
general formulation of an incremental Earley al-
gorithm, using the framework of parsing as de-
duction. In order to reduce the search space, we
introduce different types of filters on our items.
We have implemented this algorithm and tested it
on simple RCGs extracted from the German tree-
banks Negra and Tiger.
In the following section, we introduce simple
RCG and in section 3, we present an algorithm for
symbolic parsing of simple RCG. Section 4 then
presents different filtering techniques to reduce the
number of items. We close discussing future work.
2 Grammar Formalism
A range concatenation grammar (RCG) is a 5-
tupleG = (N,T, V, P, S). N is a finite set of non-
terminals (predicate names) with an arity function
dim: N ? N+, T and V are disjoint finite sets of
terminals and variables. P is a finite set of clauses
of the form ?0 ? ?1 . . . ?m, where m ? 0 and
each of the ?i, 0 ? i ? m, is a predicate of the
form Ai(?i1, . . . , ?idim(A)). Each ?ij ? (T ? V )?,
1 ? j ? dim(A) and 0 ? i ? k, is an argument.
As a shorthand notation for Ai(?1, . . . , ?dim(A)),
we use Ai(~?). S ? N is the start predicate name
with dim(S) = 1.
Note that the order of right-hand side (RHS)
predicates in a clause is of no importance. Sub-
classes of RCGs are introduced for further ref-
erence: An RCG G = (N,T, V, P, S) is sim-
ple if for all c ? P , it holds that every variable
X occurring in c occurs exactly once in the left-
hand side (LHS) and exactly once in the RHS, and
each argument in the RHS of c contains exactly
one variable. A simple RCG is ordered if for all
?0 ? ?1 ? ? ??m ? P , it holds that if a variable X1
precedes a variable X2 in a ?i, 1 ? i ? m, then
X1 also precedes X2 in ?0. The ordering require-
ment does not change the expressive power, i.e.,
ordered simple RCG is equivalent to simple RCG
(Villemonte de la Clergerie, 2002). An RCG is
?-free if it either contains no ?-rules or there is ex-
actly one rule S(?) ? ? and S does not appear in
any of the righthand sides of the rules in the gram-
mar. A rule is an ?-rule if one of the arguments
61
of the lefthand side is the empty string ?. (Boul-
lier, 1998) shows that for every simple RCG, one
can construct an equivalent ?-free simple RCG. An
RCG G = (N,T, V, P, S) is a k-RCG if for all
A ? N, dim(A) ? k.
The language of RCGs is based on the notion
of range. For a string w1 ? ? ?wn a range is a pair
of indices ?i, j? with 0 ? i ? j ? n, i.e., a
string span, which denotes a substring wi+1 ? ? ?wj
in the source string or a substring vi+1 ? ? ? vj in
the target string. Only consecutive ranges can be
concatenated into new ranges. Terminals, vari-
ables and arguments in a clause are bound to
ranges by a substitution mechanism. An instan-
tiated clause is a clause in which variables and ar-
guments are consistently replaced by ranges; its
components are instantiated predicates. For ex-
ample A(?g ? ? ?h?) ? B(?g + 1 ? ? ? h?) is an in-
stantiation of the clause A(aX1) ? B(X1) if
the target string is such that wg+1 = a. A de-
rive relation ? is defined on strings of instanti-
ated predicates. If an instantiated predicate is the
LHS of some instantiated clause, it can be replaced
by the RHS of that instantiated clause. The lan-
guage of an RCG G = (N,T, V, P, S) is the set
L(G) = {w1 ? ? ?wn | S(?0, n?) ?? ?}, i.e., an in-
put string w1 ? ? ?wn is recognized if and only if the
empty string can be derived from S(?0, n?). In this
paper, we are dealing only with ordered simple
RCGs. The ordering requirement does not change
the expressive power (Villemonte de la Clergerie,
2002). Furthermore, without loss of generality, we
assume that for every clause, there is a k ? 0 such
that the variables occurring in the clause are ex-
actly X1, . . . ,Xk.
We define derivation trees for simple RCGs as
unordered trees whose internal nodes are labelled
with predicate names and whose leaves are la-
belled with ranges such that all internal nodes
are licensed by RCG clause instantiations: given
a simple RCG G and a string w, a tree D =
?V,E, r? is a derivation tree of w = a1 . . . an
iff 1. there are exactly n leaves in D labelled
?0, 1?, . . . , ?n ? 1, n? and 2. for all v0 ? V with
v1, . . . , vn ? V , n ? 1 being all vertices with
?v0, vi? ? E (1 ? i ? n) such that the leftmost
range dominated by vi precedes the leftmost range
dominated by vi+1 (1 ? i < n): there is a clause
instantiation A0(~?0) ? A1(~?1) . . . An( ~?n) such
that a) l(vi) = Ai for 0 ? i ? n and b) the yield
of the leaves dominates by vi is ~?i.
3 Parsing
Our parsing algorithm is a modification of the
?incremental algorithm? of Burden and Ljunglo?f
(2005) with a strategy very similar to the strategy
adopted by Thread Automata (Villemonte de la
Clergerie, 2002). It assumes the grammar to be
ordered and ?-free. We refrain from supporting
non-?-free grammars since the treebank grammars
used with our implementation are all ?-free. How-
ever, note that only minor modifications would be
necessary in order to support non-?-free grammars
(see below).
We process the arguments of LHS of clauses in-
crementally, starting from an S-clause. Whenever
we reach a variable, we move into the clause of
the corresponding RHS predicate (predict or re-
sume). Whenever we reach the end of an argu-
ment, we suspend this clause and move into the
parent clause that has called the current one. In
addition, we treat the case where we reach the end
of the last argument and move into the parent as a
special case. Here, we first convert the item into
a passive one and then complete the parent item
with this passive item. This allows for some addi-
tional factorization.
The item form for passive items is [A, ~?] where
A a predicate of some arity k, ~? is a range vector of
arity k. The item form for active items: [A(~?) ?
A1( ~?1) . . . Am( ~?m), pos, ?i, j?, ~?] where A(~?) ?
A1( ~?1) . . . Am( ~?m) ? P ; pos ? {0, . . . , n} is the
position up to which we have processed the input;
?i, j? ? N2 marks the position of our dot in the
arguments of the predicate A: ?i, j? indicates that
we have processed the arguments up to the jth ele-
ment of the ith argument; ~? is an range vector con-
taining the bindings of the variables and terminals
occurring in the lefthand side of the clause (~?(i)
is the range the ith element is bound to). When
first predicting a clause, it is initialized with a vec-
tor containing only symbols ??? for ?unknown?.
We call such a vector (of appropriate arity) ~?init.
We introduce an additional piece of notation. We
write ~?(X) for the range bound to the variable X
in ~?. Furthermore, we write ~?(?i, j?) for the range
bound to the jth element in the ith argument of the
clause lefthand side.
Applying a range vector ~? containing variable
bindings for a given clause c to the argument vec-
tor of the lefthand side of c means mapping the ith
element in the arguments to ~?(i) and concatenat-
ing adjacent ranges. The result is defined iff every
62
argument is thereby mapped to a range.
We start by predicting the S-predicate:
[S(~?) ? ~?, 0, ?1, 0?, ~?init] S(
~?) ? ~? ? P
Scan: Whenever the next symbol after the dot
is the next terminal in the input, we can scan it:
[A(~?) ? ~?, pos, ?i, j?, ~?]
[A(~?) ? ~?, pos+ 1, ?i, j + 1?, ~??]
~?(i, j+1) = wpos+1
where ~?? is ~? updated with ~?(i, j + 1) =
?pos, pos+ 1?.
In order to support ?-free grammars, one would
need to store the pair of indices a ? is mapped to
in the range vector, along with the mappings of
terminals and variables. The indices could be ob-
tained through a Scan-? operation, parallel to the
Scan operation.
Predict: Whenever our dot is left of a variable
that is the first argument of some RHS predicate
B, we predict new B-clauses:
[A(~?) ? . . . B(X, . . . ) . . . , pos, ?i, j?, ~?A]
[B(~?) ? ~?, pos, ?1, 0?, ~?init]
with the side condition ~?(i, j + 1) = X,B(~?) ?
~? ? P .
Suspend: Whenever we arrive at the end of an
argument that is not the last argument, we suspend
the processing of this clause and we go back to the
item that was used to predict it.
[B(~?) ? ~?, pos?, ?i, j?, ~?B ],
[A(~?) ? . . . B(~?) . . . , pos, ?k, l?, ~?A]
[A(~?) ? . . . B(~?) . . . , pos?, ?k, l + 1?, ~?]
where the dot in the antecedent A-item precedes
the variable ~?(i), |~?(i)| = j (the ith argument has
length j and has therefore been completely pro-
cessed), |~?| < i (the ith argument is not the last
argument of B), ~?B(~?(i)) = ?pos, pos?? and for
all 1 ? m < i: ~?B(~?(m)) = ~?A(~?(m)). ~? is ~?A
updated with ~?A(~?(i)) = ?pos, pos??.
Convert: Whenever we arrive at the end of the
last argument, we convert the item into a passive
one:
[B(~?) ? ~?, pos, ?i, j?, ~?B ]
[B, ?]
|~?(i)| = j, |~?| = i,
~?B(~?) = ?
Complete: Whenever we have a passive B item
we can use it to move the dot over the variable of
the last argument of B in a parent A-clause that
was used to predict it.
[B, ~?B], [A(~?) ? . . . B(~?) . . . , pos, ?k, l?, ~?A]
[A(~?) ? . . . B(~?) . . . , pos?, ?k, l + 1?, ~?]
where the dot in the antecedent A-item precedes
the variable ~?(|~?B |), the last range in ~?B is
?pos, pos??, and for all 1 ? m < |~?B |: ~?B(m) =
~?A(~?(m)). ~? is ~?A updated with ~?A(~?(|~?B |)) =
?pos, pos??.
Resume: Whenever we are left of a variable
that is not the first argument of one of the RHS
predicates, we resume the clause of the RHS pred-
icate.
[A(~?) ? . . . B(~?) . . . , pos, ?i, j?, ~?A],
[B(~?) ? ~?, pos?, ?k ? 1, l?, ~?B]
[B(~?) ? ~?, pos, ?k, 0?, ~?B]
where ~?(i)(j + 1) = ~?(k), k > 1 (the next el-
ement is a variable that is the kth element in ~?,
i.e., the kth argument of B), |~?(k ? 1)| = l, and
~?A(~?(m)) = ~?B(~?)(m) for all 1 ? m ? k ? 1.
The goal item has the form [S, ?0, n?].
Note that, in contrast to a purely bottom-up
CYK algorithm, the Earley algorithm presented
here is prefix valid, provided that the grammar
does not contain useless symbols.
4 Filters
During parsing, various optimizations known from
(P)CFG parsing can be applied. More concretely,
because of the particular form of our simple
RCGs, we can use several filters to reject items
very early that cannot lead to a valid parse tree for
a given input w = w1 . . . wn.
Since our grammars are ?-free, we know that
each variable or occurrence of a terminal in the
clause must cover at least one terminal in the in-
put. Furthermore, since separations between ar-
guments are generated only in cases where be-
tween two terminals belonging to the yield of a
non-terminal, there is at least one other terminals
that is not part of the yield, we know that between
different arguments of a predicate, there must be at
least one terminal in the input. Consequently, we
obtain as a filtering condition on the validity of an
active item that the length of the remaining input
must be greater or equal to the number of variables
and terminal occurrences plus the number of argu-
ment separations to the right of the dot in the left-
hand side of the clause. More formally, an active
item [A(~?) ? A1( ~?1) . . . Am( ~?m), pos, ?i, j?, ~?]
satisfies the length filter iff
(n? pos)
? (|~?(i)| ? j) + ?dim(A)k=i+1 |~?(k)| + (dim(A) ? i)
The length filter is applied to results of predict,
resume, suspend and complete.
A second filter, first proposed in Klein and
Manning (2003), checks for the presence of re-
quired preterminals. In our case, we assume the
63
preterminals to be treated as terminals, so this fil-
ter amounts to checking for the presence of all
terminals in the predicted part of a clause (the
part to the right of the dot) in the remaining in-
put. Furthermore, we check that the terminals
appear in the predicted order and that the dis-
tance between two of them is at least the num-
ber of variables/terminals and argument separa-
tions in between. In other words, an active item
[A(~?) ? A1( ~?1) . . . Am( ~?m), pos, ?i, j?, ~?] satis-
fies the terminal filter iff we can find an injec-
tive mapping fT : Term = {?k, l? | ~?(k)(l) ? T
and either k > i or (k = i and l > j)} ?
{pos+ 1, . . . , n} such that
1. wfT (?k,l?) = ~?(k)(l) for all ?k, l? ? Term;
2. for all ?k1, l1?, ?k2, l2? ? Term with k1 = k2
and l1 < l2: fT (?k2, l2?) ? fT (?k1, l1?) +
(l2 ? l1);
3. for all ?k1, l1?, ?k2, l2? ? Term with k1 <
k2: fT (?k2, l2?) ? fT (?k1, l1?) + (|~?(k1)| ?
l1) + ?k2?1k=k1+1|~?(k)| + l2 + (k2 ? k1).
Checking this filter amounts to a linear traversal
of the part of the lefthand side of the clause that
is to the right of the dot. We start with index i =
pos + 1, for every variable or gap we increment
i by 1. For every terminal a, we search the next
a in the input, starting at position i. If it occurs
at position j, then we set i = j and continue our
traversal of the remaining parts of the lefthand side
of the clause.
The preterminal filter is applied to results of the
predict and resume operations.
We have implemented the incremental Earley
parser with the filtering conditions on items. In
order to test it, we have extracted simple RCGs
from the first 1000 sentences of Negra and Tiger
(with removed punctuation) using the algorithm
described in Maier and S?gaard (2008) and parsed
the sentences 1001-1100 with it. The grammars
contained 2474 clauses (Negra) and 2554 clauses
(Tiger). The following table contains the to-
tal number of sentences for different length and
resp. the number of sentences for which a parse
was found, along with the average parsing times
of those that had a parse:
Negra Tiger
parse/tot av. t. parse/tot av. t.
|w| ? 20 73/84 0.40 sec. 50/79 0.32
20 ?
|w| ? 35 14/16 2.14 sec. 10/19 2.16
5 Conclusion and Future Work
We have presented an Earley-style algorithm for
simple range concatenation grammar, formulated
as deduction system. Furthermore, we have pre-
sented a set of filters on the chart reducing the
number of items. An implementation and a test
with grammars extracted from treebanks showed
that reasonable parsing times can be achieved.
We are currently working on a probabilistic
k-best extension of our parser which resumes
comparable work for PCFG (Huang and Chiang,
2005). Unfortunately, experiments with the Ear-
ley algorithm have shown that with grammars of a
reasonable size for data-driven parsing (> 15, 000
clauses), an exhaustive parsing is no longer ef-
ficient, due to the highly ambiguous grammars.
Algorithms using only passive items seem more
promising in this context since they facilitate the
application of A? parsing techniques.
References
Pierre Boullier. 1998. Proposal for a natural lan-
guage processing syntactic backbone. Rapport de
Recherche RR-3342, INRIA.
Ha?kan Burden and Peter Ljunglo?f. 2005. Parsing lin-
ear context-free rewriting systems. In Proceedings
of IWPT 2005.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT 2005.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
Stochastic multiple context-free grammar for RNA
pseudoknot modeling. In Proceedings of TAG+8.
Dan Klein and Christopher D. Manning. 2003. A*
Parsing: Fast Exact Viterbi Parse Selection. In Pro-
ceedings of HLT-NAACL.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings of EACL.
Wolfgang Maier and Anders S?gaard. 2008. Tree-
banks and mild context-sensitivity. In Proceedings
of Formal Grammar 2008.
Hiroyuki Seki, Takahashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science.
K. Vijay-Shanker, David Weir, and Aravind Joshi.
1987. Characterising structural descriptions used by
various formalisms. In Proceedings of ACL.
Eric Villemonte de la Clergerie. 2002. Parsing mildly
context-sensitive languages with thread automata.
In Proceedings of COLING.
64
