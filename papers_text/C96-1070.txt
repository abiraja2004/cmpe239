I nc rementa l  T rans la t ion  
Ut i l i z ing  Const i tuent  Boundary  Pat terns  
Osamu FURUSE,  Hitoshi I IDA 
ATR In terpret ing  Te lecommunicat ions  Research  Laborator ies  
2-2 t l ikar ida i ,  Seika-cho, Soraku-gun,  Kyoto ,  619-02, Japan  
{furuse, iida}@itl.atr.co.jp 
Abstract 
We have proposed an incremental trans- 
lation method in Transfer-Driven Ma- 
chine Translation (TDMT). In this 
method, constituent boundary patterns 
are applied to an input in a bottom-up 
fashion. Also, by dealing with best-only 
substructures, the explosion of structural 
ambiguity is constrained and an efficient 
translation of a lengthy input can be 
achieved. Through preliminary exper- 
imentation our new TDMT has been 
shown to be more efficient while main- 
tMning translation quality. 
1 Introduction 
A system dealing with spoken language requires a 
quick response in order to provide smooth com- 
munication between humans or between a hu- 
man and a computer. Thereibre, assuring effi- 
ciency in spoken-language translation is one of 
the most crucial tasks in devising such a system. 
In spoken language, the translation of lengthy 
utterances can yield a huge amount of struc- 
tural ambiguity, which needs to be efficiently pro- 
cessed by the system. As a solution for achiev- 
ing an efficient spoken-language system, several 
techniques, such as incremental generation (Fin- 
kler, 1992; Kempen, 1987) and marker-passing 
memory-based translation (Kitano, 1994), have 
been proposed. Many of these techniques adopt 
a left-to-right strategy to handle an input incre- 
mentally and a best-first strategy to avoid the ex- 
plosion of structural ambiguity. These strategies 
(:an be achieved with bottom-up rocessing. 
We have already proposed Transfer-Driven Ma- 
chine Translation (TDMT) for efficient and ro- 
bust spoken-language translation (Furuse, 1994a; 
Furuse, 1994b). However, the top-down and 
breadth-first ranslation strategy in the earlier 
versions of TDMT, which yields a quick response 
for inputs with restricted lengths, may show poor 
efficiency when processing a very lengthy input or 
inputs having many competing structures. 
In a top-down and breadth-first application, all 
the possible structures are retained until the whole 
input string is parsed. This requires many compu- 
tations and results in inefficient ranslation. For 
instance, the sentence below has many competing 
structures, mainly because of possible combina- 
tions within noun sequences. If this expression is 
combined with another expression, the structurM 
ambiguity will be further compounded. 
With bacon chicken eggs lettuce and tomato on it. 
In contrast, if structural ambiguities of sub- 
strings are always settled and are never inherited 
to the upper structures, the explosion of struc- 
turM ambiguity could be constrained. Thus, an 
incremental strategy that fixes partial results is 
necessary for efficient processing and is achieved 
by bottom-up rocessing in left-to-right order. 
This paper proposes TDMT using an incremen- 
tal strategy for achieving efficient ranslation of a 
lengthy input or one having a lot of structural 
ambiguity. In this method, several constituent 
boundary patterns are applied to an input string 
in a bottom-up fashion. This bottom-up applica- 
tion, based on the concept of chart parsing, can 
constrain the explosion of structural ambiguity by 
dealing with best-only substructures u ing seman- 
tic distance calculations. 
In this paper, we will first; outline our new trans- 
lation strategy. We will then explain how con- 
stituent boundary patterns can be used to de- 
scribe the structure of an input string in TDMT. 
Then we will describe the bottom-up attern ap- 
plication, based on chart parsing. Next, we will 
show how the explosion of structural ambiguity 
is constrained by dealing with the best-only sub- 
structures, based on semantic distance calcula- 
tions. By comparing the preliminary experimen- 
tal results from the former top-down method and 
those from our new method, we will demonstrate 
the usefulness of our new method. A summary of 
our approach will conclude the paper. 
2 Translation strategy 
In TDMT, translation is performed by applying 
stored empirical transfer knowledge, which de- 
412  
scribes the correspondence between source lan- 
guage expressions and target language xpressions 
at various linguistic levels. The source and target 
expressions of tile transfer knowledge in TDMT 
arc ext)ressed by constituent boundary patterns, 
which represent meaningful units for linguistic 
structure and transfer. An efficient application of 
transfer knowledge source parts to an input string 
plays a key role in achieving quick translation. 
The procedure R)r applying constituent bound- 
ary patterns is perfomed after the assignment of 
morphological information to each word of an in- 
put string, and is as follows: 
(a) Insertion of constituent boundary marker; 
(b) 1)eriw~tion of possible structures; 
(e) Structural disambiguation 1)y semantic dis- 
lance calculation. 
In the top-down and breadth-tirst pattern ap- 
plication, the above procedure is executed in the 
described order. Because the selection of the best 
structure might have to be postponed until all pos- 
sible structm'es are derived, the costs of transla- 
tion could be high. 
In contrast, the incremental method determines 
the best structure locally and (-an constrain the 
number of competing structures fbr the whole in- 
put by performing (b) in l)arallel with (c); conse- 
quently, translation costs are reduced. 
The structure selected in (c) (:ontains its trans- 
t~rred result and head word infbrination, which is 
used for semantic distance calculation when com- 
bining with other structures. The output sentence 
is generated as a translation result Dora the struc- 
ture for the whole inl)ut, which is composed of 
best-first substructures. 
In the three subsequent sections, we will explain 
(a), (b), and (c), focusing on the bottoir>up and 
best-first ranslation strategy. 
3 Const i tuent  boundary  pattern 
In this section we will briefly explain how con- 
stituent boundary patterns are used to describe 
the structure of an int)ut string in TI)MT and 
what procedures arc applied before constituent 
boundary pattern applications (Furuse, 1994b). 
We will show bottom-up attern application by 
translating the following sample English sentence 
into Japanese: 
Thc bus goes to Chinatown at ten a.m. 
First, all the words in this sequence are assigned 
the following parts-of-speech. 
article, noun, verb, preposition, proper-noun, 
preposition, numeral, postnominal 
A constituent boundary pattern is defined as 
a sequence that; consists of variables and sym- 
bols representing constituent boundaries. A vari- 
able corresponds to some linguistic constituent 
and is expressed as a capital letter (e.g. X). 
A constituent boundary is expressed by either 
a functional word or a part-of-speech bigram 
marker (e.g. noun-verb). Variables in tile source 
language expression must be separated by con- 
stituent boundaries. 
For instance, the expression "goes to China- 
town" is divided into two constituents, i.e. "goes" 
and "Chinalown". The preposition "1o" can be 
identified as a constituent boundary. Therefor(;, 
in parsing "goes to Chinatown", we use the pat- 
tern "X to Y' ,  which has two variables X and Y 
and a constituent boundary "to". 
'l'he expression "the b~zs goes" can be divided 
into two constituents "the bud' and "goes". How- 
eve.r, there is no flmctional surface word that di- 
vides the expression into two constituents. In 
such ('ases, we emt)loy part-of-speech bigrams as 
boundary markers. "bus" and "goes" are a noun 
and a verb, respectively. Thus the marker noun- 
verb can be inserted as a boundary marker into the 
input "the bus goes", giving "The bus noun-verb 
goes". This sequence will now match tile general 
transfer knowledge pattern "X noun-verb Y". 
Of the possible bigrams in the above part-of~ 
speech sequence, only "noun-verb" is an eligi- 
ble constituent boundary marker (Fro:use, 1994b). 
This marker is inserted into the above sentence: 
The bus noun-verb goes to Chinatown at ten a.m. 
Indices to possible patterns are obtained from 
several words and bigrams in the above m~rker- 
inserted string (Table 1). 
Table 1 : Retrieved patterns 
word 
th c 
7~o?tn-vcrb 
to 
at 
retrieved pattern (linguistic level) 
the X ((:ompound noun) 
X noun-verb Y (simple sentence) 
X to Y (verb phrase, noun phrase) 
X at Y (verb phrase, noun phrase) 
X a.ra. (compound noun) 
The procedure xt)lained so far is the part that; 
the top-down and bot;tom-up attern application 
methods have in common. 
4 Incrementa l  pattern application 
In this section, we will show the application of con- 
stituent boundary patterns based on the concept 
of bottom-up chart parsing. 
4.1 L inguist ic  level 
In order to limit the combinations of patterns 
during pattern application, we distinguish pattern 
levels and for each linguistic level, we specify the 
linguistic sublevels which are permitted to be used 
in the assigned variables. 
Table 2 shows examples of the relationships be- 
tween linguistic levels. A variable on a given level 
413  
is instantiated by a string on the lingustic levels 
in the second column of Table 2. For instance, in 
the noun phrase "X of F' ,  the variables X and Y 
cannot be instantiated by a simple sentence, but 
can be instatiated by a noun phrase, a compound 
noun, and so on. 
Table 2: Possible linguistic sublevels in variables 
linguistic level sublevels of variables 
simple sentence VP, NP, ... 
verb phrase (VP) VP, NP, verb, ... 
noun phrase (NP) NP, CN, proper-noun . . . .  
compound noun (CN) CN, noun,.. .  
According to the regulation of the linguistic lev- 
els' relations hown in Table 2, a marker-inserted 
string is parsed using the constituent boundary 
patterns. 
4.2 Act ive  and  pass ive  arcs 
A chart parsing method (Kay, 1980) can avoid re- 
peatedly recomputing partial results and achieve 
incremental processing by using a bottom-up and 
left-to-right strategy. In chart parsing, an input 
string is parsed by combining active and passive 
arcs. These can be assigned to a substring of an 
input string when a pattern is applied to it. If all 
the variables of the applied pattern are instanti- 
ated or a substring can be matched to a pattern 
whose variables are all instantiated, a passive arc 
is created for the substring. When a substring can 
be matched to the left part of a pattern and the 
right variables of the pattern are not instatiated, 
an active arc is created for the substring. 
In conventional chart parsing, many arcs can 
be created because every word can create ac- 
tive and passive arcs based on its part-of-speech. 
Also, many arcs can be chained via non-terminal 
symbols such as a part-of-speech and NP (noun 
phrase). For instance, the pronoun, "f' can create 
many active arcs relevant o the rules "Pronoun 
1", "NP ~ Pronoun" and "S --+ NP VP", which 
can be chained. Therefore, a lot of computation 
is required in conventional chart parsing. 
In contrast, chart parsing with constituent 
boundary patterns can constrain the number of 
arc creations because only an constituent bound- 
ary creates active arcs while a variable (e.g. X) 
never creates an arc. We obtain indices to pat- 
terns from each word of the sentence. With these 
indices, patterns are retrieved and checked to de- 
termine whether each of them can create an arc. 
4.3 Pat tern  app l i ca t ion  a lgor i thm 
Our algorithm for bottom-up application of pat- 
terns is as follows. If the whole input string can 
be covered with a passive arc, the parsing will suc- 
ceed and the derivation of the passive arc will be 
the parsed result. 
1. If the processed string is a content word (e.g. 
noun, verb) create a passive arc. 
2. If the processed string is a constituent bound- 
ary "a", create each kind of arc as follows, 
according to the pattern I retrieved from the 
constituent boundary. 
2a. If the retrieved pattern is of the type "X a Y" 
and a left-neighboring passive arc can satisfy 
the condition for X's instantiation, create an 
active arc for "X a F ' ,  in which Y has not 
yet been instantiated. 
2b. If the retrieved pattern is of the type "X a" 
and a left-neighboring passive arc can satisfy 
the condition for X's instantiation, create a 
passive are for "X a". 
2c. If the retrieved pattern is of the type "a ~ ' ,  
create an active arc for "a ~ ' .  
3. If the created passive arc satisfies the leftmost 
part of an uninstantiated variable in the pat- 
tern of neighboring active arcs, the variable is 
instantiated with the passive arc, and a new 
passive or active arc is created. If a passive 
arc is generated in this operation, repeat the 
procedure until a new arc can no longer be 
created. 
Figure 1 shows how an input string is parsed 
using our bottom-up chart method. A solid line 
denotes a passive arc that covers a substring of 
the input below, while a dotted line denotes an 
active arc. 
The content words "bus", "goes", "Chinatown" 
and "ten" create passive arcs. The functional 
word "the", which is relevant o the pattern "a 
X", creates an active arc. The assignment of the 
functional word "a.m." to the pattern "X a" cre- 
ates a passive arc by combining another passive 
arc. The boundary markers "noun-verb", "to" and 
"at", which are relevant o the pattern "X a Y", 
create active arcs by combining left-neighboring 
passive arcs. 
First "the" creates the active arc (1) relevant o 
the pattern "the X". "bug' creates the passive arc 
(2). The passive arc (3) is created by combining 
(1) and (2). "noun-verb" creates the act ive arc 
(4), whereby the variable X of "X noun-verb F' 
is matched against (3). "bus" creates the passive 
are (5), and the passive arc (6) is created by com- 
bining (4) and (5). "to" creates the active are (7), 
whereby the variable X of "X to ~' at verb phrase 
is matched against (5). 
1There are other types of patterns, such as "X a 
Y fl ~',  where ce and /3 are constituent boundaries. 
They can be easily processed by slightly extending the 
algorithm. 
414  
(20) 
(16) 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  (12) 
. . . . . . . . . . . . . . . . .  (11) 
(lO) 
. . . . . . . . . . .  (7) 
(6) 
. . . . . . . . . . . . . . . . . . . .  (4) 
the bus noun-verb goes 
(9) 
18) 
to Chinatown at ten 
(15) 
(14) 
Figure 1: Chart diagram 
(19) 
(18) 
(1~) 
a .n2 .  
We continue the procedure incrementally. 
When the rightmost word has been processed, the 
derivation of the passive arc of the whole input 
gives the parsed result, in our example the de- 
rived process of the passive arc (20), which is the 
combination of (4) and (19). 
5 P re ference  o f  subst ructure  
The passive arc (19), which is relevant o "goes 
to Chinatown at ten a.m.", h~ two competing rc- 
suits. One is the combination of (7) and (18), 
where "X at F' is a noun phrase. The other is 
the combination of' (12) and (17), where "X at 
1("' is a verb phrase. Thus, (19) has two possible 
structures by the application of "X at F'.  "X to 
F' at the verb phrase level and "X a.m." at the 
compound noun level are also applied. 
The technique for obtaining substructure pref- 
erence is the determination of the best substruc- 
ture when a relative passive arc is created. Only 
the best substructure can be retained and com- 
bined with other arcs. 
5.1 Semantic distance 
The most appropriate st~ructure is selected by 
computing tile total sum of all possible combi- 
nations of partial semantic distance values. The 
structure with the least total distmme is judged 
most consistent with empirical knowledge and is 
chosen as the most plausible structure. 
The semantic distance between words is calcu- 
lated according to the relationship of tim positions 
of words' semantic attributes in the thesaurus. 
The distance between expressions i the sum of 
the distance between the words comprising the 
expressions, multiplied by some weights (Sumita, 
1992). 
5.2 Head word  in fo rmat ion  
The head words within variable bindings serve as 
input for distance calculations. An input for dis- 
tance calculation consists of head words in vari- 
able parts. The head part is designated in each 
pattern. Table 3 shows the head parts of the pos- 
sible substructures for "goes to Chinatown at ten 
a.m.", which corresponds to the passive arc (19). 
Table 3: Ilead words for (19)'s substructures 
passive matched designated head 
arc pattern head word 
(9),(19) X to Y X goes 
(17) X a.m. a.m. a.m. 
(18) X at Y X Ch inatown 
(19) X at Y X goes 
In "X at F' for the substring "goes to China- 
town at ten a.m" combined with (12) and (17), 
the variables X and Y are substituted for the com- 
pound expressions "goes to Chinatown" and "ten 
a.m.", respectively. Thus, in "X at Y" for the 
structure in (19), the input for distance calcula- 
tion is "goes" for "3;"' and "a.m." for "Y". Since 
the head of "X at Y" is designated as "X',  "goes" 
becomes the \[lead word for (19). This informa- 
tion is used when (19) is combined with another 
substring. 
5.3 S t ructure  select ion 
The difference in total distance value between the 
two possible structures i due only to the distance 
value of "X at F' .  Table 4 shows the results of the 
distance calculation in "X at Y" for the combina- 
tion of (7) and (18), and for that of (12) and (17). 
(goes, a.m.) expresses the bindings for variables X
415 
and Y, where X ="goeg', and Y ="a.m.".  "X'" 
is the target expression corresponding to "~' .  
Table 4: Distance calculation in "X at F'  
level 
input 
closest example 
target 
distance 
(7)+(18) 02)+07) 
noun phrase verb phrase 
(Chinatown, a.m.) (goes, a.m.) 
(morning, a.m.) (depart, a.m.) 
V no X' V ni X' 
0.50 0.21 
According to the distance calculation in the 
combination of (7) and (18), "I/' no 3;'", with the 
distance value 0.50, is selected as a target expres- 
sion. In the combination of (12) and (17), "Y' ni 
X'" with the distance value 0.21 is selected as a 
target expression. Thus, the combination of (12) 
and (17) is selected as the structure of the passive 
arc (19). Based on the results of distance cab 
culations, other partial source patterns for (19), 
"X to Y" and "X a.m", are transferred to "Y' ni 
3('" with the distance value 0.12, and "gozen X ~ 
j t '  with the distance value 0.00. Thus, the pas- 
sive arc (19) has its source and target structure 
through the combination of (12) and (17), the to- 
tal distance value 0.33, and the head word "goes". 
Then, the structure of the whole input string, 
which corresponds to (20), is constructed by com- 
bining (19) with (4). In this combination, "X 
noun-verb Y' matches the input string and is 
transferred to "X' wa Y'" based on the result of 
distance calulation. From the combined structure 
for (20), the sentence below is generated after ad- 
justment necessary for Japanese grammar. The 
words "bus", "goes", and "Chinalown" are trans- 
ferred to "basu", "iku", and "Chainalaun ''2, re- 
spectively. 
Basu wa gozen i0 ji ni Chainataun ni iki masu 
"ik~" is the conjugated form of "iku" followed 
by masu, a polite sentential-final form. 
6 P re l iminary  Exper iment  
In this section, we perform Fmglish-to-Japanese 
translation to compare the efficiency of the top- 
down pattern application with that of our new 
method, based on the bottom-up application and 
substructure preference in the TDMT prototype 
system. 
6.1 TDMT prototype  sys tem 
The TDMT prototype system, whose domain 
is travel conversations, is designed to achieve 
2The prototype system assigns a default arget ex- 
pression to a surface source expression. Another tar- 
get expression is selected when a specific example in 
the transfer knowledge is closest o the input. 
multi-lingual spoken-language translation (Fu- 
ruse, 1995). While language-oriented modules, 
such as morphological analysis and generation, 
are provided to treat multi-lingual translation, the 
transfer module, which is a central component, is
a common part of the translation system for ev- 
ery language pair. The system is written in LISP 
and runs on a UNIX machine. Presently, the pro- 
totype system can translate bilingually between 
Japanese and English and between Japanese and 
Korean. In English-to-Japanese translation, the 
present vocabulary size is about 3,000 words 3 and 
the number of training sentences i about 2,000. 
6.2 Exper imenta l  resul ts  
We have compared translation times in the TDMT 
prototype system for two cases. One case utilizes 
top-down application; the other case utilizes the 
new application method presented in this paper, 
which adopts bottom-up attern application and 
retains only one substructure using semantic dis- 
tance calculation. The translation times are mea- 
sured using a Spare10 workstation. 
We have experimented with the translation 
times of some English sentences into Japanese. 
The following sentences cause only minor struc- 
tural ambiguity. Note that a comma is not used 
in the input sentence, because it is assumed to 
be a spoken-language input such as the output of 
speech recognition. 
(1) 1 have a reservation for tomorrow. 
(2) Will my laundry be ready by tomorrow? 
(3) You can walk there in about three minutes. 
(4) Then may I have your credit card number please? 
Table 5 shows the translation time of the above 
sentences. For these translations, not much dif- 
ference could be seen between the new bottom-up 
method and the top-down method. For such in- 
puts TDMT can quickly produce the same trans- 
lation results with either method. 
'Fable 5: 'Danslation time for short sentences 
input 
sentence 
(1) 
(2) 
(3) 
(4) 
# of translation time (see) 
structures top~ new ~ - -  
2 0.18 0.17 
4 0.17 0.20 
4 0.38 0.35 
11 0.85 0.70 
The following sentences cause much structural 
ambiguity because of PP-attaehment, relative 
clauses, conjunctions, etc. 
3In the Japanese-to-English translation system, 
the present vocabulary size is about 5,000 words. 
416 
(5) 7'his sales clerk doesn't understand anything 1 say 
and i 'm wondering if you wouhl help me explain 
what \[ want. 
(6) Could I please have your name the date of arrival 
and the number of persons in your party? 
(7) 7bll somcone at the, fl'ont desk what game you 
want to scc and what type of seat you want and 
they'll get the tickets for you. 
(8) I h,fl somc laundry to be cleaned bul I can't re- 
member where the clcaners is and I was wonder- 
ing if you could help me. 
Table 6 shows the translation time of the above 
sentences, hi the above translations the same 
translation results could again be obtained for 
both methods, llowever the new method can 
achieve a far more efficient translation than the 
tol> down metho(t. 
Table 6: 'l'ranslation time for long sentences 
i t ,  p U L 
S(!ll ~oell C(\] 
(0) 
(r) 
(8) 
\[~ translation ti,ne 
\] 
! 4.oa / 
/ 2.at / 
1 le , . to ~.~7_ 3 
# of 
sLr l I ( :L  u res  
312 
442 
544 
696 
Average tramslation times in the top-down 
method were 1.15 seconds for a 10-word input and 
10.87 seconds for a 20-word input. Average trans- 
lation times in the bottom-up method were 0.55 
se(:onds for a 10-word input and 2.04 seconds for 
a 20-word inl)ut. The translation time in the top- 
down method is considere, d to t)e (:h)sely relate(l 
to the nnmber of possibh~ stru(;tures, while l,he 
translation time in our new method is not direcdy 
retle(-ted by this number. The inc.rease in the. num- 
ber of substructures retained will, the. new method 
is much smaller than that of the number of possi- 
ble structures in the top-down method. Therefore, 
our new method can efficiently translate a longer 
input string having many (-ompeting structures. 
Also, we have performed a small translation- 
quality experiment on the two pattern application 
methods with the 95 untrained sentences within 
the system's vocabulary. Both the tOl)-down 
method and the proposed bottom-up method gave 
the correct translation \[br the same 60 sentences 
with a success rate of 63.2%. ~'o,. only two sen- 
tences, difl>rent structures we.re produced by the 
two methods; however, all of them were incorrect 
translations. This experimental result shows that 
our new translation strategy maintains translation 
quMity. 
Similar results, which show the llSe~llhlesS of 
the new T I )MT tbr spokenJanguage translation, 
were obtained in other tyl)es of translation such 
as Jal)anese-to-English (or,-Korean) translation. 
7 Conc lus ion  
We have proposed an increlnental translation 
method in Transfer-Driven Machine 'lYanslation 
(TI)MT). in this method, constituent boundary 
patterns are applied to an input it, a bottom-up 
and left-to-right fhshion. Additionally, by deal- 
ing with best-only substructures, the explosion of 
structural ambiguity is constrained and eflq(-icnt 
translation of ~ lengthy input can be achieved. 
Through preliminary exl)erimentation , our new 
T I )MT has b('~e.n shown to be efficient and partic- 
ularly promising for spokendangnage translation. 
One important future research goal is tile in- 
('orporation of incremental n.orphologieal analy- 
sis and generation into the prot)osed translation 
strategy, which would provide a sinmltaneous in- 
terpretation mechanism tbr N)plication to a t)ra('- 
ti('al spoken-lm,guage translation system. Also 
important is the introduction of a repair mech- 
anism to correct the I)est-first results. 
Re ferences  
W. Finkler and A. S('hauder 1992. FAfects of In. 
(-remental ()utl)ut on lncrementM Natural \],an 
guage (~eneration. In IOlh I,\]uropean Confer? 
enee on Artif icial Intelligence, \[)ages 505- 507, 
Vienna, Austria. 
O. l"uruse, l",. Sull'lita, and H. \[ida. 1994a. 
'l'ransfi:r--Driven Machine 'lYansladon Utilizing 
l,~mpirical Knowledge (in Japanese). 7'rans~ 
actions of lnformahon Processing Sot,c@ of 
Japan, Vol. 35, No. 3, pages 414 425. 
O. Furuse, and il. \]ida. 1994b. Constituent 
Ik)undary Parsing for lCxample-llased Ma('hine 
Translation. In l)roe. 4 Uoling '9~, pages 105 
I I I .  
O. li'uruse, J. Kawai, H. Iida, S. Akamine, 
and I).B. Kim. 1995. Multi-lingual Spokem 
l,anguage Translation Utilizing Translation Ex- 
amples. In Prec. of NLPRS'95,  pages 544 549. 
M. Kay. 1980. Algorithm Schemata nd Data 
Structures in Syntactic Processing. 7>chnical 
Report USL-80-1~2, XI~;ROX Pale Alto Research 
Center. 
C. Kempen and l'\]. lh)enkamt). 1987. An \]n('re- 
mental l~rocedural GraHunar for Sentence For- 
mulation. Co.qnitivc Science, 2(11): pages 20:l 
258. 
il. Kitano. :1994. The g<DMDIALOG System. 
In Speech-2b-Spcech 75"anslation, 11. Kitano, 
Kluwer Academic Publishers, pages 47 113. 
lie Sumita and 1t. 1ida. 1992. Example-Based 
Transfer of Japanese Adnominal Particles into 
English. IEICIs' 7'ransaclions on Information 
and Syslems, F75-1), No.4, pages 585 594. 
417 
