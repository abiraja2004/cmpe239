Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 38?45,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Fast and simple semantic class assignment for biomedical text
K. Bretonnel Cohen
Computational Bioscience Program
U. Colorado School of Medicine
and
Department of Linguistics
U. of Colorado at Boulder
kevin.cohen@gmail.com
Tom Christiansen
Comput. Bioscience Prog.
U. Colorado Sch. of Medicine
tchrist@perl.com
William A. Baumgartner Jr.
Computational Bioscience Program
U. Colorado School of Medicine
william.baumgartner@ucdenver.edu
Karin Verspoor
Computational Bioscience Program
U. Colorado School of Medicine
karin.verspoor@ucdenver.edu
Lawrence E. Hunter
Computational Bioscience Program
U. Colorado School of Medicine
larry.hunter@ucdenver.edu
Abstract
A simple and accurate method for assigning
broad semantic classes to text strings is pre-
sented. The method is to map text strings
to terms in ontologies based on a pipeline of
exact matches, normalized strings, headword
matching, and stemming headwords. The
results of three experiments evaluating the
technique are given. Five semantic classes
are evaluated against the CRAFT corpus of
full-text journal articles. Twenty semantic
classes are evaluated against the correspond-
ing full ontologies, i.e. by reflexive match-
ing. One semantic class is evaluated against
a structured test suite. Precision, recall,
and F-measure on the corpus when evaluat-
ing against only the ontologies in the cor-
pus is micro-averaged 67.06/78.49/72.32 and
macro-averaged 69.84/83.12/75.31. Accuracy
on the corpus when evaluating against all
twenty semantic classes ranges from 77.12%
to 95.73%. Reflexive matching is generally
successful, but reveals a small number of er-
rors in the implementation. Evaluation with
the structured test suite reveals a number of
characteristics of the performance of the ap-
proach.
1 Introduction
Broad semantic class assignment is useful for a
number of language processing tasks, including
coreference resolution (Hobbs, 1978), document
classification (Caporaso et al, 2005), and informa-
tion extraction (Baumgartner Jr. et al, 2008). A
limited number of semantic classes have been stud-
ied extensively, such as assigning text strings to the
category gene or protein (Yeh et al, 2005;
Smith et al, 2008), or the PERSON, ORGANI-
ZATION, and LOCATION categories introduced in
the Message Understanding Conferences (Chinchor,
1998). A larger number of semantic classes have re-
ceived smaller amounts of attention, e.g. the classes
in the GENIA ontology (Kim et al, 2004), vari-
ous event types derived from the Gene Ontology
(Kim et al, 2009), and diseases (Leaman and Gon-
zalez, 2008). However, many semantic types have
not been studied at all. In addition, where ontolo-
gies are concerned, although there has been work
on finding mentions or evidence of specific terms in
text (Blaschke et al, 2005; Stoica and Hearst, 2006;
Davis et al, 2006; Shah et al, 2009), there has been
no work specifically addressing assigning multiple
very broad semantic classes with potential overlap.
In particular, this paper examines the problem of tak-
ing a set of ontologies and a text string (typically,
but not necessarily, a noun phrase) as input and de-
termining which ontology defines the semantic class
that that text string refers to. We make an equiva-
lence here between the notion of belonging to the
domain of an ontology and belonging to a specific
semantic class. For example, if a string in text refers
to something in the domain of the Gene Ontology,
we take it as belonging to a Gene Ontology seman-
tic class (using the name of the ontology only for
convenience); if a string in text refers to something
belonging to the domain of the Sequence Ontology,
we take it as belonging to a Sequence Ontology se-
mantic class. We focus especially on rapid, simple
methods for making such a determination.
The problem is most closely related to multi-class
38
classification, where in the case of this study we are
including an unusually large number of categories,
with possible overlap between them. A text string
might refer to something that legitimately belongs
to the domain of more than one ontology. For exam-
ple, it might belong to the semantic classes of both
the Gene Ontology and the Gene Regulation Ontol-
ogy; regulation is an important and frequent concept
in the Gene Ontology. This fact has consequences
for defining the notion of a false positive class as-
signment; we return to this issue in the Results sec-
tion.
2 Methods
2.1 Target semantic classes
The following ontologies were used to define se-
mantic classes:
? Gene Ontology
? Sequence Ontology
? Foundational Model of Anatomy
? NCBI Taxonomy
? Chemical Entities of Biological Interest
? Phenotypic Quality
? BRENDA Tissue/Enzyme Source
? Cell Type Ontology
? Gene Regulation Ontology
? Homology Ontology
? Human Disease Ontology
? Human Phenotype Ontology
? Mammalian Phenotype Ontology
? Molecule Role Ontology
? Mouse Adult Gross Anatomy Ontology
? Mouse Pathology Ontology
? Protein Modification Ontology
? Protein-Protein Interaction Ontology
? Sample Processing and Separation Techniques
Ontology
? Suggested Ontology for Pharmacogenomics
2.2 Methodology for assigning semantic class
We applied four simple techniques for attempting to
match a text string to an ontology. They are arranged
in order of decreasing stringency. That is, each sub-
sequent method has looser requirements for a match.
This both allows us to evaluate the contribution of
each component more easily and, at run time, allows
the user to set a stringency level, if the default is not
desired.
2.2.1 Exact match
The first and most stringent technique is exact
match. (This is essentially the only technique used
by the NCBO (National Center for Biomedical On-
tology) Annotator (Jonquet et al, 2009), although
it can also do substring matching.) We normalize
terms in the ontology and text strings in the input
for case and look for a match.
2.2.2 Stripping
All non-alphanumeric characters, including
whitespace, are deleted from the terms in the
ontology and from text strings in the input (e.g.
cadmium-binding and cadmium binding both
become cadmiumbinding) and look for a match.
2.2.3 Head nouns
This method involves a lightweight linguistic
analysis. We traversed each ontology and deter-
mined the head noun (see method below) of each
term and synonym in the ontology. We then pre-
pared a dictionary mapping from head nouns to lists
of ontologies in which those head nouns were found.
Head nouns were determined by two simple
heuristics (cf. (Collins, 1999)). For terms fitting the
pattern X of... (where of represents any preposi-
tion) the term X was taken as the head noun. For
all other terms, the rightmost word was taken as the
head noun. These two heuristics were applied in se-
quence when applicable, so that for example positive
regulation of growth (GO:0045927) becomes posi-
tive regulation by application of the first heuristic
and regulation by application of the second heuris-
tic. In the case of some ontologies, very limited pre-
39
processing was necessary?for example, it was nec-
essary to delete double quotes that appeared around
synonyms, and in some ontologies we had to delete
strings like [EXACT SYNONYM] from some terms
before extracting the head noun.
2.2.4 Stemming head nouns
In this technique, the headwords obtained by the
previous step were stemmed with the Porter stem-
mer.
2.3 Corpus and other materials
We made use of three sources in our evaluation.
One is the CRAFT (Colorado Richly Annotated Full
Text) corpus (Verspoor et al, 2009; Cohen et al,
2010a). This is a collection of 97 full-text journal
articles, comprising about 597,000 words, each of
which has been used as evidence for at least one an-
notation by the Mouse Genome Informatics group.
It has been annotated with a number of ontologies
and database identifiers, including:
? Gene Ontology
? Sequence Ontology
? Cell Type Ontology
? NCBI Taxonomy
? Chemical Entities of Biological Interest
(ChEBI)
In total, there are over 119,783 annotations. (For
the breakdown across semantic categories, see Ta-
ble 1.) All of these annotations were done by biolog-
ical scientists and have been double-annotated with
inter-annotator agreement in the nineties for most
categories.
The second source is the full sets of terms from
the twenty ontologies listed in the Introduction. All
of the twenty ontologies that we used were obtained
from the OBO portal. Version numbers are omitted
here due to space limitations, but are available from
the authors on request.
The third source is a structured test suite based on
the Gene Ontology (Cohen et al, 2010b). Structured
test suites are developed to test the performance
of a system on specific categories of input types.
This test set was especially designed to test diffi-
cult cases that do not correspond to exact matches
of Gene Ontology terms, as well as the full range of
types of terms. The test suite includes 300 concepts
from GO, as well as a number of transformations of
their terms, such as cells migrated derived from the
term cell migration and migration of cells derived
from cell migration, classified according to a num-
ber of linguistic attributes, such as length, whether
or not punctuation is included in the term, whether
or not it includes function (stop) words, etc. This
test suite determines at least one semantic category
that should be returned for each term. Unlike using
the entire ontologies, this evaluation method made
detailed error analysis possible. This test suite has
been used by other groups for broad characteriza-
tions of successes and failures of concept recogniz-
ers, and to tune the parameters of concept recogni-
tion systems.
2.4 Evaluation
We did three separate evaluations. In one, we com-
pared the output of our system against manually-
generated gold-standard annotations in the CRAFT
corpus (op. cit.). This was possible only for the on-
tologies that have been annotated in CRAFT, which
are listed above.
In the second evaluation, we used the entire on-
tologies themselves as inputs. In this method, all
responses should be the same?for example, every
term from the Gene Ontology should be classified
as belonging to the GO semantic class.
In the third, we utilized the structured test suite
described above.
2.4.1 Baselines
Two baselines are possible, but neither is optimal.
The first would be to use MetaMap (Aronson, 2001),
the industry standard for semantic category assign-
ment. (Note that MetaMap assigns specific cate-
gories, not broad ones.) However, MetaMap out-
puts only semantic classes that are elements of the
UMLS, which of the ontologies that we looked at,
includes only the Gene Ontology. The other is the
NCBO Annotator. The NCBO Annotator detects
only exact matches (or substring matches) to ontol-
ogy terms, so it is not clear that it is a strong enough
baseline to allow for a stringent analysis of our ap-
40
proach.
3 Results
We present our results in three sections:
? For the CRAFT corpus
? For the ontologies themselves
? For the Gene Ontology test suite
3.1 Corpus results
Table 1 (see next page) shows the results on the
CRAFT corpus if only the five ontologies that were
actually annotated in CRAFT are used as inputs.
The results are given for stemmed heads. Perfor-
mance on the four techniques that make up the ap-
proach is cumulative, and results for stemmed heads
reflects the application of all four techniques. In this
case, where we evaluate against the corpus, it is pos-
sible to determine false positives, so we can give
precision, recall, and F-measures for each semantic
class, as well as for the corpus as a whole. Micro-
averaged results were 67.06 precision, 78.49 recall,
and 72.32 F-measure. Macro-averaged results were
69.84 precision, 83.12 recall, and 75.31 F-measure.
Table 2 (see next page) shows the results for
the CRAFT corpus when all twenty ontologies are
matched against the corpus data, including the many
ontologies that are not annotated in the data. We
give results for just the five annotated ontologies
below. Rather than calculating precision, recall,
and F-measure, we calculate only accuracy. This
is because when classes other than the gold stan-
dard class is returned, we have no way of know-
ing if they are incorrect without manually examin-
ing them?that is, we have no way to identify false
positives. If the set of classes returned included the
gold standard class, a correct answer was counted. If
the classifier returned zero or more classes and none
of them was the gold standard, an incorrect answer
was counted. Results are given separately for each
of the four techniques. This allows us to evaluate
the contribution of each technique to the overall re-
sults; the value in each column is cumulative, so the
value for Stemmed head includes the contribution of
all four of the techniques that make up the general
approach. Accuracies of 77.12% to 95.73% were
achieved, depending on the ontology. We see that
the linguistic technique of locating the head noun
makes a contribution to all categories, but makes an
especially strong contribution to the Gene Ontology
and Cell Type Ontology classes. Stemming of head-
words is also effective for all five categories. We see
that exact match is effective only for those semantic
classes for which terminology is relatively fixed, i.e.
the NCBI taxonomy and chemical names. In some
of the others, matching natural language text is very
difficult by any technique. For example, of the 8,665
Sequence Ontology false negatives in the data re-
flected in the P/R/F values in Table 1, a full 2,050
are due to the single character +, which does not
appear in any of the twenty ontologies that we ex-
amined and that was marked by the annotators as a
Sequence Ontology term, wild type (SO:0000817).
3.2 Ontology results
As the second form of evaluation, we used the
terms from the ontologies themselves as the inputs
to which we attempted to assign a semantic class. In
this case, no annotation is required, and it is straight-
forwardly the case that each term in a given ontology
should be assigned the semantic class of that ontol-
ogy. We used only the head noun technique. We did
not use the exact match or stripping heuristics, since
they are guaranteed to return the correct answer, nor
did we use stemming. Thus, this section of the eval-
uation gives us a good indication of the performance
of the head noun approach.
As might be expected, almost all twenty on-
tologies returned results in the 97-100% correct
rate. However, we noted much lower performance
in two ontologies, the Sequence Ontology and the
Molecule Role Ontology. This lower performance
reflects a number of preprocessing errors or omis-
sions. The fact that we were able to detect these low-
performing ontologies indicates that our evaluation
technique in this experiment?trying to match terms
from an ontology against that ontology itself?is a
robust evaluation technique and should be used in
similar studies.
3.2.1 Structured test suite results
The third approach to evaluation involved use of
the structured test suite. The structured test suite re-
vealed a number of trends in the performance of the
system.
41
Ontology Annotations Precision Recall F-measure
Gene Ontology 39,626 66.31 73.06 69.52
Sequence Ontology 40,692 63.00 72.21 67.29
Cell Type Ontology 8,383 53.58 87.27 66.40
NCBI Taxonomy 11,775 96.24 92.51 94.34
ChEBI 19,307 70.07 90.53 79.00
Total (micro-averaged) 119,783 67.06 78.49 72.32
Total (macro-averaged) 69.84 83.12 75.31
Table 1: Results on the CRAFT corpus when only the CRAFT ontologies are used as input. Results are for stemmed
heads. Precision, recall, and F-measure are given for each semantic category in the corpus. Totals are micro-averaged
(over all tokens) and macro-averaged (over all categories), respectively. P/R/F are cumulative, so that the results for
stemmed heads reflect the application of all four techniques.
Ontology Exact Stripped Head noun Stemmed head
Gene Ontology 24.26 24.68 59.18 77.12
Sequence Ontology 44.28 47.63 56.63 73.33
Cell Type Ontology 25.26 25.80 70.09 88.38
NCBI Taxonomy 84.67 84.71 90.97 95.73
ChEBI 86.93 87.44 92.43 95.49
Table 2: Results on the CRAFT corpus when all twenty ontologies are used as input. Accuracy is given for each
technique. Accuracy is cumulative, so that accuracy in the final column reflects the application of all four techniques.
? The headword technique works very well for
recognizing syntactic variants. For example, if
the GO term induction of apoptosis is written
as apoptosis induction, the headword technique
allows it to be picked up.
? The headword technique works in situations
where text has been inserted into a term. For
example, if the GO term ensheathment of neu-
rons appears as ensheathment of some neu-
rons, the headword technique will allow it to be
picked up. If the GO term regulation of growth
shows up as regulation of vascular growth, the
headword technique will allow it to be picked
up.
? The headword stemming technique allows us to
pick up many verb phrases, which is important
for event detection and event coreference. For
example, if the GO term cell migration appears
in text as cells migrate, the technique will de-
tect it. The test suite also showed that failures
to recognize verb phrases still occur when the
morphological relationship between the nomi-
nal term and the verb are irregular, as for exam-
ple between the GO term growth and the verb
grows.
? The technique?s ability to handle coordination
is very dependent on the type of coordination.
For example, simple coordination (e.g. cell mi-
gration and proliferation) is handled well, but
complex coordination (e.g. cell migration, pro-
liferation and adhesion) is handled poorly.
? Stemming is necessary for recognition of plu-
rals, regardless of the length of the term in
words.
? The approach currently fails on irregular plu-
rals, due to failure of the Porter stemmer to han-
dle plurals like nuclei and nucleoli well.
? The approach handles classification of terms
that others have characterized as ?ungram-
matical,? such as transposition, DNA-mediated
(GO:0006313). This is important, because ex-
act matches will always fail on these terms.
42
4 Discussion
4.1 Related work
We are not aware of similar work that tries to assign
a large set of broad semantic categories to individ-
ual text strings. There is a body of work on selecting
a single ontology for a domain or text. (Mart??nez-
Romero et al, 2010) proposes a method for selecting
an ontology given a list of terms, all of which must
appear in the ontology. (Jonquet et al, 2009) de-
scribes an ontology recommender that first annotates
terms in a text with the Open Biomedical Annotator
service, then uses the sum of the scores of the indi-
vidual annotations to recommend a single ontology
for the domain as a whole.
4.2 Possible alternate approaches
Three possible alternative approaches exist, all of
which would have as their goal the returning of a sin-
gle best semantic class for every input. However, for
the use cases that we have identified?coreference
resolution, document classification, information ex-
traction, and curator assistance?we are more inter-
ested in wide coverage of a broad range of semantic
classes, so these approaches are not evaluated here.
However, we describe them for completeness and
for the use of researchers who might be interested
in pursuing single-class assignment.
4.2.1 Frequent words
One alternative approach would be to use simple
word frequencies. For example, for each ontology,
one could determine the N most frequent words, fil-
tering out stop words. At run time, check the words
in each noun phrase in the text against the lists of fre-
quent words. For every word from the text that ap-
peared in the list of frequent words from some ontol-
ogy, assign a score to each ontology in which it was
found, weighting it according to its position in the
list of frequent words. In theory, this could accom-
modate for the non-uniqueness of word-to-ontology
mappings, i.e. the fact that a single word might ap-
pear in the lists for multiple ontologies. However,
we found the technique to perform very poorly for
differentiating between ontologies and do not rec-
ommend it.
4.2.2 Measuring informativeness
If the system is desired to return only one sin-
gle semantic class per text string, then one approach
would be to determine the informativeness of each
word in each ontology. That is, we want to find the
maximal probability of an ontology given a word
from that ontology. This approach is very difficult
to normalize for the wide variability in size of the
many ontologies that we wanted to be able to deal
with.
4.2.3 Combining scores
Finally, one could conceivably combine scores for
matches obtained by the different strategies, weight-
ing them according to their stringency, i.e. exact
match receiving a higher weight than head noun
match, which in turn would receive a higher weight
than stemmed head noun match. This weighting
might also include informativeness, as described
above.
4.3 Why the linguistic method works
As pointed out above, the lightweight linguistic
method makes a large contribution to the perfor-
mance of the approach for some ontologies, partic-
ularly those for which the exact match and stripping
techniques do not perform well. It works for two
reasons, one related to the approach itself and one
related to the nature of the OBO ontologies. From
a methodological perspective, the approach is effec-
tive because headwords are a good reflection of the
semantic content of the noun phrase and they are
relatively easy to access via simple heuristics. Of
course simple heuristics will fail, as we can observe
most obviously in the cases where we failed to iden-
tify members of the ontologies in the second eval-
uation step. However, overall the approach works
well enough to constitute a viable tool for coref-
erence systems and other applications that benefit
from the ability to assign broad semantic classes to
text strings.
The approach is also able to succeed because of
the nature of the OBO ontologies. OBO ontologies
are meant to be orthogonal (Smith et al, 2007). A
distributional analysis of the distribution of terms
and words between the ontologies (data not shown
here, although some of it is discussed below), as well
as the false positives found in the corpus study, sug-
43
gests that orthogonality between the OBO ontolo-
gies is by no means complete. However, it holds
often enough for the headword method to be effec-
tive.
4.4 Additional error analysis
In the section on the results for the structured test
suite, we give a number of observations on contribu-
tions to errors, primarily related either to the char-
acteristics of individual words or to particular syn-
tactic instantiations of terms. Here, we discuss some
aspects of the distribution of lexical items and of the
corpus that contributed to errors.
? The ten most common headwords appear in
from 6-16 of the twenty ontologies. However,
they typically appear in one ontology at a fre-
quency many orders of magnitude greater than
their frequency in the other ontologies. Taking
this frequency data into account for just these
ten headwords would likely decrease false pos-
itives quite significantly.
? More than 50% of Gene Ontology terms share
one of only ten headwords. Many of our Gene
Ontology false negatives on the corpus are be-
cause the annotated text string does not contain
a word such as process or complex that is the
head word of the canonical term.
4.5 Future work
The heuristics that we implemented for extracting
headwords from OBO terms were very simple, in
keeping with our initial goal of developing an easy,
fast method for semantic class assignment. How-
ever, it is clear that we could achieve substantial per-
formance improvements from improving the heuris-
tics. We may pursue this track, if it becomes clear
that coreference performance would benefit from
this when we incorporate the semantic classification
approach into a coreference system.
On acceptance of the paper, we will make Perl and
Java versions of the semantic class assigner publicly
available on SourceForge.
4.6 Conclusion
The goal of this paper was to develop a simple ap-
proach to assigning text strings to an unprecedent-
edly large range of semantic classes, where mem-
bership in a semantic class is equated with belonging
to the semantic domain of a specific ontology. The
approach described in this paper is able to do that
at a micro-averaged F-measure of 72.32 and macro-
averaged F-measure of 75.31 as evaluated on a man-
ually annotated corpus where false positives can be
determined, and with an accuracy of 77.12-95.73%
when only true positives and false negatives can be
determined.
References
A. Aronson. 2001. Effective mapping of biomedical text
to the UMLS Metathesaurus: The MetaMap program.
In Proc AMIA 2001, pages 17?21.
William A. Baumgartner Jr., Zhiyong Lu, Helen L. John-
son, J. Gregory Caporaso, Jesse Paquette, Anna Linde-
mann, Elizabeth K. White, Olga Medvedeva, K. Bre-
tonnel Cohen, and Lawrence Hunter. 2008. Concept
recognition for extracting protein interaction relations
from biomedical text. Genome Biology, 9.
Christian Blaschke, Eduardo A. Leon, Martin Krallinger,
and Alfonso Valencia. 2005. Evaluation of BioCre-
ative assessment of task 2. BMC Bioinformatics, 6
Suppl 1.
J. Gregory Caporaso, William A. Baumgartner Jr..,
K. Bretonnel Cohen, Helen L. Johnson, Jesse Paque-
tte, and Lawrence Hunter. 2005. Concept recognition
and the TREC Genomics tasks. In The Fourteenth Text
REtrieval Conference (TREC 2005) Proceedings.
Nancy A. Chinchor. 1998. Overview of MUC-7/MET-2.
K. Bretonnel Cohen, Helen L. Johnson, Karin Verspoor,
Christophe Roeder, and Lawrence E. Hunter. 2010a.
The structural and content aspects of abstracts versus
bodies of full text journal articles are different. BMC
Bioinformatics, 11(492).
K. Bretonnel Cohen, Christophe Roeder, William
A. Baumgartner Jr., Lawrence Hunter, and Karin Ver-
spoor. 2010b. Test suite design for biomedical ontol-
ogy concept recognition systems. In Proceedings of
the Language Resources and Evaluation Conference.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
N. Davis, H. Harkema, R. Gaizauskas, Y. K. Guo,
M. Ghanem, T. Barnwell, Y. Guo, and J. Ratcliffe.
2006. Three approaches to GO-tagging biomedical
abstracts. In Proceedings of the Second International
Symposium on Semantic Mining in Biomedicine, pages
21?28, Jena, Germany.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
44
C. Jonquet, N.H. Shah, and M.A. Musen. 2009. Pro-
totyping a biomedical ontology recommender ser-
vice. In Bio-Ontologies: Knowledge in Biology,
ISMB/ECCB SIG.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,
Yuka Tateisi, and Nigel Collier. 2004. Introduction
to the bio-entity recognition task at JNLPBA. In Pro-
ceedings of the international joint workshop on natu-
ral language processing in biomedicine and its appli-
cations, pages 70?75.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
BioNLP 2009 Companion Volume: Shared Task on En-
tity Extraction, pages 1?9.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survey of advances in biomedical
named entity recognition. In Pac Symp Biocomput.
Marcos Mart??nez-Romero, Jose? Va?zquez-Naya, Cris-
tian R. Munteanu, Javier Pereira, and Alejandro Pazos.
2010. An approach for the automatic recommendation
of ontologies using collaborative knowledge. In KES
2010, Part II, LNAI 6277, pages 74?81.
Nigam H. Shah, Nipun Bhatia, Clement Jonquet, Daniel
Rubin, Annie P. Chiang, and Mark A. Musen. 2009.
Comparison of concept recognizers for building the
Open Biomedical Annotator. BMC Bioinformatics,
10.
Barry Smith, Michael Ashburner, Cornelius Rosse,
Jonathan Bard, William Bug, Werner Ceusters,
Louis J. Goldberg, Karen Eilbeck, Amelia Ireland,
Christopher J. Mungall, The OBI Consortium, Neo-
cles Leontis, Philippe Rocca-Serra, Alan Ruttenberg,
Susanna-Assunta Sansone, Richard H. Scheuermann,
Nigam Shah, Patricia L. Whetzel, and Suzanna Lewis.
2007. The OBO Foundry: coordinated evolution of
ontologies to support biomedical data integration. Na-
ture Biotechnology, 25:1251?1255.
Larry Smith, Lorraine Tanabe, Rie Johnson nee Ando,
Cheng-Ju Kuo, I-Fang Chung, Chun-Nan Hsu, Yu-
Shi Lin, Roman Klinger, Christof Friedrich, Kuzman
Ganchev, Manabu Torii, Hongfang Liu, Barry Had-
dow, Craig Struble, Richard Povinelli, Andreas Vla-
chos, William Baumgartner, Jr., Lawrence Hunter,
Bob Carpenter, Richard Tzong-Han Tsai, Hong-
Jie Dai, Feng Liu, Yifei Chen, Chengjie Sun,
Sophia Katrenko, Pieter Adriaans, Christian Blaschke,
Rafael Torres Perez, Mariana Neves, Preslav Nakov,
Anna Divoli, Manuel Mana, Jacinto Mata-Vazquez,
and W. John Wilbur. 2008. Overview of BioCreative
II gene mention recognition. Genome Biology.
E. Stoica and M. Hearst. 2006. Predicting gene functions
from text using a cross-species approach. In Proceed-
ings of the 11th Pacific Symposium on Biocomputing.
Karin Verspoor, K. Bretonnel Cohen, and Lawrence
Hunter. 2009. The textual characteristics of traditional
and Open Access scientific journals are similar. BMC
Bioinformatics, 10.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. BioCreatve task 1A: gene mention finding eval-
uation. BMC Bioinformatics, 6(Suppl. 1).
45
