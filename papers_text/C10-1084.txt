Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 743?751,
Beijing, August 2010
Enhancing Morphological Alignment for Translating
Highly Inflected Languages ?
Minh-Thang Luong
School of Computing
National University of Singapore
luongmin@comp.nus.edu.sg
Min-Yen Kan
School of Computing
National University of Singapore
kanmy@comp.nus.edu.sg
Abstract
We propose an unsupervised approach uti-
lizing only raw corpora to enhance mor-
phological alignment involving highly in-
flected languages. Our method focuses on
closed-class morphemes, modeling their
influence on nearby words. Our language-
independent model recovers important
links missing in the IBM Model 4 align-
ment and demonstrates improved end-to-
end translations for English-Finnish and
English-Hungarian.
1 Introduction
Modern statistical machine translation (SMT)
systems, regardless of whether they are word-,
phrase- or syntax-based, typically use the word as
the atomic unit of translation. While this approach
works when translating between languages with
limited morphology such as English and French,
it has been found inadequate for morphologically-
rich languages like Arabic, Czech and Finnish
(Lee, 2004; Goldwater and McClosky, 2005;
Yang and Kirchhoff, 2006). As a result, a line
of SMT research has worked to incorporate mor-
phological analysis to gain access to information
encoded within individual words.
In a typical MT process, word aligned data is
fed as training data to create a translation model.
In cases where a highly inflected language is
involved, the current word-based alignment ap-
proaches produce low-quality alignment, as the
statistical correspondences between source and
?This work was supported by a National Research Foun-
dation grant ?Interactive Media Search? (grant # R-252-000-
325-279)
target words are diffused over many morpholog-
ical forms. This problem has a direct impact on
end translation quality.
Our work addresses this shortcoming by
proposing a morphologically sensitive approach
to word alignment for language pairs involving
a highly inflected language. In particular, our
method focuses on a set of closed-class mor-
phemes (CCMs), modeling their influence on
nearby words. With the model, we correct er-
roneous alignments in the initial IBM Model 4
runs and add new alignments, which results in im-
proved translation quality.
After reviewing related work, we give a case
study for morpheme alignment in Section 3. Sec-
tion 4 presents our four-step approach to construct
and incorporate our CCM alignment model into
the grow-diag process. Section 5 describes exper-
iments, while Section 6 analyzes the system mer-
its. We conclude with suggestions for future work.
2 Related Work
MT alignment has been an active research area.
One can categorize previous approaches into those
that use language-specific syntactic information
and those that do not. Syntactic parse trees
have been used to enhance alignment (Zhang and
Gildea, 2005; Cherry and Lin, 2007; DeNero
and Klein, 2007; Zhang et al, 2008; Haghighi et
al., 2009). With syntactic knowledge, modeling
long distance reordering is possible as the search
space is confined to plausible syntactic variants.
However, they generally require language-specific
tools and annotated data, making such approaches
infeasible for many languages. Works that follow
non-syntactic approaches, such as (Matusov et al,
743
i1 declare2 resumed3 the4 session5 of6 the7 european8 parliament9 adjourned10 on11 1312 december13 199614
-1 julistan2 euroopan3 parlamentin4 perjantaina5 136 joulukuuta7 19968 keskeytyneen9 istuntokauden10 uudelleen11 avatuksi12
Direct: 1-2 2-2 3-9 4-3 5-10 6-10 7-3 8-12 9-12 10-12 11-5 12-6 13-7 14-8Inverse: 1-1 2-2 8-3 9-4 10-5 12-6 13-7 14-8 10-9 10-10 10-11 10-12
(a) Gloss: -1 declare2 european3 parliament 4 on-friday5 136 december7 19968 adjourned9 session10 resumed11,12
i1 declare2 resume+3 d4 the5 session6 of7 the8 european9 parliament10 adjourn+11 ed12 on13 1314 december15 199616
- julist+ a+ n euroopa+ n parlament+ in perjantai+ n+ a 13 joulukuu+ ta 1996 keskeyty+ neen istunto+ kauden uude+ lle+ en avatuksi1   2      3   4        5          6          7           8        9          10 11 12      13        14   15         16           17        18            19        20       21  22       23Direct: 1-23 2-23 3-23 4-23 5-22 6-23 7-22 8-6 9-5 10-7 11-16 12-16 13-9 14-12 15-13 16-15Inverse: 1-1 2-2 2-3 5-4 9-5 8-6 10-7 10-8 11-9 0-10 7-11 14-12 15-13 15-14 16-15 11-16 11-17 11-18 11-19 11-20 11-21 0-22 11-23
(b)
Figure 1: Sample English-Finnish IBM Model 4 alignments: (a) word-level and (b) morpheme-level. Solid lines indicate
intersection alignments, while the exhaustive asymmetric alignments are listed below. In (a), translation glosses for Finnish
are given; the dash-dot line is the incorrect alignment. In (b), bolded texts are closed-class morphemes (CCM), while bolded
indices indicate alignments involving CCMs. The dotted lines are correct CCM alignments not found by IBM Model 4.
2004; Liang et al, 2006; Ganchev et al, 2008),
which aim to achieve symmetric word alignment
during training, though good in many cases, are
not designed to tackle highly inflected languages.
Our work differs from these by taking a middle
road. Instead of modifying the alignment algo-
rithm directly, we preprocess asymmetric align-
ments to improve the input to the symmetrizing
process later. Also, our approach does not make
use of specific language resources, relying only on
unsupervised morphological analysis.
3 A Case for Morpheme Alignment
The notion that morpheme based alignment would
be useful in highly inflected languages is intu-
itive. Morphological inflections might indicate
tense, gender or number that manifest as separate
words in largely uninflected languages. Capturing
these subword alignments can yield better word
alignments that otherwise would be missed.
Let us make this idea concrete with a case study
of the benefits of morpheme based alignment. We
show the intersecting alignments of an actual En-
glish (source) ? Finnish (target) sentence pair in
Figure 1, where (a) word-level and (b) morpheme-
level alignments are shown. The morpheme-
level alignment is produced by automatically seg-
menting words into morphemes and running IBM
Model 4 on the resulting token stream.
Intersection links (i.e., common to both direct
and inverse alignments) play an important role in
creating the final alignment (Och and Ney, 2004).
While there are several heuristics used in the sym-
metrizing process, the grow-diag(onal) process is
common and prevalent in many SMT systems,
such as Moses (Koehn et al, 2007). In the grow-
diag process, intersection links are used as seeds
to find other new alignments within their neigh-
borhood. The process continues iteratively, until
no further links can be added.
In our example, the morpheme-level intersec-
tion alignment is better as it has no misalignments
and adds new alignments. However it misses
some key links. In particular, the alignments of
closed-class morphemes (CCMs; later formally
defined) as indicated by the dotted lines in (b) are
overlooked in the IBM Model 4 alignment. This
difficulty in aligning CCMs is due to:
1. Occurrences of garbage-collector words
(Moore, 2004) that attract CCMs to align to
them. Examples of such links in (b) are 1?23
or 11?21 with the occurrences of rare words
adjourn+11 and avatuksi23. We further
characterize such errors in Section 6.1.
2. Ambiguity among CCMs of the same surface
that causes incorrect matchings. In (b), we
observe multiple occurrence of the and n
on the source and target sides respectively.
While the link 8?6 is correct, 5?4 is not as i1
should be aligned to n4 instead. To resolve
such ambiguity, context information should
be considered as detailed in Section 4.3.
The fact that rare words and multiple affixes
often occur in highly inflected languages exacer-
bates this problem, motivating our focus on im-
proving CCM alignment. Furthermore, having ac-
cess to the correct CCM alignments as illustrated
744
in Figure 1 guides the grow-diag process in find-
ing the remaining correct alignments. For exam-
ple, the addition of CCM links i1?n4 and d4?
lle21 helps to identify declare2?julist2
and resume3?avatuksi23 as admissible align-
ments, which would otherwise be missed.
4 Methodology
Our idea is to enrich the standard IBM Model 4
alignment by modeling closed-class morphemes
(CCMs) more carefully using global statistics and
context. We realize our idea by proposing a four-
step method. First, we take the input parallel cor-
pus and convert it into morphemes before training
the IBM Model 4 morpheme alignment. Second,
from the morpheme alignment, we induce auto-
matically bilingual CCM pairs. The core of our
approach is in the third and fourth steps. In Step 3,
we construct a CCM alignment model, and apply
it on the segmented input corpus to obtain an au-
tomatic CCM alignment. Finally, in Step 4, we in-
corporate the CCM alignment into the symmetriz-
ing process via our modified grow-diag process.
4.1 Step 1: Morphological Analysis
The first step presupposes morphologically seg-
mented input to compute the IBM Model 4 mor-
pheme alignment. Following Virpioja et al
(2007), we use Morfessor, an unsupervised an-
alyzer which learns morphological segmentation
from raw tokenized text (Creutz and Lagus, 2007).
The tool segments input words into labeled
morphemes: PRE (prefix), STM (stem), and SUF
(suffix). Multiple affixes can be proposed for
each word; word compounding is allowed as well,
e.g., uncarefully is analyzed as un/PRE+
care/STM+ ful/SUF+ ly/SUF. We append a
?+? sign to each nonfinal tag to distinguish word-
internal morphemes from word-final ones, e.g.,
?x/STM? and ?x/STM+? are considered different
tokens. The ?+? annotation enables the restoration
of the original words, a key point to enforce word
boundary constraints in our work later.
4.2 Step 2: Bilingual CCM Pairs
We observe that low and highly inflected lan-
guages, while intrinsically different, share more
en fi en fi en fi
the1 -n?1 in6 -ssa?15 me166 -ni?60-s2 -t?9 is7 on?2 me166 minun?282to3 -a?6 that8 etta??7 why168 siksi?187to3 maan91 that8 ettei?283 view172 mielta??162of4 -a4 we10 -mme?10 still181 viela??108of4 -en?5 we10 meida?n?52 where183 jossa?209of4 -sta?19 we10 me?113 same186 samaa?334and5 ja?3 we10 emme123 he187 ha?n?184and5 seka??122 we10 meilla??231 good189 hyva??321and5 eika?203 . . . . . . over-408 yli-?391
Table 1: English(en)-Finnish(fi) Bilingual CCM pairs
(N=128). Shown are the top 19 and last 10 of 168 bilingual
CCM pairs extracted. Subscript i indicates the ith most fre-
quent morpheme in each language. ? marks exact correspon-
dence linguistically, whereas ? suggests rough correspon-
dence w.r.t http://en.wiktionary.org/wiki/.
in common at the morpheme level. The many-
to-one relationships among words on both sides
is often captured better by one-to-one correspon-
dences among morphemes. We wish to model
such bilingual correspondence in terms of closed-
class morphemes (CCM), similar to Nguyen and
Vogel (2008)?s work that removes nonaligned af-
fixes during the alignment process. Let us now
formally define CCM and an associative measure
to gauge such correspondence.
Definition 1. Closed-class Morphemes (CCM)
are a fixed set of stems and affixes that ex-
hibit grammatical functions just like closed-class
words. In highly inflected languages, we observe
that grammatical meanings may be encoded in
morphological stems and affixes, rather than sep-
arate words. While we cannot formally identify
valid CCMs in a language-independent way (as
by definition they manifest language-dependent
grammatical functions), we can devise a good ap-
proximation. Following Setiawan et al (2007),
we induce the set of CCMs for a language as the
top N frequent stems together with all affixes1.
Definition 2. Bilingual Normalized PMI
(biPMI) is the averaged normalized PMI com-
puted on the asymmetric morpheme alignments.
Here, normalized PMI (Bouma, 2009), known to
be less biased towards low-frequency data, is de-
fined as: nPMI(x, y) = ln p(x,y)p(x)p(y))/- ln p(x, y),
where p(x), p(y), and p(x, y) follow definitions
in the standard PMI formula. In our case, we only
1Note that we employ length and vowel sequence heuris-
tics to filter out corpus-specific morphemes.
745
compute the scores for x, y being morphemes fre-
quently aligned in both asymmetric alignments.
Given these definitions, we now consider a pair
of source and target CCMs related and termed a
bilingual CCM pair (CCM pair, for short) if they
exhibit positive correlation in their occurrences
(i.e., positive nPMI2 and frequent cooccurrences).
We should note that relying on a hard thresh-
old of N as in (Setiawan et al, 2007) is brittle
as the CCM set varies in sizes across languages.
Our method is superior in the use of N as a start-
ing point only; the bilingual correspondence of the
two languages will ascertain the final CCM sets.
Take for example the en and fi CCM sets with
154 and 214 morphemes initially (each consist-
ing of N=128 stems). As morphemes not having
their counterparts in the other language are spu-
rious, we remove them by retaining only those in
the CCM pairs. This effectively reduces the re-
spective sizes to 91 and 114. At the same time,
these final CCMs cover a much larger range of top
frequent morphemes than N , up to 408 en and 391
fi morphemes, as evidenced in Table 1.
4.3 Step 3: The CCM Alignment Model
The goal of this model is to predict when appear-
ances of a CCM pair should be deemed as linking.
With an identified set of CCM pairs, we know
when source and target morphemes correspond.
However, in a sentence pair there can be many in-
stances of both the source and target morphemes.
In our example, the the?n pair corresponds to
definite nouns; there are two the and three -n in-
stances, yielding 2? 3=6 possible links.
Deciding which instances are aligned is a deci-
sion problem. To solve this, we inspect the IBM
Model 4 morpheme alignment to construct a CCM
alignment model. The CCM model labels whether
an instance of a CCM pair is deemed semantically
related (linked). We cast the modeling problem as
supervised learning, where we choose a maximum
entropy (ME) formulation (Berger et al, 1996).
We first discuss sample selection from the IBM
Model 4 morpheme alignment, and then give de-
tails on the features extracted. The processes de-
scribed below are done per sentence pair with fm1 ,
2nPMI has a bounded range of [?1, 1] with values 1 and
0 indicating perfect positive and no correlation, respectively.
en1 and U denoting the source, target sentences and
the union alignments, respectively.
Class labels. We base this on the initial IBM
Model 4 alignment to label each CCM pair in-
stance as a positive or negative example: Positive
examples are simply CCM pairs in U. To be pre-
cise, links j?i in U are positive examples if fj?ei
is a CCM pair. To find negative examples, we in-
ventory other potential links that share the same
lexical items with a positive one. That is, a link
j??i? not in U is a negative example, if a positive
link j?i such that fj = f ?j and ei = e?i exists.
We stress that our collection of positive exam-
ples contains high-precision but low-recall IBM
Model 4 links, which connect the reliable CCM
pairs identified before. The model then general-
izes from these samples to detect incorrect CCM
links and to recover the correct ones, enhancing
recall. We later detail this process in ?4.4.
Feature Set. Given a CCM pair instance, we
construct three feature types: lexical, monolin-
gual, and bilingual (See Table 2). These features
capture the global statistics and contexts of CCM
pairs to decide if they are true alignment links.
? Lexical features reflect the tendency of the
CCM pair being aligned to themselves. We use
biPMI, which aggregates the global alignment
statistics, to determine how likely source and tar-
get CCMs are associated with each other.
? Monolingual context features measure the
association among tokens of the same language,
capturing what other stems and affixes co-occur
with the source/target CCM:
1. within the same word (intra). The aim is to
disambiguate affixes as necessary in highly
inflected languages where same stems could
generate different roles or meanings.
2. outside the CCM?s word boundary (inter).
This potentially capture cues such as tense,
or number agreement. For example, in En-
glish, the 3sg agreement marker on verbs -s
often co-occurs with nearby pronouns e.g.,
he, she, it; whereas the same marker on
nouns (-s), often appears with plural deter-
miners e.g., these, those, many.
To accomplish this, we compute two monolin-
gual nPMI scores in the same spirit as biPMI, but
using the morphologically segmented input from
746
Feature Description Examples
Lexical ? biPMI: None [?1, 0], Low (0, 1/3], Medium (1/3, 2/3], High (2/3, 1] pmid?lle=LowMonolingual Context ? Capture morpheme cooccurrence with the src/tgt CCMIntra ? Within the same word srcWd?lle=resume, tgtWd?lle=en, tgtWd?lle=uudeInter ? To the Left & Right, in different words srcLd?lle=i, srcRd?lle=the, tgtRd?lle=avatuksiBilingual context ? Capture neighbor links? cooccurrence with the CCM pair linkbi0 ? Most descriptive, capturing in terms of surface forms only ? maybe sparse bi0d?lle=resume?avatuksibi1 ? Generalizes morphemes into relative locations (Left, Within, Right) bi1d?lle=W?avatuksi, bi1d?lle=resume?Rbi2 ? Most general, coupling token types (Close, Open) /w relative positions bi2d?lle=O?WR
Table 2: Maximum entropy feature set. Shown are feature types, descriptions and examples. Most examples are given for
the alignment d4?lle+21 of the same running example in ?3. Note that we only partially list the bilingual context features.
each language separately. Two morphemes are
?linked? if within a context window of wc words.
? Bilingual context features model cross-
lingual reordering, capturing the relationships be-
tween the CCM pair link and its neighbor3 links.
Consider a simple translation between an English
phrase of the form we ?verb? and the Finnish
one ?verb? -mme, where -mme is the 1pl verb
marker. We aim to capture movements such as
?the open-class morphemes on the right of we and
on the left of -mme are often aligned?. These will
function as evidence for the ME learner to align
the CCM pair (we, -mme). We encode the bilin-
gual context at three different granularities, from
most specific to most general ones (cf Table 2).
4.4 Step 4: Incorporate CCM Alignment
At test time, we apply the trained CCM alignment
model to all CCM pairs occurring in each sentence
pair to find CCM links. On our running exam-
ple in Figure 1, the CCM classifier tests 17 CCM
pairs, identifying 6 positive CCM links of which
4 are true positives (dotted lines in (b)).
Though mostly correct, we note that some of
the predicted links conflict: (d4?lle21, ed12?
neen17 and ed12?lle21) share alignment end-
points. Such sharing in CCM alignments is rare
and we believe should be disallowed. This moti-
vates us to resolve all CCM link conflicts before
incorporating them into the symmetrizing process.
Resolving link conflicts. As CCM pairs are
classified independently, they possess classifica-
tion probabilities which we use as evidence to re-
solve the conflicts. In our example, the classifica-
tion probabilities for (d4?lle21, ed12?neen17,
ed12?lle21) are (0.99, 0.93, 0.79) respectively.
We use a simple, ?best-first? greedy approach
3Within a context window of wc words as in monolingual.
to determine which links are kept and which are
dropped to satisfy our assumption. In our case,
we pick the most confident link, d4?lle21 with
probability 0.99. This precludes the incorrect link,
ed12?lle21, but admits the other correct one
ed12?neen17, probability 0.93. As a result, this
resolution successfully removes the incorrect link.
Modifying grow-diag. We incorporate the set
of conflict-resolved CCM links into the grow-diag
process. This step modifies the input alignments
as well as the growing process. U and I denote the
IBM Model 4 union and intersection alignments.
In our view, the resolved CCM links can serve
as a quality mark to ?upgrade? links before input
into the grow-diag process. We upgrade resolved
CCM links: (a) those ? U ? part of I , treating
them as alignment seeds; (b) those /? U ? part
of U , using them for exploration and growing. To
reduce spurious alignments, we discarded links in
U that conflict with the resolved CCM links.
In the usual grow-diag, links immediately adja-
cent to a seed link l are considered candidates to
be appended into the alignment seeds. While suit-
able for word-based alignment, we believe it is too
small a context when the input are morphemes.
For morpheme alignment, the candidate context
makes more sense in terms of word units. We thus
enforce word boundaries in our modified grow-
diag. We derive word boundaries for end points in
l using the morphological tags and the ?+? word-
end marker mentioned in ?4.1. Using such bound-
aries, we can then extend the grow-diag to con-
sider candidate links within a neighborhood of wg
words; hence, enhancing the candidate coverage.
5 Experiments
We use English-Finnish and English-Hungarian
data from past shared tasks (WPT05 and WMT09)
747
to validate our approach. Both Finnish and Hun-
garian are highly inflected languages, with numer-
ous verbal and nominal cases, exhibiting agree-
ment. Dataset statistics are given in Table 3.
en-fi # en-hu #
Train Europarl-v1 714K Europarl-v4 1,510K
LM Europarl-v1-fi 714K News-hu 4,209K
Dev wpt05-dev 2000 news-dev2009 2051
Test wpt05-test 2000 news-test2009 3027
Table 3: Dataset Statistics: the numbers of parallel sen-
tences for training, LM training, development and test sets.
We use the Moses SMT framework for our
work, creating both our CCM-based systems and
the baselines. In all systems built, we obtain
the IBM Model 4 alignment via GIZA++ (Och
and Ney, 2003). Results are reported using case-
insensitive BLEU (Papineni et al, 2001).
Baselines. We build two SMT baselines:
w-system: This is a standard phrase-based
SMT, which operates at the word level. The sys-
tem extracts phrases of maximum length 7 words,
and uses a 4-gram word-based LM.
wm-system: This baseline works at the word
level just like the w-system, but differs at the
alignment stage. Specifically, input to the IBM
Model 4 training is the morpheme-level corpus,
segmented by Morfessor and augmented with ?+?
to provide word-boundary information (?4.1). Us-
ing such information, we constrain the alignment
symmetrization to extract phrase pairs of 7 words
or less in length. The morpheme-based phrase ta-
ble is then mapped back to word forms. The pro-
cess continues identically as in the w-system.
CCM-based systems. Our CCM-based sys-
tems are similar in spirit to the wm system: train at
the morpheme, but decode at the word level. We
further enhance the wm-system at the alignment
stage. First, we train our CCM model based on
the initial IBM Model 4 morpheme alignment, and
apply it to the morpheme corpus to obtain CCM
alignment, which are input to our modified grow-
diag process. The CCM approach defines the set-
ting of three parameters: ?N , wc, wg? (Section 4).
Due to our resource constraints, we set N=128,
similar to (Setiawan et al, 2007), and wc=1 ex-
perimentally. We only focus on the choice of wg,
testing wg={1, 2} to explore the effect of enforc-
ing word boundaries in the grow-diag process.
5.1 English-Finnish results
We test the translation quality of both directions
(en-fi) and (fi-en). We present results in Table 4 for
7 systems, including: our baselines, three CCM-
based systems with word-boundary knowledge
wg={0, 1, 2} and two wm-systems wg={1, 2}.
Results in Table 4 show that our CCM approach
effectively improves the performance. Compared
to the wm-system, it chalks up a gain of 0.46
BLEU points for en-fi, and a larger improvement
of 0.93 points for the easier, reverse direction.
Further using word boundary knowledge in our
modified grow-diag process demonstrates that the
additional flexibility consistently enhances BLEU
for wg = 1, 2. We achieve the best performance
at wg = 2 with improvements of 0.67 and 1.22
BLEU points for en-fi and fi-en, respectively.
en-fi fi-en
w-system 14.58 23.56
wm-system 14.47 22.89
wm-system + CCM 14.93+0.46 23.82+0.93
wm-system + CCM + wg = 1 15.01 23.95
wm-system + CCM + wg = 2 15.14+0.67 24.11+1.22
wm-system + wg = 1 14.44 22.92
wm-system + wg = 2 14.28 23.01
(Ganchev, 2008) - Base 14.72 22.78
(Ganchev, 2008) - Postcat 14.74 23.43+0.65
(Yang, 2006) - Base N/A 22.0
(Yang, 2006) - Backoff N/A 22.3+0.3
Table 4: English/Finnish results. Shown are BLEU
scores (in %) with subscripts indicating absolute improve-
ments with respect to the wm-system baseline.
Interestingly, employing the word boundary
heuristic alone in the original grow-diag does not
yield any improvement for en-fi, and even worsens
as wg is enlarged (as seen in Rows 6?7). There
are only slight improvements for fi-en with larger
wg.This attests to the importance of combining the
CCM model and the modified grow-diag process.
Our best system outperforms the w-system
baseline by 0.56 BLEU points for en-fi, and yields
an improvement of 0.55 points for fi-en.
Compared to works experimenting en/fi trans-
lation, we note the two prominent ones by Yang
and Kirchhoff (2006) and recently by Ganchev
et al (2008). The former uses a simple back-off
method experimenting only fi-en, yielding an im-
provement of 0.3 BLEU points. Work in the op-
748
posite direction (en-fi) is rare, with the latter pa-
per extending the EM algorithm using posterior
constraints, but showing no improvement; for fi-
en, they demonstrate a gain of 0.65 points. Our
CCM method compares favorably against both ap-
proaches, which use the same datasets as ours.
5.2 English-Hungarian results
To validate our CCM method as language-
independent, we also perform preliminary exper-
iments on en-hu. Table 5 shows the results using
the same CCM setting and experimental schemes
as in en/fi. An improvement of 0.35 BLEU points
is shown using the CCM model. We further im-
prove by 0.44 points with word boundary wg=1,
but performance degrades for the larger window.
Due to time constraints, we leave experiments
for the reverse, easier direction as future work.
Though modest, the best improvement for en-hu
is statistical significant at p<0.01 according to
Collins? sign test (Collins et al, 2005).
System BLEU
w-system 9.63
wm-system 9.47
wm-system + CCM 9.82 +0.35
wm-system + CCM + wg = 1 9.91 +0.44
wm-system + CCM + wg = 2 9.87
Table 5: English/Hungarian results. Subscripts indicate
absolute improvements with respect to the wm-system.
We note that MT experiments for en/hu 4 are
very limited, especially for the en to hu direction.
Nova?k (2009) obtained an improvement of 0.22
BLEU with no distortion penalty; whereas Koehn
and Haddow (2009) enhanced by 0.5 points us-
ing monotone-at-punctuation reordering, mini-
mum Bayes risk and larger beam size decoding.
While not directly comparable in the exact set-
tings, these systems share the same data source
and splits similar to ours. In view of these com-
munity results, we conclude that our CCM model
does perform competitively in the en-hu task, and
indeed seems to be language independent.
6 Detailed Analysis
The macroscopic evaluation validates our ap-
proach as improving BLEU over both baselines,
4Hungarian was used in the ACL shared task 2008, 2009.
but how do the various components contribute?
We first analyze the effects of Step 4 in produc-
ing the CCM alignment, and then step backward
to examine the contribution of the different feature
classes in Step 3 towards the ME model.
6.1 Quality of CCM alignment
To evaluate the quality of the predicted CCM
alignment, we address the following questions:
Q1: What is the portion of CCM pairs being
misaligned in the IBM Model 4 alignment?
Q2: How does the CCM alignment differ from
the IBM Model 4 alignment?
Q3: To what extent do the new links introduced
by our CCM model address Q1?
Given that we do not have linguistic expertise in
Finnish or Hungarian, it is not possible to exhaus-
tively list all misaligned CCM pairs in the IBM
Model 4 alignment. As such, we need to find other
form of approximation in order to address Q1.
We observe that correct links that do not exist
in the original alignment could be entirely miss-
ing, or mistakenly aligned to neighboring words.
With morpheme input, we can also classify mis-
takes with respect to intra- or inter-word errors.
Figure 2 characterizes errors T1, T2 and T3, each
being a more severe error class than the previous.
Focusing on ei in the figure, links connecting ei
to fj? or fj?? are deemed T1 errors (misalignments
happen on one side). A T2 error aligns f ??j within
the same word, while a T3 error aligns it outside
the current word but still within its neighborhood.
This characterization is automatic, cheap and has
the advantage of being language-independent.
fj fj' fj??
1 word
T1T2T3
1 word
ei ei' ei??
Figure 2: Categorization of CCM missing links. Given
that a CCM pair link (fj?ei) is not present in the IBM Model
4, occurrences of any nearby link of the types T[1?3] can be
construed as evidence of a potential misalignment.
Statistics in Table 6(ii) answers Q1, suggest-
ing a fairly large number of missing CCM links:
3, 418K for en/fi and 6, 216K for en/hu, about
12.35% and 12.06% of the IBM Model 4 union
alignment respectively. We see that T1 errors con-
749
stitute the majority, a reasonable reflection of the
garbage- collector5 effect discussed in Section 3.
General (i) Missing CCM links (ii)en/fi en/hu en/fi en/hu
Direct 17,632K 34,312K T1 2,215K 3,487KInverse 18,681K 34,676K T2 358K 690K
D ? I 8,643K 17,441K T3 845K 2,039K
D ? I 27,670K 51,547K Total 3,418K 6,216K
Table 6: IBM Model 4 alignment statistics. (i) General
statistics. (ii) Potentially missing CCM links.
Q2 is addressed by the last column in Ta-
ble 7. Our CCM model produces about 11.98%
(1,035K/8,643K) new CCM links as compared to
the size of the IBM Model 4 intersection align-
ment for en/fi, and similarly, 9.52% for en/hu.
Orig. Resolved I U\I New
en/fi 5,299K 3,433K 1065K 1,332K 1,035K
en/hu 9,425K 6,558K 2,752K 2,146K 1,660K
Table 7: CCM vs IBM Model 4 alignments. Orig. and
Resolved give # CCM links predicted in Step 4 before and
after resolving conflicts. Also shown are the number of re-
solved links present in the Intersection, Union excluding I
(U\I) of the IBM Model 4 alignment and New CCM links.
Lastly, figures in Table 8 answer Q3, revealing
that for en/fi, 91.11% (943K/1,035K) of the new
CCM links effectively cover the missing CCM
alignments, recovering 27.59% (943K/3,418K) of
all missing CCM links. Our modified grow-diag
realizes a majority 76.56% (722K/943K) of these
links in the final alignment.
We obtain similar results in the en/hu pair for
link recovery, but a smaller percentage 22.59%
(330K/1,461K) are realized through the modified
symmetrization. This partially explains why im-
provements are modest for en/hu.
New CCM Links (i) Modified grow-diag (ii)
en/fi en/hu en/fi en/hu
T1 707K 1,002K 547K 228K
T2 108K 146K 79K 22K
T3 128K 313K 96K 80KTotal 943K 1,461K 722K 330K
Table 8: Quality of the newly introduced CCM links.
Shown are # new CCM links addressing the three error types
before (i) and after (ii) the modified grow-diag process.
6.2 Contributions of ME Feature Classes
We also evaluate the effectiveness the ME features
individually through ablation tests. For brevity,
5E.g., ei prefers f?j or f??j (garbage collectors) over fj .
we only examine the more difficult translation di-
rection, en to fi. Results in Table 9 suggest that
all our features are effective, and that removing
any feature class degrades performance. Balanc-
ing specificity and generality, bi1 is the most
influential feature in the bilingual context group.
For monolingual context, inter, which captures
larger monolingual context, outperforms intra.
The most important feature overall is pmi, which
captures global alignment preferences. As feature
groups, bilingual and monolingual context fea-
tures are important sources of information, as re-
moving them drastically decreases system perfor-
mance by 0.23 and 0.16 BLEU, respectively.
System BLEU
all (wm-system+CCM) 14.93
?bi2 14.90 ?intra 14.89
?bi1 14.84??0.09 ?pmi 14.81??0.12
?bi0 14.89 ?bi{2/1/0} 14.70??0.23
?inter 14.85 ?in{ter/tra} 14.77??0.16
Table 9: ME feature ablation tests for English-Finnish
experiments. ? mark results statistically significant at p <
0.05, differences are subscripted.
7 Conclusion and Future Work
In this work, we have proposed a language-
independent model that addresses morpheme
alignment problems involving highly inflected
languages. Our method is unsupervised, requiring
no language specific information or resources, yet
its improvement on BLEU is comparable to much
semantically richer, language-specific work. As
our approach deals only with input word align-
ment, any downstream modifications of the trans-
lation model also benefit.
As alignment is a central focus in this work, we
plan to extend our work over different and mul-
tiple input alignments. We also feel that better
methods for the incorporation of CCM alignments
is an area for improvement. In the en/hu pair, a
large proportion of discovered CCM links are dis-
carded, in favor of spurious links from the union
alignment. Automatic estimation of the correct-
ness of our CCM alignments may improve end
translation quality over our heuristic method.
750
References
Berger, Adam L., Stephen D. Della Pietra, and Vin-
cent J. D. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Bouma, Gerlof. 2009. Normalized (pointwise) mutual
information in collocation extraction. In Proceed-
ings of the Biennial GSCL Conference, Tu?bingen,
Gunter Narr Verlag.
Cherry, Colin and Dekang Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In SSST.
Collins, Michael, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In ACL.
Creutz, Mathias and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Trans. Speech Lang. Pro-
cess., 4(1):3.
DeNero, John and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In
ACL.
Ganchev, Kuzman, Joa?o V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations? In
ACL-HLT.
Goldwater, Sharon and David McClosky. 2005. Im-
proving statistical mt through morphological analy-
sis. In HLT.
Haghighi, Aria, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised itg models. In ACL.
Koehn, Philipp and Barry Haddow. 2009. Edinburgh?s
submission to all tracks of the WMT2009 shared
task with reordering and speed improvements to
Moses. In EACL.
Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In ACL, Demonstration Session.
Lee, Young-Suk. 2004. Morphological analysis for
statistical machine translation. In HLT-NAACL.
Liang, Percy, Ben Taskar, and Dan Klein. 2006.
Alignment by agreement. In HLT-NAACL.
Matusov, Evgeny, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In COLING.
Moore, Robert C. 2004. Improving IBM word-
alignment model 1. In ACL.
Nguyen, Thuy Linh and Stephan Vogel. 2008.
Context-based Arabic morphological analysis for
machine translation. In CoNLL.
Nova?k, Attila. 2009. MorphoLogic?s submission for
the WMT 2009 shared task. In EACL.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Setiawan, Hendra, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
ACL.
Virpioja, Sami, Jaakko J. Vyrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In MT Summit
XI.
Yang, Mei and Katrin Kirchhoff. 2006. Phrase-based
backoff models for machine translation of highly in-
flected languages. In EACL.
Zhang, Hao and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In ACL.
Zhang, Hao, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL-HLT.
751
