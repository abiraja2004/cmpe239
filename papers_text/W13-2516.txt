Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 129?137,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
 
 
Learning Comparable Corpora from Latent Semantic Analysis  
Simplified Document Space 
 
 
Ekaterina Stambolieva 
euroscript Luxembourg S.?. r.l. 
55, rue de Luxembourg, L-8077 
Luxembourg 
ekaterina.stambolieva@euroscript.lu 
 
  
 
Abstract 
Focusing on a systematic Latent Semantic 
Analysis (LSA) and Machine Learning (ML) 
approach, this research contributes to the de-
velopment of a methodology for the automatic 
compilation of comparable collections of doc-
uments. Its originality lies within the delinea-
tion of relevant comparability characteristics 
of similar documents in line with an estab-
lished definition of comparable corpora. These 
innovative characteristics are used to build a 
LSA vector-based representation of the texts. 
In accordance with this new reduced in dimen-
sionality document space, an unsupervised 
machine learning algorithm gathers similar 
texts into comparable clusters. On a monolin-
gual collection of less than 100 documents, the 
proposed approach assigns comparable docu-
ments to different comparable corpora with 
high confidence. 
 
1 Introduction 
The problem of collecting comparable corpora is 
challenging and yet enchanting. Many can bene-
fit from the availability of such corpora as trans-
lation professionals, machine learning research-
ers and computational linguistics specialists. Yet 
there is not an even consent about the notion 
covered by the term comparable corpora. The 
degree of similarity between comparable corpora 
documents has not been formalized strictly and 
leaves space for different interpretations of simi-
larity, contributing to abundant text collections 
of similar and semi-similar documents. The cur-
rent research endeavors to contribute to an ap-
proach, which assembles a collection of compa-
rable documents that are closely related to each 
other on the basis of a strict definition of compa-
rable corpora. The proposed approach incorpo-
rates originally a Latent Semantic Analysis tech-
nique in order to match similar concepts instead 
of words thus contributing to better automatic 
learning of comparability between documents.  
2 Comparable Corpora Definition 
Maia (2003) discusses the characteristics of 
comparable corpora. Nevertheless, the adopted 
definition of comparable corpora in this study is 
given by McEnery (2003): 
?Comparable corpora are corpora where series 
of monolingual corpora are collected for a range 
of languages, preferably using the same sampling 
and frame and with similar balance and repre-
sentativeness, to enable the study of those lan-
guages in contrast.? 
 
McEnery (2003) characterizes comparable 
corpora as ?corpora where series of monolingual 
corpora are collected for the range of languages?. 
In the views of McEnery (2003), a monolingual 
corpus is a corpus that is not collected for a range 
of languages, but instead the documents selected 
are written in one language. In the context of the 
current research, a comparable corpus, a sub-
language corpus, can be constructed from docu-
ments in one language under the condition they 
are compliant with the preferred guidelines pro-
vided by McEnery (2003). These preferred 
guidelines are similar sampling frame, balance 
and representativeness. 
A document feature corresponding to text 
sampling is explicated taking into consideration 
the domain and genre of the documents. Addi-
129
  
tionally, similar terminology vocabulary insures 
genre correspondence. Therefore, the same sam-
pling scheme in collecting documents is evaluat-
ed considering domain and genre and viewed as 
document features.  
Language is rapidly changing and evolving 
throughout the years (Crystal 2001). As a result, 
restricting the time period a document has been 
published increases the chances of it being com-
parable to another one written during the same 
time frame. When events are reported in the 
newspaper domain, their date of publication is 
strong similarity evidence and is used as a filter 
between weakly comparable and non-comparable 
text articles (Skadi?a et al 2010a). 
The question of how representativeness of a 
corpus is decided upon is answered in different 
ways depending on the specific corpus purpose. 
For the purposes of this research, a corpus is 
considered representative when corresponding 
texts are similar in size. As reported by Manning 
and Sch?tze (1999), a balanced corpus is one, 
which is assembled ?as to give each subtype of 
text a share of the corpus that is proportional to 
some predetermined criterion of importance?. 
Skadina et al (2010b) present a good summary 
of the advantages of exploiting comparable cor-
pora. It is discussed that ?they can draw on much 
richer, more available and more diverse sources 
which are produced every day (e.g. multilingual 
news feeds) and are available on the Web in 
large quantities for many languages and do-
mains.? (Skadina et al 2010b). 
3 Related Work 
The most closely-related to machine learning 
work that mines comparable corpora is that by 
Sharoff (2010). His research incorporates intelli-
gent self-learning techniques to the compilation 
of comparable documents. Unlike other re-
searchers that experiment with Cross-Lingual 
Information Retrieval (CLIR) techniques as in 
Tao and Zhai (2005), Sharoff (2010) estimates 
the document collection?s internal subgroup sys-
tem in search for structure. The possible structure 
and grouping of a set of documents is most easily 
defined by ranked words that are representative 
for the subsets in the collection. Sharoff's ap-
proach relies heavily on keywords and keyword 
estimation. One thing Sharoff (2010) does not 
elaborate on in details is the definition of a com-
parable corpus. A possible reason for that is that 
unsupervised machine learning approaches pro-
duce related sets of documents in an environment 
where the selection process is automated and not 
supervised by any linguistically-dependent rules. 
What is written by Goeuriot et al (2009) is al-
so an influential and relevant material to the cur-
rent research. Their paper is on the compilation 
of comparable corpora in a specialized domain 
with a focus on English and Japanese. The article 
is significant for the reason the authors investi-
gate ways of building comparable corpora using 
machine learning classification algorithms, 
namely Support Vector Machine and C4.5. The 
experimental setup in the work of Goeuriot et al 
(2009) relies on manually labeled data, which is 
then fed to the machine learning algorithm core. 
The paper by Goeuriot et al (2009) is directed 
towards building a tool to automatically compile 
comparable corpora in a predefined set of docu-
ments and languages. The text comparability 
characteristics extracted, which allow compari-
son between the documents, are external and in-
ternal to the textual data. Goeuriot et al (2009) 
emphasize on selecting ways to automatic recog-
nition of useful features similar texts have and 
experiment with these features to test and predict 
their reliability. The comparability of the docu-
ments defined by them is on three levels - type of 
discourse, topic and domain, focusing on locu-
tive, ellocutive and allocutive act labels. 
Bekavac et al (2004) discuss the grounds of a 
methodology describing similarity comparison of 
under-resourced monolingual corpora. Contrary 
to other methodologies that exploit seed words or 
seed texts as a basis for search, the researchers 
have at their disposal two monolingual docu-
ments sets from which they aim to mine compa-
rable documents. The advantage of their ap-
proach is that it is applicable to texts collection 
written in one language for the reason that they 
are easily mined and compiled from the available 
textual resources nowadays. The concept behind 
their research is to align comparable documents 
that are found in pre-collected different monolin-
gual corpora. Content features are used to test the 
degree to which two texts are similar to each 
other in the sense of sharing the same infor-
mation and common words. These features, 
composition features, need to be representative 
for the texts. The composition features, extracted 
from the data, monitor the size, the format and 
the time span of the documents. 
Clustering based on semantic keyword extrac-
tion is performed by Finkelstein et al (2001). 
This approach is relevant to the current research 
as it suggests a different methodology of feeding 
texts to machine learning algorithms. The re-
130
  
searchers aim to generate new content based on 
input user queries by using context ? ?a body of 
words surrounding a user-selected phrase? 
(Finkelstein et al 2001). They emphasise on the 
significance of using context when developing 
Natural Language Processing (NLP) applica-
tions. The keyword extraction algorithm present-
ed relies on a precisely-designed clustering algo-
rithm, different than k-means, to recursively 
clean clustering results and present refined statis-
tical output. 
With regards to evaluation metrics of compa-
rable corpora, one of the main focuses of the 
ACCURAT Project (Skadina et al 2010b) is to 
design metrics of comparability estimation be-
tween texts. The ACCURAT researchers (Skadi-
na et al 2010b) concentrate on the development 
of comparable corpora criteria for different texts 
and different types of parallelism between the 
texts. Saralegi et al (2008) suggest measures 
based on distribution of topics or time with re-
gards to publication dates. Kilgariff (2001) aims 
to measure the level of comparability between 
two collections of documents. He focuses addi-
tionally on the shortcoming of known corpus 
similarity metrics. He discusses evaluation meth-
ods for corpus comparability measures, which 
are based on Spearman rank correlation co-
efficient, perplexity and cross-entropy, ?2 and 
others. To his knowledge, the ?2 test performs the 
best when comparing two sets of documents. It is 
important to note that the approach adopted by 
Kilgariff (2001) relies on words and n-gram se-
quence features. Not only does he regard the 
texts as bag-of-words, but also he incorporates n-
gram characteristics in his evaluation metric 
analysis.  
Mining word similarity techniques are dis-
cussed in the work of Deerwester et al (1990); 
Baeza-Yates and Ribeiro-Netto (1999); and Da-
gan, Lee and Pereira (1999). Deerwester et al 
(1990) incorporate LSA as a technique to identi-
fy word relatedness. LSA ?identifies a number of 
most prominent dimensions in the data, which 
are assumed to correspond to ?latent concepts?.? 
(Radinsky et al 2011). Radinsky et al (2011) 
indicate that LSA vector space models are ?diffi-
cult to interpret?. Consequently, the current re-
search focuses not only on the incorporation of 
LSA to mapping content, but also of the em-
ployment of a machine learning technique to 
group projected into the two-dimensional space 
documents into similar clusters. Baeza-Yates and 
Ribeiro-Netto (1999), as Sharoff (2009) and 
Goeuriot et al (2010), consider texts as bag-of-
words as the least complex word similarity ap-
proaches can be incorporated. Mapping distri-
butional similarity, Lee (1999) opts for similar 
word co-occurrence probability estimation im-
provement. Dagan et al (1999) also aim for 
better estimation of word co-occurrence likeli-
hood not based on empirical methods, but in-
stead relying on distributional similarity for the 
generation of language models. WordNet-
based and distributional-similarity compari-
sons of word similarity are presented in Agirre 
et al (2009). They suggest different views of 
word relatedness comparison ? bag-of-words, 
context windows and syntactic dependency 
approaches. They describe their findings as 
yielding best results on known test sets. What 
is important to be remarked is that their meth-
odology requires minor fine-tuning in order to 
give good results on cross-lingual word simi-
larity. 
4 Methodology 
The novelty of our approach is the incorpora-
tion of the Latent Semantic Analysis tech-
nique, which matches concepts, or information 
units, from one document to another instead of 
approximating word similarity. LSA expects 
and constructs a new vector-based representa-
tion of the documents to be compared. A con-
cept holds not only textual, but also morpho-
logical information about each word present in 
the texts. By employing LSA, the document 
space is projected into the two-dimensional 
space in correspondence with the latent rela-
tionships between the words in the texts. In the 
two-dimensional space, clusters of similar 
documents are compiled together using a sim-
ple, but powerful unsupervised machine learn-
ing algorithm, k-means clustering. Clustering 
evaluation metrics such as precision, recall and 
purity are employed towards automatic evalua-
tion and analysis of the resulting comparable 
corpora. 
In order to compile comparable corpora with 
the current settings, a set of pre-collected doc-
uments is needed. From this set of documents, 
two to five comparable corpora are identified 
and texts with similar topics, domains and fea-
tures are assigned to relevant comparable cor-
pora. 
LSA has its known limitations. It acknowl-
edges documents as bags-of-words and mines 
131
  
the latent relationships between the words in 
the bags-of-words. Working with information 
units overcomes this limitation of LSA. The 
information units contain additional linguistic 
information about the syntactic and morpho-
logical relationships between words, therefore 
forming concepts of these words. The order of 
the words, or the information units, is not im-
perative, therefore it is not controlled by the 
methodology. 
LSA allows words to have only one mean-
ing thus restricting the robustness of the natu-
ral languages. This limitation is tackled by 
suggesting different word sense candidates for 
words and constructing a separate information 
unit for each promoted word sense.  
5 Data Feature Selection 
The innovation of the discussed research ap-
proach lays in its basic concept of perceiving 
texts as bags of interrelated concepts. The sur-
face-form words found in the texts are en-
riched with linguistic information that furnish-
es better matching procedure of the concepts 
lying within the texts for comparison. 
Unlike previous work, which regards docu-
ments as bags-of-words (Sharoff 2009, 
Goeuriot et al 2010) the methodology treats 
documents as collections of concepts, each 
concept containing comparable textual infor-
mation. The concepts are represented by in-
formation units. The process of recognizing 
such units happens at document level, where 
each document is viewed as a separate text 
with its own context. Each information unit is 
defined as the inseparable pair of lemma and 
its context-dependent part-of-speech (POS) 
tag. A lemmatization technique is applied to 
transform the texts into linguistically-
simplified versions of the originals, where each 
word (infected or not) is substituted by its cor-
responding lexeme. 
As stated before, the information units in-
corporate POS output. A POS tagger is used to 
process the texts before linguistically-
simplifying it using lemmatization techniques. 
The idea of enriching the words by POS infor-
mation is not new to the research of Natural 
Language Processing, but it is new for the re-
search of compiling comparable corpora. By 
identifying the POS information of a sentence, 
lexical ambiguity is reduced. The accompany-
ing POS tag to each lemma assists the disam-
biguation of the information units. For exam-
ple, run as being the action of walking fast has 
a verb POS tag opposed to run as the period of 
some event happening has a noun POS tag. In 
this example, the POS tag provides the needed 
information for disambiguating the two differ-
ent meanings of a word. In the current research 
scenario, the POS tagging module 1  emulates 
the results of a basic Word Sense Disambigua-
tion technique. 
Furthermore, the input set of documents is 
transformed into a set of lists of information 
units as described, where a single list of units 
corresponds to a single document. When com-
pared, the units are matched for correspond-
ence both based on the lemma's lexical catego-
ry in the sentence and its base form. 
Another feature, which helps build context re-
lated concepts, is the identification of Noun 
Phrases (NP) in the texts. Noun Phrase recogni-
tion is imperative since it further develops the 
simple word sense disambiguation method. Some 
words to have a different meaning when occur-
ring in a chain of words such as a noun phrase. 
Unlike the proposed by Su and Babych (2012) 
approach to NP recognition, NPs are identified 
following linguistically-derived rules, which rep-
resent common constructions of the language 
under consideration. When a NP is identified, it 
is listed as a new information unit with a corre-
sponding NP POS tag. All POS annotations as 
well as lemma information of its constituent 
words are removed from the documents' list of 
information units.  
6 Experiments  
6.1 Experimental Corpus 
A pre-collected corpus of documents, part of the 
NPs for Events (NP4E) corpus (Hasler et al 
2006), is used for experimenting. The NP4E cor-
pus is collected for the special purpose of ex-
tracting coreference resolution in English. Never-
theless, the structure and the organization of the 
corpus are suitable for the needs of acquisition of 
a test corpus for the current study. The NP4E 
corpus contains five different groups of news 
articles based on topic gathered from the Reuters. 
The news articles are collected in the time frame 
                                                 
1 TreeTagger http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/ 
 
132
  
of two years ? 1996 and 1997 (Rose, Stevenson 
and Whitehead 2002). Four of the five NP4E 
news article groups are used to compile an exper-
imental corpus containing roughly 40000 words 
or 520 words per text. The chosen experimental 
collection consists of sub-corpora that 
have documents comparable to the others in their 
sub-corpora based on domain. The domain of 
these comparable corpora is terrorism, and the 
four distinct topics are connected with terrorism, 
bombing and suicide respectively for events in 
Israel, Tajikistan, China and Peru. In total, the 
experimental corpus consists of 77 newswire ar-
ticles. The distribution of the documents in this 
selected corpus is 20 on Israel topic, 19 on Tajik-
istan topic, 19 for China topic and 19 on Peru 
topic. These sub-corpora are referred to as Israel 
(I), Tajikistan (T), China (C) and Peru (P) on-
wards.  
6.2 Experimental Set-up 
The experimental set-up is structured as a chain 
of two simple procedures. They are respectively 
an experimental setup data selection 
and experimental setup clustering distribution. 
6.2.1 Data Selection Frame 
The data selection frame describes how docu-
ment features are selected. The documents are 
afterwards preprocessed in order to extract all 
underlying text features and binary vectors are 
constructed to represent each separate document. 
The document features on focus consist of all 
identified information units enriched with the 
noun phrases that were recognized in the texts. 
The binary vectors then are used as an input to 
the LSA algorithm. 
6.2.2 Cluster Distribution 
The number of resulting clusters, or comparable 
corpora, should be set in advance for unsuper-
vised machine learning algorithms. An experi-
ments with k, k is in the range of 2 to 5, are con-
ducted. Testing with number of clusters greater 
or equal to two comes logical. In the case of ex-
pecting two resulting clusters, the methodology 
groups all similar documents in one comparable 
corpus, and withdraws the non-similar docu-
ments to the second collection. When k is chosen 
to be 2 or 3, the resulting comparable corpora 
tend to be weakly-comparable (Skadi?a et al 
2010a) for the reason the algorithms are forced to 
gather documents with four distinct topics into 
only two or three comparable collections. It is 
interesting to analyze the research methodolo-
gy?s performance in the case four output compa-
rable corpora are expected, meaning when the 
learning algorithm is asked to suggest four com-
parable sets of documents.  
To evaluate clustering performance in terms of 
forcing the system to split the document collec-
tion into more comparable corpora than present, 
k equals to 5 is also used in the experiments. 
Consequently, the number of clusters varies be-
tween 2 and 5.  
6.2.3 Evaluation Metrics 
Three metrics are chosen to evaluate results - the 
standard precision and recall, and additionally - 
purity. Precision shows how many documents in 
the resulting collections are identified correctly 
as comparable to the majority of documents on a 
specific topic in the cluster. For example, when 
16 out of 19 documents are recognized to be 
comparable to each other, the precision of this 
clustering result is 0.84. Recall shows how many 
false negatives are identified as comparable to a 
certain topic-related collection of texts. The false 
negatives are the documents on a different topic, 
which the machine learning algorithm falsely 
lists to be comparable to documents on another 
topic. When 21 documents are grouped in one 
similarity cluster, 19 of them being on a related 
topic, 3 of them being on another topic, the recall 
of the learning performance is 0.86. 
Purity is an evaluation metric used to estimate 
the purity of the resulting clusters (Figure 1.). A 
cluster is recognized as pure when it contains a 
number of documents with the same label (mean-
ing they are listed to be comparable to each other 
by a human evaluator) and as less as possible 
documents that have a different label from the 
dominant label (Manning et al 2008): 
 
Figure 1. Purity score formula 
 
where nomcluster i is the number of the majority 
class members in each resulting cluster i, and 
noclustrers is the number of resulting clusters, or k. 
As Manning et al (2008) warn ?High purity is 
easy to achieve when the number of clusters is 
large - in particular, purity is 1 if each document 
gets its own cluster?. The number of clusters for 
the current research is not big. Nonetheless, the 
results are evaluated based on two other metrics. 
133
  
The other metrics for measuring the 
comparability between documents that are 
chosen for exploitation in the current research, 
are Mutual Infromation (MI) and Normalized 
Mutual Infroamtion (NMI). The formula for 
NMI is as follows and shown in Figure 2.:  
 
 
Figure 2. NMI score formula 
 
MI is explained in details in Kalgariff (2001) 
and (Manning et al 2008). Manning et al (2008) 
discuss additionally the formula for the entropy 
H, and NMI. ? is the group of clusters addressed 
in the experiments, and C is the group of labels ? 
namely the different characteristics of the com-
parable corpora.  
In the current scenario, no human evaluation is 
performed. Rather than that the corpus is pre-
designed in a way to contain four different com-
parable corpora that need not to be manually la-
beled 
6.3 Evaluation 
Results are obtained after conducting different 
set-up experiments. One set-up focuses on evalu-
ating comparable corpus collection having as an 
input part of the experimental corpus. This part 
contains documents on two out of the four differ-
ent topics. The two-topic collections are com-
piled by combining all combinations possible of 
two topic-based sets together from the four dis-
tinct topic sub-corpora. In this experimental sce-
nario, the total of different corpora for evaluation 
is 6 (according to the combination?s formula ) 
- Peru and China, Peru and Tajikistan, Peru and 
Israel, Tajikistan and China, Tajikistan and Isra-
el, China and Israel. Table 1 shows the results of 
running LSA with k-means clustering on the dis- 
 
 
cussed sub-groups. As seen on Table 1. the learn-
ing algorithm performance is excellent when the 
number of comparable corpora that are expected 
is greater than two. When three or more compa-
rable clusters are elected, each similar by topic 
document is grouped with all other documents 
that are comparable to it in the same resulting 
comparable corpus. In the case of expecting three 
comparable corpora with Precision and Recall 
equal to 1.0, one of these corpora contains all 
documents of two different sub-corpora and the 
rest contain all documents of one of the pre-
defined experimental sub-corpora. In the case of 
expecting five comparable corpora with Preci-
sion and Recall equal to 1.0, one sub-corpus is 
split into two comparable clusters, these clusters 
containing documents on the same topic. What is 
interesting in this experimental set-up are the 
results the learning algorithm obtains when it 
aims to produce only two comparable clusters. 
For three of the test sets - China and Israel, Peru 
and China and Tajikistan and Israel, grouping of 
documents on different topics into the same simi-
lar collection is seen. The lowest results obtained 
are for the test set Tajikistan and Israel, where 3 
of the 19 documents on an Israel topic are 
grouped together with the texts on the Tajikistan 
topic. The reason behind this automatic learning 
confusion originates from the fact the Tajikistan 
and Israel topic documents contain many similar 
concepts, which make good clustering harder to 
achieve.  
The purity of the resulting corpora is very 
high, above 0.9, indicating that comparable doc-
uments are identified correctly with high rele-
vance. The only exception is the results on the 
Tajikistan and Israel test set with purity 0.56. 
This exception occurs because of poor clustering 
results, which have been discussed.  
 
 
 
Sub-
corpus 
Topic Precision    Recall    Purity 
  2Cl 3Cl 4Cl 5Cl 2Cl 3Cl 4Cl 5Cl  
P Peru 0.84 1 1 1 1 1 1 1 0.921 
C China 1 1 1 1 0.86 1 1 1  
P Peru 0.84 1 1 1 1 1 1 1 0.921 
T Tajikistan 1 1 1 1 0.86 1 1 1  
P Peru 1 1 1 1 1 1 1 1 1.00 
I Israel 1 1 1 1 1 1 1 1  
T Tajikistan 1 1 1 1 1 1 1 1 1.00 
C China 1 1 1 1 1 1 1 1  
T Tajikistan 1 1 1 1 0.52 1 1 1 0.56 
I Israel 0.15 1 1 1 1 1 1 1  
C China 0.86 1 1 1 1 1 1 1 0.923 
I Israel 1 1 1 1 0.85 1 1 1  
Table 1. Clustering results for test sets of combinations of two topic sub-corpora 
(nCl pointing to the numbers of clusters identified ) 
134
  
Another set-up focuses on the analysis and 
evaluation of the results on clusters containing 
documents on three of the four different topics. 
The same way as the two-topic collections are 
constructed, combining three topic sub-corpora 
into one results in the development of the input 
for the LSA and k-means clustering algorithms.  
In this experimental scenario, a total of 4 distinct 
input collections are compiled -Tajikistan, Israel 
and China; Tajikistan, Israel and Peru; Peru, 
China and Israel; and Tajikistan, China and Peru.   
The results of the learning comparable corpora 
from them are listed in Table 2. As it can be easi-
ly seen, the clustering performance is impecca-
ble. Therefore, providing more documents, more 
data features, helps identifying better similar 
documents applying the proposed research ap-
proach. 
 
 
 
 
 Precision Recall Purity 
 2cl 3cl 4cL 5cl 2cl 3cl 4cl 5cl  
T 1 1 1 1 1 1 1 1  
C 1 1 1 1 1 1 1 1 1.00 
I 1 1 1 1 1 1 1 1  
P 1 1 1 1 1 1 1 1  
Table 3.  Clustering results on the whole experi-
mental corpus 
 
 Mutual 
Information 
H(?) H(C) NMI 
2CL 2CL 2CL 2Cl 
Peru 
China 
0.6866 0.9927 1 0.6916 
Peru 
Tajikistan 
0.6866 0.9927 1 0.6916 
Peru 
Israel 
1.0230 1.0074 1.0074 0.9522 
Tajikistan 
China 
1 1 1 1 
Tajikistan 
Israel 
0.0844 0.3912 1.0074 0.1262 
China 
Israel 
0.6855 0.9744 1.0074 0.6917 
Table 4.  MI and NMI scores results for test sets of 
combinations of two topic sub-corpora 
Table 3. Shows the clustering results when all 
texts of the experimental corpus are suggested as 
an input. The algorithms once more do not have 
problems collecting the similar documents into 
comparable corpora with high precision and re-
call. 
MI and NMI are computed only for the results 
presented in Table 1. The reasoning behind is 
that Table 2. And Table 3. show perfect cluster-
ing results of comparable corpora obtained on 
the whole set of input documents described in 
Section 6.1.   
The results of the comparable texts grouping 
are estimated using a clustering quality trade-off 
metric, NMI. Table 4. shows the NMI results of 
the clustering performance on the two-topic col-
lections described in the first experimental set-up 
at the beginning of  Section 6.3.  
 
 
 
Consequently, the results shown on Table 4. 
are obtained with respects to the precision, recall 
and purity scores presented in Table 1. The NMI 
score is evidence of the identified comparable 
corpora quality. As seen on Table 4., the lowest 
NMI score correspond to the clustering results on 
the Peru- and China- topic texts. As shown on 
Table 1., the proposed approach is not confident 
when grouping the Peru- and China- topic texts  
into comparable collections. The results of the 
NMI metric shown on Table 4. only confirm this 
conclusion. The best results obtained according 
to the NMI score are NMI is dependent on the 
mutual information and the entropy the texts to 
be clustered share. MI is a metric, which esti-
mates how the amount of information presented 
in the documents affect the clustering output. 
When the MI score is low, as in the example of 
grouping the Tajikistan- and Israel- topic texts, 
the information contained in the documents does 
not contribute to highly-comparable clusters of 
corpora. When the MI score obtained is high, as 
Sub-
corpus 
Topic Precision    Recall    Purity 
  2Cl 3Cl 4CL 5Cl 2Cl 3Cl 4Cl 5Cl  
T Tajikistan 1 1 1 1 1 1 1 1  
I Israel 1 1 1 1 1 1 1 1 1.00 
C China 1 1 1 1 1 1 1 1  
T Tajikistan 1 1 1 1 1 1 1 1  
I Israel 1 1 1 1 1 1 1 1 1.00 
P Peru 1 1 1 1 1 1 1 1  
P Peru 1 1 1 1 1 1 1 1  
C China 1 1 1 1 1 1 1 1 1.00 
I Israel 1 1 1 1 1 1 1 1  
T Tajikistan 1 1 1 1 1 1 1 1  
C China 1 1 1 1 1 1 1 1 1.00 
P Peru 1 1 1 1 1 1 1 1  
Table 2. Clustering results for test sets of combinations of three topic sub-corpora 
 
135
  
in the Tajikistan- and China- topic documents 
experiment, the information in these documents 
is a strong evidence of the text relatedness. Table 
4. lists the intermediate calculations of the entro-
py based on the available labels H(C) and the 
resulting clusters H(?). 
7 Remarks 
The problems identified in the current methodol-
ogy are classified into two different groups: text 
processing resources errors and clustering output 
errors. The processing resources are taken as off-
the-shelf modules and the development focus of 
the study in not concentrating on improving their 
performance. The second type of errors is the 
clustering errors. Their size can be reduced by 
improving the performance of the text prepro-
cessing resources. Additionally, enhanced clus-
tering output evaluation metrics can reveal learn-
ing algorithm?s weaknesses and suggest ways for 
improvement. 
8 Future Work 
More can be done in the future to improve the 
proposed methodology. One idea for further in-
vestigation is experimenting with larger collec-
tions of data. The results on the experimental 
corpus are promising, but the document collec-
tion is not big and contains less than 80 texts. It 
would be interesting to experiment with corpora 
that consist of hundreds of documents to test 
clustering performance. Additionally, a new ex-
perimental collection of documents is being 
compiled. It contains psycholinguistics texts both 
in Spanish and English. As the collection of this 
document set is still in progress, the results ob-
tained on it are not presented in the current pa-
per. These results will be reported in future work 
publications.  
Furthermore, a new translation equivalent 
source can be added. In the case of compiling 
specialized collections of comparable docu-
ments, a specialized bilingual or multilingual 
dictionary can prove to be a valuable resource. 
An untested interesting experimental setup can 
be investigating the resulting clustering perfor-
mance when more than 50% or more of the most 
relevant lemmas (with noun phrases) are selected 
as document features. A Named Entity Recog-
nizer (NER) and a synonymy suggestion module 
have the possibility to serve as good text pro-
cessing resources and further improve grouping 
outcomes. In connection with NER, it is interest-
ing additionally to investigate if the test corpus 
contains local names, which make clustering bet-
ter easier. Lastly, potential source for further de-
velopment is the automatic recognition of diasys-
tematic text features, such as diachronic, diatopic 
or diatechnic information. 
Clustering results of comparable corpora are 
obtained when the document characteristics are 
filtered by best keyword estimation metric - 
TF.BM25, explained in P?rez-Iglesias et al 
(2009). The results show decrease in good clus-
tering performance. A future work aspect is to 
investigate the cause this lower performance. 
9 Conclusion 
An innovative approach to the problem of 
compilation of comparable corpora is described. 
The approach suggests guidelines to textual 
characteristics selection scheme. Additionally, 
the approach incorporates LSA and unsupervised 
ML techniques. Different evaluation metrics, 
such as precision, purity and normalized mutual 
information, are employed to estimate compara-
ble corpus clustering results. These metrics show 
good results when evaluating comparable clus-
ters from a predefined set of less than 100 docu-
ments. The methodology suggested is applied for 
monolingual selection of documents; nonetheless 
it is readily extendable to more languages.  
References  
Agirre, Eneko, Alfonseca, Enrique, Hall, Keith, 
Kravalova, Jana, Pa?ca, Marius and Soroa, Ai-
tor.2009. A study of Similarity and Relatedness 
Using Distributional and WordNet-based ap-
proaches. In  NAACL ?09, pages 19-27. 
Baeza-Yates, Ricardo and Ribeiro-Neto, Betrhier. 
1999. Modern Infromation Retieval, Addison 
Wesley. 
Bekavac, Bo?o, Osenova, Petya, Simov, Kiril and 
Tadic, Marco. 2004. Making Monolingual Corpora 
Comparable: a Case Study of Bulgarian and Croa-
tian. In Proceedings of LREC2004, pages 
1187-1190, Lisbon. 
Crystal, David. 2001. Language and the Internet. 
Cambidge University, Press. Cambidge.UK, pages 
91-93. 
Dagan, Igo, Lee, Lillian and Pereira, Fernando. 1999. 
Similarity-based models of word co-occurrence 
probabilities. Machine Learning. 34(1-3), pages 
43-69. 
Deerwester, Scott, Dumais, Susan, Furnas, George, 
Landauer, Thomas and Harshman, Richard. 1990. 
Indexing by latent semantic analysis. Journal of 
136
  
the Americal Society for Information Science. 
41(6), pages 391-407. 
Finkelstein, Lev, Gabrilovich, Evgeniy, Matias, Yos-
si, Rivlin, Ehud, Solan, Zach, Wolfman, Gadi and 
Ruppin, Eytan. 2001. Placing Search in Context: 
The Concept Revisited. In WWW?01, pages 406-
414. 
Goeuriot, Lorraine, Emmanuel Morin and B?atrice 
Daille. 2009. Compilation of specialized compara-
ble corpora in French and Japanese. In Proceed-
ings of the 2nd workshop on Building and Us-
ing Comparable Corpora: from Parallel to 
Non-parallel Corpora, August 06, 2009, Suntec, 
Singapore.  
Hasler, Laura, Constantin Orasan and Karin Nau-
mann. 2006. NPs for Events: Experiments in Con-
ference Annotation. In Proceedings of the 5th 
edition of the International Conference on 
Language Resources and Evaluation 
(LREC2006),pages 1167-1172, 24-26 May 2006, 
Genoa, Italy. 
Ion, Radu, Dan Tufi?, Tiberiu Boro?, Ru Ceau?u and 
Dan ?tef?nescu. 2010. On-line Compilation of 
Comparable Corpora and Their Evaluation. In 
Proceedingds of the 7th International Confer-
ence of Formal Approaches to South Slavic 
and Balkan Languages (FASSBL7), pages 29-
34, Dubrovnic, Croatia. 
Kilgarriff, Adam. 2001. Comparing corpora. Interna-
tional Journal of Corpus Lingusitics, 6(1), pag-
es 97-133. 
Lee, Lillian. 1999. Measures of distributional similari-
ty. Proceedings of ACL 1999, pages 25-32. 
Maia, Belinda. 2003. What are Comparable Corpora?    
Electronic resource: 
http://web.letras.up.pt/bhsmaia/ 
belinda/pubs/CL2003%20workshop.doc. 
Manning, Christopher D. and Hinrich Sch?tze. 1999. 
Introduction to Information Retrieval. Cam-
bridge University Press, Cambridge, UK. 
Manning, Christopher D., Prabhakan Raghavan, and 
Hinrich Sch?tze. 2008. Introduction to Infor-
mation Retrieval, Cambridge University Press, 
pages 356-358.  
McEnery, Tony. 2003. Corpus Linguistics. In Ruslan 
Mitkov, editor, The Handbook of Computation-
al Lingustics. Oxford University Press, Oxford, 
UK, pages 448-464. 
Radinsky, Kira, Agichtein, Eugene, Gabrilovich, 
Evgeniy and Markovitch, Shaul. 2011. A word at a 
time: Computing Word Relatedness using Tem-
poral Semantic Analysis. In  WWW?11, pages 337-
346. 
P?rez-Iglesias, Joaqu?n, P?rez-Ag?era, Jos?, Fresno, 
V?ctor and Feinstein, Yuval. 2009. Integrating the 
probabilistic model BM25/BM25F into Lucene. In  
CoRR, abs/0911.5046. 
Rose, Tony, Mark Stevenson and Miles Whitehead. 
2002. The Reuters Corpus Volume 1 ? from Yes-
terday?s News to Tomorrow?s Language Resource. 
In  Proceedings of  LREC2002, pages 827-833. 
Sarageli, Xabier., San Vincente, Inaki, Gurrutxaga. 
Antton 2002.Automatic Extraction of bilingual 
terms from comparable corpora in a popular sci-
ence domain. In  Proceedings of  the workshop 
on Comparable Corpora, LREC?08. 
Sharoff, Serge. 2010. Analysing similarities and dif-
ferences between corpora. In Proceedings of the 
7th Conference of Language Technologies 
(Jezikovne Tehnologije), pages 5-11, 
Ljubljiana. Slovenia. 
Skadi?a, Inguna, Ahmet Aker, Voula Giouli, Dan 
Tufis, Robert Gaizauskas, Madara Mieri?a and Ni-
kos Mastropavlos. 2010a. A Collection of Compa-
rable Corpora for Under-Resourced Languages. In 
Inguna Skadi?a and Dan Tufis, editors, Human 
Language Technologies. The Baltic Perspec-
tive. Proceedings of the 4th International Con-
ference Baltic HLT 2010, pages 161-168. 
Skadi?a, Inguna, Vasiljeiv, Andrejs, Skadi??, Raivis, 
Gaizauskas, Robert, Tufi?, Dan and Gornostay, 
Tatiana. 2010b. Analysis and Evaluation of Com-
poarable Corpora for Under Resourced Areas of 
Machine Translation. In Proceedings of the 3rd 
Workshop on Building and Using Comparable 
Corpora. Applications of Parallel and Com-
parable Corpora in Natural Language PEngi-
neering and the Humanities, pages 6-14. 
Su, Fangzhoung and Bogdan Babych. 2012. Measur-
ing Comparability of Documents in Non-Parallel 
Corpora for Efficient Extraction of (Semi-)Parallel 
Translation Equivalents. In Proceedings of the 
Joint Workshop on Exploiting Synergies be-
tween Information Retrieval and Machine 
Translation (ESIRMT) and Hybrid Approach-
es to Machine Translation (HyTra), pages 10-
19, Avignon, France. 
Tao, Tao and Cheng Xiang Zhai. 2005. Mining Com-
parable Bilingual Text Corpora for Cross-
Language Information Integration. In Proceedings 
of the eleventh ACM SIGKDD international 
conference on Knowledge discovery in data 
mining, pages 691-696. 
 
137
