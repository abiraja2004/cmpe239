Generation as Dependency Parsing
Alexander Koller and Kristina Striegnitz
Dept. of Computational Linguistics, Saarland University
{koller|kris}@coli.uni-sb.de
Abstract
Natural-Language Generation from flat
semantics is an NP-complete problem.
This makes it necessary to develop al-
gorithms that run with reasonable effi-
ciency in practice despite the high worst-
case complexity. We show how to con-
vert TAG generation problems into de-
pendency parsing problems, which is use-
ful because optimizations in recent de-
pendency parsers based on constraint pro-
gramming tackle exactly the combina-
torics that make generation hard. Indeed,
initial experiments display promising run-
times.
1 Introduction
Existing algorithms for realization from a flat input
semantics all have runtimes which are exponential in
the worst case. Several different approaches to im-
proving the runtime in practice have been suggested
in the literature ? e.g. heuristics (Brew, 1992) and
factorizations into smaller exponential subproblems
(Kay, 1996; Carroll et al, 1999). While these solu-
tions achieve some measure of success in making re-
alization efficient, the contrast in efficiency to pars-
ing is striking both in theory and in practice.
The problematic runtimes of generation algo-
rithms are explained by the fact that realization is an
NP-complete problem even using just context-free
grammars, as Brew (1992) showed in the context of
shake-and-bake generation. The first contribution of
our paper is a proof of a stronger NP-completeness
result: If we allow semantic indices in the grammar,
realization is NP-complete even if we fix a single
grammar. Our alternative proof shows clearly that
the combinatorics in generation come from essen-
tially the same sources as in parsing for free word
order languages. It has been noted in the literature
that this problem, too, becomes NP-complete very
easily (Barton et al, 1987).
The main point of this paper is to show how to
encode generation with a variant of tree-adjoining
grammars (TAG) as a parsing problem with depen-
dency grammars (DG). The particular variant of DG
we use, Topological Dependency Grammar (TDG)
(Duchier, 2002; Duchier and Debusmann, 2001),
was developed specifically with efficient parsing for
free word order languages in mind. The mere exis-
tence of this encoding proves TDG?s parsing prob-
lem NP-complete as well, a result which has been
conjectured but never formally shown so far. But it
turns out that the complexities that arise in gener-
ation problems in practice seem to be precisely of
the sort that the TDG parser can handle well. Initial
experiments with generating from the XTAG gram-
mar (XTAG Research Group, 2001) suggest that our
generation system is competitive with state-of-the-
art chart generators, and indeed seems to run in poly-
nomial time in practice.
Next to the attractive runtime behaviour, our ap-
proach to realization is interesting because it may
provide us with a different angle from which to
look for tractable fragments of the general realiza-
tion problem. As we will show, the computation that
takes place in our system is very different from that
in a chart generator, and may be more efficient in
some cases by taking into account global informa-
tion to guide local choices.
Plan of the Paper. We will define the problem we
want to tackle in Section 2, and then show that it is
NP-complete (Section 3). In Section 4, we sketch
the dependency grammar formalism we use. Sec-
tion 5 is the heart of the paper: We show how to
encode TAG generation as TDG parsing, and dis-
cuss some examples and runtimes. We compare our
approach to some others in Section 6, and conclude
and discuss future research in Section 7.
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 17-24.
                         Proceedings of the 40th Annual Meeting of the Association for
2 The Realization Problem
In this paper, we deal with the subtask of natural
language generation known as surface realization:
given a grammar and a semantic representation, the
problem is to find a sentence which is grammatical
according to the grammar and expresses the content
of the semantic representation.
We represent the semantic input as a multiset
(bag) of ground atoms of predicate logic, such
as {buy(e,a,b), name(a,mary) car(b)}. To en-
code syntactic information, we use a tree-adjoining
grammar without feature structures (Joshi and Sch-
abes, 1997). Following Stone and Doran (1997) and
Kay (1996), we enhance this TAG grammar with
a syntax-semantics interface in which nonterminal
nodes of the elementary trees are equipped with in-
dex variables, which can be bound to individuals in
the semantic input. We assume that the root node,
all substitution nodes, and all nodes that admit ad-
junction carry such index variables. We also assign
a semantics to every elementary tree, so that lexi-
cal entries are pairs of the form (?, T ), where ? is
a multiset of semantic atoms, and T is an initial or
auxiliary tree, e.g.
( {buy(x,y,z)},
S:x
NP:y  VP:x
V:x
buys
NP:z 
)
When the lexicon is accessed, x, y, z get bound
to terms occurring in the semantic input, e.g. e, a, b
in our example. Since we furthermore assume that
every index variable that appears in T also appears
in ?, this means that all indices occurring in T get
bound at this stage.
The semantics of a complex tree is the multiset
union of the semantics of the elementary trees in-
volved. Now we say that the realization problem of
a grammar G is to decide for a given input semantics
S and an index i whether there is a derivation tree
which is grammatical according to G, is assigned
the semantics S, and has a root node with index i.
3 NP-Completeness of Realization
This definition is the simplest conceivable formal-
ization of problems occurring in surface realization
as a decision problem: It does not even require us
to compute a single actual realization, just to check
?1 B:i
N:i  E:k
e
B:k 
sem: edge(i,k)
?2 C
eating C 
sem: edge(i,k)
?3 N:i
n
sem: node(i)
?4 B:1
eat C 
sem: start-eating
?5 C
ate
sem: end-eating
Figure 1: The grammar G
ham
.
whether one exists. Every practical generation sys-
tem generating from flat semantics will have to ad-
dress this problem in one form or another.
Now we show that this problem is NP-complete.
A similar result was proved in the context of shake-
and-bake generation by Brew (1992), but he needed
to use the grammar in his encoding, which leaves
the possibility open that for every single grammar
G, there might be a realization algorithm tailored
specifically to G which still runs in polynomial time.
Our result is stronger in that we define a single
grammar G
ham
whose realization problem is NP-
complete in the above sense. Furthermore, we find
that our proof brings out the sources of the complex-
ity more clearly. G
ham
does not permit adjunction,
hence the result also holds for context-free gram-
mars with indices.
1
 
2
 3
It is clear that the problem is in
NP: We can simply guess the ele-
mentary trees we need and how to
combine them, and then check in
polynomial time whether they verbalize the seman-
tics. The NP-hardness proof is by reducing the well-
known HAMILTONIAN-PATH problem to the realiza-
tion problem. HAMILTONIAN-PATH is the problem
of deciding whether a directed graph has a cycle that
visits each node exactly once, e.g. (1,3,2,1) in the
graph shown above.
We will now construct an LTAG grammar G
ham
such that every graph G = (V,E) can be encoded
as a semantic input S for the realization problem of
G
ham
, which can be verbalized if and only if G has
a Hamiltonian cycle. S is defined as follows:
S = {node(i) | i ? V }
? {edge(i, k) | (i, k) ? E}
? {start-eating, end-eating}.
B:1
N:1 
N:1
n
E:3
e
B:3 
B:3
N:3 
N:3
n
E:2
e
B:2 
B:2
N:2 
N:2
n
E:1
e
B:1 
B:1
eat C 
C
eating C 
C
ate
Figure 2: A derivation with G
ham
corresponding to
a Hamiltonian cycle.
The grammar G
ham
is given in Fig. 1; the start
symbol is B, and we want the root to have index 1.
The tree ?
1
models an edge transition from node i
to the node k by consuming the semantic encodings
of this edge and (by way of a substitution of ?
3
) of
the node i. The second substitution node of ?
1
can
be filled either by another ?
1
, in which way a path
through the graph is modelled, or by an ?
4
, in which
case we switch to an ?edge eating mode?. In this
mode, we can arbitrarily consume edges using ?
2
,
and close the tree with ?
5
when we?re done. This
is illustrated in Fig. 2, the tree corresponding to the
cycle in the example graph above.
The Hamiltonian cycle of the graph, if one exists,
is represented in the indices of the B nodes. The list
of these indices is a path in the graph, as the ?
1
trees
model edge transitions; it is a cycle because it starts
in 1 and ends in 1; and it visits each node exactly
once, for we use exactly one ?
1
tree for each node
literal. The edges which weren?t used in the cycle
can be consumed in the edge eating mode.
The main source for the combinatorics of the re-
alization problem is thus the interaction of lexical
ambiguity and the completely free order in the flat
semantics. Once we have chosen between ?
1
and ?
2
in the realization of each edge literal, we have deter-
mined which edges should be part of the prospective
Hamiltonian cycle, and checking whether it really
is one can be done in linear time. If, on the other
hand, the order of the input placed restrictions on
the structure of the derivation tree, we would again
have information that told us when to switch into the
edge eating mode, i.e. which edges should be part
peter likes mary
subj obj
Figure 3: TDG parse tree for ?Peter likes Mary.?
of the cycle. A third source of combinatorics which
does not become so clear in this encoding is the con-
figuration of the elementary trees. Even when we
have committed to the lexical entries, it is conceiv-
able that only one particular way of plugging them
into each other is grammatical.
4 Topological Dependency Grammar
These factors are exactly the same that make depen-
dency parsing for free word order languages diffi-
cult, and it seems worthwhile to see whether op-
timized parsers for dependency grammars can also
contribute to making generation efficient. We now
sketch a dependency formalism which has an effi-
cient parser and then discuss some of the important
properties of this parser. In the next section, we will
see how to employ the parser for generation.
4.1 The Grammar Formalism
The parse trees of topological dependency grammar
(TDG) (Duchier and Debusmann, 2001; Duchier,
2002) are trees whose nodes correspond one-to-one
to the words of the sentence, and whose edges are la-
belled, e.g. with syntactic relations (see Fig. 3). The
trees are unordered, i.e. there is no intrinsic order
among the children of a node. Word order in TDG
is initially completely free, but there is a separate
mechanism to specify constraints on linear prece-
dence. Since completely free order is what we want
for the realization problem, we do not need these
mechanisms and do not go into them here.
The lexicon assigns to each word a set of lexical
entries; in a parse tree, one of these lexical entries
has to be picked for each node. The lexical entry
specifies what labels are allowed on the incoming
edge (the node?s labels) and the outgoing edges (the
node?s valency). Here are some examples:
word labels valency
likes ? {subj, obj, adv?}
Peter {subj, obj} ?
Mary {subj, obj} ?
The lexical entry for ?likes? specifies that the corre-
sponding node does not accept any incoming edges
(and hence must be the root), must have precisely
one subject and one object edge going out, and can
have arbitrarily many outgoing edges with label adv
(indicated by ?). The nodes for ?Peter? and ?Mary?
both require their incoming edge to be labelled with
either subj or obj and neither require nor allow any
outgoing edges.
A well-formed dependency tree for an input sen-
tence is simply a tree with the appropriate nodes,
whose edges obey the labels and valency restric-
tions specified by the lexical entries. So, the tree in
Fig. 3 is well-formed according to our lexicon.
4.2 TDG Parsing
The parsing problem of TDG can be seen as a search
problem: For each node, we must choose a lexi-
cal entry and the correct mother-daughter relations it
participates in. One strength of the TDG approach is
that it is amenable to strong syntactic inferences that
tackle specifically the three sources of complexity
mentioned above.
The parsing algorithm (Duchier, 2002) is stated
in the framework of constraint programming (Koller
and Niehren, 2000), a general approach to coping
with combinatorial problems. Before it explores all
choices that are possible in a certain state of the
search tree (distribution), it first tries to eliminate
some of the choices which definitely cannot lead to a
solution by simple inferences (propagations). ?Sim-
ple? means that propagations take only polynomial
time; the combinatorics is in the distribution steps
alone. That is, it can still happen that a search tree
of exponential size has to be explored, but the time
spent on propagation in each of its node is only poly-
nomial. Strong propagation can reduce the size of
the search tree, and it may even make the whole al-
gorithm run in polynomial time in practice.
The TDG parser translates the parsing prob-
lem into constraints over (variables denoting) fi-
nite sets of integers, as implemented efficiently in
the Mozart programming system (Oz Development
Team, 1999). This translation is complete: Solutions
of the set constraint can be translated back to cor-
rect dependency trees. But for efficiency, the parser
uses additional propagators tailored to the specific
inferences of the dependency problem. For instance,
in the ?Peter likes Mary? example above, one such
propagator could contribute the information that nei-
ther the ?Peter? nor the ?Mary? node can be an adv
child of ?likes?, because neither can accept an adv
edge. Once the choice has been made that ?Peter? is
the subj child of ?likes?, a propagator can contribute
that ?Mary? must be its obj child, as it is the only
possible candidate for the (obligatory) obj child.
Finally, lexical ambiguity is handled by selection
constraints. These constraints restrict which lexical
entry should be picked for a node. When all pos-
sible lexical entries have some information in com-
mon (e.g., that there must be an outgoing subj edge),
this information is automatically lifted to the node
and can be used by the other propagators. Thus it
is sometimes even possible to finish parsing without
committing to single lexical entries for some nodes.
5 Generation as Dependency Parsing
We will now show how TDG parsing can be used to
enumerate all sentences expressing a given input se-
mantics, thereby solving the realization problem in-
troduced in Section 2. We first define the encoding.
Then we give an example and discuss some runtime
results. Finally, we consider a particular restriction
of our encoding and ways of overcoming it.
5.1 The Encoding
Let G be a grammar as described in Section 2;
i.e. lexical entries are of the form (?, T ), where
? is a flat semantics and T is a TAG elementary
tree whose nodes are decorated with semantic in-
dices. We make the following simplifying assump-
tions. First, we assume that the nodes of the elemen-
tary trees of G are not labelled with feature struc-
tures. Next, we assume that whenever we can adjoin
an auxiliary tree at a node, we can adjoin arbitrarily
many trees at this node. The idea of multiple adjunc-
tion is not new (Schabes and Shieber, 1994), but it
is simplified here because we disregard complex ad-
junction constraints. We will discuss these two re-
strictions in the conclusion. Finally, we assume that
every lexical semantics ? has precisely one member;
this restriction will be lifted in Section 5.4.
Now let?s say we want to find the realizations of
the input semantics S = {?
1
, . . . , ?
n
}, using the
grammar G. The input ?sentence? of the parsing
start mary buy car indef red
sub
st NP,
m,1
substS,e,1
substN,c,1
substNP,c,1
adjN,c
Figure 4: Dependency tree for ?Mary buys a red
car.?
problem we construct is the sequence {start} ? S,
where start is a special start symbol. The parse
tree will correspond very closely to a TAG deriva-
tion tree, its nodes standing for the instantiated ele-
mentary trees that are used in the derivation.
To this end, we use two types of edge labels ?
substitution and adjunction labels. An edge with a
substitution label subst
A,i,p
from the node ? to the
node ? (both of which stand for elementary trees)
indicates that ? should be plugged into the p-th sub-
stitution node in ? that has label A and index i. We
write subst(A) for the maximum number of occur-
rences of A as the label of substitution nodes in any
elementary tree of G; this is the maximum value that
p can take.
An edge with an adjunction label adj
A,i
from ? to
? specifies that ? is adjoined at some node within ?
carrying label A and index i and admitting adjunc-
tion. It does not matter for our purposes to which
node in ? ? is adjoined exactly; the choice cannot af-
fect grammaticality because there is no feature uni-
fication involved.
The dependency grammar encodes how an ele-
mentary tree can be used in a TAG derivation by
restricting the labels of the incoming and outgoing
edges via labels and valency requirements in the lex-
icon. Let?s say that T is an elementary tree of G
which has been matched with the input atom ?
r
, in-
stantiating its index variables. Let A be the label
and i the index of the root of T . If T is an auxiliary
tree, it accepts incoming adjunction edges for A and
i, i.e. it gets the labels value {adj
A,i
}. If T is an
initial tree, it will accept arbitrary incoming substi-
tution edges for A and i, i.e. its labels value is
{subst
A,i,p
| 1 ? p ? subst(A)}
In either case, T will require precisely one out-
going substitution edge for each of its substitution
nodes, and it will allow arbitrary numbers of outgo-
ing adjunction edges for each node where we can
adjoin. That is, the valency value is as follows:
{subst
A,i,p
| ex. substitution node N in T s.t. A
is label, i is index of N , and N is
pth substitution node for A:i in T}
? {adj
A,i
? | ex. node with label A, index i
in T which admits adjunction}
We obtain the set of all lexicon entries for the
atom ?
r
by encoding all TAG lexicon entries which
match ?
r
as just specified. The start symbol, start,
gets a special lexicon entry: Its labels entry is the
empty set (i.e. it must be the root of the tree), and its
valency entry is the set {subst
S,k,1
}, where k is the
semantic index with which generation should start.
5.2 An Example
Now let us go through an example to make these def-
initions a bit clearer. Let?s say we want to verbalize
the semantics
{name(m, mary), buy(e,m, c),
car(c), indef(c), red(c)}
The LTAG grammar we use contains the elemen-
tary trees which are used in the tree in Fig. 5, along
with the obvious semantics; we want to generate a
sentence starting with the main event e. The encod-
ing produces the following dependency grammar;
the entries in the ?atom? column are to be read as
abbreviations of the actual atoms in the input seman-
tics.
atom labels valency
start ? {subst
S,e,1
}
buy {subst
S,e,1
} {subst
NP,c,1
, subst
NP,m,1
,
adj
V P,e
?, adj
V,e
?}
mary {subst
NP,m,1
, {adj
NP,1
?, adj
PN,m
?}
subst
NP,m,2
}
indef {subst
NP,c,1
, {adj
NP,c
?}
subst
NP,c,2
}
car {subst
N,c,1
} {adj
N,c
?}
red {adj
N,c
} ?
If we parse the ?sentence?
start mary buy car indef red
with this grammar, leaving the word order com-
pletely open, we obtain precisely one parse tree,
shown in Fig. 4. Reading this parse as a TAG
derivation tree, we can reconstruct the derived tree
in Fig. 5, which indeed produces the string ?Mary
buys a red car?.
S:e
NP:m 
NP:m
PN:m
Mary
VP:e
V:e
buys
NP:c 
NP:c
Detnoad j
a
N:c 
N:c
Adjnoad j
red
N:c
N:c
car
Figure 5: Derived tree for ?Mary buys a red car.?
5.3 Implementation and Experiments
The overall realization algorithm we propose en-
codes the input problem as a DG parsing problem
and then runs the parser described in Section 4.2,
which is freely available over the Web, as a black
box. Because the information lifted to the nodes by
the selection constraints may be strong enough to
compute the parse tree without ever committing to
unique lexical entries, the complete parse may still
contain some lexical ambiguity. This is no problem,
however, because the absence of features guarantees
that every combination of choices will be grammat-
ical. Similarly, a node can have multiple children
over adjunction edges with the same label, and there
may be more than one node in the upper elemen-
tary tree to which the lower tree could be adjoined.
Again, all remaining combinations are guaranteed to
be grammatical.
In order to get an idea of the performance of
our realization algorithm in comparison to the state
of the art, we have tried generating the following
sentences, which are examples from (Carroll et al,
1999):
(1) The manager in that office interviewed a new
consultant from Germany.
(2) Our manager organized an unusual additional
weekly departmental conference.
We have converted the XTAG grammar (XTAG
Research Group, 2001) into our grammar format,
automatically adding indices to the nodes of the el-
ementary trees, removing features, simplifying ad-
junction constraints, and adding artificial lexical se-
mantics that consists of the words at the lexical an-
chors and the indices used in the respective trees.
XTAG typically assigns quite a few elementary trees
to one lemma, and the same lexical semantics can of-
ten be verbalized by more than hundred elementary
trees in the converted grammar. It turns out that the
dependency parser scales very nicely to this degree
of lexical ambiguity: The sentence (1) is generated
in 470 milliseconds (as opposed to Carroll et al?s 1.8
seconds), whereas we generate (2) in about 170 mil-
liseconds (as opposed to 4.3 seconds).1 Although
these numbers are by no means a serious evaluation
of our system?s performance, they do present a first
proof of concept for our approach.
The most encouraging aspect of these results is
that despite the increased lexical ambiguity, the
parser gets by without ever making any wrong
choices, which means that it runs in polynomial
time, on all examples we have tried. This is possible
because on the one hand, the selection constraint au-
tomatically compresses the many different elemen-
tary trees that XTAG assigns to one lemma into very
few classes. On the other hand, the propagation that
rules out impossible edges is so strong that the free
input order does not make the configuration prob-
lem much harder in practice. Finally, our treatment
of modification allows us to multiply out the possi-
ble permutations in a postprocessing step, after the
parser has done the hard work. A particularly strik-
ing example is (2), where the parser gives us a single
solution, which multiplies out to 312 = 13 ? 4! dif-
ferent realizations. (The 13 basic realizations corre-
spond to different syntactic frames for the main verb
in the XTAG grammar, e.g. for topicalized or pas-
sive constructions.)
5.4 More Complex Semantics
So far, we have only considered TAG grammars in
which each elementary tree is assigned a semantics
that contains precisely one atom. However, there
are cases where an elementary tree either has an
empty semantics, or a semantics that contains mul-
tiple atoms. The first case can be avoided by ex-
ploiting TAG?s extended domain of locality, see e.g.
(Gardent and Thater, 2001).
The simplest possible way for dealing with the
second case is to preprocess the input into several
1A newer version of Carroll et al?s system generates (1) in
420 milliseconds (Copestake, p.c.). Our times were measured
on a 700 MHz Pentium-III PC.
different parsing problems. In a first step, we collect
all possible instantiations of LTAG lexical entries
matching subsets of the semantics. Then we con-
struct all partitions of the input semantics in which
each block in the partition is covered by a lexical en-
try, and build a parsing problem in which each block
is one symbol in the input to the parser.
This seems to work quite well in practice, as there
are usually not many possible partitions. In the worst
case, however, this approach produces an exponen-
tial number of parsing problems. Indeed, using a
variant of the grammar from Section 3, it is easy
to show that the problem of deciding whether there
is a partition whose parsing problem can be solved
is NP-complete as well. An alternative approach is
to push the partitioning process into the parser as
well. We expect this will not hurt the runtime all
that much, but the exact effect remains to be seen.
6 Comparison to Other Approaches
The perspective on realization that our system takes
is quite different from previous approaches. In this
section, we relate it to chart generation (Kay, 1996;
Carroll et al, 1999) and to another constraint-based
approach (Gardent and Thater, 2001).
In chart based approaches to realization, the main
idea is to minimize the necessary computation by
reusing partial results that have been computed be-
fore. In the setting of fixed word order parsing, this
brings an immense increase in efficiency. In genera-
tion, however, the NP-completeness manifests itself
in charts of worst-case exponential size. In addition,
it can happen that substructures are built which are
not used in the final realization, especially when pro-
cessing modifications.
By contrast, our system configures nodes into a
dependency tree. It solves a search problem, made
up by choices for mother-daughter relations in the
tree. Propagation, which runs in polynomial time,
has access to global information (illustrated in Sec-
tion 4.2) and can thus rule out impossible mother-
daughter relations efficiently; every propagation step
that takes place actually contributes to zooming in
on the possible realizations. Our system can show
exponential runtimes when the distributions span a
search tree of exponential size.
Gardent and Thater (2001) also propose a con-
straint based approach to generation working with
a variant of TAG. However, the performance of their
system decreases rapidly as the input gets larger
even when when working with a toy grammar. The
main difference between their approach and ours
seems to be that their algorithm tries to construct
a derived tree, while ours builds a derivation tree.
Our parser only has to deal with information that
is essential to solve the combinatorial problem, and
not e.g. with the internal structure of the elementary
trees. The reconstruction of the derived tree, which
is cheap once the derivation tree has been computed,
is delegated to a post-processing step. Working with
derived trees, Gardent and Thater (2001) cannot ig-
nore any information and have to keep track of the
relationships between nodes at points where they are
not relevant.
7 Conclusion
Generation from flat semantics is an NP-complete
problem. In this paper, we have first given an al-
ternative proof for this fact, which works even for
a fixed grammar and makes the connection to the
complexity of free word order parsing clearly visi-
ble. Then we have shown how to translate the re-
alization problem of TAG into parsing problems of
topological dependency grammar, and argued how
the optimizations in the dependency parser ? which
were originally developed for free word order pars-
ing ? help reduce the runtime for the generation sys-
tem. This reduction shows in passing that the pars-
ing problem for TDG is NP-complete as well, which
has been conjectured, but never proved.
The NP-completeness result for the realization
problem explains immediately why all existing com-
plete generation algorithms have exponential run-
times in the worst case. As our proof shows, the
main sources of the combinatorics are the interac-
tion of lexical ambiguity and tree configuration with
the completely unordered nature of the input. Mod-
ification is important and deserves careful treatment
(and indeed, our system deals very gracefully with
it), but it is not as intrinsically important as some
of the literature suggests; our proof gets by without
modification. If we allow the grammar to be part
of the input, we can even modify the proof to show
NP-hardness of the case where semantic atoms can
be verbalized more often than they appear in the in-
put, and of the case where they can be verbalized
less often. The case where every atom can be used
arbitrarily often remains open.
By using techniques from constraint program-
ming, the dependency parser seems to cope rather
well with the combinatorics of generation. Propaga-
tors can rule out impossible local structures on the
grounds of global information, and selection con-
straints greatly alleviate the proliferation of lexical
ambiguity in large TAG grammars by making shared
information available without having to commit to
specific lexical entries. Initial experiments with the
XTAG grammar indicate that we can generate prac-
tical examples in polynomial time, and may be com-
petitive with state-of-the-art realization systems in
terms of raw runtime.
In the future, it will first of all be necessary to lift
the restrictions we have placed on the TAG gram-
mar: So far, the nodes of the elementary trees are
only equipped with nonterminal labels and indices,
not with general feature structures, and we allow
only a restricted form of adjunction constraints. It
should be possible to either encode these construc-
tions directly in the dependency grammar (which al-
lows user-defined features too), or filter out wrong
realizations in a post-processing step. The effect of
such extensions on the runtime remains to be seen.
Finally, we expect that despite the general NP-
completeness, there are restricted generation prob-
lems which can be solved in polynomial time, but
still contain all problems that actually arise for nat-
ural language. The results of this paper open up a
new perspective from which such restrictions can be
sought, especially considering that all the natural-
language examples we tried are indeed processed
in polynomial time. Such a polynomial realiza-
tion algorithm would be the ideal starting point
for algorithms that compute not just any, but the
best possible realization ? a problem which e.g.
Bangalore and Rambow (2000) approximate using
stochastic methods.
Acknowledgments. We are grateful to Tilman
Becker, Chris Brew, Ann Copestake, Ralph Debus-
mann, Gerald Penn, Stefan Thater, and our reviewers
for helpful comments and discussions.
References
Srinivas Bangalore and Owen Rambow. 2000. Using
tags, a tree model, and a language model for genera-
tion. In Proc. of the TAG+5 Workshop, Paris.
G. Edward Barton, Robert C. Berwick, and Eric Sven
Ristad. 1987. Computational Complexity and Natu-
ral Language. MIT Press, Cambridge, Mass.
Chris Brew. 1992. Letting the cat out of the bag: Gen-
eration for Shake-and-Bake MT. In Proceedings of
COLING-92, pages 610?616, Nantes.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznanski. 1999. An efficient chart generator for
(semi-)lexicalist grammars. In Proceedings of the 7th
European Workshop on NLG, pages 86?95, Toulouse.
Denys Duchier and Ralph Debusmann. 2001. Topolog-
ical dependency trees: A constraint-based account of
linear precedence. In Proceedings of the 39th ACL,
Toulouse, France.
Denys Duchier. 2002. Configuration of labeled trees un-
der lexicalized constraints and principles. Journal of
Language and Computation. To appear.
Claire Gardent and Stefan Thater. 2001. Generating with
a grammar based on tree descriptions: A constraint-
based approach. In Proceedings of the 39th ACL,
Toulouse.
Aravind Joshi and Yves Schabes. 1997. Tree-Adjoining
Grammars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, chapter 2, pages 69?
123. Springer-Verlag, Berlin.
Martin Kay. 1996. Chart generation. In Proceedings of
the 34th Annual Meeting of the ACL, pages 200?204,
Santa Cruz.
Alexander Koller and Joachim Niehren. 2000. Con-
straint programming in computational linguistics. To
appear in Proceedings of LLC8, CSLI Press.
Oz Development Team. 1999. The Mozart Programming
System web pages. http://www.mozart-oz.
org/.
Yves Schabes and Stuart Shieber. 1994. An alterna-
tive conception of tree-adjoining derivation. Compu-
tational Linguistics, 20(1):91?124.
Matthew Stone and Christy Doran. 1997. Sentence plan-
ning as description using tree-adjoining grammar. In
Proceedings of the 35th ACL, pages 198?205.
XTAG Research Group. 2001. A lexicalized tree adjoin-
ing grammar for english. Technical Report IRCS-01-
03, IRCS, University of Pennsylvania.
