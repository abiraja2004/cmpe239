2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 513?517,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Space Efficiencies in Discourse Modeling via Conditional Random Sampling
Brian Kjersten
CLSP
Johns Hopkins University
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Abstract
Recent exploratory efforts in discourse-level
language modeling have relied heavily on cal-
culating Pointwise Mutual Information (PMI),
which involves significant computation when
done over large collections. Prior work has
required aggressive pruning or independence
assumptions to compute scores on large col-
lections. We show the method of Condi-
tional Random Sampling, thus far an underuti-
lized technique, to be a space-efficient means
of representing the sufficient statistics in dis-
course that underly recent PMI-based work.
This is demonstrated in the context of induc-
ing Shankian script-like structures over news
articles.
1 Introduction
It has become common to model the distributional
affinity between some word or phrase pair, (wi, wj),
as a function of co-occurance within some con-
text boundary. Church and Hanks (1990) suggested
pointwise mutual information: PMI(wi, wj) =
log Pr(wi,wj)Pr(wi) Pr(wj) , showing linguistically appealing
results using contexts defined by fixed width n-gram
windows, and syntactic dependencies derived from
automatically parsed corpora. Later work such as
by Lin (1999) continued this tradition. Here we con-
sider document, or discourse-level contexts, such as
explored by Rosenfeld (1994) or Church (2000), and
more recently by those such as Chambers and Juraf-
sky (2008) or Van Durme and Lall (2009b).
In the spirit of recent work in randomized algo-
rithms for large-scale HLT (such as by Ravichandran
et al. (2005), Talbot and Osborne (2007), Goyal et
al. (2010), Talbot and Brants (2008),Van Durme and
Lall (2009a), Levenberg and Osborne (2009), Goyal
et al. (2010), Petrovic et al. (2010), Van Durme and
Lall (2010), or Goyal and Daume? (2011)), we pro-
pose the method of Conditional Random Sampling
(CRS) by Li and Church (2007) as an efficient way
to store approximations of the statistics used to cal-
culate PMI for applications in inducing rudimentary
script-like structures.
Efficiently storing such structures is an impor-
tant step in integrating document-level statistics into
downstream tasks, such as characterizing complex
scenarios (Chambers and Jurafsky, 2011), or story
understanding (Gordon et al., 2011).
2 Background
Conditional Random Sampling (CRS) Li and
Church (2007) proposed CRS to approximate the
contingency table between elements in a query, to
be used in distributional similarity measures such
as cosine similarity, correlation, and PMI. Central
is the idea of the postings list, which is made up
of the identifiers of each document that contains a
given word or phrase. A set of such lists, one per
type in the underlying vocabulary, is known as an
inverted index. To reduce storage costs, a CRS trun-
cates these lists, now called sketches, such that each
sketch is no larger than some length parameter k.
Formally, assume an ordered list of document
identifiers, ? = (1, 2, ...), where each referenced
document is a bag of words drawn from a vocabu-
lary of size V . Let Pi ? ? be the postings list for
some element wi ? V . The function pi represents a
513
random permutation on the space of identifiers in ?.
The sketch, Si, is defined as the first k elements of
the permuted list: Si = mink(pi(Pi)). 1
Let q be a two-element query, (wi, wj). Given
the postings lists for wi, wj , we can construct
a four-cell contingency table containing the fre-
quency of documents that contained only wi, only
wj , both together, or neither. A CRS allows for
approximating this table in O(k) time by rely-
ing on a sample of ?, specific to q: pi(?)q =
(1, 2, 3, ...,min(max(Si),max(Sj))).
The PMI of q, given ?, can be estimated from
pi(?)q using the approximate word occurrence,
Pr(wi) = |Si?pi(?)q|/|pi(?)q|, and co-occurrence,
Pr(wi ? wj) = |Si ? Sj ? pi(?)q|/|pi(?)q|.
This scheme generalizes to longer queries of
length m, where storage costs remain O(V k), and
query time scales at O(mk). Li and Church (2007)
proved that CRS produces an unbiased estimate of
the probabilities, and showed empirically that vari-
ance is a function of k and m.
Despite its simplicity and promise for large-scale
data mining in NLP, CRS has thus-far seen minimal
application in the community.
Trigger Language Models As here, Rosenfeld
(1994)?s work on trigger language models was con-
cerned with document level context. He identified
trigger pairs: pairs of word sequences where the
presence of the first word sequence affects the prob-
ability of the other, possibly at long distances. He
recommended selecting a small list of trigger pairs
based on the highest average mutual information
(often simply called mutual information), although
intuitively PMI could also be used. Computational
constraints forced him to apply heavy pruning to the
bigrams in his model.
Scripts A script, proposed by Schank (1975), is a
form of Minsky-style frame that captures common-
sense knowledge regarding typical events. For ex-
ample, if a machine were to reason about eating at a
restaurant, it should associate to this event: the ex-
1For example, assume some word wi that appears in doc-
uments d1, d4, d10 and d12. The identifiers are then randomly
permuted via pi such that: d?3 = d1, d
?
2 = d4, d
?
7 = d10 and
d?1 = d12. Following permutation, the postings list for wi is
made up of identifiers that map to the same underlying docu-
ments as before, but now in a different order. If we let k = 3,
then Si = (1, 2, 3), corresponding to documents: (d12, d4, d1).
istence of a customer or patron that usually pays for
the meal that is ordered by the patron, then served
by the waiter, etc.
Chambers and Jurafsky (2008) suggested induc-
ing a similar structure called a narrative chain: fo-
cus on the situational descriptions explicitly pertain-
ing to a single protagonist, a series of references
within a document that are automatically labeled
as coreferent. With a large corpus, one can then
find those sets of verbs (as anchors of basic sit-
uational descriptions) which tend to co-occur, and
share a protagonist, leading to an approximate sub-
set of Schank?s original conception.2
Underlying the co-occurrence framework of
Chambers and Jurafsky was finding those verbs with
high PMI. Starting with some initial element, chains
were built greedily by adding the term, x, that max-
imized the average of the pairwise PMI between x
and every term already in the chain:
Wn+1 = arg max
W
1
n
n?
j=1
pmi(W,Wj)
By relying on the average pairwise PMI, they are
making independence assumptions that are not al-
ways valid. In order to consider more nuanced joint
effects between more than two terms, more efficient
methods would need to be considered.
3 Experiments
Setup Following Chambers and Jurafsky (2008),
we extracted and lemmatized the verbs from the
New York Times section of the Gigaword Corpus us-
ing the Stanford POS tagger (Toutanova et al., 2004)
and the Morpha lemmatizer (Minnen et al., 2000).
After filtering various POS tagger errors and setting
a minimum document frequency (df) of 50, we went
from a vocabulary of 94,803 words to 8,051.3 For
various values of k we built sketches over 1,655,193
documents, for each resulting word type.
2Given a large collection of news articles, some on the topic
of local crime, one might see a story such as: ?... searched for
Michaeli ... hei was arrested ... Mikei plead guilty ... convicted
himi ...?, helping to support an induced chain: (search, arrest,
plead, acquit, convict, sentence).
3Types containing punctuation other than hyphens and un-
derscores were discarded as tagger-error.
514
Table 1: Top-n by approximate PMI, for varying k. Subscripts denote rank under true PMI, when less than 50.
plead plead, admit plead, admit, convict
1 sentence4 sentence4 sentence4 abuse? sentence5 owe? sentence2
2 commit? defraud5 misbrand2 convict22 prosecute15 admitt11 prosecute3
3 indict10 indict10 defraud5 owe? testify20 engage? arrest8
4 prosecute33 arraign6 arraign6 investigate? indict10 investigate28 testify5
5 abuse? conspire11 manslaughter1 understand? defraud7 prey? acquit1
6 convict24 convict24 bilk8 defraud7 convict22 defraud? indict4
k = 100 1,000 10,000 1,000 10,000 1,000 10,000
We use a generalized definition of PMI for three
or more items as the logarithm of the joint probabil-
ity divided by the product of the marginals.
Subjective Quality We first consider the lemma-
tized version of the motivating example by Cham-
bers and Jurafsky (2008): [plead, admit, convict],
breaking it into 1-, 2-, and 3-element seeds. They
reported the top 6 elements that maximize average
pairwise PMI as: sentence, parole, fire, indict, fine,
deny. We see similar results in Table 1, while not-
ing again the distinction in underlying statistics: we
did not restrict ourselves to cooccurrence based on
shared coreferring arguments.
These results show intuitive discourse-level rela-
tionships with a sketch size as small as k = 100
for the unary seed. In addition, when examining the
true PMI rank of each of these terms (reflected as
subscripts), we see that highly ranked items in the
approximate lists come from the set of items highly
ranked in the non-approximate version.4 A major
benefit of the approach is that it allows for approxi-
mate scoring of larger sets of elements jointly, with-
out the traditionally assumed storage penalty.5
Accuracy 1 We measured the trade-off between
PMI approximation accuracy and sketch size.
Triples of verb tokens were sampled at random from
the narrative cloze test set of Chambers and Jurafsky
(2008). Seed terms were limited to verbs with df be-
tween 1,000 and 100,000 to extract lists of the top-
25 candidate verbs by joint, approximate PMI. For
4The word ?sentence? is consistently higher ranked in the
approximate PMI list than it is in the true PMI list: results stem
from a given shared permutation across the queries, and thus
approximation errors are more likely to be correlated.
5For example, we report that PMI(plead, admit, convict)
> PMI(plead, admit, owe), when k = 1, 000, as com-
pared to: avg(PMI(plead, convict), PMI(admit, convict)) >
avg(PMI(plead, owe), PMI(admit, owe)).
a given rank r, we measured the overlap of the true
top-3 PMI and the approximate list, rank r or higher
(see Figure 1(a)). If query size is 2, k = 10, 000,
the true top-3 true PMI items tend to rank well in
the approximate PMI list. We observe that these
randomly assembled queries tax the sketch-based
approximation, motivating the next experiment on
non-uniformly sampled queries.
Accuracy 2 In a more realistic scenario, we might
have more discretion in selecting terms of interest.
Here we chose the first word of each seed uniformly
at random from each document, and selected subse-
quent seed words to maximize the true PMI with the
established words in the seed. We constrained the
seed terms to have df between 1,000 and 100,000.
Then, for each seed of length 1, 2, and 3 words,
we found the 25-best list of terms using approximate
PMI, considering only terms that occur in more than
50 documents. Figure 1(b) shows the results of this
PMI approximation tradeoff. With a sketch size of
10,000, a rank of 5 is enough to contain two out of
the top three items, and the number gradually con-
tinues to grow as rank size increases.
Memory Analysis Accuracy in a CRS is a func-
tion of the aggressiveness in space savings: as k ap-
proaches the true length of the posting list for wi,
the resulting approximations are closer to truth, at
the cost of increased storage. When k =?, CRS is
the same as using an inverted index: Fig. 2 shows the
percent memory required for our data, compared to a
standard index, as the sketch size increases. For our
data, a full index involves storing 95 million docu-
ment numbers. For the k = 10, 000 results, we see
that 23% of a full index was needed.
Figure 1(c) shows the quality of approximate best
PMI lists as memory usage is varied. A 2-word
query needs about 20% of the memory for 2.5 of the
515
Rank
Mean.
Observ
ed
0.0
0.5
1.0
1.5
2.0
2.5
3.0
l l l l l l
l l
l l l l
5 10 15 20 25
k1000 10000ml 2 3 4
(a)
Rank
Mean.
Observ
ed
0.0
0.5
1.0
1.5
2.0
2.5
3.0
l l
l l l l
l l l l l l
5 10 15 20 25
k1000 10000ml 2 3 4
(b)
Percent.Memory
Mean.
Observ
ed
0.0
0.5
1.0
1.5
2.0
2.5
3.0 llllll
l
l
ll0 20 40 60 80 100
ml 234
(c)
Figure 1: (a) Average number of true top-3 PMI items when seed terms have 1,000? df? 100,000 and are chosen uni-
formly at random from documents. (b) Average number of true top-3 PMI items when seeds are moderate-frequency
high-PMI tuples. (c) Average number of true top-3 PMI items in the top ten approximate PMI list, as a function of
memory usage, when seeds are moderate-frequency high-PMI tuples.
log10(k)
Percen
t.Total
20
40
60
80
100
2 3 4 5 6
Figure 2: % of inverted index stored, as function of k.
top 3 true PMI items to appear in the top 10. Over
40% memory is needed for a 4-word query. 2.5 of
the top 3 true PMI items appear in the top 50 when
the memory is about 35%. This suggests that CRS
allows us to use a fraction of the memory of storing
a full inverted index, but that memory requirements
grow with query size.
Discussion Storing exact PMIs of three or four
words would be expensive to store in memory for
any moderately sized vocabulary, because it would
involve storing on the order of V m count statis-
tics. If we are approximating this with a CRS, we
store sketches of length k or less for every word
in the vocabulary, which is O(kV ). Table 1 and
Fig. 1(b) show that the two-word queries start to
get good performance when k is near 10,000. This
requires 22.7% of the memory of a complete in-
verted index, or 21.5 million postings. The three
and four word queries get good performance near
k = 100, 000. With this sketch size, 60.5 million
postings are stored.
4 Conclusion
We have proposed using Conditional Random Sam-
pling for approximating PMI in the discourse under-
standing community. We have shown that the ap-
proximate PMI rank list produces results that are in-
tuitive and consistent with the exact PMI even with
significant memory savings. This enables us to ap-
proximate PMI for tuples longer than pairs without
undue independence assumptions. One future av-
enue is to explore the use of this structure in appli-
cations such as machine translation, as potentially
enabling greater use of long distance dependencies
than in prior work, such as by Hasan et al. (2008).
5 Acknowledgements
We acknowledge support from the National Science
Foundation PIRE Grant No. OISE-0530118. We
acknowledge the Army Research Laboratory for its
support to the first author under SCEP (the Student
Career Experience Program). Any opinions, find-
ings, conclusions, or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the views of the supporting agencies.
516
References
