Proceedings of the Second Workshop on Statistical Machine Translation, pages 159?166,
Prague, June 2007. c?2007 Association for Computational Linguistics
Context-aware Discriminative Phrase Selection
for Statistical Machine Translation
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
In this work we revise the application
of discriminative learning to the problem
of phrase selection in Statistical Machine
Translation. Inspired by common tech-
niques used in Word Sense Disambiguation,
we train classifiers based on local context
to predict possible phrase translations. Our
work extends that of Vickrey et al (2005) in
two main aspects. First, we move from word
translation to phrase translation. Second, we
move from the ?blank-filling? task to the ?full
translation? task. We report results on a set
of highly frequent source phrases, obtaining
a significant improvement, specially with re-
spect to adequacy, according to a rigorous
process of manual evaluation.
1 Introduction
Translations tables in Phrase-based Statistical Ma-
chine Translation (SMT) are often built on the ba-
sis of Maximum-likelihood Estimation (MLE), be-
ing one of the major limitations of this approach that
the source sentence context in which phrases occur
is completely ignored (Koehn et al, 2003).
In this work, inspired by state-of-the-art Word
Sense Disambiguation (WSD) techniques, we sug-
gest using Discriminative Phrase Translation (DPT)
models which take into account a wider feature
context. Following the approach by Vickrey et al
(2005), we deal with the ?phrase translation? prob-
lem as a classification problem. We use Support
Vector Machines (SVMs) to predict phrase transla-
tions in the context of the whole source sentence.
We extend the work by Vickrey et al (2005) in two
main aspects. First, we move from ?word transla-
tion? to ?phrase translation?. Second, we move from
the ?blank-filling? task to the ?full translation? task.
Our approach is fully described in Section 2. We
apply it to the Spanish-to-English translation of Eu-
ropean Parliament Proceedings. In Section 3, prior
to considering the ?full translation? task, we ana-
lyze the impact of using DPT models for the iso-
lated ?phrase translation? task. In spite of working
on a very specific domain, a large room for improve-
ment, coherent with WSD performance, and results
by Vickrey et al (2005), is predicted. Then, in Sec-
tion 4, we tackle the full translation task. DPT mod-
els are integrated in a ?soft? manner, by making them
available to the decoder so they can fully interact
with other models. Results using a reduced set of
highly frequent source phrases show a significant
improvement, according to several automatic eval-
uation metrics. Interestingly, the BLEU metric (Pap-
ineni et al, 2001) is not able to reflect this improve-
ment. Through a rigorous process of manual eval-
uation we have verified the gain. We have also ob-
served that it is mainly related to adequacy. These
results confirm that better phrase translation proba-
bilities may be helpful for the full translation task.
However, the fact that no gain in fluency is reported
indicates that the integration of these probabilities
into the statistical framework requires further study.
2 Discriminative Phrase Translation
In this section we describe the phrase-based SMT
baseline system and how DPT models are built and
integrated into this system in a ?soft? manner.
159
2.1 Baseline System
The baseline system is a phrase-based SMT sys-
tem (Koehn et al, 2003), built almost entirely us-
ing freely available components. We use the SRI
Language Modeling Toolkit (Stolcke, 2002) for lan-
guage modeling. We build trigram language models
applying linear interpolation and Kneser-Ney dis-
counting for smoothing. Translation models are
built on top of word-aligned parallel corpora linguis-
tically annotated at the level of shallow syntax (i.e.,
lemma, part-of-speech, and base phrase chunks)
as described by Gime?nez and Ma`rquez (2005).
Text is automatically annotated, using the SVM-
Tool (Gime?nez and Ma`rquez, 2004), Freeling (Car-
reras et al, 2004), and Phreco (Carreras et al, 2005)
packages. We used the GIZA++ SMT Toolkit1 (Och
and Ney, 2003) to generate word alignments. We
apply the phrase-extract algorithm, as described by
Och (2002), on the Viterbi alignments output by
GIZA++ following the ?global phrase extraction?
strategy described by Gime?nez and Ma`rquez (2005)
(i.e., a single phrase translation table is built on top
of the union of alignments corresponding to dif-
ferent linguistic data views). We work with the
union of source-to-target and target-to-source align-
ments, with no heuristic refinement. Phrases up to
length five are considered. Also, phrase pairs ap-
pearing only once are discarded, and phrase pairs
in which the source/target phrase is more than three
times longer than the target/source phrase are ig-
nored. Phrase pairs are scored on the basis of un-
smoothed relative frequency (i.e., MLE). Regard-
ing the argmax search, we used the Pharaoh beam
search decoder (Koehn, 2004), which naturally fits
with the previous tools.
2.2 DPT for SMT
Instead of relying on MLE estimation to score the
phrase pairs (fi, ej) in the translation table, we
suggest considering the translation of every source
phrase fi as a multi-class classification problem,
where every possible translation of fi is a class.
We use local linear SVMs 2. Since SVMs are bi-
nary classifiers, the problem must be binarized. We
1http://www.fjoch.com/GIZA++.html
2We use the SVMlight package, which is freely available at
http://svmlight.joachims.org (Joachims, 1999).
have applied a simple one-vs-all binarization, i.e., a
SVM is trained for every possible translation candi-
date ej . Training examples are extracted from the
same training data as in the case of MLE models,
i.e., an aligned parallel corpus, obtained as described
in Section 2.1. We use each sentence pair in which
the source phrase fi occurs to generate a positive ex-
ample for the classifier corresponding to the actual
translation of fi in that sentence, according to the
automatic alignment. This will be as well a negative
example for the classifiers corresponding to the rest
of possible translations of fi.
2.2.1 Feature Set
We consider different kinds of information, al-
ways from the source sentence, based on standard
WSD methods (Yarowsky et al, 2001). As to the
local context, inside the source phrase to disam-
biguate, and 5 tokens to the left and to the right,
we use n-grams (n ? {1, 2, 3}) of: words, parts-
of-speech, lemmas and base phrase chunking IOB
labels. As to the global context, we collect topical
information by considering the source sentence as a
bag of lemmas.
2.2.2 Decoding. A Trick.
At translation time, we consider every instance of
fi as a separate case. In each case, for all possi-
ble translations of fi, we collect the SVM score, ac-
cording to the SVM classification rule. We are in
fact modeling P (ej |fi). However, these scores are
not probabilities. We transform them into proba-
bilities by applying the softmax function described
by Bishop (1995). We do not constrain the decoder
to use the translation ej with highest probability. In-
stead, we make all predictions available and let the
decoder choose. We have avoided implementing a
new decoder by pre-computing all the SVM pre-
dictions for all possible translations for all source
phrases appearing in the test set. We input this in-
formation onto the decoder by replicating the entries
in the translation table. In other words, each distinct
occurrence of every single source phrase has a dis-
tinct list of phrase translation candidates with their
corresponding scores. Accordingly, the source sen-
tence is transformed into a sequence of identifiers,
160
in our case a sequence of (w, i) pairs3, which allow
us to uniquely identify every distinct instance of ev-
ery word in the test set during decoding, and to re-
trieve DPT predictions in the translation table. For
that purpose, source phrases in the translation table
must comply with the same format.
This imaginative trick4 saved us in the short run
a gigantic amount of work. However, it imposes a
severe limitation on the kind of features which the
DPT system may use. In particular, features from
the target sentence under construction and from
the correspondence between source and target (i.e.,
alignments) can not be used.
3 Phrase Translation
Analogously to the ?word translation? definition by
Vickrey et al (2005), rather than predicting the sense
of a word according to a given sense inventory, in
?phrase translation?, the goal is to predict the correct
translation of a phrase, for a given target language,
in the context of a sentence. This task is simpler than
the ?full translation? task, but provides an insight to
the gain prospectives.
We used the data from the Openlab 2006 Initia-
tive5 promoted by the TC-STAR Consortium6. This
test suite is entirely based on European Parliament
Proceedings. We have focused on the Spanish-to-
English task. The training set consists of 1,281,427
parallel sentences. Performing phrase extraction
over the training data, as described in Section 2.1,
we obtained translation candidates for 1,729,191
source phrases. We built classifiers for all the source
phrases with more than one possible translation and
more than 10 occurrences. 241,234 source phrases
fulfilled this requirement. For each source phrase,
we used 80% of the instances for training, 10% for
development, and 10% for test.
Table 1 shows ?phrase translation? results over
the test set. We compare the performance, in terms
of accuracy, of DPT models and the ?most fre-
quent translation? baseline (?MFT?). The MFT base-
3w is a word and i corresponds to the number of instances
of word w seen in the test set before the current instance.
4We have checked that results following this type of decod-
ing when translation tables are estimated on the basis of MLE
are identical to regular decoding results.
5http://tc-star.itc.it/openlab2006/
6http://www.tc-star.org/
phrase set model macro micro
all MFT 0.66 0.70
DPT 0.68 0.76
frequent MFT 0.76 0.75
DPT 0.86 0.86
Table 1: ?Phrase Translation? Accuracy (test set).
line is equivalent to selecting the translation candi-
date with highest probability according to MLE. The
?macro? column shows macro-averaged results over
all phrases, i.e., the accuracy for each phrase counts
equally towards the average. The ?micro? column
shows micro-averaged accuracy, where each test ex-
ample counts equally. The ?all? set includes results
for the 241,234 phrases, whereas the ?frequent? set
includes results for a selection of 41 very frequent
phrases ocurring more than 50,000 times.
A priori, DPT models seem to offer a significant
room for potential improvement. Although phrase
translation differs from WSD in a number of as-
pects, the increase with respect to the MFT baseline
is comparable. Results are also coherent with those
attained by Vickrey et al (2005).
-1
-0.5
 0
 0.5
 1
 0  50000  100000  150000  200000  250000  300000
a
cc
u
ra
cy
(D
PT
) -
 ac
cu
rac
y(M
LE
)
#examples
Figure 1: Analysis of ?Phrase Translation? Results
on the development set (Spanish-to-English).
Figure 1 shows the relationship between the accu-
racy7 gain and the number of training examples. In
general, with a sufficient number of examples (over
10,000), DPT outperforms the MFT baseline.
7We focus on micro-averaged accuracy.
161
4 Full Translation
In the ?phrase translation? task the predicted phrase
does not interact with the rest of the target sentence.
In this section we analyze the impact of DPT models
when the goal is to translate the whole sentence.
For evaluation purposes we count on a set of 1,008
sentences. Three human references per sentence are
available. We randomly split this set in two halves,
and use them for development and test, respectively.
4.1 Evaluation
Evaluating the effects of using DPT predictions, di-
rected towards a better word selection, in the full
translation task presents two serious difficulties.
In first place, the actual room for improvement
caused by a better translation modeling is smaller
than estimated in Section 3. This is mainly due to
the SMT architecture itself which relies on a search
over a probability space in which several models co-
operate. For instance, in many cases errors caused
by a poor translation modeling may be corrected by
the language model. In a recent study, Vilar et al
(2006) found that only around 25% of the errors are
related to word selection. In half of these cases er-
rors are caused by a wrong word sense disambigua-
tion, and in the other half the word sense is correct
but the lexical choice is wrong.
In second place, most conventional automatic
evaluation metrics have not been designed for this
purpose. For instance, metrics such as BLEU (Pa-
pineni et al, 2001) tend to favour longer n-gram
matchings, and are, thus, biased towards word or-
dering. We might find better suited metrics, such
as METEOR (Banerjee and Lavie, 2005), which is
oriented towards word selection8. However, a new
problem arises. Because different metrics are biased
towards different aspects of quality, scores conferred
by different metrics are often controversial.
In order to cope with evaluation difficulties we
have applied several complementary actions:
1. Based on the results from Section 3, we focus
on a reduced set of 41 very promising phrases
trained on more than 50,000 examples. This
set covers 25.8% of the words in the test set,
8METEOR works at the unigram level, may consider word
stemming and, for the case of English is also able to perform a
lookup for synonymy in WordNet (Fellbaum, 1998).
and exhibits a potential absolute accuracy gain
around 11% (See Table 1).
2. With the purpose of evaluating the changes re-
lated only to this small set of very promis-
ing phrases, we introduce a new measure, Apt,
which computes ?phrase translation? accuracy
for a given list of source phrases. For every
test case, Apt counts the proportion of phrases
from the list appearing in the source sentence
which have a valid9 translation both in the tar-
get sentence and in any of the reference trans-
lations. In fact, because in general source-to-
target algnments are not known, Apt calculates
an approximate10 solution.
3. We evaluate overall MT quality on the basis
of ?Human Likeness?. In particular, we use
the QUEEN11 meta-measure from the QARLA
Framework (Amigo? et al, 2005). QUEEN op-
erates under the assumption that a good trans-
lation must be similar to all human references
according to all metrics. Given a set of auto-
matic translations A, a set of similarity metrics
X, and a set of human references R, QUEEN is
defined as the probability, over R?R?R, that
for every metric in X the automatic translation
a is more similar to a reference r than two other
references r? and r?? to each other. Formally:
QUEENX,R(a) = Prob(?x ? X : x(a, r) ? x(r?, r??))
QUEEN captures the features that are common
to all human references, rewarding those auto-
matic translations which share them, and pe-
nalizing those which do not. Thus, QUEEN pro-
vides a robust means of combining several met-
rics into a single measure of quality. Following
the methodology described by Gime?nez and
Amigo? (2006), we compute the QUEEN mea-
sure over the metric combination with high-
est KING, i.e., discriminative power. We have
considered all the lexical metrics12 provided by
9Valid translations are provided by the translation table.
10Current Apt implementation searches phrases from left to
right in decreasing length order.
11QUEEN is available inside the IQMT package for MT
Evaluation based on ?Human Likeness? (Gime?nez and Amigo?,
2006). http://www.lsi.upc.edu/?nlp/IQMT
12Consult the IQMT Technical Manual v1.3 for a detailed de-
scription of the metric set. http://www.lsi.upc.edu/
?nlp/IQMT/IQMT.v1.3.pdf
162
QUEEN Apt BLEU METEOR ROUGE
P (e) + PMLE(f |e) 0.43 0.86 0.59 0.77 0.42
P (e) + PMLE(e|f) 0.45 0.87 0.62 0.77 0.43
P (e) + PDPT (e|f) 0.47 0.89 0.62 0.78 0.44
Table 2: Automatic evaluation of the ?full translation? results on the test set.
IQMT. The optimal set is:
{ METEORwnsyn, ROUGEw 1.2 }
which includes variants of METEOR, and
ROUGE (Lin and Och, 2004).
4.2 Adjustment of Parameters
Models are combined in a log-linear fashion:
logP (e|f) ? ?lmlogP (e) + ?glogPMLE(f |e)
+ ?dlogPMLE(e|f) + ?DPT logPDPT (e|f)
P (e) is the language model probability.
PMLE(f |e) corresponds to the MLE-based
generative translation model, whereas PMLE(e|f)
corresponds to the analogous discriminative model.
PDPT (e|f) corresponds to the DPT model which
uses SVM-based predictions in a wider feature
context. In order to perform fair comparisons,
model weights must be adjusted.
Because we have focused on a reduced set of fre-
quent phrases, in order to translate the whole test set
we must provide alternative translation probabilities
for all the source phrases in the vocabulary which
do not have a DPT prediction. We have used MLE
predictions to complete the model. However, inter-
action between DPT and MLE models is problem-
atic. Problems arise when, for a given source phrase,
fi, DPT predictions must compete with MLE pre-
dictions for larger phrases fj overlapping with or
containing fi (See Section 4.3). We have alleviated
these problems by splitting DPT tables in 3 subta-
bles: (1) phrases with DPT prediction, (2) phrases
with DPT prediction only for subphrases of it, and
(3) phrases with no DPT prediction for any sub-
phrase; and separately adjusting their weights.
Counting on a reliable automatic measure of qual-
ity is a crucial issue for system development. Opti-
mal configurations may vary very significantly de-
pending on the metric governing the optimization
process. We optimize the system parameters over
the QUEEN measure, which has proved to lead to
more robust system configurations than BLEU (Lam-
bert et al, 2006). We exhaustively try all possible
parameter configurations, at a resolution of 0.1, over
the development set and select the best one. In order
to keep the optimization process feasible, in terms of
time, the search space is pruned13 during decoding.
4.3 Results
We compare the systems using the generative and
discriminative MLE-based translation models to the
discriminative translation model which uses DPT
predictions for the set of 41 very ?frequent? source
phrases. Table 2 shows automatic evaluation re-
sults on the test set, according to several metrics.
Phrase translation accuracy (over the ?frequent? set
of phrases) and MT quality are evaluated by means
of the Apt and QUEEN measures, respectively. For
the sake of informativeness, BLEU, METEORwnsyn
and ROUGEw 1.2 scores are provided as well.
Interestingly, discriminative models outperform
the (noisy-channel) default generative model. Im-
provement in Apt measure also reveals that DPT pre-
dictions provide a better translation for the set of
?frequent? phrases than the MLE models. This im-
provement remains when measuring overall transla-
tion quality via QUEEN. If we take into account that
DPT predictions are available for only 25% of the
words in the test set, we can say that the gain re-
ported by the QUEEN and Apt measures is consistent
with the accuracy prospectives predicted in Table 1.
METEORwnsyn and ROUGEw 1.2 reflect a slight im-
provement as well. However, according to BLEU
there is no difference between both systems. We
suspect that BLEU is unable to accurately reflect the
possible gains attained by a better ?phrase selection?
over a small set of phrases because of its tendency
13For each phrase only the 30 top-scoring translations are
used. At all times, only the 100 top-scoring solutions are kept.
We also disabled distortion and word penalty models. There-
fore, translations are monotonic, and source and target tend to
have the same number of words (that is not mandatory).
163
to reward long n-gram matchings. In order to clar-
ify this scenario a rigorous process of manual evalu-
ation has been conducted. We have selected a subset
of sentences based on the following criteria:
? sentence length between 10 and 30 words.
? at least 5 words have a DPT prediction.
? DPT and MLE outputs differ.
A total of 114 sentences fulfill these require-
ments. In each translation case, assessors must judge
whether the output by the discriminative ?MLE? sys-
tem is better, equal to or worse than the output by
the ?DPT? system, with respect to adequacy, fluency,
and overall quality. In order to avoid any bias in the
evaluation, we have randomized the respective posi-
tion in the display of the sentences corresponding to
each system. Four judges participated in the evalua-
tion. Each judge evaluated only half of the cases.
Each case was evaluated by two different judges.
Therefore, we count on 228 human assessments.
Table 3 shows the results of the manual system
comparison. Statistical significance has been deter-
mined using the sign-test (Siegel, 1956). According
to human assessors, the ?DPT? system outperforms
the ?MLE? system very significantly with respect to
adequacy, whereas for fluency there is a slight ad-
vantage in favor of the ?MLE? system. Overall, there
is a slight but significant advantage in favor of the
?DPT? system. Manual evaluation confirms our sus-
picion that the BLEU metric is less sensitive than
QUEEN to improvements related to adequacy.
Error Analysis
Guided by the QUEEN measure, we carefully inspect
particular cases. We start, in Table 4, by show-
ing a positive case. The three phrases highlighted
in the source sentence (?tiene?, ?sen?ora? and ?una
cuestio?n?) find a better translation with the help of
the DPT models: ?tiene? translates into ?has? instead
of ?i give?, ?sen?ora? into ?mrs? instead of ?lady?, and
?una cuestio?n? into ?a point? instead of ?a ... motion?.
In contrast, Table 5 shows a negative case. The
translation of the Spanish word ?sen?ora? as ?mrs? is
acceptable. However, it influences very negatively
the translation of the following word ?diputada?,
whereas the ?MLE? system translates the phrase
?sen?ora diputada?, which does not have a DPT pre-
diction, as a whole. Similarly, the translation of
Adequacy Fluency Overall
MLE > DPT 39 84 83
MLE = DPT 100 76 46
MLE < DPT 89 68 99
Table 3: Manual evaluation of the ?full translation?
results on the test set. Counts on the number of
translation cases for which the ?MLE? system is bet-
ter than (>), equal to (=), or worse than (<) the
?DPT? system, with respect to adequacy, fluency,
and overall MT quality, are presented.
?cuestio?n? as ?matter?, although acceptable, is break-
ing the phrase ?cuestio?n de orden? of high cohe-
sion, which is commonly translated as ?point of or-
der?. The cause underlying these problems is that
DPT predictions are available only for a subset of
phrases. Thus, during decoding, for these cases our
DPT models may be in disadvantage.
5 Related Work
Recently, there is a growing interest in the appli-
cation of WSD technology to MT. For instance,
Carpuat and Wu (2005b) suggested integrating
WSD predictions into a SMT system in a ?hard?
manner, either for decoding, by constraining the set
of acceptable translation candidates for each given
source word, or for post-processing the SMT sys-
tem output, by directly replacing the translation of
each selected word with the WSD system predic-
tion. They did not manage to improve MT quality.
They encountered several problems inherent to the
SMT architecture. In particular, they described what
they called the ?language model effect? in SMT:
?The lexical choices are made in a way that heav-
ily prefers phrasal cohesion in the output target sen-
tence, as scored by the language model.?. This prob-
lem is a direct consequence of the ?hard? interaction
between their WSD and SMT systems. WSD pre-
dictions cannot adapt to the surrounding target con-
text. In a later work, Carpuat and Wu (2005a) ana-
lyzed the converse question, i.e. they measured the
WSD performance of SMT models. They showed
that dedicated WSD models significantly outper-
form current state-of-the-art SMT models. Conse-
quently, SMT should benefit from WSD predictions.
Simultaneously, Vickrey et al (2005) studied the
164
Source tiene la palabra la sen?ora mussolini para una cuestio?n de orden .
Ref 1 mrs mussolini has the floor for a point of order .
Ref 2 you have the floor , missus mussolini , for a question of order .
Ref 3 ms mussolini has now the floor for a point of order .
P (e) + PMLE(e|f) i give the floor to the lady mussolini for a procedural motion .
P (e) + PDPT (e|f) has the floor the mrs mussolini on a point of order .
Table 4: Case of Analysis of sentence #422. DPT models help.
Source sen?ora diputada , e?sta no es una cuestio?n de orden .
Ref 1 mrs mussolini , that is not a point of order .
Ref 2 honourable member , this is not a question of order .
Ref 3 my honourable friend , this is not a point of order .
P (e) + PMLE(e|f) honourable member , this is not a point of order .
P (e) + PDPT (e|f) mrs karamanou , this is not a matter of order .
Table 5: Case of Analysis of sentence #434. DPT models fail.
application of discriminative models based on WSD
technology to the ?blank-filling? task, a simplified
version of the translation task, in which the target
context surrounding the word translation is avail-
able. They did not encounter the ?language model
effect? because they approached the task in a ?soft?
way, i.e., allowing their WSD models to interact
with other models during decoding. Similarly, our
DPT models are, as described in Section 2.2, softly
integrated in the decoding step, and thus do not suf-
fer from the detrimental ?language model effect? ei-
ther, in the context of the ?full translation? task. Be-
sides, DPT models enforce phrasal cohesion by con-
sidering disambiguation at the level of phrases.
6 Conclusions and Further Work
Despite the fact that measuring improvements in
word selection is a very delicate issue, we have
showed that dedicated discriminative translation
models considering a wider feature context provide
a useful mechanism in order to improve the qual-
ity of current phrase-based SMT systems, specially
with regard to adequacy. However, the fact that no
gain in fluency is reported indicates that the integra-
tion of these probabilities into the statistical frame-
work requires further study.
Moreover, there are several open issues. First, for
practical reasons, we have limited to a reduced set of
?frequent? phrases, and we have disabled reordering
and word penalty models. We are currently studying
the impact of a larger set of phrases, covering over
99% of the words in the test set. Experiments with
enabled reordering and word penalty models should
be conducted as well. Second, automatic evalua-
tion of the results revealed a low agreement between
BLEU and other metrics. For system comparison, we
solved this through a process of manual evaluation.
However, this is impractical for the adjustment of
parameters, where hundreds of different configura-
tions are tried. In this work we have relied on auto-
matic evaluation based on ?Human Likeness? which
allows for metric combinations and provides a sta-
ble and robust criterion for the metric set selection.
Other alternatives could be tried. The crucial issue,
in our opinion, is that the metric guiding the opti-
mization is able to capture the changes.
Finally, we argue that, if DPT models considered
features from the target side, and from the corre-
spondence between source and target, results could
further improve. However, at the short term, the in-
corporation of these type of features will force us to
either build a new decoder or extend an existing one,
or to move to a new MT architecture, for instance,
in the fashion of the architectures suggested by Till-
mann and Zhang (2006) or Liang et al (2006).
Acknowledgements
This research has been funded by the Span-
ish Ministry of Education and Science, projects
OpenMT (TIN2006-15307-C03-02) and TRAN-
165
GRAM (TIN2004-07925-C03-02). We are recog-
nized as a Quality Research Group (2005 SGR-
00130) by DURSI, the Research Department of the
Catalan Government. Authors are thankful to the
TC-STAR Consortium for providing such very valu-
able data sets.
References
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Sumarization. In Proceed-
ings of the 43th Annual Meeting of the Association for
Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Christopher M. Bishop. 1995. 6.4: Modeling conditional
distributions. In Neural Networks for Pattern Recog-
nition, page 215. Oxford University Press.
Marine Carpuat and Dekai Wu. 2005a. Evaluating the
Word Sense Disambiguation Performance of Statisti-
cal Machine Translation. In Proceedings of IJCNLP.
Marine Carpuat and Dekai Wu. 2005b. Word Sense Dis-
ambiguation vs. Statistical Machine Translation. In
Proceedings of ACL.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings of the 4th LREC.
Xavier Carreras, Llu??s Ma?rquez, and Jorge Castro. 2005.
Filtering-ranking perceptron learning for partial pars-
ing. Machine Learning, 59:1?31.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
Framework for Automatic Machine Translation Eval-
uation. In Proceedings of the 5th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of 4th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2005. Combining
Linguistic Data Views for Phrase-based SMT. In Pro-
ceedings of the Workshop on Building and Using Par-
allel Texts, ACL.
T. Joachims. 1999. Making large-Scale SVM Learning
Practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. The MIT Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA.
Patrik Lambert, Jesu?s Gime?nez, Marta R. Costa-jussa?,
Enrique Amigo?, Rafael E. Banchs, Llu??s Ma?rquez, and
J.A. R. Fonollosa. 2006. Machine Translation Sys-
tem Development based on Human Likeness. In Pro-
ceedings of IEEE/ACL 2006 Workshop on Spoken Lan-
guage Technology.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, , and
Ben Taskar. 2006. An End-to-End Discriminative
Approach to Machine Translation. In Proceedings of
COLING-ACL06.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation, rc22176. Technical report,
IBM T.J. Watson Research Center.
Sidney Siegel. 1956. Nonparametric Statistics for the
Behavioral Sciences. McGraw-Hill.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP.
Christoph Tillmann and Tong Zhang. 2006. A Discrim-
inative Global Training Algorithm for Statistical MT.
In Proceedings of COLING-ACL06.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-Sense Disambiguation for Machine Translation.
In Proceedings of HLT/EMNLP.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In Proceedings of the 5th LREC.
David Yarowsky, Silviu Cucerzan, Radu Florian, Charles
Schafer, and Richard Wicentowski. 2001. The Johns
Hopkins Senseval2 System Descriptions. In Proceed-
ings of Senseval-2: Second International Workshop on
Evaluating Word Sense Disambiguation Systems.
166
