Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 110?114,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Exodus ? Exploring SMT for EU Institutions
Michael Jellinghaus1,2, Alexandros Poulis1, David Kolovratn??k3
1: European Parliament, Luxembourg
2: Saarland University, Saarbru?cken, Germany
3: Charles University in Prague, Czech Republic
micha@coli.uni-sb.de, apoulis@europarl.europa.eu, david@kolovratnik.net
Abstract
In this paper, we describe Exodus, a joint
pilot project of the European Commission?s
Directorate-General for Translation (DGT)
and the European Parliament?s Directorate-
General for Translation (DG TRAD) which
explores the potential of deploying new ap-
proaches to machine translation in European
institutions. We have participated in the
English-to-French track of this year?s WMT10
shared translation task using a system trained
on data previously extracted from large in-
house translation memories.
1 Project Background
1.1 Translation at EU Institutions
The European Union?s policy on multilingualism1 re-
quires enormous amounts of documents to be trans-
lated into the 23 official languages (which yield 506
translation directions). To cope with this task, the EU
has the biggest translation service in the world, em-
ploying almost 5000 internal staff as translators (out of
which 1750 at the European Commission (EC) and 760
at the European Parliament (EP) alone), backed up by
more than 2000 support staff. In 2009, the total output
of the Commission?s Directorate-General for Transla-
tion (DGT) and the Parliament?s Directorate-General
for Translation (DG TRAD) together was more than 3
million translated pages. Thus, it is not surprising that
the cost of all translation and interpreting services of
all the EU institutions amounts to 1% of the annual EU
budget (2008 figures). According to our estimations,
this is more than e 1 billion per year.
1.2 Machine Translation and Other Translation
Technologies at EU Institutions
In order to make the translators? work more efficient so
that they can translate more pages in the same time,
a number of tools like terminology databases, bilin-
gual concordancers, and, most importantly, translation
memories are at their disposition, most of which are
heavily used.
1http://ec.europa.eu/education/
languages/eu-language-policy/index en.htm
In real translation production scenarios, Machine
Translation is usually used to complement transla-
tion memory tools (TM tool). Translation memories
are databases that contain text segments (usually sen-
tences) that are stored together with their translations.
Each such pair of source and target language segments
is called a translation unit. Translation units also con-
tain useful meta-data (creation date, document type,
client, etc.) that allow us to filter the data both for trans-
lation and machine translation purposes.
A TM tool tries to match the segments within a doc-
ument that needs to be translated with segments in the
translation memory and propose translations. If the
memory contains an identical string then we have a so-
called exact or 100% match which yields a very reliable
translation. Approximate or partial matches are called
fuzzy matches and usually, the minimum value of a
fuzzy match is set to 65%?70%. Lower matches are
not considered as usable since they demand more edit-
ing time than typing a translation from scratch. First
experiments have shown that the quality of SMT out-
put for certain language pairs is equal or similar to 70%
fuzzy matches.
Consequently, the cases where machine translation
can play a helpful role in this context is when, for a seg-
ment to be translated, there is no exact match and the
available fuzzy matches do not exceed a certain thresh-
old. This threshold in our case is expected to be 85% or
lower. To this end, there exists a system called ECMT
(European Commission Machine Translation; also ac-
cessible to other European institutions) which is a rule-
based system.
However, only certain translation directions are cov-
ered by ECMT, and its maintenance is quite compli-
cated and requires quite a lot of dedicated and special-
ized human resources. In the light of these facts and
with the addition of the languages of (prospective) new
member states, statistical approaches to machine trans-
lation seem to offer a viable alternative.
First of all, SMT is data-driven, i.e. it exploits par-
allel corpora of which there are plenty at the EU in-
stitutions in the form of translation memories. Trans-
lation memories have two main advantages over other
parallel corpora. First of all, they contain almost ex-
clusively perfectly aligned segments, as each segment
is stored together with its translation, and secondly,
110
they contain cleaner data since their content is regu-
larly maintained by linguists and database administra-
tors. SMT systems are quicker to develop and easier
to maintain than rule-based systems. The availability
of free, open-source software like Moses2 (Koehn et
al., 2007), GIZA++3 (Och and Ney, 2003) and the like
constitutes a further argument in their favor.
Early experiments with Moses were started by mem-
bers of DGT?s Portuguese Language Department as
early as summer 2008 (Leal Fontes and Machado,
2009), then turned into a wider interinstitutional project
with the codename Exodus, currently combining re-
sources from European Commission?s DGT and Euro-
pean Parliament?s DGTRAD. Exodus is the first joint
project of the interinstitutional Language Technology
Watch group where a number of EU institutions join
forces in the field of language technology.
2 Participation in WMT 2010 Shared
Task
After the English-Portuguese experiments, the first lan-
guage pair for which we developed a system with
a sizeable amount of training data was English-to-
French. This system has been developed for testing
at the European Parliament. As English-to-French is
also one of the eight translation directions evaluated in
this year?s shared translation task, we decided to partic-
ipate. The reasons behind this decision are manifold:
We would like to
? know where we stand in comparison to other sys-
tems,
? learn about what system adaptations are the most
beneficial,
? make our project known to potential collaborators,
? compare the WMT10 evaluation results to the out-
come of our in-house evaluation.
There is, however, one major difference between the
evaluation as carried out in WMT10 and our in-house
evaluation: The test data of WMT10 consists exclu-
sively of news articles and is thus out-of-domain for
our system intended for use within the European Parlia-
ment. This means that the impact of training our system
on the in-domain data we obtain from our translation
memories cannot be assessed properly, i.e. taking into
consideration our specific translation production needs.
Therefore, we would like to invite other interested
groups to also translate our in-domain test data with
the goal of seeing how our translation scenario could
benefit from their setups. Due to legal issues, however,
we unfortunately cannot provide our internal training
data at this moment.
2http://www.statmt.org/moses/
3http://www.fjoch.com/GIZA++.html
3 Data Used
To build our English-to-French MT system, we did
not use any of the data provided by the organizers of
the WMT10 shared translation task. Instead, we used
data that was extracted from the translation memories
at the core of EURAMIS (European Advanced Multi-
lingual Information System; (Theologitis, 1997; Blatt,
1998)) which are the fruit of thousands of man-years
contributed by translators at EU institutions who, each
day, upload the majority of the segments they translate.
Initially (before pre-processing), our EN-FR cor-
pus contained 10,446,450 segments and included doc-
uments both from the Commission and the EP from
common legislative procedures. These segments were
extracted in November 2009 from 7 translation memo-
ries hosted in Euramis. Currently, we do not have in-
formation about the exact document types coming from
the Commission?s databases. The Parliament?s docu-
ment types used include, among others:
? legislative documents such as draft reports, final
reports, amendments, opinions, etc.,
? documents for the plenary such as questions, res-
olutions or session amendments,
? committee and delegation documents,
? documents concerning the ACP4 and the EMPA5,
? internal documents such as budget estimates, staff
regulations, rules of procedure, etc.,
? calls for tender.
Any sensitive or classified documents or
Commission-internal documents that do not be-
long to common legislative procedures have been
excluded from the data.
In terms of preprocessing, we performed several
steps. First, we obtained translation memory exchange
(TMX) files from EURAMIS and converted them to
UTF-8 text as the Euramis native character encoding
is UCS-2. Then we removed certain control charac-
ters which otherwise would have halted processing, we
extracted the two single-language corpora into a plain-
text file, tokenized and lowercased the data. Finally,
we separated the corpus into training data (9,300,682
segments), and data for tuning and testing ? 500 seg-
ments each. These segments did not exceed a max-
imum length of 60 tokens and were excluded from
the preparation of the translation and language models.
The models were then trained on the remaining seg-
ments. The maximum length of 60 tokens was applied
here as well.
4African, Caribbean and Pacific Group of States
5Euro-Mediterranean Parliamentary Assembly
111
Metric Score
BLEU 18.8
BLEU-cased 16.9
TER 0.747
Table 1: Automatic scores calculated for Exodus in
WMT10
4 Building the Models and Decoding
The parallel data described above was used to train an
English-to-French translation model and a French tar-
get language model. This was done on a server running
Sun Solaris with 64 GB of RAM and 8 double core
CPU?s @1800 Mhz (albeit shared with other processes
running simultaneously).
In general, we simply used a vanilla Moses instal-
lation at this point, leaving the integration of more so-
phisticated features to a later moment, i.e. after a thor-
ough analysis of the results of the present evaluation
campaign when we will know which adaptations yield
the most significant improvements.
For the word alignments, we chose MGIZA (Gao
and Vogel, 2008), using seven threads per MGIZA in-
stance, with the parallel option, i.e. one MGIZA in-
stance per pair direction running in parallel. The target
language model is a 7-gram, binarized IRSTLM (Fed-
erico et al, 2008). The weights of the distortion, trans-
lation and language models were optimized with re-
spect to BLEU scores (Papineni et al, 2002) on a given
held-out set of sentences with Minimum Error Rate
Training (MERT; (Och, 2003)) in 15 iterations.
After the actual translation with Moses, an additional
recasing ?translation? model was applied in the same
manner. Finally, the translation output underwent min-
imal automatic postprocessing based on regular expres-
sion replacements. This was mainly undertaken in or-
der to fix the distribution of whitespace and some re-
maining capitalization issues.
5 Results
5.1 WMT10 Evaluation
In one of the tasks of the WMT10 human evaluation
campaign, people were asked to rank competing trans-
lations. From each 1-through-5 ranking of a set of 5
system outputs, 10 pairwise comparisons are extracted.
Then, for each system, a score is computed that tells
how often it was ranked equally or better than the other
system. For our system, this score is 32.35%, meaning
it ranked 17th out of 19 systems for English-to-French.
A number of automatic scores were also calculated and
appear in Table 1.
5.2 Evaluation at the European Parliament
As the goal behind building our system has been to pro-
vide a tool to translators at EU institutions, we have
also had it evaluated by two of our colleagues, both
Evaluator A Evaluator B Overall
Reference 1.75 2.06 1.97
ECMT 3.34 3.31 3.32
Google 3.59 3.28 3.37
Exodus 3.52 3.45 3.47
Table 2: Average relative rank (on a scale from 1 to 5)
OK Edited Bad
Reference 29 30 2
ECMT 8 57 2
Google 7 33 5
Exodus 13 62 12
Table 3: Results of Editing Task (?OK? means ?No cor-
rections needed?; ?Bad? means ?Unable to correct?)
native speakers of French and working as professional
translators of the French Language Unit at the Parlia-
ment?s DG TRAD.
For this purpose, we had 1742 sentences of in-house
documents translated by our system as well as by
the rule-based ECMT and the statistics-based Google
Translate.6,7 We developed an online evaluation tool
based on the one used by the WMT evaluation cam-
paign in the last years (Callison-Burch et al, 2009)
where we asked the evaluators to perform three differ-
ent tasks.
In the first one, they were shown the three automatic
translations plus a human reference in random order
and asked to rank the four versions relative to each
other on a scale from 1 to 5. The average relative ranks
can be seen in Table 2.
The second task consisted of post-editing a given
translation. Again, the sentence might come from one
of three MT systems, or be a human translation. The
absolute number of items that did not need any correc-
tions, had to be edited, or were impossible to correct
are shown in Table 3.
For the third and last task, only translations of our
own system were displayed. Here, the evaluators
should simply assign them to one of four quality cat-
egories as proposed by (Roturier, 2009), and addition-
ally tick boxes standing for the presence of 13 different
types of errors in the sentence concerning word order,
punctuation, or different types of syntactic/semantic
problems. A total of 150 segments were judged. For
the categorization results, see Tables 4 and 5.
5.3 Evaluation at the European Commission
On a side note, the Portuguese Language Department
also performed a manual evaluation (Leal Fontes and
Machado, 2009) which involved 14 of their managers
and translators, comparing their Moses-based system to
6http://translate.google.com
7As about a third of the source documents are not public,
we could not send those to Google Translate.
112
Items Proportion
Excellent 28 18.6%
Good 42 28%
Medium 45 30%
Poor 35 23.3%
Table 4: Results of Categorization Task: Quality Cate-
gories
Error type Occurrences
Word order
Single word 11
Sequence of words 42
Incorrect word(s)
Wrong lexical choice 51
Wrong terminology choice 6
Incorrect form 77
Extra word(s) 21
Missing word(s) 14
Style 44
Idioms 1
Untranslated word(s) 5
Punctuation 24
Letter case 7
Other 5
Table 5: Results of Categorization Task: Error Types
ECMT and Google. Table 6 shows how many people
considered Moses better, similar, or worse compared to
ECMT and Google, respectively.
Moses-based SMT did well in fields where ECMT
is systematically used (e.g. Justice and Home Affairs
and Trade) and showed a big improvement over ECMT
in terminology-intensive domains (e.g. Fisheries). As
of early 2009, more than half of their translators (58%)
now already use ECMT systematically in production,
i.e. for all English and French originals. 85% use it for
specific language combinations or for certain domains
only. On a voluntary basis, they have been replacing
ECMT with Moses-based SMT for the translation of
day-to-day incoming documents. Over a three-month
period, more than 2500 pages have been translated in
this manner, and the translators of the Portuguese de-
partment declared themselves ready to switch over to
an SMT system as soon as it should become available.
Compared to Better Similar Worse
ECMT 7 5 2
Google 5 5 3
Table 6: Portuguese Language Department evaluation
results of Moses-based MT system
6 Discussion of Results
As expected, our system did not rank among the top
competitors in the WMT10 shared task. This is mainly
due to the data we trained on, which is of a very spe-
cific domain (common legislative procedures of Eu-
ropean Institutions) and relatively small in size when
compared to what others used for this language combi-
nation. In addition, we more or less used Moses out-of-
the-box with no sophisticated add-ons or optimization.
In the internal evaluation, our system beat neither
Google Translate nor ECMT overall but it did show a
similar performance. This is all the more encourag-
ing as Exodus has been built within less than a month,
while ECMT has been developed and maintained in ex-
cess of 30 years, and while Google Translate is based
on manpower and computing resources that a public
administration body usually cannot provide.
Finally, the successful trials of SMT software at the
EC?s Portuguese department seem to indicate that such
a system holds enormous potential, especially when a
serious adaptation to specific language combinations
and domains is taken into consideration.
7 Outlook
Further use and development of SMT at EU institutions
depends on the outcome of internal evaluations, among
other factors. We plan to extend our activities to other
language pairs, an English-to-Greek machine transla-
tion project already having started. Given a continu-
ation of the currently promising results, Exodus will
eventually be integrated into the CAT (computer-aided
translation) tools used by EU translators.8 Further-
more, we would like to release an extended EuroParl
corpus not only containing parliamentary proceedings
but also other types of public documents. We estimate
that such a step should foster research to the benefit of
both EU institutions and machine translation in gen-
eral.
8 Conclusions
We have presented Exodus, a joint pilot project of
the European Commission?s Directorate-General for
Translation (DGT) and the European Parliament?s
Directorate-General for Translation (DG TRAD) with
the aim of exploring the potential of deploying new
approaches to machine translation in European insti-
tutions.
Our system is based on a fairly vanilla Moses instal-
lation and trained on data extracted from large in-house
translation memories covering a range of EU docu-
ments. The obtained models use 7-grams.
We applied the Exodus system to this year?s WMT10
shared English-to-French translation task. As the test
8However, speed issues will have to be addressed before
as the current system is not able to provide translations in real
time.
113
data stems from a different domain than the one tar-
geted by our system, we did not outperform the com-
petitors. However, results from in-house evaluation are
promising and indicate the big potential of SMT for
European Institutions.
Acknowledgments
We would very much like to thank (in alphabetical or-
der) Manuel Toma?s Carrasco Ben??tez, Dirk De Paepe,
Alfons De Vuyst, Peter Hjorts?, Herman Jenne?, Hila?rio
Leal Fontes, Maria Jose? Machado, Spyridon Pilos, Joa?o
Rosas, Helmut Spindler, Filiep Spyckerelle, and Ange-
lika Vaasa for their invaluable help and support.
David Kolovratn??k was supported by the Czech Sci-
ence Foundation under contract no. 201/09/H057 and
by the Grant Agency of Charles University under con-
tract no. 100008/2008.
References
A. Blatt. 1998. EURAMIS : Added value by integra-
tion. In T&T Terminologie et Traduction, 1.1998,
pages 59?73.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an Open Source Toolkit for
Handling Large Scale Language Models. In Pro-
ceedings of Interspeech, Brisbane, Australia.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software En-
gineering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177?180, Jun.
Hila?rio Leal Fontes and Maria Jose? Machado. 2009.
Contribution of the Portuguese Langauge Depart-
ment to the Evaluation of Moses Machine Transla-
tion System. Technical report, Portuguese Language
Department, DGT, European Commission, Decem-
ber.
Franz Josef Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings
of ACL, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In ACL ?02:
Proceedings of the 40th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 311?
318, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Johann Roturier. 2009. Deploying novel MT tech-
nology to raise the bar for quality: A review of key
advantages and challenges. In The twelfth Machine
Translation Summit, Ottawa, Canada, August. Inter-
national Association for Machine Translation.
D. Theologitis. 1997. EURAMIS, the platform of the
EC translator. In EAMT Workshop, pages 17?32.
114
