Coling 2010: Poster Volume, pages 1131?1139,
Beijing, August 2010
Using Clustering to Improve Retrieval Evaluation without 
Relevance Judgments 
Zhiwei Shi 
Institute of Computing Technology 
Chinese Academy of Science 
shizhiwei@ict.ac.cn
Peng Li 
Institute of Computing Technology 
Chinese Academy of Science 
lipeng01@ict.ac.cn
Bin Wang 
Institute of Computing Technology 
Chinese Academy of Science 
wangbin@ict.ac.cn
Abstract
Retrieval evaluation without relevance 
judgments is a hard but also very mean-
ingful work. In this paper, we use clus-
tering technique to improve the per-
formance of judgment free retrieval 
evaluation. By using one system to rep-
resent all the systems that are similar to 
it, we can largely reduce the negative ef-
fect of similar retrieval results in Re-
trieval evaluation. Experimental results 
demonstrated that our method outper-
formed all the previous judgment free 
evaluation methods significantly. Its 
overall average performance outper-
formed the best previous result by 
20.5%. Besides, our work is a general 
framework that can be applied to any 
other judgment free evaluation method 
for performance improvement. 
1 Introduction 
Generally, to compare the effectiveness of in-
formation retrieval systems, we need to prepare 
a test collection composed of a set of documents, 
a set of query topics, and a set of relevance 
judgments indicating which documents are rele-
vant to which topics. Among these requirements, 
relevance judgment is the most human resource 
exhausting and time consuming part. It even 
becomes infeasible when the test collection is 
extremely large. To address this problem, the 
TREC conferences used a pooling technology 
(Voorhees and Harman, 1999), where the top n
(e.g., n=100) documents retrieved by each par-
ticipating system are collected into a pool and 
then only the documents in the pool are judged 
for system comparison. Zobel (1998) has shown 
that this pooling method leads to reliable results 
in term of determining the effectiveness of re-
trieval systems and their relative rankings. Yet, 
the relevance determination process is still very 
resource intensive especially when the test col-
lection reaches or exceeds terabyte, or much 
more queries are included. More seriously, 
when we change to a new document collection, 
we have to redo the entire evaluation process.  
There are two possible solutions to the prob-
lem above, evaluation with incomplete rele-
vance judgments and evaluation without rele-
vance judgments. The former is well studied.  
Many well designed ranking methods with in-
complete judgments were carried out. Two of 
them, Minimal Test Collection (MTC) method 
(Carterette et al, 2006) and Statistical evalua-
tion (statMAP) method (Aslam et al, 2006), 
even got practical application in the Million 
Query (1MQ) track in TREC 2007 (Allan et al,
2007), and achieved satisfactory evaluation per-
formance. The latter is comparatively less stud-
ied. Only a few papers concentrate on the issue 
of evaluating retrieval systems without rele-
vance judgments. In Section 2 of this paper, we 
will briefly review some representative methods. 
We will see what they are and how they work.  
1131
In this paper, we focus our effort on the re-
trieval evaluation without relevance judgments. 
Although ?blind? evaluation is really a hard 
problem and its evaluation performance is far 
less than that of methods with incomplete 
judgments, it is undeniable that non-judgment 
evaluation has its own advantages. In some 
cases, relevance judgments are non-attainable. 
For example, when researchers compare their 
novel retrieval algorithms to existing methods, 
or search for optimal parameters of their algo-
rithms, or conduct data fusion in a dynamic en-
vironment, relevance judgment usually seems 
impossible. Besides, to construct a good evalua-
tion method without relevance judgments, re-
searchers need to mine the retrieval results thor-
oughly, and try to find laws that indicate the 
correlation between the effectiveness of a sys-
tem and features of its retrieval result. These 
laws are not only useful for ?blind? evaluation 
methods but also valuable for evaluation meth-
ods with incomplete judgments. 
One of the useful laws for ?blind? evaluation 
methods is Authority Effect (Spoerri, 2005). Yet 
it always ruined by multiple similar results. 
In this work, we use clustering technique to 
solve this problem. By selecting one system to 
represent all the systems that are similar to it, 
we can largely reduce the negative effect of 
similar retrieval results. Details of this method 
will be presented Section 3. Experimental re-
sults, which are reported in Section 4, also veri-
fied that our idea is feasible and effective. Our 
method outperformed all the previous judgment 
free evaluation methods on every test bed.  The 
overall average performance outperformed the 
best previous result by 20.5%. Finally, we con-
clude our work in Section 5. 
2 Related Work 
In 2001, Soboroff et al (2001) firstly proposed 
the concept of evaluating retrieval systems in 
the absence of relevance judgments. They 
generated a set of pseudo-relevance judgments 
by randomly selecting and declaring some 
documents from the pool of top 100 documents 
as relevant. This set of pseudo-relevance 
judgments (instead of a set of human relevance 
judgments) was then used to determine the 
effectiveness of the retrieval systems. Four 
versions of this random pseudo-relevance 
method were designed and tested on data from 
the ad hoc track in TREC 3, 5, 6, 7 and 8. They 
were simple random pseudo-relevance method, 
the variant with duplicate documents, the 
variant with Shallow pools and the variant with 
Exact-fraction sampling. All their resulting 
system assessments and rankings were well 
correlated with actual TREC rankings, and the 
variant with duplicate documents in pools got 
the best performance, with an average Kendall?s 
tau value 0.50 over the data of TREC 3, 5, 6, 7 
and 8. 
Soboroff et al?s idea came from two results 
in retrieval evaluation. One is that incomplete 
judgments do not harm evaluation results 
greatly. Zobel?s (1998) research had showed 
that the results obtained using pooling technol-
ogy were quite reliable given a pool depth of 
100. He also found that even though the pool 
depth was limited to 10, the relative perform-
ance among systems changed little, although 
actual precision scores did change for some sys-
tems. The other is that partially incorrect rele-
vance judgments do not harm evaluation results 
greatly. Voorhees (1998) ascertained that de-
spite a low average overlap between assessment 
sets, and wide variation in overlap among par-
ticular topics, the relative rankings of systems 
remained largely unchanged across the different 
sets of relevance judgments. These two points 
are bases of Soboroff et al?s random pseudo-
relevance method, and give explanation to the 
result that their rankings were positively related 
to that of the actual TRECs. As a matter of fact, 
the two points are bases of all the retrieval 
evaluation methods without or with incomplete 
relevance judgments. 
Aslam and Savell (2003) devised a method to 
measure the relative retrieval effectiveness of 
systems through system similarity computation. 
In their work, the similarity between two re-
trieval systems was the ratio of the number of 
documents in their intersection and union. Each 
system was scored by the average similarity 
between it and all other systems. This measure-
ment produced results that were highly corre-
lated with the random pseudo-relevance method. 
Aslam and Savell hypothesized that this was 
caused by ?tyranny of the masses? effect, and 
these two related methods were assessing the 
systems based on ?popularity? instead of ?per-
formance?. The analysis by Spoerri (2005) sug-
1132
gested that the ?popularity? effect was caused by 
considering all the runs submitted by a retrieval 
system, instead of only selecting one run per 
system. Our later experimental results will show 
that this point of view is partially correct. The 
?popularity? effect could not be avoided com-
pletely by only selecting one run per system. 
This is indeed a hard problem for all the evalua-
tion methods without relevance judgments. 
Wu and Crestani (2003) developed multiple 
?reference count? based methods to rank re-
trieval systems. They made the distinction be-
tween an ?original? document and its duplicates 
in all other lists, called the ?reference? docu-
ments, when computing a document?s score. A 
system?s score is the (weighted) sum of the 
scores of its ?original? documents. Several ver-
sions of reference count method were carried 
out and tested. The basic method (Basic) scored 
each ?original? document by the number of its 
?reference? documents. The first variant (V1) 
assigned different weights to ?reference? docu-
ments based on their ranking positions. The 
second variant (V2) assigned different weights 
to the ?original? document based on its ranking 
position. The third variant (V3) assigned differ-
ent weights to both the ?original? documents and 
the ?reference? documents based on their rank-
ing positions. The fourth variant (V4) was simi-
lar to V3, except that it normalized the weights 
to ?reference? documents. Wu and Crestani?s 
method output similar evaluation performance 
to that of the random pseudo-relevance method. 
Their work also showed that the similarity be-
tween the multiple runs submitted by the same 
retrieval system affected the ranking process. If 
only one run was selected for any of the partici-
pant system for any query, for 3-9 systems, V3 
outperformed random pseudo-relevance method 
by 45.6%; for 10-15 systems, random pseudo-
relevance method outperformed V3 by 6.5%. 
Nuray and Can (2006) introduced a method 
to rank retrieval systems automatically using 
data fusion. Their method consists of two parts. 
One is selecting systems for data fusion, and the 
other is selecting documents as pseudo relevant 
documents as the fusion result. In the former 
part, they hypothesized that systems returning 
documents different from the majority could 
provide better discrimination among the docu-
ments and systems. In return, this could lead to 
a more accurate pseudo relevant documents and 
more accurate rankings. To find proper systems, 
they introduced the ?bias? concept for system 
selection. In their work, bias was 1 minus the 
similarity between a system and the majority, 
where the similarity is a normalized dot product 
of two vectors. In the latter part, Nuray and Can 
tested three criterions, namely Rank position, 
Borda count and Condorcet. Experimental re-
sults on data from TREC 3, 5, 6 and 7 showed 
that bias plus Condorcet got the best evaluation 
results and it outperformed the reference count 
method and random pseudo relevance method 
greatly. 
More recently, Spoerri (2007) proposed a 
method using the structure of overlap between 
search results to rank retrieval systems. This 
method provides us a new view on how to rank 
retrieval systems without relevance judgments. 
He used local statistics of retrieval results as 
indicators of relative effectiveness of retrieval 
systems. Concretely, if there are N systems to be 
ranked, N groups are constructed randomly with 
the constraint that each group contains five sys-
tems and each system will appear in five groups; 
then the percentages of a system?s documents 
not found by other systems (Single%) as well as 
the difference between the percentages of docu-
ments found by a single system and all five sys-
tems (Single%-AllFive%) are calculated as in-
dicators of relative effectiveness respectively. 
Spoerri found that these two local statistics were 
highly and negatively correlated with the mean 
average precision and precision at 1000 scores 
of the systems. By utilizing the two statistics to 
rank systems from subsets of TREC 3, 6, 7 and 
8, Spoerri obtained appealing evaluation results. 
The overlap structure of the top 50 documents 
were sufficient to rank retrieval systems and 
produced the best results, which outperformed 
previous attempts to rank retrieval systems 
without relevance judgments significantly. 
So far, we have reviewed 5 representatives of 
non-judgment evaluation methods. All these 
methods faced the same serious problem: simi-
lar runs harmed the effectiveness of ranking 
process. Different methods handled this prob-
lem differently. Aslam and Savell (2003) called 
this the ?tyranny of the masses? and provided no 
solution. Wu and Crestani (2003) addressed this 
problem by selecting only one run for any of the 
participant system for any query. Nuray and 
Can (2006) selected systems that were less simi-
1133
lar to the majority for data fusion. Spoerri (2007) 
performed his method on a selected subset of all 
the systems. All these treatments led to evalua-
tion performance improvement. Yet we will say 
it could be improved more. In the next section, 
we will present a new solution to this problem. 
Its performance is examined in Section 4. 
3 Using Clustering to Improve Re-
trieval Evaluation without Relevance 
Judgments
3.1 Problem
As we reviewed in Section 2, previous research 
had shown that incomplete relevance judgments 
and partially incorrect relevance judgments do 
not harm retrieval evaluation greatly. This is 
why pooling technique can lead to reliable 
retrieval evaluation results. It is also the 
theoretical foundation of evaluation without 
relevance judgments.
Besides, non-judgments methods armed with 
more laws inside retrieval results. These laws 
indicate the correlation between retrieval effec-
tiveness of a system and features in its retrieval 
results. One of the most important laws used in 
non-judgments evaluation is Authority Effect 
(Spoerri, 2005): document, which is retrieval by 
more systems, is more likely being relevant. 
Unfortunately, similar retrieval results ruined 
this law. Aslam and Savell (2003) called this the 
?tyranny of the masses?. So, how to alleviate the 
negative effect of similar retrieval results is a 
big issue in non-judgments evaluation.  
3.2 Solution
Generally, our solution to the ?tyranny of the 
masses? is removing similar systems by cluster-
ing. The whole process is as follows: 
Firstly, all systems to be evaluated are clus-
tered into several subsets. 
Secondly, for each subset, one system is se-
lected as a representative. 
Thirdly, all the information used for system 
evaluation comes from these representatives. 
Finally, score every system according to the 
information collected in the previous step.
This is the general framework of our method-
ology. Notice that, in the third step, only se-
lected systems contribute to the information 
required for system evaluation. So we can elimi-
nate the negative effect caused by similar re-
trieval results. 
This solution can be applied to any method of 
retrieval evaluation without relevance judg-
ments. To illustrate how to apply it to a retrieval 
evaluation method, we will describe using clus-
tering to improve Average System Similarity, 
which is proposed by Aslam and Savell (2003), 
in detail as an example. 
3.3 Average System Similarity Based on 
Clustering
In Aslam and Savell?s (2003) method, each sys-
tem is evaluated based on a criterion named Av-
erage System Similarity. The average system 
similarity of a given system S0 is calculated ac-
cording to formula (1). 
?
z
 
0
),(
1
1
)(AvgSysSim
0
0
SS
SSSysSim
n
S
(1)
where n is the number of systems to be evalu-
ated, and similarity between two systems S and 
S0, SysSim(S, S0), is calculated based on for-
mula (2). 
21
21
21 RetRet
RetRet
),(SysSim
?
?
 SS (2)
where Reti indicates the set of documents re-
turned by System i (i = 1, 2). 
When applying clustering technique to the 
system similarity method, we need to define an 
equivalence relation first. 
Definition 1 (System Equivalence): Suppose 
that all systems are clustered into m clusters 
namely C1, C2, ?, Cm. Two systems S1 and S2
are equivalent if and only if there exists k (1 ? k
? m) so that S1?Ck and S2?Ck.
kk CSCSmkk
iff
SS
??dd
 
21
21
,,1,
(3)
Given the definition of System Equivalence, 
we get the average system similarity based on 
clustering as follows: 
?
z
 
0
),(
1
1
)(AvgSysSim
0
0
SR
SRSysSim
m
S
(4)
where m is the number of clusters and R is the 
representative system of a cluster. 
1134
Replacing formula (1) with formula (4), we 
get the retrieval evaluation method Average 
System Similarity Based on Clustering, shortly 
ASSBC.
There are two important issues for ASSBC 
that need to be addressed. Issue 1: How to select 
representative system from a cluster? Issue 2: 
How to decide the number of clusters we need? 
Before we address Issue 1, we introduce an-
other definition, Cluster Similarity. 
Definition 2 (Cluster Similarity): for any 
given two clusters C1 and C2, with their respec-
tive representative systems S1 and S2, the cluster 
similarity between C1 and C2 is the system simi-
larity between S1 and S2.
),(SysSim),(ClusterSim 2121 SSCC  (5)
Now we come to selecting representative sys-
tems for clusters. Here, we utilize a hierarchical 
bottom up clustering technique. The entire clus-
tering process is as follows. 
Initially, each system forms a cluster.
Loop Until the number of clusters is m 
Two most similar clusters merge, and 
one of their representatives with higher 
average system similarity survives as 
the representative of the new cluster. 
End Loop. 
In the initial step, since every cluster contains 
only one system, the representative system is 
unquestionable. Within each loop, two represen-
tative systems of the old clusters are candidates 
of the new cluster, and the one with higher score, 
which means higher retrieval performance, be-
comes the representative of the new cluster. 
For Issue 2, technically, how to decide the 
number of clusters is always a problem for clus-
tering. Yet, we do not have to rush in the deci-
sion. Let us examine the evaluation perform-
ance on different values of m first. 
4 Experiments 
In this section, we will illustrate the evaluation 
performance of Average System Similarity 
Based on Clustering vs. different values of m.
Before we come to the experimental results, we 
would like to make some details clear first. 
4.1 Some Clarification 
4.1.1 Dataset
We perform our experiments on the ad hoc tasks 
of TREC-3, -5, -6 and -7. Most existing works 
on retrieval evaluation without judgments are 
tested on these tasks. To make a direct compari-
son with these work mentioned in Section 2 
later, we also choose these tasks as our test bed. 
4.1.2 Performance Measurement 
One of the measures of retrieval effectiveness 
used by TREC is mean non-interpolated average 
precision (MAP). Since average precision is 
based on much more information than other ef-
fectiveness measures such as R-precision or 
P(10) and known to be a more powerful and 
more stable effectiveness measure (Buckley and 
Voorhees, 2000), we utilize MAP as the effec-
tive measurement of retrieval systems in our 
experiments. 
The correlation of the ranking with our pro-
posed methods, as well as other methods, to the 
TREC official rankings is measured using the 
Spearman?s rank correlation coefficient. One 
reason is that it suits better for evaluating corre-
lation between ratio sequences, e.g. MAP, than 
Kendall?s tau. The other reason is that we can 
directly compare our results with those of pre-
vious attempts reviewed in Section 2, since 
most of them provided Spearman?s rank correla-
tion coefficient results. 
4.1.3 Substitute for Number of Clusters 
TREC Runs 
3 40 
5 61 
6 74 
7 103 
Table 1. Number of TREC runs 
As we know, the number of systems (runs) var-
ies in different TREC dataset (see Table 1 for 
details). Instead of examining the evaluation 
performance variation when absolute number of 
clusters m changes, we illustrate the evaluation 
performance vs. the percentage of m. Actually, 
for the sake of convenience, we will plot the 
correlation of our method to the TREC official 
rankings vs. the percentage of systems removed 
from the representative group in the following 
subsection.
1135
4.2 Experimental results 
Figure 1-4 show the plots of the correlation 
of our method to the TREC official rankings vs. 
the percentage of systems removed from the 
representative group on TREC-3, -5, -6 and -7 
respectively. The percentage of systems re-
moved goes from 0 to 85%, where 0 means no 
system removed and represents the original Av-
erage System Similarity method, and 85% is an 
up bound in our experiments. The horizontal 
line indicates the original performance. The 
tagged number on the curve says when the per-
formance curve reaches its peak and the peak 
value.







     
5HPRYHG3HUFHQWDJH
6S
HD
UP
DQ
&
RH
II
LF
LH
QW
Figure 1. Spearman Coefficient of ASSBC vs. different 
percentage of removed systems on TREC -3. 
In Figure 1, the Spearman coefficients of 
ASSBC vs. different percentage of removed 
systems on TREC-3 are presented. Except for 
the beginning, almost all the points are above 
the horizontal line. The curve reaches its top at 
65%-67%, where the Spearman coefficient is 
0.8929. 












     
5HPRYHG3HUFHQWDJH
6S
HD
UP
DQ
&
RH
II
LF
LH
QW
Figure 2. Spearman Coefficient of ASSBC vs. different 
percentage of removed systems on TREC -5. 
Figure 2 depicts the evaluation performance 
on TREC-5. From 0 to 63%, the performance 
curve fluctuates around the horizontal line. This 
means deficient clustering does not bring sub-
stantial performance variation. After 63%, the 
curve begins to rise and reaches its peak at 78%, 
where the performance is 0.8691. Then it drops 
dramatically as more systems removed from the 
representative group. 
The situation on TREC-6 is plotted in Figure 
3. In this case, the curve rises gently in the in-
terval between 0 and 70% except for some fluc-
tuation. After 70%, the curve starts to climb and 
reaches the peak at 75% with the peak value of 
0.8576. It remains high performance until 80%, 
and then decline quickly. 











     
5HPRYHG3HFHQWDJH
6S
HD
UP
DQ
&
RH
II
LF
LH
QW
Figure 3. Spearman Coefficient of ASSBC vs. different 
percentage of removed systems on TREC -6. 
Figure 4 presents the evaluation performance 
on TREC-7. The trend in this figure is pretty 
much like that in Figure 2. The curve fluctuates 
first, and then climbs the hill, where the peak 
value is 0.6557 and 75% systems are removed. 
The only difference is in this figure the curve is 
gentler. This means on TREC-7 ASSBC does 
not obtain as much improvement as on TREC-5. 











     
5HPRYHG3HUFHQWDJH
6S
HD
UP
DQ
&
RH
II
LF
HQ
W
Figure 4. Spearman Coefficient of ASSBC vs. different 
percentage of removed systems on TREC -7. 
According Figure1-4, we can say that cluster-
ing systems does bring us evaluation perform-
ance improvement. Generally, obvious im-
provement occurs in the interval between 65% 
and 80%. TREC-3 is an exception. The curve on 
TREC-3 reaches its peak at 65%. Notice that in 
TREC-3 there are only 40 systems (runs), and 
1136
65% indicates 26 systems removed and 14 sys-
tems left as representatives. Interestingly, for 
other TRECs, 78% (the biggest peak position) 
means at least 14 systems left as well. So, this 
can be interpreted as the minimum number of 
clusters.
To examine the general effect on evaluation 
performance of cluster number, we also plot the 
average performance of TREC -3, -5, -6 and -7 
vs. the percentage of systems removed from the 
representative group in Figure 5. With slight 
fluctuation, the average performance curve 
climbs stably, and reaches its peak 0.7754 at the 
position 78%. Then it drops dramatically.  







     
5HPRYHG3HUFHQWDJH
6S
HD
UP
DQ
&
RH
II
LF
LH
QW
Figure 5. Average Spearman Coefficient of ASSBC vs. 
different percentage of removed systems on TREC -3, -
5, -6 and -7. 
To make the result more intuitive, we present 
a comparison of the performance of original 
Average System Similarity (ASS) and the best 
performance of Average System Similarity 
Based on Clustering (ASSBC) in Table 2. Ac-
cording to the table, we can see that clustering 
systems improve the evaluation performance 
significantly. 
 ASS ASSBC Improvement
Trec3 0.7086 0.8929 26.0% 
Trec5 0.5277 0.8691 64.7% 
Trec6 0.6300 0.8576 36.1% 
Trec7 0.5855 0.6557 12.0% 
Avg 0.6129 0.7754 26.5% 
Table 2. Spearman coefficients of original Average 
System Similarity (ASS) and the best performance of 
Average System Similarity Based on Clustering 
(ASSBC) on TREC -3, -5, -6, -7 and the over all aver-
age.
4.3 Comparison with All Previous At-
tempts
Meanwhile, we also provide a comparison 
among the ASSBC method and all the existing 
non-judgment evaluation methods mentioned in 
Section 2. The result is given in Table 3. 
  RS RC CB Single% ASS ASSBC optimal 
(78% Removed) 
Trec3 0.627  0.587  0.867  0.824  0.709  0.893 
Trec5 0.429  0.421  0.657*  0.563  0.528  0.869  
Trec6 0.436  0.384  0.717  0.618  0.630  0.854  
Trec7 0.411  0.382  0.453  0.550  0.585  0.631  
Avg 0.476  0.444  0.674  0.639  0.613  0.812 
Table 3. Spearman coefficients for best results from different evaluation methods 
In Table 3, RS represents the result of ran-
dom pseudo relevance method, where relevance 
ratio is set to 10% rather than the actual ratio in 
its original version; RC is the best result pro-
duced by reference count method; BC accounts 
for the best result of Bias plus Condorcet 
method, a data fusion based method. Results of 
these three methods are cited from Nuray and 
Can?s (2006) paper. For the number with a ?*? 
(BC on TREC 5), in their original paper, same 
result in different tables conflict, and we pick 
the higher value presenting in Table 3. Single% 
is the representative of Spoerri?s overlap struc-
ture based method. Different from its original 
version, the result in Table 3 is gained on all the 
systems opposite to on a selected subset, except 
that runs submitted by the same system are 
counted only once. ASS is short for Average 
System Similarity. ASSBC optimal is the best 
result of our method. Here we utilize both 78% 
1137
as the percentage of removed systems and 14 as 
the minimum number of clusters1. Clearly, our 
method outperforms all the previous attempts on 
every TREC.  The overall average performance 
outperforms the best previous result (from CB) 
by 20.5%. 
5 Conclusion
Retrieval evaluation without relevance judg-
ments is a hard problem. Meanwhile it is also an 
important problem that we can not avoid it in 
many research areas and applications.  
One of the main factors that depress the per-
formance of judgments free evaluation is: simi-
lar retrieval results ruined the Authority Effect, 
which is one of the important bases for all the 
judgment free evaluation methods. 
In this paper, we use clustering technique to 
address this problem. By using one system to 
represent all the systems that are similar to it, 
we can largely reduce the negative effect of 
similar retrieval results. Experimental results 
also verified our idea. Our method outperforms 
all the previous judgment free evaluation meth-
ods on every test bed.  The overall average per-
formance outperforms the best previous result 
by 20.5%. 
Besides, improving judgment free evaluation 
via clustering is more than just a method. It is a 
general framework that can be applied to any 
judgment free evaluation method. The Average 
System Similarity Based on Clustering method 
is an example. It works well means that the 
framework is feasible and successful. We will 
apply it to other judgment free evaluation meth-
ods in our future work. 
Acknowledgement This work is supported 
by the National Science Foundation of China 
under Grant No. 60776797, the Major State Ba-
sic Research Project of China (973 Program) 
under Grant No. 2007CB311103 and the Na-
tional High Technology Research and Devel-
opment Program of China (863 Program) under 
Grant No. 2006AA010105. 
1 Since we add a terminal criterion for clustering 
with 14 as the minimum number of clusters, the av-
erage performance in Table 3 gains an improvement 
compared to that presented in Figure 5 and Table 2.  
References 
Allan J., Carterette B., Aslam J. A., Pavlu V., 
Dachev B., and Kanoulas E. 2007 Overview of 
the TREC 2007 Million Query Track, Proceedings 
of TREC.
Aslam J. A., Pavlu V. and Yilmaz E. 2006 A statisti-
cal method for system evaluation using incom-
plete judgments, Proceedings of the 29th annual 
international ACM SIGIR conference on Research 
and development in information retrieval, August 
06-11, 2006, Seattle, Washington 
Aslam J. A. and Savell R. 2003 On the effectiveness 
of evaluating retrieval systems in the absence of 
relevance judgments, Proceedings of the 26th an-
nual international ACM SIGIR conference on Re-
search and development in informaion retrieval, 
July 28-August 01, 2003, Toronto, Canada 
Buckley, C. and Voorhees, E. M. 2000 Evaluating 
evaluation measure stability, Proceedings of the 
23rd ACMSIGIR conference pp. 33?40 
Carterette B., Allan J. and Sitaraman R. 2006 Mini-
mal test collections for retrieval evaluation, Pro-
ceedings of the 29th annual international ACM 
SIGIR conference on Research and development 
in information retrieval, August 06-11, 2006, Se-
attle, Washington, USA  
Nuray R. and Can F. 2006 Automatic ranking of 
information retrieval systems using data fusion, 
Information Processing and Management: an In-
ternational Journal, v.42 n.3, p.595-614, May 
2006 
Soboroff I., Nicholas C. and Cahan P. 2001 Ranking 
retrieval systems without relevance judgments, 
Proceedings of the 24th annual international ACM 
SIGIR conference on Research and development 
in information retrieval, p.66-73, September 2001, 
New Orleans, Louisiana, United States 
Spoerri A. 2005 How the overlap between search 
results correlates with relevance. In: Proceedings 
of the 68th annual meeting of the American Soci-
ety for Information Science and Technology 
(ASIST 2005). 
Spoerri A. 2007 Using the structure of overlap be-
tween search results to rank retrieval systems 
without relevance judgments, Information Proc-
essing and Management: an International Journal, 
v.43 n.4, pp.1059-1070, July, 2007 
Voorhees E. M. 1998 Variations in relevance judg-
ments and the measurement of retrieval effective-
ness, Proceedings of the 21st annual international 
ACM SIGIR conference on Research and devel-
1138
opment in information retrieval, p.315-323, Au-
gust 24-28, 1998, Melbourne, Australia 
Voorhees E. M. and Harman, D. 1999 Overview of 
the eighth text retrieval conference (TREC-8). 
The eighth text retrieval conference (TREC-8), 
Gaithersburg, MD, USA, 1999. U.S. Government 
Printing Office, Washington 
Wu S. and Crestani F. 2003 Methods for ranking 
information retrieval systems without relevance 
judgments, Proceedings of the 2003 ACM sympo-
sium on Applied computing, March 09-12, 2003, 
Melbourne, Florida 
Zobel J. 1998 How reliable are the results of large-
scale information retrieval experiments?, Proceed-
ings of the 21st annual international ACM SIGIR 
conference on Research and development in in-
formation retrieval, p.307-314, August 24-28, 
1998, Melbourne, Australia 
1139
