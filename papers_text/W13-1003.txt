Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 21?30,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Improving Word Translation Disambiguation by
Capturing Multiword Expressions with Dictionaries
Lars Bungum, Bjo?rn Gamba?ck, Andre? Lynum, Erwin Marsi
Norwegian University of Science and Technology
Sem S?lands vei 7?9; NO?7491 Trondheim, Norway
{bungum,gamback,andrely,emarsi}@idi.ntnu.no
Abstract
The paper describes a method for identifying
and translating multiword expressions using a
bi-directional dictionary. While a dictionary-
based approach suffers from limited recall,
precision is high; hence it is best employed
alongside an approach with complementing
properties, such as an n-gram language model.
We evaluate the method on data from the
English-German translation part of the cross-
lingual word sense disambiguation task in the
2010 semantic evaluation exercise (SemEval).
The output of a baseline disambiguation sys-
tem based on n-grams was substantially im-
proved by matching the target words and their
immediate contexts against compound and
collocational words in a dictionary.
1 Introduction
Multiword expressions (MWEs) cause particular
lexical choice problems in machine translation
(MT), but can also be seen as an opportunity to both
generalize outside the bilingual corpora often used
as training data in statistical machine translation ap-
proaches and as a method to adapt to specific do-
mains. The identification of MWEs is in general
important for many language processing tasks (Sag
et al, 2002), but can be crucial in MT: since the se-
mantics of many MWEs are non-compositional, a
suitable translation cannot be constructed by trans-
lating the words in isolation. Identifying MWEs
can help to identify idiomatic or otherwise fixed lan-
guage usage, leading to more fluent translations, and
potentially reduce the amount of lexical choice an
MT system faces during target language generation.
In any translation effort, automatic or otherwise,
the selection of target language lexical items to in-
clude in the translation is a crucial part of the fi-
nal translation quality. In rule-based systems lex-
ical choice is derived from the semantics of the
source words, a process which often involves com-
plex semantic composition. Data-driven systems
on the other hand commonly base their translations
nearly exclusively on cooccurrences of bare words
or phrases in bilingual corpora, leaving the respon-
sibility of selecting lexical items in the translation
entirely to the local context found in phrase trans-
lation tables and language models with no explicit
notion of the source or target language semantics.
Still, systems of this type have been shown to pro-
duce reasonable translation quality without explic-
itly considering word translation disambiguation.
Bilingual corpora are scarce, however, and un-
available for most language pairs and target do-
mains. An alternative approach is to build systems
based on large monolingual knowledge sources and
bilingual lexica, as in the hybrid MT system PRE-
SEMT (Sofianopoulos et al, 2012). Since such
a system explicitly uses a translation dictionary, it
must at some point in the translation process decide
which lexical entries to use; thus a separate word
translation disambiguation module needs to be in-
corporated. To research available methods in such a
module we have identified a task where we can use
public datasets for measuring how well a method is
able to select the optimal of many translation choices
from a source language sentence.
21
In phrase-based statistical MT systems, the trans-
lation of multiword expressions can be a notable
source of errors, despite the fact that those systems
explicitly recognize and use alignments of sequen-
tial chunks of words. Several researchers have ap-
proached this problem by adding MWE translation
tables to the systems, either through expanding the
phrase tables (Ren et al, 2009) or by injecting the
MWE translations into the decoder (Bai et al, 2009).
Furthermore, there has been some interest in auto-
matic mining of MWE pairs from bilingual corpora
as a task in itself: Caseli et al (2010) used a dic-
tionary for evaluation of an automatic MWE extrac-
tion procedure using bilingual corpora. They also
argued for the filtering of stopwords, similarly to the
procedure described in the present paper. Sharoff
et al (2006) showed how MWE pairs can be ex-
tracted from comparable monolingual corpora in-
stead of from a parallel bilingual corpus.
The methodology introduced in this paper em-
ploys bilingual dictionaries as a source of multi-
word expressions. Relationships are induced be-
tween the source sentence and candidate transla-
tion lexical items based on their correspondence in
the dictionary. Specifically, we use a determinis-
tic multiword expression disambiguation procedure
based on translation dictionaries in both directions
(from source to target language and vice versa),
and a baseline system that ranks target lexical items
based on their immediate context and an n-gram
language model. The n-gram model represents a
high-coverage, low-precision companion to the dic-
tionary approach (i.e., it has complementary proper-
ties). Results show that the MWE dictionary infor-
mation substantially improves the baseline system.
The 2010 Semantic Evaluation exercise (Sem-
Eval?10) featured a shared task on Cross-Lingual
Word Sense Disambiguation (CL-WSD), where the
focus was on disambiguating the translation of a sin-
gle noun in a sentence. The participating systems
were given an English word in its context and asked
to produce appropriate substitutes in another lan-
guage (Lefever and Hoste, 2010b). The CL-WSD
data covers Dutch, French, Spanish, Italian and Ger-
man; however, since the purpose of the experiments
in this paper just was to assess our method?s abil-
ity to choose the right translation of a word given its
context, we used the English-to-German part only.
The next section details the employed disam-
biguation methodology and describes the data sets
used in the experiments. Section 3 then reports on
the results of experiments applying the methodology
to the SemEval datasets, particularly addressing the
impact of the dictionary MWE correspondences. Fi-
nally, Section 4 sums up the discussion and points to
issues that can be investigated further.
2 Methodology
The core of the disambiguation model introduced
in this paper is dictionary-based multiword extrac-
tion. Multiword extraction is done in both a direct
and indirect manner: Direct extraction uses adjacent
words in the source language in combination with
the word to be translated, if the combination has an
entry in the source-to-target language (SL?TL) dic-
tionary. Indirect extraction works in the reverse di-
rection, by searching the target-to-source (TL?SL)
dictionary and looking up translation candidates for
the combined words. Using a dictionary to identify
multiword expressions after translation has a low re-
call of target language MWEs, since often there ei-
ther are no multiword expressions to be discovered,
or the dictionary method is unable to find a trans-
lation for an MWE. Nevertheless, when an MWE
really is identified by means of the dictionary-based
method, the precision is high.
Due to the low recall, relying on multiword ex-
pressions from dictionaries would, however, not be
sufficient. Hence this method is combined with an
n-gram language model (LM) based on a large tar-
get language corpus. The LM is used to rank trans-
lation candidates according to the probability of the
n-gram best matching the context around the transla-
tion candidate. This is a more robust but less precise
approach, which servers as the foundation for the
high-precision but low-recall dictionary approach.
In the actual implementation, the n-gram method
thus first provides a list of its best suggestions
(currently top-5), and the dictionary method then
prepends its candidates to the top of this list. Con-
sequently, n-gram matching is described before
dictionary-based multiword extraction in the follow-
ing section. First, however, we introduce the data
sets used in the experiments.
22
(a) AGREEMENT in the form of an exchange of letters between
the European Economic Community and the Bank for Interna-
tional Settlements concerning the mobilization of claims held by
the Member States under the medium-term financial assistance
arrangements
{bank 4; bankengesellschaft 1; kreditinstitut 1; zentralbank 1; fi-
nanzinstitut 1}
(b) The Office shall maintain an electronic data bank with the par-
ticulars of applications for registration of trade marks and entries
in the Register. The Office may also make available the contents
of this data bank on CD-ROM or in any other machine-readable
form.
{datenbank 4; bank 3; datenbanksystem 1; daten 1}
(c) established as a band of 1 km in width from the banks of a
river or the shores of a lake or coast for a length of at least 3 km.
{ufer 4; flussufer 3}
Table 1: Examples of contexts for the English word bank
with possible German translations
2.1 The CL-WSD Datasets
The data sets used for the SemEval?10 Cross-
Lingual Word Sense Disambiguation task were con-
structed by making a ?sense inventory? of all pos-
sible target language translations of a given source
language word based on word-alignments in Eu-
roparl (Koehn, 2005), with alignments involving the
relevant source words being manually checked. The
retrieved target words were manually lemmatised
and clustered into translations with a similar sense;
see Lefever and Hoste (2010a) for details.
Trial and test instances were extracted from two
other corpora, JRC-Acquis (Steinberger et al, 2006)
and BNC (Burnard, 2007). The trial data for each
language consists of five nouns (with 20 sentence
contexts per noun), and the test data of twenty nouns
(50 contexts each, so 1000 in total per language,
with the CL-WSD data covering Dutch, French,
Spanish, Italian and German). Table 1 provides ex-
amples from the trial data of contexts for the English
word bank and its possible translations in German.
Gold standard translations were created by hav-
ing four human translators picking the contextually
appropriate sense for each source word, choosing 0?
3 preferred target language translations for it. The
translations are thus restricted to those appearing in
Europarl, probably introducing a slight domain bias.
Each translation has an associated count indicating
how many annotators considered it to be among their
top-3 preferred translations in the given context.
bank, bankanleihe, bankanstalt, bankdarlehen, bankenge-
sellschaft, bankensektor, bankfeiertag, bankgesellschaft, bankin-
stitut, bankkonto, bankkredit, banknote, blutbank, daten, daten-
bank, datenbanksystem, euro-banknote, feiertag, finanzinstitut,
flussufer, geheimkonto, geldschein, gescha?ftsbank, handelsbank,
konto, kredit, kreditinstitut, nationalbank, notenbank, sparkasse,
sparkassenverband, ufer, weltbank, weltbankgeber, west-bank,
westbank, westjordanien, westjordanland, westjordanufer, west-
ufer, zentralbank
Table 2: All German translation candidates for bank as
extracted from the gold standard
In this way, for the English lemma bank, for ex-
ample, the CL-WSD trial gold standard for German
contains the word Bank itself, together with 40 other
translation candidates, as shown in Table 2. Eight
of those are related to river banks (Ufer, but also,
e.g., Westbank and Westjordanland), three concern
databases (Datenbank), and one is for blood banks.
The rest are connected to different types of finan-
cial institutions (such as Handelsbank and Finanz-
institut, but also by association Konto, Weldbank-
geber, Banknote, Geldschein, Kredit, etc.).
2.2 N-Gram Context Matching
N-gram matching is used to produce a ranked list
of translation candidates and their contexts, both in
order to provide robustness and to give a baseline
performance. The n-gram models were built using
the IRSTLM toolkit (Federico et al, 2008; Bungum
and Gamba?ck, 2012) on the DeWaC corpus (Baroni
and Kilgarriff, 2006), using the stopword list from
NLTK (Loper and Bird, 2002). The n-gram match-
ing procedure consists of two steps:
1. An nth order source context is extracted and the
translations for each SL word in this context
are retrieved from the dictionary. This includes
stopword filtering of the context.
2. All relevant n-grams are inspected in order
from left to right and from more specific (5-
grams) to least specific (single words).
For each part of the context with matching n-grams
in the target language model, the appropriate target
translation candidates are extracted and ranked ac-
cording to their language model probability. This
results in an n-best list of translation candidates.
23
Since dictionary entries are lemma-based, lemma-
tization was necessary to use this approach in com-
bination with the dictionary enhancements. The
source context is formed by the lemmata in the sen-
tence surrounding the focus word (the word to be
disambiguated) by a window of up to four words
in each direction, limited by a 5-gram maximum
length. In order to extract the semantically most rel-
evant content, stopwords are removed before con-
structing this source word window. For each of the
1?5 lemmata in the window, the relevant translation
candidates are retrieved from the bilingual dictio-
nary. The candidates form the ordered translation
context for the source word window.
The following example illustrates how the trans-
lation context is created for the focus word ?bank?.
First the relevant part of the source language sen-
tence with the focus word in bold face:
(1) The BIS could conclude stand-by credit
agreements with the creditor countries? cen-
tral bank if they should so request.
For example, using a context of two words in front
and two words after the focus word, the following
source language context is obtained after a prepro-
cessing involving lemmatization, stopword removal,
and insertion of sentence start (<s>) and end mark-
ers (</s>):
(2) country central bank request </s>
From this the possible n-grams in the target side con-
text are generated by assembling all ordered com-
binations of the translations of the source language
words for each context length: the widest contexts
(5-grams) are looked up first before moving on to
narrower contexts, and ending up with looking up
only the translation candidate in isolation.
Each of the n-grams is looked up in the language
model and for each context part the n-grams are or-
dered according to their language model probability.
Table 3 shows a few examples of such generated n-
grams with their corresponding scores from the n-
gram language model.1 The target candidates (ital-
ics) are then extracted from the ordered list of target
language n-grams. This gives an n-best list of trans-
1There are no scores for 4- and 5-grams; as expected when
using direct translation to generate target language n-grams.
n n-gram LM score
5 land mittig bank nachsuchen </s> Not found
4 mittig bank nachsuchen </s> Not found
3 mittig bank nachsuchen Not found
3 kredit anfragen </s> -0.266291
2 mittig bank -3.382560
2 zentral blutbank -5.144870
1 bank -3.673000
Table 3: Target language n-gram examples from look-
ups of stopword-filtered lemmata country central bank
request reported in log scores. The first 3 n-grams were
not found in the language model.
lation candidates from which the top-1 or top-5 can
be taken. Since multiple senses in the dictionary can
render the same literal output, duplicate translation
candidates are filtered out from the n-best list.
2.3 Dictionary-Based Context Matching
After creating the n-gram based list of translation
candidates, additional candidates are produced by
looking at multiword entries in a bilingual dictio-
nary. The existence of multiword entries in the dic-
tionary corresponding to adjacent lemmata in the
source context or translation candidates in the target
context is taken as a clear indicator for the suitability
of a particular translation candidate. Such entries are
added to the top of the n-best list, which represents
a strong preference in the disambiguation system.
Dictionaries are used in all experiments to look up
translation candidates and target language transla-
tions of the words in the context, but this approach is
mining the dictionaries by using lookups of greater
length. Thus is, for example, the dictionary entry
Community Bank translated to the translation candi-
date Commerzbank; this translation candidate would
be put on top of the list of prioritized answers.
Two separate procedures are used to find such in-
dicators, a direct procedure based on the source con-
text and an indirect procedure based on the weaker
target language context. These are detailed in pseu-
docode in Algorithms 1 and 2, and work as follows:
Source Language (SL) Method (Algorithm 1)
If there is a dictionary entry for the source word
and one of its adjacent words, search the set
of translations for any of the translation candi-
dates for the word alone. Specifically, transla-
24
Algorithm 1 SL algorithm to rank translation candidates (tcands) for SL lemma b given list of tcands
1: procedure FINDCAND(list rlist,SL-lemma b, const tcands) . rlist is original ranking
2: comblemmas? list(previouslemma(b) + b, b + nextlemma(b)) . Find adjacent lemmata
3: for lem ? comblemmas do
4: c? sl-dictionary-lookup(lem) . Look up lemma in SL?TL dict.
5: if c ? tcands then rlist? list(c + rlist) . Push lookup result c onto rlist if in tcands
6: end if
7: end for
8: return rlist . Return new list with lemmata whose translations were in tcands on top
9: end procedure
Algorithm 2 TL algorithm to rank translation candidates (tcands) for SL lemma b given list of tcands
[The ready-made TL tcands from the dataset are looked up in TL-SL direction. It is necessary to keep a list of the
reverse-translation of the individual tcand as well as the original tcand itself, in order to monitor which tcand it was.
If the SL context is found in either of these reverse lookups the matching tcand is ranked high.]
1: procedure FINDCAND(list rlist,SL-lemma b, const tcands) . rlist is original ranking
2: for cand ? tcands do . Assemble list of TL translations
3: translist? list(cand, tl-dictionary-lookup(cand)) + translist
4: . Append TL?SL lookup results of tcands with cand as id
5: end for
6: for cand, trans ? translist do
7: if previouslemma(b)?nextlemma(b) ? trans then . If trans contains either SL lemma
8: rlist? list(cand) + rlist . append this cand onto rlist
9: end if
10: end for
11: return rlist
12: . Return tcands list; top-ranking tcands whose SL-neighbours were found in TL?SL lookup
13: end procedure
tions of the combination of the source word and
an adjacent word in the context are matched
against translation candidates for the word.
Target Language (TL) Method (Algorithm 2)
If a translation candidate looked up in the re-
verse direction matches the source word along
with one or more adjacent words, it is a good
translation candidate. TL candidates are looked
up in a TL?SL dictionary and multiword results
are matched against SL combinations of disam-
biguation words and their immediate contexts.
For both methods the dictionary entry for the tar-
get word or translation candidate is matched against
the immediate context. Thus both methods result
in two different lookups for each focus word, com-
bining it with the previous and next terms, respec-
tively. This is done exhaustively for all combina-
tions of translations of the words in the context win-
dow. Only one adjacent word was used, since very
few of the candidates were able to match the context
even with one word. Hence, virtually none would
be found with more context, making it very unlikely
that larger contexts would contribute to the disam-
biguation procedure, as wider matches would also
match the one-word contexts.
Also for both methods, translation candidates are
only added once, in case the same translation candi-
date generates hits with either (or both) of the meth-
ods. Looking at the running example, stopword fil-
tered and with lemmatized context:
(3) country central bank request
This example generates two source language multi-
word expressions, central bank and bank request. In
the source language method, these word combina-
25
tions are looked up in the dictionary where the zen-
tralbank entry is found for central bank, which is
also found as a translation candidate for bank.
The target language method works in the reverse
order, looking up the translation candidates in the
TL?SL direction and checking if the combined lem-
mata are among the candidates? translations into the
source language. In the example, the entry zentral-
bank:central bank is found in the dictionary, match-
ing the source language context, so zentralbank is
assumed to be a correct translation.
2.4 Dictionaries
Two English-German dictionaries were used in the
experiments, both with close to 1 million entries
(translations). One is a free on-line resource, while
the other was obtained by reversing an existing pro-
prietary German-English dictionary made available
to the authors by its owners:
? The GFAI dictionary (called ?D1? in Section 3
below) is a proprietary and substantially ex-
tended version of the Chemnitz dictionary, with
549k EN entries including 433k MWEs, and
552k DE entries (79k MWEs). The Chem-
nitz electronic German-English dictionary2 it-
self contains over 470,000 word translations
and is available under a GPL license.
? The freely available CC dictionary3 (?D2? be-
low) is an internet-based German-English and
English-German dictionary built through user
generated word definitions. It has 565k/440k
(total/MWE) EN and 548k/210k DE entries.
Note that the actual dictionaries are irrelevant to the
discussion at hand, and that we do not aim to point
out strengths or weaknesses of either dictionary, nor
to indicate a bias towards a specific resource.
3 Results
Experiments were carried out both on the trial and
test data described in Section 2.1 (5 trial and 20 test
words; with 20 resp. 50 instances for each word; in
total 1100 instances in need of disambiguation). The
results show that the dictionaries yield answers with
2http://dict.tu-chemnitz.de/
3http://www.dict.cc/
high precision, although they are robust enough to
solve the SemEval WSD challenge on their own.
For measuring the success rate of the developed
models, we adopt the ?Out-Of-Five? (OOF) score
(Lefever and Hoste, 2010b) from the SemEval?10
Cross-Lingual Word Sense Disambiguation task.
The Out-Of-Five criterion measures how well the
top five candidates from the system match the top
five translations in the gold standard:
OOF (i) =
?
a?Ai
freq i(a)
|Hi|
where Hi denotes the multiset of translations pro-
posed by humans for the focus word in each source
sentence si (1 ? i ? N , N being the number
of test items). Ai is the set of translations produced
by the system for source term i. Since each transla-
tion has an associated count of how many annotators
chose it, there is for each si a function freq i return-
ing this count for each term in Hi (0 for all other
terms), and max freq i gives the maximal count for
any term in Hi. For the first example in Table 1:
?
??????????
??????????
H1 = {bank, bank, bank, bank, zentralbank,
bankengesellschaft, kreditinstitut, finanzinstitut}
freq1(bank) = 4
. . .
freq1(finanzinstitut) = 1
maxfreq1 = 4
and the cardinality of the multiset is: |H1| = 8. This
equates to the sum of all top-3 preferences given to
the translation candidates by all annotators.
For the Out-Of-Five evaluation, the CL-WSD sys-
tems were allowed to submit up to five candidates
of equal rank. OOF is a recall-oriented measure
with no additional penalty for precision errors, so
there is no benefit in outputting less than five can-
didates. With respect to the previous example from
Table 1, the maximum score is obtained by system
output A1 = {bank, bankengesellschaft, kreditinstitut,
zentralbank, finanzinstitut}, which gives OOF (1) =
(4 + 1 + 1 + 1 + 1)/8 = 1, whereas A2 = {bank,
bankengesellschaft, nationalbank, notenbank, sparkasse}
would give OOF (1) = (4 + 1)/8 = 0.625.4
4Note that the maximum OOF score is not always 1 (i.e., it
is not normalized), since the gold standard sometimes contains
more than five translation alternatives.
26
Source language Target language All
Dictionary D1 D2 comb D1 D2 comb comb
Top 8.89 6.99 8.89 22.71 24.43 25.34 24.67
Low 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Mean 2.71 0.99 3.04 8.35 7.10 9.24 10.13
Table 4: F1-score results for individual dictionaries
Source language Target language All
Dictionary D1 D2 comb D1 D2 comb comb
coach 1.00 0.00 1.00 0.21 0.00 0.21 0.21
education 0.83 0.67 0.83 0.47 0.62 0.54 0.53
execution 0.00 0.00 0.00 0.17 0.22 0.17 0.17
figure 1.00 0.00 1.00 0.51 0.57 0.55 0.55
job 0.88 0.80 0.94 0.45 0.78 0.46 0.44
letter 1.00 0.00 1.00 0.66 0.75 0.62 0.66
match 1.00 1.00 1.00 0.80 0.50 0.80 0.80
mission 0.71 0.33 0.71 0.46 0.37 0.36 0.36
mood 0.00 0.00 0.00 0.00 0.00 0.00 0.00
paper 0.68 0.17 0.68 0.53 0.35 0.55 0.55
post 1.00 1.00 1.00 0.39 0.48 0.45 0.48
pot 0.00 0.00 0.00 1.00 1.00 1.00 1.00
range 1.00 1.00 1.00 0.28 0.37 0.30 0.30
rest 1.00 0.67 1.00 0.60 0.56 0.56 0.58
ring 0.09 0.00 0.09 0.37 0.93 0.38 0.38
scene 1.00 0.00 1.00 0.50 0.42 0.44 0.50
side 1.00 0.00 1.00 0.21 0.16 0.23 0.27
soil 1.00 0.00 1.00 0.72 0.58 0.66 0.69
strain 0.00 0.00 0.00 0.51 0.88 0.55 0.55
test 1.00 1.00 1.00 0.62 0.52 0.57 0.61
Mean 0.84 0.74 0.84 0.50 0.56 0.49 0.51
Table 5: Precision scores for all terms filtering out those
instances for which no candidates were suggested
For assessing overall system performance in
the experiments, we take the best (?Top?), worst
(?Low?), and average (?Mean?) of the OOF scores
for all the SL focus words, with F1-score reported
as the harmonic mean of the precision and recall of
the OOF scores. Table 4 shows results for each dic-
tionary approach on the test set, with ?D1? being
the GFAI dictionary, ?D2? the CC dictionary, and
?comb? the combination of both. Target language
look-up contributes more to providing good transla-
tion candidates than the source language methodol-
ogy, and also outperforms a strategy combining all
dictionaries in both directions (?All comb?).
Filtering out the instances for which no candi-
date translation was produced, and taking the aver-
age precision scores only over these, gives the re-
sults shown in Table 5. Markedly different preci-
sion scores can be noticed, but the source language
Source language Target language
Dictionary D1 D2 D1 D2
Mean 3.25 1.5 12.65 11.45
Total 223 256 1,164 880
Table 6: Number of instances with a translation candidate
(?Mean?) and the total number of suggested candidates
Most Most Freq 5-gram 5-gram All Dict VSM
Freq Aligned + Dict Comb Model
Top 51.77 68.71 52.02 52.74 24.67 55.92
Low 1.76 9.93 14.09 15.40 0.00 10.73
Mean 21.18 34.61 30.36 36.38 10.13 30.30
Table 7: Overview of results (F1-scores) on SemEval data
method again has higher precision on the sugges-
tions it makes than the target language counterpart.
As shown in Table 6, this higher precision is offset
by lower coverage, with far fewer instances actually
producing a translation candidate with the dictionary
lookup methods. There is a notable difference in the
precision of the SL and TL approaches, coinciding
with more candidates produced by the latter. Several
words in Table 5 give 100% precision scores for at
least one dictionary, while a few give 0% precision
for some dictionaries. The word ?mood? even has
0% precision for both dictionaries in both directions.
Table 7 gives an overview of different approaches
to word translation disambiguation on the dataset.
For each method, the three lines again give both
the best and worst scoring terms, and the mean
value for all test words. The maximum attainable
score for each of those would be 99.28, 90.48 and
95.47, respectively, but those are perfect scores not
reachable for all items, as described above (OOF-
scoring). Instead the columns Most Freq and Most
Freq aligned give the baseline scores for the Sem-
Eval dataset: the translation most frequently seen
in the corpus and the translation most frequently
aligned in a word-aligned parallel corpus (Europarl),
respectively. Then follows the results when using
only a stopword-filtered 5-gram model built with the
IRSTLM language modeling kit (Federico and Cet-
tolo, 2007), and when combining the 5-gram model
with the dictionary approach (5-gram + Dict).
The next column (All Dict Comb) shows how the
dictionary methods fared on their own. The com-
27
bined dictionary approach has low recall (see Ta-
ble 6) and does not alone provide a good solution to
the overall problem. Due to high precision, however,
the approach is able to enhance the n-gram method
that already produces acceptable results. Finally, the
column VSM Model as comparison gives the results
obtained when using a Vector Space Model for word
translation disambiguation (Marsi et al, 2011).
Comparing the dictionary approach to state-of-
the-art monolingual solutions to the WTD problem
on this dataset shows that the approach performs bet-
ter for the Lowest and Mean scores of the terms, but
not for the Top scores (Lynum et al, 2012). As can
be seen in Table 7, the vector space model produced
the overall best score for a single term. However, the
method combining a 5-gram language model with
the dictionary approach was best both at avoiding
really low scores for any single term and when com-
paring the mean scores for all the terms.
4 Discussion and Conclusion
The paper has presented a method for using dictio-
nary lookups based on the adjacent words in both
the source language text and target language candi-
date translation texts to disambiguate word transla-
tion candidates. By composing lookup words by us-
ing both neighbouring words, improved disambigua-
tion performance was obtained on the data from the
SemEval?10 English-German Cross-Lingual Word
Sense Disambiguation task. The extended use of
dictionaries proves a valuable source of informa-
tion for disambiguation, and can introduce low-cost
phrase-level translation to quantitative Word Sense
Disambiguation approaches such as N-gram or Vec-
tor Space Model methods, often lacking the phrases-
based dimension.
The results show clear differences between the
source and target language methods of using dictio-
nary lookups, where the former has very high preci-
sion (0.84) but low coverage, while the TL method
compensates lower precision (0.51) with markedly
better coverage. The SL dictionary method pro-
vided answers to only between 1.5 and 3.25 of 50
instances per word on average, depending on the dic-
tionary. This owes largely to the differences in algo-
rithms, where the TL method matches any adjacent
lemma to the focus word with the translation of the
pre-defined translation candidates, whereas the SL
method matches dictionaries of the combined lem-
mata of the focus word and its adjacent words to the
same list of translation candidates. False positives
are expected with lower constraints such as these.
On the SemEval data, the contribution of the dictio-
nary methods to the n-grams is mostly in improving
the average score.
The idea of acquiring lexical information from
corpora is of course not new in itself. So did, e.g.,
Rapp (1999) use vector-space models for the pur-
pose of extracting ranked lists of translation can-
didates for extending a dictionary for word trans-
lation disambiguation. Chiao and Zweigenbaum
(2002) tried to identify translational equivalences
by investigating the relations between target and
source language word distributions in a restricted
domain, and also applied reverse-translation filtering
for improved performance, while Sadat et al (2003)
utilised non-aligned, comparable corpora to induce
a bilingual lexicon, using a bidirectional method
(SL?TL, TL?SL, and a combination of both).
Extending the method to use an arbitrary size win-
dow around all words in the context of each focus
word (not just the word itself) could identify more
multiword expressions and generate a more accurate
bag-of-words for a data-driven approach. Differ-
ences between dictionaries could also be explored,
giving more weight to translations found in two or
more dictionaries. Furthermore, the differences be-
tween the SL and TL methods could explored fur-
ther, investigating in detail the consequences of us-
ing a symmetrical dictionary, in order to study the
effect that increased coverage has on results. Test-
ing the idea on more languages will help verify the
validity of these findings.
Acknowledgements
This research has received funding from NTNU and from
the European Community?s 7th Framework Programme
under contract nr 248307 (PRESEMT). Thanks to the
other project participants and the anonymous reviewers
for several very useful comments.
28
References
Bai, M.-H., You, J.-M., Chen, K.-J., and Chang,
J. S. (2009). Acquiring translation equivalences of
multiword expressions by normalized correlation
frequencies. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 478?486, Singapore. ACL.
Baroni, M. and Kilgarriff, A. (2006). Large
linguistically-processed web corpora for multiple
languages. In Proceedings of the 11th Conference
of the European Chapter of the Association for
Computational Linguistics, pages 87?90, Trento,
Italy. ACL.
Bungum, L. and Gamba?ck, B. (2012). Efficient n-
gram language modeling for billion word web-
corpora. In Proceedings of the 8th International
Conference on Language Resources and Evalua-
tion, pages 6?12, Istanbul, Turkey. ELRA. Work-
shop on Challenges in the Management of Large
Corpora.
Burnard, L., editor (2007). Reference Guide for the
British National Corpus (XML Edition). BNC
Consortium, Oxford, England. http://www.
natcorp.ox.ac.uk/XMLedition/URG.
Caseli, H. d. M., Ramisch, C., das Grac?as
Volpe Nunes, M., and Villavicencio, A. (2010).
Alignment-based extraction of multiword expres-
sions. Language Resources and Evaluation, 44(1-
2):59?77. Special Issue on Multiword expression:
hard going or plain sailing.
Chiao, Y.-C. and Zweigenbaum, P. (2002). Look-
ing for candidate translational equivalents in spe-
cialized comparable corpora. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics, volume 2, pages 1?5,
Philadelphia, Pennsylvania. ACL. Also published
in AMIA Annual Symposium 2002, pp. 150?154.
Federico, M., Bertoldi, N., and Cettolo, M. (2008).
Irstlm: an open source toolkit for handling large
scale language models. In INTERSPEECH, pages
1618?1621. ISCA.
Federico, M. and Cettolo, M. (2007). Efficient han-
dling of n-gram language models for statistical
machine translation. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 88?95, Prague, Czech
Republic. ACL. 2nd Workshop on Statistical Ma-
chine Translation.
Koehn, P. (2005). Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the 10th Machine Translation Summit, pages 79?
86, Phuket, Thailand.
Lefever, E. and Hoste, V. (2010a). Construction
of a benchmark data set for cross-lingual word
sense disambiguation. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation, pages 1584?1590, Valetta, Malta.
ELRA.
Lefever, E. and Hoste, V. (2010b). SemEval-2010
Task 3: Cross-lingual word sense disambiguation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
15?20, Uppsala, Sweden. ACL. 5th International
Workshop on Semantic Evaluation.
Loper, E. and Bird, S. (2002). NLTK: the natu-
ral language toolkit. In Proceedings of the ACL-
02 Workshop on Effective tools and methodolo-
gies for teaching natural language processing and
computational linguistics - Volume 1, ETMTNLP
?02, pages 63?70, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
LREC06 (2006). Proceedings of the 5th Interna-
tional Conference on Language Resources and
Evaluation, Genova, Italy. ELRA.
Lynum, A., Marsi, E., Bungum, L., and Gamba?ck,
B. (2012). Disambiguating word translations with
target language models. In Proceedings of the
15th International Conference on Text, Speech
and Dialogue, pages 378?385, Brno, Czech Re-
public. Springer.
Marsi, E., Lynum, A., Bungum, L., and Gamba?ck,
B. (2011). Word translation disambiguation with-
out parallel texts. In Proceedings of the Inter-
national Workshop on Using Linguistic Informa-
tion for Hybrid Machine Translation, pages 66?
74, Barcelona, Spain.
Rapp, R. (1999). Automatic identification of word
translations from unrelated English and German
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 519?526, Madrid, Spain. ACL.
29
Ren, Z., Lu?, Y., Cao, J., Liu, Q., and Huang, Y.
(2009). Improving statistical machine translation
using domain bilingual multiword expressions. In
Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics, pages
47?54, Singapore. ACL. Workshop on Multiword
Expressions: Identification, Interpretation, Dis-
ambiguation and Applications.
Sadat, F., Yoshikawa, M., and Uemura, S. (2003).
Learning bilingual translations from comparable
corpora to cross-language information retrieval:
Hybrid statistics-based and linguistics-based ap-
proach. In Proceedings of the 41th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 57?64, Sapporo, Japan. ACL. 6th
International Workshop on Information Retrieval
with Asian languages; a shorter version published
in ACL Annual Meeting 2003, pp. 141?144.
Sag, I., Baldwin, T., Bond, F., Copestake, A., and
Flickinger, D. (2002). Multiword expressions:
A pain in the neck for NLP. In Gelbukh, A.,
editor, Computational Linguistics and Intelligent
Text Processing: Proceedings of the 3rd Interna-
tional Conference, number 2276 in Lecture Notes
in Computer Science, pages 189?206, Mexico
City, Mexico. Springer-Verlag.
Sharoff, S., Babych, B., and Hartley, A. (2006). Us-
ing collocations from comparable corpora to find
translation equivalents. In LREC06 (2006), pages
465?470.
Sofianopoulos, S., Vassiliou, M., and Tambouratzis,
G. (2012). Implementing a language-independent
MT methodology. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1?10, Jeju, Korea. ACL.
First Workshop on Multilingual Modeling.
Steinberger, R., Pouliquen, B., Widiger, A., Ignat,
C., Erjavec, T., Tufis?, D., and Varga, D. (2006).
The JRC-Acquis: A multilingual aligned parallel
corpus with 20+ languages. In LREC06 (2006),
pages 2142?2147.
30
