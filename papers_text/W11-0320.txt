Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 172?180,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Probabilistic Word Alignment under the L0-norm
Thomas Schoenemann
Center for Mathematical Sciences
Lund University, Sweden
Abstract
This paper makes two contributions to the
area of single-word based word alignment for
bilingual sentence pairs. Firstly, it integrates
the ? seemingly rather different ? works of
(Bodrumlu et al, 2009) and the standard prob-
abilistic ones into a single framework.
Secondly, we present two algorithms to opti-
mize the arising task. The first is an iterative
scheme similar to Viterbi training, able to han-
dle large tasks. The second is based on the in-
exact solution of an integer program. While it
can handle only small corpora, it allows more
insight into the quality of the model and the
performance of the iterative scheme.
Finally, we present an alternative way to
handle prior dictionary knowledge and dis-
cuss connections to computing IBM-3 Viterbi
alignments.
1 Introduction
The training of single word based translation mod-
els (Brown et al, 1993b; Vogel et al, 1996) is an es-
sential building block for most state-of-the-art trans-
lation systems. Indeed, even more refined transla-
tion models (Wang and Waibel, 1998; Sumita et al,
2004; Deng and Byrne, 2005; Fraser and Marcu,
2007a) are initialized by the parameters of single
word based ones. The exception is here the joint
approach of Marcu and Wong (2002), but its refine-
ment by Birch et al (2006) again relies on the well-
known IBM models.
Traditionally (Brown et al, 1993b; Al-Onaizan
et al, 1999) single word based models are trained
by the EM-algorithm, which has the advantageous
property that the collection of counts can be de-
composed over the sentences. Refinements that also
allow symmetrized models are based on bipartite
graph matching (Matusov et al, 2004; Taskar et al,
2005) or quadratic assignment problems (Lacoste-
Julien et al, 2006). Recently, Bodrumlu et al
(2009) proposed the first method that treats a non-
decomposable problem by handling all sentence
pairs at once and via integer linear programming.
Their (non-probabilistic) approach finds dictionaries
with a minimal number of entries. However, the ap-
proach does not include a position model.
In this work we combine the two strategies into
a single framework. That is, the dictionary sparsity
objective of Bodrumlu et al will become a regu-
larity term in our framework. It is combined with
the maximal alignment probability of every sentence
pair, where we consider the models IBM-1, IBM-2
and HMM. This allows us to write dictionary spar-
sity as the (non-convex) L0 norm of the dictionary
parameters of the respective models.
For supervised training, regularity terms are quite
common, e.g. (Taskar et al, 2005; Lacoste-Julien et
al., 2006). For the unsupervised problem addressed
in this paper they have recently been introduced in
the form of posterior constraints (Ganchev et al,
2010). In related fields of NLP lately Dirichlet pri-
ors have been investigated, e.g. (Johnson, 2007).
We present two strategies to handle the objec-
tive function addressed in this paper. One of these
schemes relies, like (Germann et al, 2004; Lacoste-
Julien et al, 2006; DeNero and Klein, 2008; Bo-
drumlu et al, 2009), on integer linear programming
172
(see e.g. (Schrijver, 1986; Achterberg, 2007)), but
due to the large-scale nature of our problem we
solve only the LP-relaxation, followed by successive
strengthening. For the latter, we develop our own,
exponentially large set of cuts and show that it can
be handled as a polynomially sized system, though
in practice this is too inefficient.
2 The Models
Before we introduce our objective function we give
a brief description of the (standard) models we con-
sider. In all cases, one is given a set of bilin-
gual sentence pairs containing a foreign language
and English. The models formalize the probabil-
ity of obtaining the foreign sentence from a given
English sentence, by considering hidden variables
called alignments:
pd,l(fs|es) =
?
as
pd,l(fs,as|es) .
Here, the subscripts d and l denote two sets of pa-
rameters: whereas the set l defines the probability of
an alignment without knowing any of the sentences,
d describes the translational probability given an
alignment and a source sentence.
For a source (English) sentence of length I and a
target (foreign) sentence of length J , the set of ad-
missible alignments is generally the set of subsets
of {1, . . . , I} ? {1, . . . , J}. However, for compu-
tational reasons the considered models only allow
restricted alignments, where each target word may
align to at most one source word. Any such align-
ment is expressed as a vector aJ1 ? {0, . . . , I}
J .
2.1 Considered models
For a source sentence es = eI1 and a target sentence
f s = fJ1 , the considered models all factor as follows:
pd,l(f
s,as|es) = (1)
J?
j=1
pd(fj |eaj ) ? pl(aj |aj?1, j, I)
In all cases, the translational probability is non-
parametric, i.e. d contains one parameter for every
co-occurring pair of source and target words. Since
the model is probabilistic, the parameters of all f for
a given e have to sum up to one.
With respect to the alignment probability, the
models differ. For the IBM-1 the set l is actually
empty, so pl(aj |aj?1, j, I) = 1/(I+1). The IBM-2
models1 p(aj |j, I), with a respective set of parame-
ters. Finally, the HMM models p(aj |aj?1, I).
It is common to further reduce the alignment pa-
rameters. In this paper we consider a nonparametric
distribution for the IBM-2, but both a nonparamet-
ric and a parametric one for the HMM. In contrast
to GIZA++, we have a parameter for every possible
difference, i.e. we do not group differences with ab-
solutes greater than 5. Also, commonly one uses a
distribution p(i|i?, I) = r(i? i?)/
?I
i??=1 r(i
?? ? i?),
but for technical reasons, we drop the denominator
and instead constrain the r(?)-parameters to sum to
1. In future work we hope to implement both the
normalization and the grouping of bins.
2.2 Word Alignment
Originally the considered models were used for the
actual translation problem. Hence, the parameters
d and l had to be inferred from a training corpus,
which was based on maximizing the probability
max
d,l
?
s
?
a
pd,l(f
s,as|es) . (2)
Today the major application of the models lies in
word alignment. Instead of estimating continuous
parameters, one is now faced with the discrete opti-
mization problem of assigning a single alignment to
every sentence pair in the corpus. This lead to the
recent innovative work of (Bodrumlu et al, 2009)
where the alignments are the only unknown quanti-
ties.
Nevertheless, the use of probabilistic models re-
mains attractive, in particular since they contribute
statistics of likely alignments. In this work, we com-
bine the two concepts into the criterion
min
d,l
? log
[
?
s
max
as
pd,l(f
s,as|es)
]
+ ? ?d?0 ,
where ? ? 0 is a weighting parameter and we now
estimate a single alignment for every sentence.
The second term denotes the L0-norm of the
translational parameters, i.e. the number of non-zero
1The original work considered a dependence on I and J , but
it is common to drop J .
173
parameters. Since we only consider a single align-
ment per sentence, this term is equivalent to Bo-
drumlu et al?s objective function. Minimizing the
first term is closely related to the common criterion
(2). For parameter estimation it is known as the max-
imum approximation, but for word alignment it is a
perfectly valid model.
For the IBM-1 model the first term alone results in
a convex, but not strictly convex minimization prob-
lem2. However, EM-like iterative methods generally
do not reach the minimum: they are doing block co-
ordinate descent (Bertsekas, 1999, chap. 2.7) which
generally gives the optimum only for strictly convex
functions. Indeed, our experiments showed a strong
dependence on initialization and lead to heavily lo-
cal solutions.
In the following we present two strategies to min-
imize the new objective. We start with an iterative
method that also handles the regularity term.
3 An Iterative Scheme
To derive our algorithms, we first switch the mini-
mum and maximum in the objective and obtain
min
{as}
min
d,l
?
?
s
log
[
pd,l(f
s,as|es)
]
+ ? ?d?0 ,
where the notation{as} denotes the alignments of
all sentence pairs. Ignoring the L0-term for the mo-
ment, we now make use of a result of (Vicente et al,
2009) in their recent work on histogram-based im-
age segmentation: for any given set of alignments,
the inner minimization over the parameters is solved
by relative frequencies. When plugging this solution
into the functional, one gets a model that does not
decompose over the sentences, but one that is still
reasonably easy to handle.
Before we get into details, we observe that this
minimizer is valid even when including the L0 term:
if two words are never linked, both terms will set the
respective parameter to 0. If they are linked, how-
ever, then setting this parameter to 0 would make
the first term infinite. All non-zero parameters are
treated equally by the L0 term, so the restriction
to relative frequencies does not change the optimal
value. In fact, this can be extended to weighted L0
2This is due to taking the maximizing alignment. Summing
over all alignments is strictly convex.
terms, and later on we exploit this to derive an al-
ternative way to handle a dictionary prior. Note that
the same principle could also be applied to the work
of (Bodrumlu et al, 2009).
3.1 Mathematical Formulation
We detail our scheme for the IBM-1 model, the ex-
tension to other models is easy. For given align-
ments we introduce the counts
Nf,e({a
s}) =
?
s
?
j
?(f, fj) ? ?(e, eaj )
Ne({as}) =
?
s
?
j
?(e, eaj ) ,
where ?(?, ?) is the Kronecker-delta. The op-
timal translation parameters are then given by
Nf,e({as})/Ne({as}), and plugging this into the
first term in the objective gives (up to a constant)
min
{as}
?
f,e
?Nf,e({a
s}) log
(
Nf,e({as})
Ne({as})
)
.
The second term is simply ?
?
f,e ?Nf,e({a
s})?0,
and since N(e) =
?
f N(f, e), in total we get
min
{as}
?
f,e
?Nf,e({a
s}) log (Nf,e({a
s}))
+
?
e
Ne({as}) log (Ne({as})) .
+ ?
?
f,e
?Nf,e({a
s})?0 (3)
In essence we are now dealing with the function
x log(x), where its value for 0 is defined as 0.
3.2 Algorithm
For the new objective, we were able to entirely get
rid of the model parameters, leaving only alignment
variables. Nevertheless, the algorithm we present
maintains these parameters, and it requires an initial
choice. While we initialize the alignment parame-
ters uniformly, for the translation parameters we use
co-occurrence counts. This performed much better
than a uniform initialization. The algorithm, called
AM (for alternating minimization), now iterates two
steps:
174
1. Starting from the current setting of d and
l, derive Viterbi alignments for all sentence
pairs. E.g. for the IBM-1 we set asj =
argmax
i
d(fj |ei). For the IBM-2 the term is
similar, while for the HMM one can use dy-
namic programming.
Note that this step does not consider the L0-
term. This term can however not increase.
2. Run the Iterated Conditional Modes (Besag,
1986), i.e. go sequentially over all alignment
variables and set them to their optimal value
when keeping the others fixed.
Here, we need to keep track of the current
alignment counts. In every step we need to
compute the objective cost associated to a count
that increases by 1, and (another) one that
decreases by 1. For the IBM-2 we need to
consider the alignment counts, too, and for
the HMM usually two alignment terms are af-
fected. In case of 0-alignments there can be
more than two. We presently do not consider
these cases and hence do not find the exact op-
timum there.
Afterwards, reestimate the parameters d and l
from the final counts.
4 Integer Linear Programming
The above algorithm is fast and can handle large cor-
pora. However, it still gets stuck in local minima,
and there is no way of telling how close to the opti-
mum one got.
This motivates the second algorithm where we
cast the objective function as an integer linear pro-
gram (ILP). In practice it is too large to be solved
exactly, so we solve its linear programming relax-
ation, followed by successive strengthening. Here
we derive our own set of cuts. Now we also get a
lower bound on the problem and obtain lower en-
ergy solutions in practice. But we can handle only
small corpora.
We limit this method to the models IBM-1 and
IBM-2. Handling an HMM would be possible,
but its first order alignment model would introduce
many more variables. Handling the IBM-3, based on
(Ravi and Knight, 2010; Schoenemann, 2010) seems
a more promising direction.
4.1 An ILP for the Regularized IBM-1
The basis of our ILP is the fact that the counts Nf,e
and Ne can only assume a finite, a-priori known set
of integral values, including 0. We introduce a bi-
nary variable ncf,e ? {0, 1} for each possible value
c, where we want ncf,e = 1 if Nf,e(a
s) = c, oth-
erwise ncf,e = 0. This is analogous for the vari-
ables nce and Ne(a
s). Finally, since the counts de-
pend on the alignments, we also need binary vari-
ables xsi,j ? {0, 1} that we want to be 1 if and only
if asj = i.
The cost function of (3) can now be written as a
linear function in terms of the integer variables ncf,e
and nce, with coefficients
wce,f = ?c log(c) + ??c?0 , w
c
e = c log(c) .
However, we need additional constraints. In particu-
lar we need to ensure that for a given f and e exactly
one variable ncf,e is 1. Equivalently we can postulate
that the sum of these variables is one. We proceed
analogous for each e and the variables nce.
Then, we need to ensure that for each source word
in each sentence f s an alignment is specified, i.e.
that for each given s and j the variables xsi,j sum
to 1. Finally, the count variables have to reflect the
counts induced by the alignment variables. For the
counts Nf,e this is expressed by
?
s,i,j:fsj =f,e
s
i=e
xsi,j =
?
c
c ? ncf,e ?f, e ,
and likewise for the counts Ne.
Altogether, we arrive at the following system:
min
{xsi,j},{n
c
f,e},{n
c
e}
?
e,c
wce n
c
e +
?
f,e,c
wcf,e n
c
f,e
s.t.
?
i
xsi,j = 1 ?s, j
?
c
ncf,e = 1 ?f, e
?
c
nce = 1 ?e
?
s,i,j:fj=f,ei=e
xsi,j =
?
c
c ? ncf,e ?f, e
?
s,i,j:ei=e
xsi,j =
?
c
c ? nce ?e
xsi,j ? {0, 1}, n
c
e ? {0, 1}, n
c
e,f ? {0, 1} .
175
4.2 Handling the IBM-2
The above mentioned system can be easily adapted
to the IBM-2 model. To this end, we introduce vari-
ables nci,j,I ? {0, 1} to express how often source
word j is aligned to target word i given that there
are I words in the target sentence. Note that the
number of times source word j is aligned given that
the target sentence has I words is known a-priori
and does not depend on the alignment to be opti-
mized. We denote it Cj,I . The cost function of
the ILP is augmented by
?
i,j,I,cw
c
i,j,I n
c
i,j,I , with
wci,j,I = c log(c/Cj,I). In addition we add the fol-
lowing constraints to the system:
?
s:Is=I
xsi,j =
?
c
c ? nci,j,I ?i, j, I .
5 Cutting Planes
Integer linear programming is an NP-hard problem
(see e.g. (Schrijver, 1986)). While for problems
with a few thousand variables it can often be solved
exactly via the branch and cut method, in our setting
none of the solvers we tried terminated. Already
solving the linear programming relaxation requires
up to 4 GB of memory for corpora with roughly
3000 sentence pairs.
So instead of looking for an exact solution, we
make use of a few iterations of the cutting planes
method (Gomory, 1958), where repeatedly an LP-
relaxation is solved, then additionally valid inequal-
ities, called cuts, are added to the system. Every
round gives a tighter relaxation, i.e. a better lower
bound on the optimal integral value.
After solving each LP-relaxation we derive an in-
tegral solution by starting the iterative method from
section 3 from the fractional LP-solution. In the end
we output the best found integral solution.
For deriving cuts we tried all the methods imple-
mented in the COIN Cut Generation Library CGL3,
based on the solver Clp from the same project line.
However, either the methods were very slow in pro-
ducing cuts or they produced very few cuts only. So
eventually we derived our own set of cuts that will
now be presented. Note that they alone do not give
an integral solution.
3http://www.coin-or.org/projects/Cgl.xml
5.1 A Set of Count-based Cuts
The derived ILP contains several constraints of the
form ?
i
yi =
?
c
c ? zc , (4)
where all variables are binary. Expressions of this
kind arise wherever we need to ensure consistency
between alignment variables and count variables.
Our cuts aim at strengthening each of these equa-
tions individually.
Assume that equation (4) involves the variables
y1, . . . , yK and hence also the variables z0, . . . , zK .
The trouble with the equation is that even if the
left hand side is integral, the right-hand side is usu-
ally not. As an example, consider the case where
?K
i=1 yi = 3. Then the fractional assignment z0 =
1?3/K, zK = 3/K and zc = 0 for all other c satis-
fies (4). Indeed, if the cost function for zc is concave
in c, as is the function?c log(c) we use, this will be
the optimal solution for the given left hand side.
Hence we want to enforce that for an integral
value k of the left hand side, all variables zc for
0 ? c < k are zero. This is ensured by the fol-
lowing system of inequalities that is exponentially
large in k:
?
i?K
yi +
k?1?
c=0
zc ? k (5)
?K ? {1, . . . ,K} : |K| = k .
It turns out that this system can be formulated quite
compactly.
5.2 Polynomial Formulation
We now formalize the result for the compact formu-
lation of (5).
Proposition 1 The union of the systems (5) for all k
can be represented by polynomially many variables
and linear constraints.
Proof: we first observe that it suffices to enforce
[
max
K:|K|=k
?
i?K
yi
]
+
k?1?
c=0
zc ? k
for all k. These are polynomially many equations
(one for each k), but each involves a maximization
176
over exponentially many sets. However, this maxi-
mization can be expressed by the auxiliary variables
?kl := max
K?{1,...,l}:|K|?k
?
i?K
yi
= max{?k?1l?1 + yl , ?
k
l?1}
Now we only have to maximize over two linear ex-
pressions for each of the new, polynomially many,
variables. We can enforce ?kl to be an upper bound
on the maximum by postulating ?kl ? ?
k?1
l?1 + yl and
?kl ? ?
k
l?1. Since the original maximum occurred on
the left hand side of a less-than constraint, this upper
bound will be tight wherever this is necessary. 2
In practice the arising system is numerically hard
to solve. Since usually only polynomially many cuts
of the form (5) are actually violated during the cut-
ting planes process, we add them in passes and get
significantly lower running times. Moreover, each
round of cuts gives a new opportunity to derive a
heuristic integral solution.
5.3 Backward Cuts
We call the cuts we have derived above forward cuts
as they focus on variables that are smaller than k. If
we could be sure that the left-hand side of (4) was
always integral, they should indeed suffice. In prac-
tice this is not the case, so we now also derive back-
ward cuts where we focus on all variables that are
larger than k, with the following reasoning: once we
know that at least K ? k variables yi are inactive
(i.e. yi = 0), we can conclude that all zc with c > k
must be zero, too. This can be expressed by the set
of inequalities
?
i?K
(1? yi) +
K?
c=k+1
zc ? K ? k
?K ? {1, . . . ,K} : |K| = K ? k ,
or equivalently
?
i?K
?yi +
K?
c=k+1
zc ? 0 ?K : |K| = K ? k .
5.4 Other Applications
A related constraint system arises in recent work
(Ravi and Knight, 2010; Schoenemann, 2010) on
computing IBM-3 Viterbi alignments. We imple-
mented4 the polynomial formulation of the above
forward cuts for this system, and got mild speed-ups
(224 instead of 237 minutes for the Hansards task
reported in the second paper). With an additionally
improved fertility exclusion stage5 this is reduced to
176 minutes.
6 Experiments
We evaluate the proposed strategies on both small
scale and (where applicable) large scale tasks. We
compare to standard EM with sums over alignments,
where for the IBM-1 and the HMM we use GIZA++.
In addition, we evaluate several variants (our imple-
mentations) of the HMM, with non-parametric and
parametric alignment models. Note that for the non-
parametric variant we estimate distributions for the
first aligned position, for the parametric all initial
positions are equally likely. For the IBM-2 we con-
sider the non-parametric variant and hence our im-
plementation. We also evaluate our schemes on the
task without regularization.
All experiments in this work were executed on a
3 GHz Core 2 Duo machine with 8 GB of memory,
where up to 4 GB were actually used. The itera-
tive scheme was run for 15 iterations, where it was
nearly converged. This setting was also used for our
own EM-implementations. Solely for GIZA++ we
used the standard setting of 5 iterations, and the im-
plemented smoothing process. For the IBM-2 and
HMM we follow the standard strategy to first train
an IBM-1 with the same objective function.
6.1 Large Scale Tasks
We consider two well-known corpora with publicly
available gold alignments, and run both translation
directions for each of them. The first task is the
Canadian Hansards task (French and English) with
roughly 1 million sentences. The second task is
Europarl Spanish-English, where we take the first
500000 sentences. Our iterative scheme runs in
4based on code available at www.maths.lth.se/
matematiklth/personal/tosch/download.html.
5In (Schoenemann, 2010) we stated that the difference be-
tween cyif and the contribution of i to the bound has to exceed
u ? l3. This can be tightened if one additionally adds the cost
of the best f alignments to i to the cost cyif .
177
Canadian Hansards
Fr? En En? Fr
HMM (Giza++) 0.918 0.918
par. HMM (our EM) 0.887 0.896
par. HMM (Viterbi) 0.873 0.897
par. HMM + L0 0.891 0.907
nonpar. HMM (our EM) 0.873 0.911
nonpar. HMM (Viterbi) 0.881 0.909
nonpar. HMM + L0 0.902 0.917
Europarl
Es? En En? Es
HMM (Giza++) 0.764 0.754
nonpar. HMM (our EM) 0.738 0.733
nonpar. HMM (Viterbi) 0.726 0.716
nonpar. HMM + L0 0.729 0.73
Table 1: For large corpora, the proposed scheme outper-
forms Viterbi training and sometimes even our EM.
roughly 5 hours (with room for improvements), us-
ing 2.5 GB memory. We found that an L0-weight of
? = 5.0 performs very well. Hence, we will use this
for all our experiments.
We compare to the standard GIZA++ implemen-
tation and our own HMM implementations with EM.
Here we ran 15 iterations for IBM-1 and HMM each.
As shown in Table 1 adding the L0 term improves
the standard Viterbi training. Our method also some-
times beats the simple EM implementation but not
GIZA++. This may be due to the special paramet-
ric model of GIZA++, its smoothing process or the
lower number of iterations. Our deficient paramet-
ric model is inferior for the Hansards task, so we did
not run it for Europarl.
6.2 Small Scale Tasks
To evaluate the ILP strategy we consider four small
scale tasks released by the European Corpus Ini-
tiative6. See (Schoenemann, 2010) for the corpus
statistics. We report weighted f-measures (Fraser
and Marcu, 2007b) on gold alignments (sure and
possible) specified by one annotator, for 144 and 110
sentences respectively. The number of cut rounds
was selected so that the execution times remained
below 2 days for all tasks. This was 50 rounds for
the IBM-1 and 2 rounds for the IBM-2. In fact, with
6http://www.elsnet.org/eci.html
these numbers the Avalanche task is processed in lit-
tle less than a day.
We tested a number of LP solvers and found that
most of them need several hours to solve the root re-
laxation. This is different for the commercial solver
FICO Xpress, which only needs around 15 minutes.
However, it is slower in processing the subsequent
cut iterations. Hence, for the IBM-1 we use the open
source Clp7.
The resulting f-measures for the tested strategies
are given in Table 2. In all cases, adding the L0
term greatly improves the standard Viterbi training.
Moreover, for the small scale tasks, the parametric
HMM is clearly the best choice when using the L0
penalty. In the majority of cases the ILP strategy
performs better than the iterative scheme. In fact, it
always found the lower energy solution. The most
extreme difference we observed for the IBM-2 on
the UBS English to German task: here AM finds an
energy of 318147, where the ILP gives 303674.
Finally, Table 3 evaluates the effectiveness of
the cut strategy exemplarily on one of the tasks.
Clearly, the gaps are reduced significantly compared
to the LP-relaxation. However, except for the IBM-
1 (which is convex for ? = 0) the lower bounds are
still quite loose.
6.3 Handling Dictionary Knowledge
The presented L0 regularity is easily modified to in-
clude dictionary knowledge8. To this end, we intro-
duce a weighted L0-norm: whenever a pair of source
and target words is listed in the dictionary, the entry
is not penalized. All remaining entries are penalized
by ?.
Note that this is different from the standard way
(Brown et al, 1993a) of handling dictionary knowl-
edge, which appends the dictionary to the corpus
(with a proper weighting factor). We tried both
schemes with several weighting factors, then chose
the best-performing for the UBS task. For the UBS
German to English task we get an accuracy of 0.445,
which beats GIZA++ both with (0.438) and without
(0.398) dictionary knowledge. In the reverse direc-
tion both schemes profit from the extra knowledge,
7http://www.coin-or.org/projects/Clp.xml
8Our data are based on www.dict.info/uddl.php
and www.ilovelanguages.com/idp and the stemming
algorithms at snowball.tartarus.org.
178
Avalanche French? German
Model EM AM ILP
IBM-1 0.603 0.619 0.591
IBM-1 + L0 ? 0.64 0.625
IBM-2 0.568 0.632 0.60
IBM-2 + L0 ? 0.680 0.636
par. HMM 0.752 0.621 ?
par. HMM + L0 ? 0.779 ?
nonpar. HMM 0.752 0.655 ?
nonpar. HMM + L0 ? 0.714 ?
Avalanche German? French
Model EM AM ILP
IBM-1 0.494 0.485 0.507
IBM-1 + L0 ? 0.497 0.488
IBM-2 0.428 0.459 0.526
IBM-2 + L0 ? 0.483 0.55
par. HMM 0.606 0.49 ?
par. HMM + L0 ? 0.592 ?
nonpar. HMM 0.582 0.501 ?
nonpar. HMM + L0 ? 0.537 ?
UBS German? English
Model EM AM ILP
IBM-1 0.381 0.359 0.335
IBM-1 + L0 ? 0.350 0.442
IBM-2 0.315 0.324 0.340
IBM-2 + L0 ? 0.383 0.462
par. HMM 0.398 0.229 ?
par. HMM + L0 ? 0.383 ?
nonpar. HMM 0.421 0.29 ?
nonpar. HMM + L0 ? 0.371 ?
UBS English? German
Model EM AM ILP
IBM-1 0.515 0.435 0.489
IBM-1 + L0 ? 0.444 0.504
IBM-2 0.417 0.40 0.435
IBM-2 + L0 ? 0.52 0.571
par. HMM 0.625 0.404 ?
par. HMM + L0 ? 0.537 ?
nonpar. HMM 0.623 0.436 ?
nonpar. HMM + L0 ? 0.524 ?
Table 2: Alignment accuracy (weighted f-measure) for
different algorithms. We use a dictionary penalty of
? = 5 and the standard EM (GIZA++ for IBM-1 and
parametric HMM, our implementation otherwise) train-
ing scheme with 5 iterations for each model.
UBS English? German
L0-weight IBM-1 IBM-2
root relaxation 0.0 1.098 7.697
after cut rounds 0.0 1.081 5.67
root relaxation 5.0 1.16 2.76
after cut rounds 5.0 1.107 2.36
Table 3: Ratios of the best known integer solution and the
best known lower bounds for all considered tasks.
but GIZA++ remains the clear winner. Applying
the same weights to the above mentioned Hansards
task slightly improved GIZA++, whereas it slightly
worsened the performance of our scheme in the one
direction and slightly improved it in the other. We
intend to investigate this more thoroughly in the fu-
ture.
7 Discussion
In this paper we have shown that an L0 prior on
the dictionary parameters greatly improves Viterbi
training. A simple iterative scheme often nearly
matches our EM-implementation of the HMM.
We have also derived two algorithms to deal with
the new objective. A simple iterative scheme gives
quite accurate results on large scale tasks. On small
scale tasks our inexact ILP strategy shows that the
iterative scheme does not find the optimum in prac-
tice, a point that may well carry over to other mod-
els trained with the maximum approximation. This
strategy also provides lower bounds, but at present
they are quite loose.
Moreover, we have presented an alternative way
of handling dictionary knowledge. Finally, we have
discussed connections to computing IBM-3 Viterbi
alignments, where we got mild speed-ups.
In future work we intend to investigate the effect
of the generated alignments on the translation qual-
ity of phrase-based approaches. We also want to ex-
plore strategies to determine the regularity weight.
Finally, we want to handle a non-deficient paramet-
ric HMM.
Acknowledgements. We thank Ben Taskar and
Joa?o Grac?a for helpful discussions. This work was
funded by the European Research Council (Glob-
alVision grant no. 209480).
179
References
T. Achterberg. 2007. Constraint Integer Programming.
Ph.D. thesis, Zuse Institut, TU Berlin, Germany, July.
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-
ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.
Smith, and D. Yarowsky. 1999. Statistical
machine translation, Final report, JHU workshop.
http://www.clsp.jhu.edu/ws99/.
D.P. Bertsekas. 1999. Nonlinear Programming, 2nd edi-
tion. Athena Scientific.
J. Besag. 1986. On the statistical analysis of dirty pic-
tures. Journal of the Royal Statistical Society, Series
B, 48(3):259?302.
A. Birch, C. Callison-Burch, and M. Osborne. 2006.
Constraining the phrase-based, joint probability statis-
tical translation model. In Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
Cambridge, Massachusetts, August.
T. Bodrumlu, K. Knight, and S. Ravi. 2009. A new ob-
jective function for word alignment. In Proceedings of
the Workshop on Integer Linear Programming for Nat-
ural Language Processing (ILP), Boulder, Colorado,
June.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, M.J.
Goldsmith, J. Hajic, R.L. Mercer, and S. Mohanty.
1993a. But dictionaries are data too. In HLT work-
shop on Human Language Technology.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L.
Mercer. 1993b. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Columbus,
Ohio, June.
Y. Deng and W. Byrne. 2005. HMM word and phrase
alignment for statistical machine translation. In HLT-
EMNLP, Vancouver, Canada, October.
A. Fraser and D. Marcu. 2007a. Getting the structure
right for word alignment: LEAF. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Prague, Czech Republic, June.
A. Fraser and D. Marcu. 2007b. Measuring word align-
ment quality for statistical machine translation. Com-
putational Linguistics, 33(3):293?303, September.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search, 11:2001?2049, July.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2004. Fast decoding and optimal decoding for
machine translation. Artificial Intelligence, 154(1?2),
April.
R.E. Gomory. 1958. Outline of an algorithm for integer
solutions to linear programs. Bulletin of the American
Mathematical Society, 64:275?278.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Conference on Empirical Methods
in Natural Language Processing (EMNLP), Prague,
Czech Republic, June.
S. Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.
2006. Word alignment via quadratic assignment. In
Human Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, New York, New York, June.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Philadelphia, Pennsylva-
nia, July.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric word
alignments for statistical machine translation. In In-
ternational Conference on Computational Linguistics
(COLING), Geneva, Switzerland, August.
S. Ravi and K. Knight. 2010. Does GIZA++ make search
errors? Computational Linguistics, 36(3).
T. Schoenemann. 2010. Computing optimal alignments
for the IBM-3 translation model. In Conference on
Computational Natural Language Learning (CoNLL),
Uppsala, Sweden, July.
A. Schrijver. 1986. Theory of Linear and Integer
Programming. Wiley-Interscience Series in Discrete
Mathematics and Optimization. John Wiley & Sons.
E. Sumita, Y. Akiba, T. Doi, A. Finch, K. Imamura,
H. Okuma, M. Paul, M. Shimohata, and T. Watanabe.
2004. EBMT, SMT, Hybrid and more: ATR spoken
language translation system. In International Work-
shop on Spoken Language Translation (IWSLT), Ky-
oto, Japan, September.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A
discriminative matching approach to word alignment.
In Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), Vancouver, Canada, Oc-
tober.
S. Vicente, V.N. Kolmogorov, and C. Rother. 2009. Joint
optimization of segmentation and appearance models.
In IEEE International Conference on Computer Vision
(ICCV), Kyoto, Japan, September.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In In-
ternational Conference on Computational Linguistics
(COLING), pages 836?841, Copenhagen, Denmark,
August.
Y.-Y. Wang and A. Waibel. 1998. Modeling with
structures in statistical machine translation. In In-
ternational Conference on Computational Linguistics
(COLING), Montreal, Canada, August.
180
