First Joint Conference on Lexical and Computational Semantics (*SEM), pages 575?578,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Tiantianzhu7:System Description of Semantic Textual Similarity (STS) in
the SemEval-2012 (Task 6)
Tiantian Zhu
Department of Computer Science and
Technology
East China Normal University
51111201046@student.ecnu.edu.cn
Man Lan
Department of Computer Science and
Technology
East China Normal University
mlan@cs.ecnu.edu.cn
Abstract
This paper briefly reports our submissions to
the Semantic Textual Similarity (STS) task
in the SemEval 2012 (Task 6). We first use
knowledge-based methods to compute word
semantic similarity as well as Word Sense Dis-
ambiguation (WSD). We also consider word
order similarity from the structure of the sen-
tence. Finally we sum up several aspects of
similarity with different coefficients and get
the sentence similarity score.
1 Introduction
The task of semantic textual similarity (STS) is to
measure the degree of semantic equivalence between
two sentences. It plays an increasingly important
role in several text-related research and applications,
such as text mining, Web page retrieval, automatic
question-answering, text summarization, and ma-
chine translation. The goal of the Semeval 2012 STS
task (task 6) is to build a unified framework for the
evaluation of semantic textual similarity modules for
different systems and to characterize their impact on
NLP applications.
Generally, there are two ways to measure sim-
ilarity of two sentences, i.e, corpus-based meth-
ods and knowledge-based methods. The corpus-
based method typically computes sentence similar-
ity based on the frequency of word occurrence or the
co-occurrence between collocated words. For ex-
ample, in (Islam and Inkpen, 2008) they proposed a
corpus-based sentence similarity measure as a func-
tion of string similarity, word similarity and com-
mon word order similarity (CWO). The knowledge-
based method computes sentence similarity based
on the semantic information collected from knowl-
edge bases. With the aid of a number of success-
ful computational linguistic projects, many seman-
tic knowledge bases are readily available, for ex-
ample, WordNet, Spatial Date Transfer Standard,
Gene Ontology, etc. Among them, the most widely
used one is WordNet, which is organized by mean-
ings and developed at Princeton University. Sev-
eral methods computed word similarity by using
WordNet, such as the Lesk method in (Banerjee and
Pedersen, 2003), the lch method in (Leacock and
Chodorow, 1998)and the wup method in (Wu and
Palmer, 1994). Generally, although the knowledge-
based methods heavily depend on the knowledge
bases, they performed much better than the corpus-
based methods in most cases. Therefore, in our STS
system, we use a knowledge-based method to com-
pute word similarity.
The rest of this paper is organized as follows. Sec-
tion 2 describes our system. Section 3 presents the
results of our system.
2 System Description
Usually, a sentence is composed of some nouns,
verbs, adjectives, adverbs and/or some stop words.
We found that these words carry a lot of informa-
tion, especially the nouns and verbs. Although the
adjectives and adverbs also make contribution to the
semantic meaning of the sentence, they are much
weaker than the nouns and verbs. So we consider
to measure the sentence semantic similarities from
three aspects. We define the following three types of
similarity from two compared sentences to measure
575
the semantic similarity: (1) Noun Similarity to mea-
sure the similarity between the nouns from the two
compared sentences, (2) Verb Similarity to measure
the similarity between Verbs, (3) ADJ-ADV Simi-
larity to measure the similarity between the adjec-
tives and adverbs from each sentence. Besides the
semantic information similarity, we also found that
the structure of the sentences carry some informa-
tion which cannot be ignored. Therefore, we define
the last aspect of the sentence similarity as Word Or-
der Similarity. In the following we will introduce the
different components of our system.
2.1 POS
As a basic natural language processing technique,
part of speech tagging is to identify the part of
speech of individual words in the sentence. In or-
der to compute the three above semantic similari-
ties, we first identify the nouns, verbs, adjectives,
and adverbs in the sentence. Then we can calculate
the Noun Similarity, Verb Similarity and ADJ-ADV
Similarity from two sentences.
2.2 Semantic similarity between words
The word similarity measurement have important
impact on the performance of sentence similarity.
Currently, many lexical resources based approaches
perform comparatively well to compute semantic
word similarities. However, the exact resources they
are based are quite different. For example, some are
based on dictionary and/or thesaurus, and others are
based on WordNet.
WordNet is a machine-readable lexical database.
The words in Wordnet are classified into four cat-
egories, i.e., nouns, verbs, adjectives and adverbs.
WordNet groups these words into sets of syn-
onyms called synsets, provides short definitions, and
records the various semantic relations between these
synsets. The synsets are interlinked by means of
conceptual-semantic and lexical relations. Word-
Net alo provides the most common relationships
include Hyponym/Hypernym (i.e., is-a relationships)
and Meronym/Holonym (i.e., part-of relationships).
Nouns and verbs are organized into hierarchies
based on the hyponymy/hypernym relation between
synsets while adjectives and adverbs are not.
In this paper, we adopt the wup method in (Wu
and Palmer, 1994) to estimate the semantic similar-
ity between two words, which estimates the seman-
tic similarity between two words based on the depth
of the two words in WordNet and the depth of their
least common subsumer (LCS), where LCS is de-
fined as the common ancestor deepest in the taxon-
omy.
For example, given two words, w1 and w2, the
semantic similarity s(w1,w2) is the function of their
depth in the taxonomy and the depth of their least
common subsumer. If d1 and d2 are the depth of
w1 and w2 in WordNet, and h is the depth of their
least common subsumer in WordNet, the semantic
similarity can be written as:
s(w1, w2) =
2.0 ? h
d1 + d2
(1)
2.3 Word Sense Disambiguation
Word Sense Disambiguation (WSD) is to identify
the actual meaning of a word according to the con-
text. In our word similarity method, we take the
nearest meaning of two words into consideration
rather than their actual meaning. More impor-
tantly, the nearest meaning does not always repre-
sent the actual meaning. In our system, we used
a WSD algorithm proposed by (Ted Pedersen et
al.,2005), which computes semantic relatedness of
word senses using extended gloss overlaps of their
dictionary definitions. We utilize this WSD algo-
rithm for each sentence to get the actual meaning of
each word before computing the word semantic sim-
ilarity.
2.4 Semantic Similarity
We adopt a similar way to compute the three types of
semantic similarities. Here we take Noun Similarity
as an example.
Suppose sentence s1 and s2 are the two sentences
to be compared, s1 has a nouns while s2 has b nouns.
Then we get a ? b noun pairs and use the word sim-
ilarity method mentioned in section 2.2 to compute
the Noun Similarity of each noun pair. After that,
for each noun, we choose its highest score in noun
pairs as its similarity score. Then we use the formula
below to compute the Noun Similarity.
SimNoun =
(
?c
i=1 ni) ? (a + b)
2ab
(2)
576
where c represents the number of noun words in
sequence a and sequence b, c = min(a, b); ni rep-
resents the highest matching similarity score of i-th
word in the shorter sequence with respect to one of
the words in the longer sequence; and
?c
i=1 ni rep-
resents the sum of the highest matching similarity
score between the words in sequence a and sequence
b. Similarly, we can get SimV erb. Since there is no
Hyponym/Hypernym relation for adjectives and ad-
verbs in WordNet, we just compute ADJ-ADV Sim-
ilarity based on the frequency of overlap of simple
words.
2.5 Word Order Similarity
We believe that word order information also make
contributions to sentence similarity. In most cases,
the longer common sequence (LCS) the two sen-
tences have, the higher similarity score the sentences
get. For example the pair of sentences s1 and s2, we
remove all the punctuation from the sentences:
? s1: But other sources close to the sale said
Vivendi was keeping the door open to further
bids and hoped to see bidders interested in in-
dividual assets team up
? s2: But other sources close to the sale said
Vivendi was keeping the door open for further
bids in the next day or two
Since the length of the longest common sequence
is 14, we use the following formula to compute the
word order similarity.
SimWordOrder =
lengthofLCS
shorterlength
(3)
where the shorter length means the length of the
shorter sentence.
2.6 Overall Similarity
After we have the Noun Similarity, Verb Similar-
ity, ADJ-ADV Similarity and Word Order Similar-
ity, we calculate the Overall Similarity of two com-
pared sentences based on these four scores of simi-
larity. We combine them in the following way:
Simsent = aSimNoun + bSimV erb+
cSimADJ?ADV + dSimWordOrder
(4)
Where a, b, c and d are the coefficients which
denote the contribution of each aspect to the over-
all sentence similarity, For different data collections,
we empirically set different coefficients, for exam-
ple, for the MSR Paraphrase data, the four coeffi-
cients are set as 0.5, 0.3, 0.1, 0.1, because it is hard
to get the highest score 5 even when the two sen-
tences are almost the same meaning, We empirically
set a threshold, if the score exceeds the threshold we
set the score 5.
3 Experiment and Results on STS
Firstly, Stanford parser1 is used to parse each
sentence and to tag each word with a part of
speech(POS). Secondly, WordNet SenseRelate All-
Words2, a WSD tool from CPAN is used to disam-
biguate and to assign a sense for each word based on
the assigned POS.
We submitted three runs: run 1 with WSD, run 2
without WSD, run 3 removing stop words and with-
out WSD. The stoplist is available online3. Table 1
lists the performance of these three systems as well
as the baseline and the rank 1 results on STS task in
SemEval 2012.
We can see that run1 gets the best result, which
means WSD has improved the accuracy of sentence
similarity. Run3 gets better result than run2, which
proves that stop words do disturb the computation of
sentence similarity, removing them is a better choice
in our system.
4 Conclusion
In our work, we adopt a knowledge-based word sim-
ilarity method with WSD to measure the seman-
tic similarity between two sentences from four as-
pects: Noun Similarity, Verb Similarity, ADJ-ADV
Similarity and Word Order Similarity. The results
show that WSD improves the pearson coefficient at
some degree. However, our system did not get a
good rank. It indicates there still exists many prob-
lems such as wrong POS tag and wrong WSD which
might lead to wrong meaning of one word in a sen-
tence.
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://search.cpan.org/Tedpederse/WordNet-SenseRelate-
AllWords-0.19
3http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-
smart-stop-list/english.stop
577
Table 1: STS system configuration and results on STS task.
Run ALL ALLnrm Mean MSRpar MSRvid SMTeur OnWN SMTnews
rank 1 .7790 .8579 .6773 .6830 .8739 .5280 .6641 .4937
baseline .3110 .6732 .4356 .4334 .2996 .4542 .5864 .3908
1 .4533 .7134 .4192 .4184 .5630 .2083 .4822 .2745
2 .4157 .7099 .3960 .4260 .5628 .1546 .4552 .1923
3 .4446 .7097 .3740 .3411 .5946 .1868 .4029 .1823
Acknowledgments
The authors would like to thank the organizers for
their invaluable support making STS a first-rank and
interesting international event.
References
Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-
dul Kadir, Shyamala C. Doraisamy. 2010. Word Sense
Disambiguation-based Sentence Similarity. In Proc.
COLING-ACL, Beijing.
Jin Feng, Yiming Zhou, Trevor Martin. 2008. Sen-
tence Similarity based on Relevance. Proceedings of
IPMU?08, Torremolinos.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O?Shea, and Keeley Crockett. 2009. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
LIN LI, XIA HU, BI-YUN HU, JUN WANG, YI-MING
ZHOU. 2009. MEASURING SENTENCE SIMILAR-
ITY FROM DIFFERENT ASPECTS.
Islam Aminul and Diana Inkpen. 2008. Semantic Text
Similarity Using Corpus-Based Word Similarity and
String Similarity. ACM Transactions on Knowledge
Discovery from Data.
Banerjee and Pedersen. 2003. Extended gloss overlaps
as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence (IJCAI-03), pages805C810,
Acapulco, Mexico.
Leacock and Chodorow. 1998. Combining local con-
text and WordNet similarity for word sense identifica-
tion. In Christiane Fellbaum, editor, WordNet: An
Electronic Lexical Database. The MIT Press, Cam-
bridge,MA.
Z.Wu and M.Palmer. 1994. Verbs semantics and
lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computional Linguis-
tics,Morristown, NJ, USA.
Ted Pedersen, Satanjeev Banerjee, Siddharth Patward-
han. 2005. Maximizing Semantic Relatedness to Per-
form Word Sense Disambiguation.
578
