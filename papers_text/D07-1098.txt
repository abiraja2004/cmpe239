Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 940?946,
Prague, June 2007. c?2007 Association for Computational Linguistics
Probabilistic Parsing Action Models for Multi-lingual Dependency 
Parsing 
Xiangyu Duan 
Institute of Automation, Chi-
nese Academy of Sciences 
xyduan@nlpr.ia.ac.cn 
Jun Zhao 
Institute of Automation, Chi-
nese Academy of Sciences 
jzhao@nlpr.ia.ac.cn 
Bo Xu 
Institute of Automation, Chi-
nese Academy of Sciences 
xubo@hitic.ia.ac.cn 
 
 
Abstract 
Deterministic dependency parsers use pars-
ing actions to construct dependencies. 
These parsers do not compute the probabil-
ity of the whole dependency tree. They 
only determine parsing actions stepwisely 
by a trained classifier. To globally model 
parsing actions of all steps that are taken on 
the input sentence, we propose two kinds 
of probabilistic parsing action models that 
can compute the probability of the whole 
dependency tree. The tree with the maxi-
mal probability is outputted. The experi-
ments are carried on 10 languages, and the 
results show that our probabilistic parsing 
action models outperform the original de-
terministic dependency parser. 
1 Introduction 
The target of CoNLL 2007 shared task (Nivre et al, 
2007) is to parse texts in multiple languages by 
using a single dependency parser that has the ca-
pacity to learn from treebank data. Among parsers 
participating in CoNLL 2006 shared task 
(Buchholz et al, 2006), deterministic dependency 
parser shows great efficiency in time and compa-
rable performances for multi-lingual dependency 
parsing (Nivre et al, 2006). Deterministic parser 
regards parsing as a sequence of parsing actions 
that are taken step by step on the input sentence. 
Parsing actions construct dependency relations be-
tween words. 
Deterministic dependency parser does not score 
the entire dependency tree as most of state-of-the-
art parsers. They only stepwisely choose the most 
probable parsing action. In this paper, to globally 
model parsing actions of all steps that are taken on 
the input sentence, we propose two kinds of prob-
abilistic parsing action models that can compute 
the entire dependency tree?s probability. Experi-
ments are evaluated on diverse data set of 10 lan-
guages provided by CoNLL 2007 shared-task 
(Nivre et al, 2007). Results show that our prob-
abilistic parsing action models outperform the 
original deterministic dependency parser. We also 
present a general error analysis across a wide set of 
languages plus a detailed error analysis of Chinese. 
Next we briefly introduce the original determi-
nistic dependency parsing algorithm that is a basic 
component of our models. 
2 Introduction of Deterministic Depend-
ency Parsing 
There are mainly two representative deterministic 
dependency parsing algorithms proposed respec-
tively by Nivre (2003), Yamada and Matsumoto 
(2003). Here we briefly introduce Yamada and 
Matsumoto?s algorithm, which is adopted by our 
models, to illustrate deterministic dependency 
parsing. The other representative method of Nivre 
also parses sentences in a similar deterministic 
manner except different data structure and parsing 
actions. 
Yamada?s method originally focuses on unla-
beled dependency parsing. Three kinds of parsing 
actions are applied to construct the dependency 
between two focus words. The two focus words are 
the current sub tree?s root and the succeeding (right) 
sub tree?s root given the current parsing state. 
Every parsing step results in a new parsing state, 
which includes all elements of the current partially 
built tree. Features are extracted about these two 
focus words. In the training phase, features and the 
corresponding parsing action compose the training
940
 
 
 
 
 
 
 
 
 
 
 
 
 
He provides confirming evidence RIGHT
He
provides confirming evidence
SHIFT
LEFT
RIGHTconfirming 
He 
provides evidence provides evidence 
He confirming 
provides 
He evidence 
confirming 
Figure 1. The example of the parsing process of Yamada and Matsumoto?s method. The input sentence 
is ?He provides confirming evidence.? 
 
data. In the testing phase, the classifier determines 
which parsing action should be taken based on the 
features. The parsing algorithm ends when there is 
no further dependency relation can be made on the 
whole sentence. The details of the three parsing 
actions are as follows: 
LEFT: it constructs the dependency that the 
right focus word depends on the left focus word. 
RIGHT: it constructs the dependency that the 
left focus word depends on the right focus word. 
SHIFT: it does not construct dependency, just 
moves the parsing focus. That is, the new left focus 
word is the previous right focus word, whose suc-
ceeding sub tree?s root is the new right focus word. 
The illustration of these three actions and the 
parsing process is presented in figure 1. Note that 
the focus words are shown as bold black box. 
We extend the set of parsing actions to do la-
beled dependency parsing. LEFT and RIGHT are 
concatenated by dependency labels, while SHIFT 
remains the same. For example in figure 1, the 
original action sequence ?RIGHT -> SHIFT -> 
RIGHT -> LEFT? becomes ?RIGHT-SBJ -> 
SHIFT -> RIGHT-NMOD -> LEFT-OBJ?. 
3 Probabilistic Parsing Action Models 
Deterministic dependency parsing algorithms are 
greedy. They choose the most probable parsing 
action at every parsing step given the current pars-
ing state, and do not score the entire dependency 
tree. To compute the probability of whole depend-
ency tree, we propose two kinds of probabilistic 
models that are defined on parsing actins: parsing 
action chain model (PACM) and parsing action 
phrase model (PAPM). 
3.1 Parsing Action Chain Model (PACM) 
The parsing process can be viewed as a Markov 
Chain. At every parsing step, there are several can-
didate parsing actions. The objective of this model 
is to find the most probable sequence of parsing 
actions by taking the Markov assumption. As 
shown in figure 1, the action sequence ?RIGHT-
SBJ -> SHIFT -> RIGHT-NMOD -> LEFT-
OBJ? constructs the right dependency tree of the 
example sentence. Choosing this action sequence 
among all candidate sequences is the objective of 
this model.  
Firstly, we should define the probability of the 
dependency tree conditioned on the input sentence. 
)1(),...|()|(
...1
10?
=
?=
ni
ii SdddPSTP  
Where T denotes the dependency tree, S denotes 
the original input sentence,  denotes the parsing 
action at time step i. We add an artificial parsing 
action  as initial action. 
id
0d
We introduce a variable  to denote the 
resulting parsing state when the action  is taken 
on .  is the original input sen-
tence. 
id
context
id
1?idcontext 0dcontext
Suppose  are taken sequentially on the 
input sentence S, and result in a sequence of pars-
ing states , then P(T|S) de-
fined in equation (1) becomes as below: 
ndd ...0
ndd
contextcontext ...
0
941
)4()|(
)3()|(
)2(),...,|(
...1
...1
...1
1
1
10
?
?
?
=
=
=
?
?
?
=
?
ni
di
ni
dd
ni
ddd
i
ii
ii
contextdP
contextcontextP
contextcontextcontextP
 
Formula (3) comes from formula (2) by obeying 
the Markov assumption. Note that formula (4) is 
about the classifier of parsing actions. It denotes 
the probability of the parsing action given the 
parsing state . If we train a classifier 
that can predict with probability output, then we 
can compute P(T|S) by computing the product of 
the probabilities of parsing actions. The classifier 
we use throughout this paper is SVM (Vapnik, 
1995). We adopt Libsvm (Chang and Lin, 2005), 
which can train multi-class classifier and support 
training and predicting with probability output 
(Chang and Lin, 2005). 
id
1?idcontext
For this model, the objective is to choose the 
parsing action sequence that constructs the de-
pendency tree with the maximal probability. 
)5()|(max)|(max
...1
... 11
?
= ?
=
ni
didd in
contextdPSTP  
Because this model chooses the most probable 
sequence, not the most probable parsing action at 
only one step, it avoids the greedy property of the 
original deterministic parsers. 
We use beam search for the decoding of this 
model. We use m to denote the beam size. Then 
beam search is carried out as follows. At every 
parsing step, all parsing states are ordered (or par-
tially m ordered) according to their probabilities. 
Probability of a parsing state is determined by 
multiplying the probabilities of actions that gener-
ate that state. Then we choose m best parsing 
states for this step, and next parsing step only con-
sider these m best parsing states. Parsing termi-
nates when the first entire dependency tree is con-
structed. To obtain a list of n-best parses, we sim-
ply continue parsing until either n trees are found, 
or no further parsing can be fulfilled. 
3.2 Parsing Action Phrase Model (PAPM) 
In the Parsing Action Chain Model (PACM), ac-
tions are competing at every parsing step. Only m 
best parsing states resulted by the corresponding 
actions are kept for every step. But for the parsing 
problem, it is reasonable that actions are competing 
for which phrase should be built. For dependency 
syntax, one phrase consists of the head word and 
all its children. Based on this motivation, we pro-
pose Parsing Action Phrase Model (PAPM), which 
divides parsing actions into two classes: construct-
ing action and shifting action. 
If a phrase is built after an action is performed, 
the action is called constructing action. In original 
Yamada?s algorithm, constructing actions are 
LEFT and RIGHT. For example, if LEFT is taken, 
it indicates that the right focus word has found all 
its children and becomes the head of this new 
phrase. Note that one word with no children can 
also be viewed as a phrase if its dependency on 
other word is constructed. In the extended set of 
parsing actions for labeled parsing, compound ac-
tions, which consist of LEFT and RIGHT con-
catenated by dependency labels, are constructing 
actions. 
If no phrase is built after an action is performed, 
the action is called shifting action. Such action is 
SHIFT. 
We denote  as constructing action and  as 
shifting action. j indexes the time step. Then we 
introduce a new concept: parsing action phrase. 
We use  to denote the ith parsing action phrase. 
It can be expanded as . That is, 
parsing action phrase  is a sequence of parsing 
actions that constructs the next syntactic phrase. 
ja jb
iA
jjkji abbA 1... ???
iA
For example, consider the parsing process in 
figure 1,  is ?RIGHT-SBJ?,  is ?SHIFT, 
RIGHT-NMOD?,  is ?LEFT-OBJ?. Note that 
 consists of a constructing action,  consists 
of a shifting action and a constructing action,  
consists of a constructing action. 
1A 2A
3A
1A 2A
3A
The indexes are different for both sides of the 
expansion ,  is the ith parsing 
action phrase corresponding to both constructing 
action  at time step j and all its preceding shift-
ing actions. Note that on the right side of the ex-
pansion, only one constructing action is allowed 
and is always at the last position, while shifting 
action can occur several times or does not occur at 
all. It is parsing action phrases, i.e. sequences of 
parsing actions, that are competing for which next 
phrase should be built. 
jjkji abbA 1... ??? iA
ja
942
The probability of the dependency tree given the 
input sentence is redefined as: 
)|())|((
)|(
)|...(
)|(
)|(
)...|(
)6(),...|()|(
1
1
1
1
1
11
...2
1
...1
...1
1
...1
...1
...1
...1
11
??
?
?
?
?
?
?
?=
=
=
?
=
=
?
?
?
?
?
?
?
=
+?
=
?
=
??
=
=
=
=
?
jtj
i
i
i
ii
ii
bj
kt
btj
ni
Akj
ni
Ajjkj
ni
Ai
ni
AA
ni
AAA
ni
ii
contextaPcontextbP
contextbP
contextabbP
contextAP
contextcontextP
contextcontextcontextP
SAAAPSTP
 
Where k represents the number of steps that shift-
ing action can be taken.  is the parsing 
state resulting from a sequence of actions 
 taken on . 
iA
context
jjkj abb 1... ?? 1?iAcontext
The objective in this model is to find the most 
probable sequence of parsing action phrases. 
)7()|(max)|(max
...1
... 11
?
= ?
=
ni
AiAA in
contextAPSTP  
Similar with parsing action chain model 
(PACM), we use beam search for the decoding of 
parsing action phrase model (PAPM). The differ-
ence is that PAPM do not keep m best parsing 
states at every parsing step. Instead, PAPM keep m 
best states which are corresponding to m best cur-
rent parsing action phrases (several steps of 
SHIFT and the last step of a constructing action). 
4 Experiments and Results 
Experiments are carried on 10 languages provided 
by CoNLL 2007 shared-task organizers (Nivre et 
al., 2007). Among these languages, Chinese (Chen 
et al, 2003), Catalan (Mart? et al, 2007) and Eng-
lish (Johansson and Nugues, 2007) have low per-
centage of non-projective relations, which are 
0.0%, 0.1% and 0.3% respectively. Except these 
three languages, we use software of projectiviza-
tion/deprojectivization provided by Nivre and 
Nilsson (2005) for other languages. Because our 
algorithm only deals with projective parsing, we 
should projectivize training data at first to prepare 
for the following training of our algorithm. During 
testing, deprojectivization is applied to the output 
of the parser. 
Considering the classifier of Libsvm (Chang and 
Lin, 2005), the features are extracted from the fol-
lowing fields of the data representation: FORM, 
LEMMA, CPOSTAG, POSTAG, FEATS and DE-
PREL. We split values of FEATS field into its 
atomic components. We only use available features 
of DEPREL field during deterministic parsing. We 
use similar feature context window as used in Ya-
mada?s algorithm (Yamada and Matsumoto, 2003). 
In detail, the size of feature context window is six, 
which consists of left two sub trees, two focus 
words related sub trees and right two sub trees. 
This feature template is used for all 10 languages. 
4.1 Results of PACM and Yamada?s Method 
After submitting the testing results of Parsing Ac-
tion Chain Model (PACM), we also perform origi-
nal deterministic parsing proposed by Yamada and 
Matsumoto (2003). The total results are shown in 
table 1. The experimental results are mainly evalu-
ated by labeled attachment score (LAS), unlabeled 
attachment score (UAS) and labeled accuracy (LA). 
Table 1 shows that Parsing Action Chain Model 
(PACM) outperform original Yamada?s parsing 
method for all languages. The LAS improvements 
range from 0.60 percentage points to 1.71 percent-
age points. Note that the original Yamada?s 
method still gives testing results above the official 
reported average performance of all languages. 
 Ara Bas Cat Chi Cze Eng Gre Hun Ita Tur 
YamLAS  69.31 69.67 83.26 81.88 74.63 84.81 72.75 76.24 80.08 73.94
YamUAS  78.93 75.86 88.53 86.17 80.11 85.83 79.45 79.97 83.69 79.79
YamLA  81.13 75.71 88.36 84.56 82.10 89.71 82.58 88.37 86.93 80.81
PACMLAS  69.91 71.26 84.95 82.58 75.34 85.83 74.29 77.06 80.75 75.03
PACMUAS  79.04 77.57 89.71 86.88 80.82 86.97 80.77 80.66 84.20 81.03
PACMLA  81.40 77.35 89.55 85.35 83.17 90.57 83.87 88.92 87.32 81.17
Table 1. The performances of Yamada?s method (Yam) and Parsing Action Chain Model (PACM). 
 
943
4.2 Results of PAPM 
Not all languages have only one root node of a 
sentence. Since Parsing Action Phrase Model 
(PAPM) only builds dependencies, and shifting 
action is not the ending action of a parsing action 
phrase, PAPM always ends with one root word. 
This property makes PAPM only suitable for 
Catalan, Chinese, English and Hungarian, which 
are unary root languages. PAPM result of Catalan 
was not submitted before deadline due to the    
shortage of time and computing resources. We 
report Catalan?s PAPM result together with that of 
other three languages in table 2.  
 
 Cat Chi Eng Hun 
PAPMLAS  87.26 82.64 86.69 76.89 
PAPMUAS  92.07 86.94 87.87 80.53 
PAPMLA  91.89 85.41 92.04 89.73 
Table 2. The performance of Parsing Action 
Phrase Model (PAPM) for Catalan, Chinese, Eng-
lish and Hungarian. 
 
Compared with the results of PACM shown in 
table 1, the performance of PAPM differs among 
different languages. Catalan and English show 
that PAPM improves 2.31% and 0.86% respec-
tively over PACM, while the improvement of Chi-
nese is marginal, and there is a little decrease of 
Hungarian. Hungarian has relatively high percent-
age of non-projective relations. If phrase consists 
of head word and its non-projective children, the 
constructing actions that are main actions in 
PAPM will be very difficult to be learned because 
some non-projective children together with their 
heads have no chance to be simultaneously as fo-
cus words. Although projectivization is also per-
formed for Hungarian, the built-in non-projective 
property still has negative influence on the per-
formance. 
5 Error Analysis 
In the following we provide a general error analy-
sis across a wide set of languages plus a detailed 
analysis of Chinese. 
5.1 General Error Analysis 
One of the main difficulties in dependency parsing 
is the determination of long distance dependencies. 
Although all kinds of evaluation scores differ 
dramatically among different languages, 69.91% 
to 85.83% regarding LAS, there are some general 
observations reflecting the difficulty of long dis-
tance dependency parsing. We study this difficulty 
from two aspects about our full submission of 
PACM: precision of dependencies of different arc 
lengths and precision of root nodes. 
For arcs of length 1, all languages give high 
performances with lowest 91.62% of Czech 
(B?hmova et al, 2003) to highest 96.8% of Cata-
lan (Mart? et al, 2007). As arcs lengths grow 
longer, various degradations are caused. For Cata-
lan, score of arc length 2 is similar with that of arc 
length 1, but there are dramatic degradations for 
longer arc lengths, from 94.94% of arc length 2 to 
85.22% of length 3-6. For English (Johansson and 
Nugues, 2007) and Italian (Montemagni et al, 
2003), there are graceful degradation for arcs of 
length 1,2 and 3-6, with 96-91-85 of English and 
95-85-75 of Italian. For other languages, long arcs 
also give remarkable degradations that pull down 
the performance. 
Precision of root nodes also reflects the per-
formance of long arc dependencies because the 
arc between the root and its children are often 
long arcs. In fact, it is the precision of roots and 
arcs longer than 7 that mainly pull down the over-
all performance. Yamada?s method is a bottom-up 
parsing algorithm that builds short distance de-
pendencies at first. The difficulty of building long 
arc dependencies may partially be resulted from 
the errors of short distance dependencies. The de-
terministic manner causes error propagation, and 
it indirectly indicates that the errors of roots are 
the final results of error propagation of short dis-
tance dependencies. But there is an exception oc-
curred in Chinese. The root precision is 90.48%, 
only below the precision of arcs of length 1. This 
phenomenon exists because the sentences in Chi-
nese data set (Chen et al, 2003) are in fact clauses 
with average length of 5.9 rather than entire sen-
tences. The root words are heads of clauses. 
Both Parsing Action Chain Model (PACM) and 
Parsing Action Phrase Model (PAPM) avoid 
greedy property of original Yamada?s method. It 
can be expected that there will be a precision im-
provement of long distance dependencies over 
original Yamada?s method. For PACM, the results 
of Basque (Aduriz et al, 2003), Catalan (Mart? et 
al., 2007), Chinese (Chen et al, 2003), English 
(Johansson and Nugues, 2007) and Greek (Pro-
944
kopidis et al, 2005) show that the root precision 
improvement over Yamada?s method is more con-
spicuous than that of other long distance depend-
encies. The largest improvement of roots precision 
is 10.7% of Greek. While for Arabic (Hajic et al, 
2004), Czech (B?hmova et al, 2003), Hungarian 
(Csendes et al, 2005), Italian (Montemagni et al, 
2003) and Turkish (Oflazer et al, 2003), the im-
provement of root precision is small, but depend-
encies of arcs longer than 1 give better scores. For 
PAPM, good performances of Catalan and English 
also give significant improvements of root preci-
sion over PACM. For Catalan, the root precision 
improvement is from 63.86% to 95.21%; for Eng-
lish, the root precision improvement is from 
62.03% to 89.25%. 
5.2 Error Analysis of Chinese 
There are mainly two sources of errors regarding 
LAS in Chinese dependency parsing. 
One is from conjunction words (C) that have a 
relatively high percentage of wrong heads (about 
20%), and therefore 19% wrong dependency la-
bels. In Chinese, conjunction words often con-
catenate clauses. Long distance dependencies be-
tween clauses are bridged by conjunction words. 
It is difficult for conjunction words to find their 
heads. 
The other source of errors comes from auxiliary 
words (DE) and preposition words (P). Unlike 
conjunction words, auxiliary words and preposi-
tion words have high performance of finding right 
head, but label accuracy (LA) decrease signifi-
cantly. The reason may lie in the large depend-
ency label set consisting of 57 kinds of depend-
ency labels in Chinese. Moreover, auxiliary words 
(DE) and preposition words (P) have more possi-
ble dependency labels than other coarse POS have. 
This introduces ambiguity for parsers. 
Most common POS including noun and verb 
contribute much to the overall performance of 
83% Labeled Attachment Scores (LAS). Adverbs 
obtain top score while adjectives give the worst. 
6 Conclusion 
We propose two kinds of probabilistic models 
defined on parsing actions to compute the prob-
ability of entire sentence. Compared with original 
Yamada and Matsumoto?s deterministic depend-
ency method which stepwisely chooses most 
probable parsing action, the two probabilistic 
models improve the performance regarding all 10 
languages in CoNLL 2007 shared task. Through 
the study of parsing results, we find that long dis-
tance dependencies are hard to be determined for 
all 10 languages. Further analysis about this diffi-
culty is needed to guide the research direction. 
Feature exploration is also necessary to provide 
more informative features for hard problems. 
Ackowledgements 
This work was supported by Hi-tech Research and 
Development Program of China under grant No. 
2006AA01Z144, the Natural Sciences Foundation 
of China under grant No. 60673042, and the Natu-
ral Science Foundation of Beijing under grant No. 
4052027, 4073043. 
References 
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 
2006. CoNLL-X shared task on multilingual de-
pendency parsing. SIGNLL. 
Chih-Chung Chang and Chih-Jen Lin. 2005. LIBSVM: 
A library for support vector machines. 
J. Nivre. 2003. An efficient algorithm for projective 
dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies 
(IWPT). 
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proc. of ACL-2005, pages 99?
106. 
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, S. Marinov. 
2006. Labeled Pseudo-Projective Dependency 
Parsing with Support Vector Machines. In Proc. of 
the Tenth Conference on Computational Natural 
Language Learning (CoNLL). 
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 
shared task on dependency parsing. In Proc. of the 
Joint Conf. on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL). 
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In 
Proceedings of the 8th International Workshop on 
Parsing Technologies (IWPT). 
V. Vapnik. 1995. The Nature of StatisticalLearning 
Theory. Springer. 
945
A. Abeill?, editor. 2003. Treebanks: Building and 
Using Parsed Corpora. Kluwer.  
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque Dependency Tree-
bank. In Proc. of the 2nd Workshop on Treebanks 
and Linguistic Theories (TLT), pages 201?204. 
A. B?hmov?, J. Hajic, E. Hajicov? and B. Hladk?. 
2003. The PDT: a 3-level annotation scenario. In 
Abeill? (2003), chapter 7, 103?127. 
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. 
Huang and Z. Gao. 2003. Sinica Treebank: Design 
Criteria, Representational Issues and Implementa-
tion. In Abeill? (2003), chapter 13, pages 231?248. 
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 
2005. The Szeged Treebank. Springer.  
J. Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. 
Beska. 2004. Prague Arabic Dependency Treebank: 
Development in Data and Tools. In Proc. of the 
NEMLAR Intern. Conf. on Arabic Language Re-
sources and Tools, pages 110?117. 
R. Johansson and P. Nugues. 2007. Extended 
constituent-to-dependency conversion for English. 
In Proc. of the 16th Nordic Conference on 
Computational Linguistics (NODALIDA).  
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313?330. 
M. A. Mart?, M. Taul?, L. M?rquez and M. Bertran. 
2007. CESS-ECE: A Multilingual and Multilevel 
Annotated Corpus. Available for download from: 
http://www.lsi.upc.edu/~mbertran/cess-ece/. 
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, 
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. 
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. 
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R. 
Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (2003), chapter 11, 
pages 189?210.  
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 
shared task on dependency parsing. In Proc. of the 
CoNLL 2007 Shared Task. Joint Conf. on Empirical 
Methods in Natural Language Processing and 
Computational Natural Language Learning 
(EMNLP-CoNLL). 
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. 
T?r. 2003. Building a Turkish treebank. In Abeill? 
(2003), chapter 15, pages 261?277.  
P. Prokopidis, E. Desypri, M. Koutsombogera, H. 
Papageorgiou, and S. Piperidis. 2005. Theoretical 
and practical issues in the construction of a Greek 
depen- dency treebank. In Proc. of the 4th 
Workshop on Treebanks and Linguistic Theories 
(TLT), pages 149?160. 
946
