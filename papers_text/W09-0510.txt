Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 74?81,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Incrementality, Speaker-Hearer Switching
and the Disambiguation Challenge
Ruth Kempson, Eleni Gregoromichelaki
King?s College London
{ruth.kempson, eleni.gregor}@kcl.ac.uk
Yo Sato
University of Hertfordshire
y.sato@herts.ac.uk
Abstract
Taking so-called split utterances as our
point of departure, we argue that a new
perspective on the major challenge of dis-
ambiguation becomes available, given a
framework in which both parsing and gen-
eration incrementally involve the same
mechanisms for constructing trees reflect-
ing interpretation (Dynamic Syntax: (Cann
et al, 2005; Kempson et al, 2001)). With
all dependencies, syntactic, semantic and
pragmatic, defined in terms of incremental
progressive tree growth, the phenomenon
of speaker/hearer role-switch emerges as
an immediate consequence, with the po-
tential for clarification, acknowledgement,
correction, all available incrementally at
any sub-sentential point in the interpreta-
tion process. Accordingly, at all interme-
diate points where interpretation of an ut-
terance subpart is not fully determined for
the hearer in context, uncertainty can be
resolved immediately by suitable clarifica-
tion/correction/repair/extension as an ex-
change between interlocutors. The result
is a major check on the combinatorial ex-
plosion of alternative structures and inter-
pretations at each choice point, and the ba-
sis for a model of how interpretation in
context can be established without either
party having to make assumptions about
what information they and their interlocu-
tor share in resolving ambiguities.
1 Introduction
A major characteristic of dialogue is effortless
switching between the roles of hearer and speaker.
Dialogue participants seamlessly shift between
parsing and generation bi-directionally across any
syntactic dependency, without any indication of
there being any problem associated with such
shifts (examples from Howes et al (in prep)):
(1) Conversation from A and B, to C:
A: We?re going
B: to Bristol, where Jo lives.
(2) A smelling smoke comes into the kitchen:
A: Have you burnt
B the buns. Very thoroughly.
A: But did you burn
B: Myself? No. Luckily.
(3) A: Are you left or
B: Right-handed.
Furthermore, in no case is there any guarantee that
the way the shared utterance evolves is what ei-
ther party had in mind to say at the outset, indeed
obviously not, as otherwise the exchange risks be-
ing otiose. This flexibility provides a vehicle for
ongoing clarification, acknowledgement, correc-
tions, repairs etc. ((6)-(7) from (Mills, 2007)):
(4) A: I?m seeing Bill.
B: The builder?
A: Yeah, who lives with Monica.
(5) A: I saw Don
B: John?
A: Don, the guy from Bristol.
(6) A: I?m on the second switch
B: Switch?
A: Yeah, the grey thing
(7) A: I?m on the second row third on the left.
B: What?
A: on the left
The fragmental utterances that constitute such in-
cremental, joint contributions have been analysed
as falling into discrete structural types according
to their function, in all cases resolved to propo-
sitional types by combining with appropriate ab-
stractions from context (Ferna?ndez, 2006; Purver,
2004). However, any such fragment and their
resolution may occur as mid-turn interruptions,
well before any emergent propositional structure
is completed:
74
(8) A: They X-rayed me, and took a urine
sample, took a blood sample.
Er, the doctor ...
B: Chorlton?
A: Chorlton, mhm, he examined me, erm,
he, he said now they were on about a slight
[shadow] on my heart. [BNC: KPY
1005-1008]
The advantage of such ongoing, incremental, joint
conversational contributions is the effective nar-
rowing down of the search space out of which
hearers select (a) interpretations to yield some
commonly shared understanding, e.g. choice
of referents for NPs, and, (b) restricted struc-
tural frames which allow (grammatical) context-
dependent fragment resolution, i.e. exact speci-
fications of what contextually available structures
resolve elliptical elements. This seems to pro-
vide an answer as to why such fragments are so
frequent and undemanding elements of dialogue,
forming the basis for the observed coordination
between participants: successive resolution at sub-
sentential stages yields a progressively jointly es-
tablished common ground, that can thereafter be
taken as a secure, albeit individual, basis for filter-
ing out interpretations inconsistent with such con-
firmed knowledge-base (see (Poesio and Rieser,
2008; Ginzburg, forthcmg) etc). All such dialogue
phenomena, illustrated in (1)-(8), jointly and in-
crementally achieved, we address with the general
term split utterances.
However, such exchanges are hard to model
within orthodox grammatical frameworks, given
that usually it is the sentence/proposition that is
taken as the unit of syntactic/semantic analysis;
and they have not been addressed in detail within
such frameworks, being set aside as deviant, given
that such grammars in principle do not specify
a concept of grammaticality that relies on a de-
scription of the context of occurrence of a certain
structure (however, see Poesio and Rieser (2008)
for German completions). In so far as fragment
utterances are now being addressed, the pressure
of compatibility with sentence-based grammars
is at least partly responsible for analyses of e.g.
clarificatory-request fragments as sentential in na-
ture (Ginzburg and Cooper, 2004). But such anal-
yses fail to provide a basis for incrementally re-
solved clarification requests such as the interrup-
tion in (8) where no sentential basis is yet avail-
able over which to define the required abstraction
of contextually provided content.
In the psycholinguistic literature, on the other
hand, there is broad agreement that incrementality
is a crucial feature of parsing with semantic inter-
pretation taking place as early as possible at the
sub-sentential level (see e.g. (Sturt and Crocker,
1996)). Nonetheless, this does not, in and of it-
self, provide a basis for explaining the ease and
frequency of split utterances in dialogue: the inter-
active coordination between the parsing and pro-
duction activities, one feeding the other, remains
as a challenge.
In NLP modelling, parsing and generation algo-
rithms are generally dissociated from the descrip-
tion of linguistic entities and rules, i.e. the gram-
mar formalisms, which are considered either to be
independent of processing (?process-neutral?) or
to require some additional generation- or parsing-
specific mechanisms to be incorporated. However,
this point of view creates obstacles for a success-
ful account of data as in (1)-(8). Modelling those
would require that, for the current speaker, the ini-
tiated generation mechanism has to be displaced
mid-production without the propositional genera-
tion task having been completed. Then the parsing
mechanism, despite being independent of, indeed
in some sense the reverse of, the generation com-
ponent, has to take over mid-sentence as though, in
some sense there had been parsing involved up to
the point of switchover. Conversely, for the hearer-
turned-speaker, it would be necessary to somehow
connect their parse with what they are now about
to produce in order to compose the meaning of the
combined sentence. Moreover, in both directions
of switch, as (2) shows, this is not a phenomenon
of both interlocutors intending to say the same
sentence: as (3) shows, even the function of the
utterance (e.g. question/answer) can alter in the
switch of roles and such fragments can play two
roles (e.g. question/completion) at the same time
(e.g. (2)). Hence the grammatical integration of
such joint contributions must be flexible enough
to allow such switches which means that such
fragment resolutions must occur before the com-
putation of intentions at the pragmatic level. So
the ability of language users to successfully pro-
cess such utterances, even at sub-sentential levels,
means that modelling their grammar requires fine-
grained grammaticality definitions able to char-
acterise and integrate sub-sentential fragments in
turns jointly constructed by speaker and hearer.
75
This can be achieved straightforwardly if fea-
tures like incrementality and context-dependent
processing are built into the grammar architecture
itself. The modelling of split utterances then be-
comes straightforward as each successive process-
ing step exploits solely the grammatical apparatus
to succeed or fail. Such a view notably does not in-
voke high-level decisions about speaker/hearer in-
tentions as part of the mechanism itself. That this
is the right view to take is enhanced by the fact that
as all of (1)-(8) show, neither party in such role-
exchanges can definitively know in advance what
will emerge as the eventual joint proposition. If,
to the contrary, generation decisions are modelled
as involving intentions for whole utterances, there
will be no the basis for modelling how such in-
complete strings can be integrated in suitable con-
texts, with joint propositional structures emerging
before such joint intentions have been established.
An additional puzzle, equally related to both
the challenges of disambiguation and the status
of modelling speaker?s intentions as part of the
mechanism whereby utterance interpretation takes
place, is the common occurrence of hearers NOT
being constrained by any check on consistency
with speaker intentions in determining a putative
interpretation, failing to make use of well estab-
lished shared knowledge:
(9) A: I?m going to cook salmon, as John?s
coming.
B: What? John?s a vegetarian.
A: Not my brother. John Smith.
(10) A: Why don?t you have cheese and noodles?
B: Beef? You KNOW I?m a vegetarian
Such examples are problematic for any account
that proposes that interpretation mechanisms for
utterance understanding solely depend on selec-
tion of interpretations which either the speaker
could have intended (Sperber and Wilson, 1986;
Carston, 2002), or ones which are compati-
ble with checking consistency with the com-
mon ground/plans established between speaker
and hearer (Poesio and Rieser, 2008; Ginzburg,
forthcmg), mutual knowledge, etc. (Clark, 1996;
Brennan and Clark, 1996). To the contrary, the
data in (9)-(10) tend to show that the full range
of interpretations computable by the grammar has
in principle to be available at all choice points for
construal, without any filter based on plausibility
measures, thus leaving the disambiguation chal-
lenge still unresolved.
In this paper we show how with speaker and
hearer in principle using the same mechanisms for
construal, equally incrementally applied, such dis-
ambiguation issues can be resolved in a timely
manner which in turn reduces the multiplication
of structural/interpretive options. As we shall see,
what connects our diverse examples, and indeed
underpins the smooth shift in the joint endeav-
our of conversation, lies in incremental, context-
dependent processing and bidirectionality, essen-
tial ingredients of the Dynamic Syntax (Cann et al,
2005) dialogue model.
2 Incrementality in Dynamic Syntax
Dynamic Syntax (DS) is a procedure-oriented
framework, involving incremental processing, i.e.
strictly sequential, word-by-word interpretation of
linguistic strings. The notion of incrementality
in DS is closely related to another of its features,
the goal-directedness of BOTH parsing and gener-
ation. At each stage of processing, structural pre-
dictions are triggered that could fulfill the goals
compatible with the input, in an underspecified
manner. For example, when a proper name like
Bob is encountered sentence-initially in English,
a semantic predicate node is predicted to follow
(?Ty(e ? t)), amongst other possibilities.
By way of introducing the reader to the DS
devices, let us look at some formal details with
an example, Bob saw Mary. The ?complete? se-
mantic representation tree resulting after the com-
plete processing of this sentence is shown in Fig-
ure 2 below. A DS tree is formally encoded with
the tree logic LOFT (Blackburn and Meyer-Viol
(1994)), we omit these details here) and is gen-
erally binary configurational, with annotations at
every node. Important annotations here, see the
(simplified) tree below, are those which represent
semantic formulae along with their type informa-
tion (e.g. ?Ty(x)?) based on a combination of the
epsilon and lambda calculi1.
Such complete trees are constructed, starting
from a radically underspecified annotation, the ax-
iom, the leftmost minimal tree in Figure 2, and
going through monotonic updates of partial, or
structurally underspecified, trees. The outline of
this process is illustrated schematically in Figure
2. Crucial for expressing the goal-directedness
are requirements, i.e. unrealised but expected
1These are the adopted semantic representation languages
in DS but the computational formalism is compatible with
other semantic-representation formats
76
0?Ty(t),
?
7?
1
?Ty(t)
?Ty(e),? ?Ty(e? t)
7?
2
?Ty(t)
Ty(e),Bob? ?Ty(e? t),?
7?
3
?Ty(t)
Ty(e),
Bob? ?Ty(e? t)
?Ty(e),
?
Ty(e? (e? t)),
See?
7?
0(gen)/4
Ty(t),?
See?(Mary?)(Bob?)
Ty(e),
Bob?
Ty(e? t),
See?(Mary?)
Ty(e),
Mary?
Ty(e? (e? t)),
See?
Figure 2: Monotonic tree growth in DS
Ty(t),
See?(Mary?)(Bob?)
Ty(e),
Bob?
Ty(e? t),
See?(Mary?)
Ty(e),
Mary?
Ty(e? (e? t)),
See?
Figure 1: A DS complete tree
node/tree specifications, indicated by ??? in front
of annotations. The axiom says that a proposition
(of type t, Ty(t)) is expected to be constructed.
Furthermore, the pointer, notated with ??? indi-
cates the ?current? node in processing, namely the
one to be processed next, and governs word order.
Updates are carried out by means of applying
actions, which are divided into two types. Compu-
tational actions govern general tree-constructional
processes, such as moving the pointer, introducing
and updating nodes, as well as compiling interpre-
tation for all non-terminal nodes in the tree. In our
example, the update of (1) to (2) is executed via
computational actions specific to English, expand-
ing the axiom to the subject and predicate nodes,
requiring the former to be processed next by the
position of the ?. Construction of only weakly
specified tree relations (unfixed nodes) can also be
induced, characterised only as dominance by some
current node, with subsequent update required. In-
dividual lexical items also provide procedures for
building structure in the form of lexical actions,
inducing both nodes and annotations. For exam-
ple, in the update from (2) to (3), the set of lexical
actions for the word see is applied, yielding the
predicate subtree and its annotations. Thus partial
trees grow incrementally, driven by procedures as-
sociated with particular words as they are encoun-
tered.
Requirements embody structural predictions as
mentioned earlier. Thus unlike the conven-
tional bottom-up parsing,2 the DS model takes
the parser/generator to entertain some predicted
goal(s) to be reached eventually at any stage of
processing, and this is precisely what makes the
formalism incremental. This is the characteri-
sation of incrementality adopted by some psy-
cholinguists under the appellation of connected-
ness (Sturt and Crocker, 1996; Costa et al, 2002):
an encountered word always gets ?connected? to a
larger, predicted, tree.
Individual DS trees consist of predicates and
their arguments. Complex structures are obtained
via a general tree-adjunction operation licensing
the construction of so-called LINKed trees, pairs
of trees where sharing of information occurs. In
its simplest form this mechanism is the same one
which provides the potential for compiling in-
2The examples in (1)-(8) also suggest the implausibility
of purely bottom-up or head-driven parsing being adopted di-
rectly, because such strategies involve waiting until all the
daughters are gathered before moving on to their projection.
In fact, the parsing strategy adopted by DS is somewhat sim-
ilar to mixed parsing strategies like the left-corner or Earley
algorithm to a degree. These parsing strategic issues are more
fully discussed in Sato (forthcmg).
77
A consultant, a friend of Jo?s, is retiring: Ty(t), Retire?((?, x, Consultant?(x) ? Friend?(Jo?)(x)))
Ty(e), (?, x, Consultant?(x) ? Friend?(Jo?)(x)) Ty(e? t), Retire?
Ty(e), (?, x, Friend?(Jo?)(x))
Ty(cn), (x, Friend?(Jo?)(x))
x Friend?(Jo?)
Jo? Friend?
Ty(cn? e), ?P.?, P
Figure 3: Apposition in DS
terpretation for apposition constructions as can
be seen in Figure (3)3. The assumption in the
construction of such LINKed structures is that at
any arbitrary stage of development, some type-
complete subtree may constitute the context for
the subsequent parsing of the following string as
an adjunct structure candidate for incorporation
into the primary tree, hence the obligatory sharing
of information in the resulting semantic represen-
tation.
More generally, context in DS is defined as the
storage of parse states, i.e., the storing of par-
tial tree, word sequence parsed to date, plus the
actions used in building up the partial tree. For-
mally, a parse state P is defined as a set of triples
?T, W, A?, where: T is a (possibly partial) tree;
W is the associated sequence of words; A is the
associated sequence of lexical and computational
actions. At any point in the parsing process, the
context C for a particular partial tree T in the set
P can be taken to consist of: a set of triples P ? =
{. . . , ?Ti, Wi, Ai?, . . .} resulting from the previ-
ous sentence(s); and the triple ?T, W, A? itself,
the subtree currently being processed. Anaphora
and ellipsis construal generally involve re-use of
formulae, structures, and actions from the set C.
Grammaticality of a string of words is then de-
fined relative to its context C, a string being well-
formed iff there is a mapping from string onto
completed tree with no outstanding requirements
given the monotonic processing of that string rela-
tive to context. All fragments illustrated above are
processed by means of either extending the current
3Epsilon terms, like ?, x, Consultant?(x), stand for wit-
nesses of existentially quantified formulae in the epsilon cal-
culus and represent the semantic content of indefinites in DS.
Defined relative to the equivalence ?(?, x, ?(x)) = ?x?(x),
their defining property is their reflection of their contain-
ing environment, and accordingly they are particularly well-
suited to expressing the growth of terms secured by such ap-
positional devices.
tree, or constructing LINKed structures and trans-
fer of information among them so that one tree
provides the context for another, and are licensed
as wellformed relative to that context. In particu-
lar, fragments like the doctor in (8) are licensed by
the grammar because they occur at a stage in pro-
cessing at which the context contains an appropri-
ate structure within which they can be integrated.
The definite NP is taken as an anaphoric device,
relying on a substitution process from the context
of the partial tree to which the node it decorates is
LINKed to achieve the appropriate construal and
tree-update:
(11) The?parse? tree licensing production of the
doctor: LINK adjunction
?Ty(t)
Chorlton? ?Ty(e? t)
(Doctor?(Chorlton?)),?
3 Bidirectionality in DS
Crucially, for our current concern, this architec-
ture allows a dialogue model in which generation
and parsing function in parallel, following exactly
the same procedure in the same order. See Fig (2)
for a (simplified) display of the transitions manip-
ulated by a parse of Bob saw Mary, as each word
is processed and integrated to reach the complete
tree. Generation of this utterance from a complete
tree follows precisely the same actions and trees
from left to right, although the complete tree is
available from the start (this is why the complete
tree is marked ?0? for generation): in this case the
eventual message is known by the speaker, though
of course not by the hearer. What generation in-
volves in addition to the parse steps is reference
78
to this complete tree to check whether each pu-
tative step is consistent with it in order not to be
deviated from the legitimate course of action, that
is, a subsumption check. The trees (1-3) are li-
censed because each of these subsumes (4). Each
time then the generator applies a lexical action, it
is licensed to produce the word that carries that ac-
tion under successful subsumption check: at Step
(3), for example, the generator processes the lex-
ical action which results in the annotation ?See??,
and upon success and subsumption of (4) license
to generate the word see at that point ensues.
For split utterances, two more assumptions are
pertinent. On the one hand, speakers may have
initially only a partial structure to convey: this is
unproblematic, as all that is required by the for-
malism is monotonicity of tree growth, the check
being one of subsumption which can be carried
out on partial trees as well. On the other hand,
the utterance plan may change, even within a sin-
gle speaker. Extensions and clarifications in DS
can be straightforwardly generated by appending
a LINKed structure projecting the added material
to be conveyed (preserving the monotonicity con-
straint)4.
(12) I?m going home, with my brother, maybe
with his wife.
Such a model under which the speaker and
hearer essentially follow the same sets of actions,
updating incrementally their semantic representa-
tions, allows the hearer to ?mirror? the same series
of partial trees, albeit not knowing in advance what
the content of the unspecified nodes will be.
4 Parser/generator implementation
The process-integral nature of DS emphasised
thus far lends itself to the straightforward imple-
mentation of a parsing/generating system, since
the ?actions? defined in the grammar directly pro-
vide a major part of its implementation. By now it
should also be clear that the DS formalism is fully
bi-directional, not only in the sense that the same
grammar can be used for generation and parsing,
but also because the two sets of activities, conven-
tionally treated as ?reverse? processes, are mod-
elled to run in parallel. Therefore, not only can the
same sets of actions be used for both processes,
4Revisions however will involve shifting to a previous
partial tree as the newly selected context: I?m going home,
to my brother, sorry my mother.
but also a large part of the parsing and generation
algorithms can be shared.
This design architecture and a prototype im-
plementation are outlined in (Purver and Otsuka,
2003), and the effort is under way to scale up the
DS parsing/generating system incorporating the
results in (Gargett et al, 2008; Gregoromichelaki
et al, to appear).5 The parser starts from the axiom
(step 0 in Fig.2), which ?predicts? a proposition to
be built, and follows the applicable actions, lexi-
cal or general, to develop a complete tree. Now,
as has been described in this paper, the genera-
tor follows exactly the same steps: the axiom is
developed through successive updates into a com-
plete tree. The only material difference from ?
or rather in addition to? parsing is the complete
tree (Step 0(gen)/4), given from the very start of
the generation task, which is then referred to at
each tree update for subsumption check. The main
point is that despite the obvious difference in their
purposes ?outputting a string from a meaning ver-
sus outputting a meaning from a string? parsing
and generation indeed share the direction of pro-
cessing in DS. Moreover, as no intervening level
of syntactic structure over the string is ever com-
puted, the parsing/generation tasks are more effi-
ciently incremental in that semantic interpretation
is directly imposed at each stage of lexical integra-
tion, irrespective of whether some given partially
developed constituent is complete.
To clarify, see the pseudocode in the Prolog
format below, which is a close analogue of the
implemented function that both does parsing and
generation of a word (context manipulation is
ignored here for reasons of space). The plus
and minus signs attached to a variable indicate it
must/needn?t be instantiated, respectively. In ef-
fect, the former corresponds to the input, the latter
to the output.
(13) parse gen word(
+OldMeaning,?Word,?NewMeaning):-
apply lexical actions(+OldMeaning, ?Word,
+LexActions, ?IntermediateMeaning ),
apply computational actions(
+IntermediateMeaning, +CompActions,
?NewMeaning )
OldMeaning is an obligatory input item, which
corresponds to the semantic structure con-
structed so far (which might be just structural
tree information initially before any lexical
5The preliminary results are described in (Sato,
forthcmg).
79
input has been processed thus advocating a
strong predictive element even compared to
(Sturt and Crocker, 1996). Now notice that
the other two variables ?corresponding to the
word and the new (post-word) meaning? may
function either as the input or output. More
precisely, this is intended to be a shorthand
for either (+OldMeaning,+Word,?NewMeaning)
i.e. Word as input and NewMeaning as out-
put, or (+OldMeaning,?Word,+NewMeaning), i.e.
NewMeaning as input and Word as output, to repeat,
the former corresponding to parsing and the latter
to generation.
In either case, the same set of two sub-
procedures, the two kinds of actions described in
(13), are applied sequentially to process the input
to produce the output. These procedures corre-
spond to an incremental ?update? from one par-
tial tree to another, through a word. The whole
function is then recursively applied to exhaust the
words in the string, from left to right, either in
parsing or generation. Thus there is no differ-
ence between the two in the order of procedures
to be applied, or words to be processed. Thus it is
a mere switch of input/output that shifts between
parsing and generation.6
4.1 Split utterances in Dynamic Syntax
Split utterances follow as an immediate conse-
quence of these assumptions. For the dialogues in
(1)-(8), therefore, while A reaches a partial tree of
what she has uttered through successive updates
as described above, B as the hearer, will follow
the same updates to reach the same representation
of what he has heard. This provides him with the
ability at any stage to become the speaker, inter-
rupting to continue A?s utterance, repair, ask for
clarification, reformulate, or provide a correction,
as and when necessary7. According to our model
of dialogue, repeating or extending a constituent
of A?s utterance by B is licensed only if B, the
hearer turned now speaker, entertains a message
6Thus the parsing procedure is dictated by the grammar to
a large extent, but importantly, not completely. More specif-
ically, the grammar formalism specifies the state paths them-
selves, but not how the paths should be searched. The DS ac-
tions are defined in conditional terms, i.e. what to do as and
when a certain condition holds. If a number of actions can be
applied at some point during a parse, i.e. locally ambiguity
is encountered, then it is up to a particular implementation
of the parser to decide which should be traversed first. The
current implementation includes suggestions of search strate-
gies.
7The account extends the implementation reported in
(Purver et al, 2006)
to be conveyed that matches or extends the parse
tree of what he has heard in a monotonic fashion.
In DS, this message is a semantic representation
in tree format and its presence allows B to only ut-
ter the relevant subpart of A?s intended utterance.
Indeed, this update is what B is seeking to clarify,
extend or acknowledge. In DS, B can reuse the
already constructed (partial) parse tree in his con-
text, rather than having to rebuild an entire propo-
sitional tree or subtree.
The fact that the parsing formalism integrates
a strong element of predictivity, i.e. the parser
is always one step ahead from the lexical in-
put, allows a straightforward switch from pars-
ing to generation thus resulting in an explana-
tion of the facility with which split utterances oc-
cur (even without explicit reasoning processes).
Moreover, on the one hand, because of incremen-
tality, the issue of interpretation-selection can be
faced at any point in the process, with correc-
tions/acknowledgements etc. able to be provided
at any point; this results in the potential exponen-
tial explosion of interpretations being kept firmly
in check. And, structurally, such fragments can
be integrated in the current partial tree represen-
tation only (given the position of the pointer) so
there is no structural ambiguity multiplication. On
the other hand, for any one of these intermedi-
ate check points, bidirectionality entails that con-
sistency checking remains internal to the individ-
ual interlocutors? system, the fact of their mir-
roring each other resulting at their being at the
same point of tree growth. This is sufficient to en-
sure that any inconsistency with their own parse
recognised by one party as grounds for correc-
tion/repair can be processed AS a correction/repair
by the other party without requiring any additional
metarepresentation of their interlocutors? informa-
tion state (at least for these purposes). This allows
the possibility of building up apparently complex
assumptions of shared content, without any neces-
sity of constructing hypotheses of what is enter-
tained by the other, since all context-based selec-
tions are based on the context of the interlocutor
themselves. This, in its turn, opens up the possi-
bility of hearers constructing interpretations based
on selections made that transparently violate what
is knowledge shared by both parties, for no pre-
sumption of common ground is essential as input
to the interpretation process (see, e.g. (9)-(10)).
80
5 Conclusion
It is notable that, from this perspective, no pre-
sumption of common ground or hypothesis as to
what the speaker could have intended is necessary
to determine how the hearer selects interpretation.
All that is required is a concept of system-internal
consistency checking, the potential for clarifica-
tion in cases of uncertainty, and reliance at such
points on disambiguation/correction/repair by the
other party. The advantage of such a proposal, we
suggest, is the provision of a fully mechanistic ac-
count for disambiguation (cf. (Pickering and Gar-
rod, 2004)). The consequence of such an analysis
is that language use is essentially interactive (see
also (Ginzburg, forthcmg; Clark, 1996)): the only
constraint as to whether some hypothesised in-
terpretation assigned by either party is confirmed
turns on whether it is acknowledged or corrected
(see also (Healey, 2008)).
Acknowledgements
This work was supported by grants ESRC RES-062-23-0962,
the EU ITALK project (FP7-214668) and Leverhulme F07-
04OU. We are grateful for comments to: Robin Cooper, Alex
Davies, Arash Eshghi, Jonathan Ginzburg, Pat Healey, Greg
James Mills. Normal disclaimers apply.
References
Patrick Blackburn and Wilfried Meyer-Viol. 1994.
Linguistics, logic and finite trees. Bulletin of the
IGPL, 2:3?31.
Susan E. Brennan and Herbert H. Clark. 1996. Con-
ceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning,
Memory and Cognition, 22:482?1493.
Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005.
The Dynamics of Language. Elsevier, Oxford.
Robyn Carston. 2002. Thoughts and Utterances: The
Pragmatics of Explicit Communication. Blackwell.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,
Patrick Sturt, and Giovanni Soda. 2002. Enhanc-
ing first-pass attachment prediction. In ECAI 2002:
508-512.
Raquel Ferna?ndez. 2006. Non-Sentential Utterances
in Dialogue: Classification, Resolution and Use.
Ph.D. thesis, King?s College London, University of
London.
Andrew Gargett, Eleni Gregoromichelaki, Chris
Howes, and Yo Sato. 2008. Dialogue-grammar cor-
respondence in dynamic syntax. In Proceedings of
the 12th SEMDIAL (LONDIAL).
Jonathan Ginzburg and Robin Cooper. 2004. Clarifi-
cation, ellipsis, and the nature of contextual updates
in dialogue. Linguistics and Philosophy, 27(3):297?
365.
Jonathan Ginzburg. forthcmg. Semantics for Conver-
sation. CSLI.
Eleni Gregoromichelaki, Yo Sato, Ruth Kempson, An-
drew Gargett, and Christine Howes. to appear. Dia-
logue modelling and the remit of core grammar. In
Proceedings of IWCS 2009.
Patrick Healey. 2008. Interactive misalignment: The
role of repair in the development of group sub-
languages. In R. Cooper and R. Kempson, editors,
Language in Flux. College Publications.
Christine Howes, Patrick G. T. Healey, and Gregory
Mills. in prep. a: An experimental investigation
into. . . b: . . . split utterances.
Ruth Kempson, Wilfried Meyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow of Language Un-
derstanding. Blackwell.
Gregory J. Mills. 2007. Semantic co-ordination in di-
alogue: the role of direct interaction. Ph.D. thesis,
Queen Mary University of London.
Martin Pickering and Simon Garrod. 2004. Toward
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences.
Massimo Poesio and Hannes Rieser. 2008. Comple-
tions, coordination, and alignment in dialogue. Ms.
Matthew Purver and Masayuki Otsuka. 2003. Incre-
mental generation by incremental parsing: Tactical
generation in Dynamic Syntax. In Proceedings of
the 9th European Workshop in Natural Language
Generation (ENLG), pages 79?86.
Matthew Purver, Ronnie Cann, and Ruth Kempson.
2006. Grammars as parsers: Meeting the dialogue
challenge. Research on Language and Computa-
tion, 4(2-3):289?326.
Matthew Purver. 2004. The Theory and Use of Clari-
fication Requests in Dialogue. Ph.D. thesis, Univer-
sity of London, forthcoming.
Yo Sato. forthcmg. Local ambiguity, search strate-
gies and parsing in dynamic syntax. In Eleni Gre-
goromichelaki and Ruth Kempson, editors, Dynamic
Syntax: Collected Papers. CSLI.
Dan Sperber and Deirdre Wilson. 1986. Relevance:
Communication and Cognition. Blackwell.
Patrick Sturt and Matthew Crocker. 1996. Monotonic
syntactic processing: a cross-linguistic study of at-
tachment and reanalysis. Language and Cognitive
Processes, 11:448?494.
81
