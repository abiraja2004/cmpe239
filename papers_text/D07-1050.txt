Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 477?485, Prague, June 2007. c?2007 Association for Computational Linguistics
Word Sense Disambiguation
Incorporating Lexical and Structural Semantic Information
Takaaki Tanaka? Francis Bond? Timothy Baldwin? Sanae Fujita? Chikara Hashimoto?
? {takaaki, sanae}@cslab.kecl.ntt.co.jp ? bond@nict.go.jp
? tim@csse.unimelb.edu.au ? ch@yz.yamagata-u.ac.jp
? NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation
? National Institute of Information and Communications Technology
? The University of Melbourne ? Yamagata University
Abstract
We present results that show that incorporat-
ing lexical and structural semantic informa-
tion is effective for word sense disambigua-
tion. We evaluated the method by using pre-
cise information from a large treebank and
an ontology automatically created from dic-
tionary sentences. Exploiting rich semantic
and structural information improves preci-
sion 2?3%. The most gains are seen with
verbs, with an improvement of 5.7% over a
model using only bag of words and n-gram
features.
1 Introduction
Recently, significant improvements have been made
in combining symbolic and statistical approaches
to various natural language processing tasks. In
parsing, for example, symbolic grammars are be-
ing combined with stochastic models (Riezler et al,
2002; Oepen et al, 2002; Malouf and van Noord,
2004). Statistical techniques have also been shown
to be useful for word sense disambiguation (Steven-
son, 2003). However, to date, there have been
few combinations of sense information together with
symbolic grammars and statistical models. Klein
and Manning (2003) show that much of the gain in
statistical parsing using lexicalized models comes
from the use of a small set of function words.
Features based on general relations provide little
improvement, presumably because the data is too
sparse: in the Penn treebank normally used to train
and test statistical parsers stocks and skyrocket never
appear together. They note that this should motivate
the use of similarity and/or class based approaches:
the superordinate concepts capital (? stocks) and
move upward (? sky rocket) frequently appear to-
gether. However, there has been little success in this
area to date. For example, Xiong et al (2005) use se-
mantic knowledge to parse Chinese, but gain only a
marginal improvement. Focusing on WSD, Steven-
son (2003) and others have shown that the use of
syntactic information (predicate-argument relations)
improve the quality of word sense disambiguation
(WSD). McCarthy and Carroll (2003) have shown
the effectiveness of the selectional preference infor-
mation for WSD. However, there is still little work
on combining WSD and parse selection.
We hypothesize that one of the reasons for the
lack of success is that there has been no resource
annotated with both syntactic (or structural seman-
tic information) and lexical semantic information.
For English, there is the SemCor corpus (Fellbaum,
1998) which is annotated with parse trees and Word-
Net senses, but it is fairly small, and does not ex-
plicitly include any structural semantic information.
Therefore, we decided to construct and use a tree-
bank with both syntactic information (e.g. HPSG
parses) and lexical semantic information (e.g. sense
tags): the Hinoki treebank (Bond et al, 2004). This
can be used to train word sense disambiguation and
parse ranking models using both syntactic and lexi-
cal semantic features. In this paper, we discuss only
word sense disambiguation. Parse ranking is dis-
cussed in Fujita et al (2007).
2 The Hinoki Corpus
The Hinoki corpus consists of the Lexeed Seman-
tic Database of Japanese (Kasahara et al, 2004) and
corpora annotated with syntactic and semantic infor-
477
mation.
2.1 Lexeed
Lexeed is a database built from on a dictionary,
which defines word senses used in the Hinoki cor-
pus and has around 49,000 dictionary definition sen-
tences and 46,000 example sentences which are syn-
tactically and semantically annotated. Lexeed con-
sists of all words with a familiarity greater than or
equal to five on a scale of one to seven. This gives
a fundamental vocabulary of 28,000 words, divided
into 46,347 different senses. Each sense has a defi-
nition sentence and example sentence written using
only these 28,000 familiar words (and some function
words). Many senses have more than one sentence
in the definition: there are 75,000 defining sentences
in all.
A (simplified) example of the entry for?U3 un-
tenshu ?chauffeur? is given in Figure 1. Each word
contains the word itself, its part of speech (POS) and
lexical type(s) in the grammar, and the familiarity
score. Each sense then contains definition and ex-
ample sentences, links to other senses in the lexicon
(such as hypernym), and links to other resources,
such as the Goi-Taikei (Ikehara et al, 1997) and
WordNet (Fellbaum, 1998). Each content word in
the definition and example sentences is annotated
with sense tags from the same lexicon.
2.2 Lexical Semantics Annotation
The lexical semantic annotation uses the sense in-
ventory from Lexeed. All words in the fundamental
vocabulary are tagged with their sense. For example,
the word d& ookii ?big? (in ookiku naru ?grow
up?) is tagged as sense 5 in the example sentence
(Figure 1), with the meaning ?elder, older?.
Each word was annotated by five annotators. We
use the majority choice in case of disagreements
(Tanaka et al, 2006). Inter-annotator agreements
among the five annotators range from 78.7% to
83.3%: the lowest agreement is for the Lexeed def-
inition sentences and the highest is for Kyoto cor-
pus (newspaper text). These agreements reflect the
difficulties in disambiguating word sense over each
corpus and can be considered as the upper bound of
precision for WSD.
Table 1 shows the distribution of word senses ac-
cording to the word familiarity in Lexeed.
Fam #Words
Poly-
semous #WS
#Mono-
semous(%)
6.5 - 368 182 4.0 186 (50.5)
6.0 - 4,445 1,902 3.4 2,543 (57.2)
5.5 - 9,814 3,502 2.7 6,312 (64.3)
5.0 - 11,430 3,457 2.5 7,973 (69.8)
Table 1: Word Senses in Lexeed
2.3 Ontology
The Hinoki corpus comes with an ontology semi-
automatically constructed from the parse results of
definitions in Lexeed (Nichols and Bond, 2005). The
ontology includes more than 80 thousand relation-
ships between word senses, e.g. synonym, hyper-
nym, abbreviation, etc. The hypernym relation for
?U3 untenshu ?chauffeur? is shown in Figure 1.
Hypernym or synonym relations exist for almost all
content words.
2.4 Thesaurus
As part of the ontology verification, all nominal and
most verbal word senses in Lexeed were linked to
semantic classes in the Japanese thesaurus, Nihongo
Goi-Taikei (Ikehara et al, 1997). These were then
hand verified. Goi-Taikei has about 400,000 words
including proper nouns, most nouns are classified
into about 2,700 semantic classes. These seman-
tic classes are arranged in a hierarchical structure
(11 levels). The Goi-Taikei Semantic Class for ?
U3 untenshu ?chauffeur? is shown in Figure 1:
?C292:driver? at level 9 which is subordinate to
?C4:person?.
2.5 Syntactic and Structural Semantics
Annotation
Syntactic annotation is done by selecting the best
parse (or parses) from the full analyses derived by
a broad-coverage precision grammar. The gram-
mar is an HPSG implementation (JACY: Siegel and
Bender, 2002), which provides a high level of de-
tail, marking not only dependency and constituent
structure but also detailed semantic relations. As the
grammar is based on a monostratal theory of gram-
mar (HPSG: Pollard and Sag, 1994) it is possible
to simultaneously annotate syntactic and semantic
structure without overburdening the annotator. Us-
ing a grammar enforces treebank consistency ? all
sentences annotated are guaranteed to have well-
478
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
INDEX ?U3 untenshu
POS noun
LEX-TYPE noun-lex
FAMILIARITY 6.2 [1?7] (? 5)
SENSE 1
?
?
?
?
?
?
?
?
?
?
?
?
DEFINITION
[
\1 ??1 k?U1 2d04 a person who drives trains and cars
]
EXAMPLE
[
d&(5 C<8b\1 G?U31 Dod6 G%?3 2
I dream of growing up and becoming a train driver
]
HYPERNYM 04 hito ?person?
SEM. CLASS ?292:driver? (? ?4:person?)
WORDNET motorman1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Dictionary Entry for ?U31 untenshu ?chauffeur?
formed parses. The flip side to this is that any sen-
tences which the parser cannot parse remain unan-
notated, at least unless we were to fall back on full
manual mark-up of their analyses. The actual anno-
tation process uses the same tools as the Redwoods
treebank of English (Oepen et al, 2002).
There were 4 parses for the definition sentence
shown in Figure 1. The correct parse, shown as a
phrase structure tree, is shown in Figure 2. The two
sources of ambiguity are the conjunction and the rel-
ative clause. The parser also allows the conjunction
to join to \ densha and 0 hito. In Japanese, rel-
ative clauses can have gapped and non-gapped read-
ings. In the gapped reading (selected here), 0 hito
is the subject of ?U unten ?drive?. In the non-
gapped reading there is some underspecified relation
between the thing and the verb phrase. This is sim-
ilar to the difference in the two readings of the day
he knew in English: ?the day that he knew about?
(gapped) vs ?the day on which he knew (some-
thing)? (non-gapped). Such semantic ambiguity is
resolved by selecting the correct derivation tree that
includes the applied rules in building the tree.
The parse results can be automatically given by
the HPSG parser PET (Callmeier, 2000) with the
Japanese grammar JACY. The current parse ranking
model has an accuracy of 70%: the correct tree is
ranked first 70% of the time (for Lexeed definition
sentences) (Fujita et al, 2007).
The full parse is an HPSG sign, containing both
syntactic and semantic information. A view of the
semantic information is given in Figure 31.
1The specific meaning representation language used in
UTTERANCE
NP
VP N
PP V
NP
PP
N CONJ N CASE-P V V
\ ? ? k ?U 2d 0
densha ya jidousha o unten suru hito
train or car ACC drive do person
?U31 ?chauffeur?: ?a person who drives a train or car?
Figure 2: Syntactic View of the Definition of ?U
31 untenshu ?chauffeur?
The semantic view shows some ambiguity has
been resolved that is not visible in the purely syn-
tactic view.
The semantic view can be further simplified into a
dependency representation, further abstracting away
from quantification, as shown in Figure 4. One of
the advantages of the HPSG sign is that it contains
all this information, making it possible to extract the
particular view needed. In order to make linking to
other resources (such as the sense annotation) easier,
predicates are labeled with pointers back to their po-
sition in the original surface string. For example, the
predicate densha n 1 links to the surface characters
between positions 0 and 3: \.
JACY is Minimal Recursion Semantics (Copestake et al, 2005).
479
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TEXT \??k?U2d0
TOP h1
RELS
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
proposition m rel
LBL h1
ARG0 e2
MARG h3
?
?
?
?
?
?
unknown rel
LBL h4
ARG0 e2
ARG x5
?
?
?
?
?
densha n
LBL h6
ARG0 x7
?
?
?
?
?
?
?
udef rel
LBL h8
ARG0 x7
RSTR h9
BODY h10
?
?
?
?
?
?
?
?
?
?
ya p
LBL h11
ARG0 x13
L-INDEX x7
R-INDEX x12
?
?
?
?
?
?
?
?
?
?
udef rel
LBL h15
ARG0 x12
RSTR h16
BODY h17
?
?
?
?
?
?
?
jidousha n
LBL h18
ARG0 x12
?
?
?
?
?
?
?
udef rel
LBL h19
ARG0 x12
RSTR h20
BODY h21
?
?
?
?
?
?
?
?
?
?
unten s
LBL h22
ARG0 e23 tense=present
ARG1 x5
ARG2 x13
?
?
?
?
?
?
?
hito n
LBL h24
ARG0 x5
?
?
?
?
?
?
?
udef rel
LBL h25
ARG0 x5
RSTR h26
BODY h27
?
?
?
?
?
?
?
?
proposition m rel
LBL h10001
ARG0 e23 tense=present
MARG h28
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
HCONS {h3 qeq h4,h9 qeq h6,h16 qeq h11,h20 qeq h18,h26 qeq h24,h28 qeq h22}
ING {h24 ing h10001}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 3: Semantic View of the Definition of ?U31 untenshu ?chauffeur?
_1:proposition_m<0:13>[MARG e2:unknown]
e2:unknown<0:13>[ARG x5:_hito_n]
x7:udef<0:3>[]
x7:densha_n_1<0:3>
x12:udef<4:7>[]
x12:_jidousha_n<4:7>
x13:_ya_p_conj<0:4>[L-INDEX x7:_densha_n_1, R-INDEX x12:_jidousha_n]
e23:_unten_s_2<8:10>[ARG1 x5:_hito_n, ARG2 x13:_ya_p_conj]
x5:udef<12:13>[]
_2:proposition_m<0:13>[MARG e23:_unten_s_2]
Figure 4: Dependency View of the Definition of ?U31 untenshu ?chauffeur?
3 Task
We define the task in this paper as ?allocating the
word sense tags for all content words included in
Lexeed as headwords, in each input sentence?. This
task is a kind of all-words task, however, a unique
point is that we focus on fundamental vocabulary
(basic words) in Lexeed and ignore other words. We
use Lexeed as the sense inventory. There are two
problems in resolving the task: how to build the
model and how to assign the word sense by using
the model for disambiguating the senses. We de-
scribe the word sense selection model we use in sec-
tion 4 and the method of word sense assignment in
section 5.
4 Word Sense Selection Model
All content words (i.e. basic words) in Lexeed are
classified into six groups by part-of-speech: noun,
verb, verbal noun, adjective, adverb, others. We
treat the first five groups as targets of disambiguat-
ing senses. We build five words sense models corre-
sponding to these groups. A model contains senses
for various words, however, features for a word are
discriminated from those for other words so that the
senses irrelevant to a target word are not selected.
For example, an n-gram feature following a target
word ?has-a-tail? for dog is distinct from that for cat.
In the remainder of this section, we describe the
features used in the word sense disambiguation.
First we used simple n-gram collocations, then a bag
of words of all words occurring in the sentence. This
was then enhanced by using ontological information
and predicate argument relations.
4.1 Word Collocations
Word collocations (WORD-Col) are basic and effec-
tive cues for WSD. They can be modelled by n-
gram and bag of words features, which are easily
extracted from a corpus. We used all unigrams, bi-
grams and trigrams which precede and follow the
target words (N-gram) and all content words in the
sentences where the target words occur (BOW).
480
# sample features
C1 ?COLWS:04?
C2 ?COLWSSC:C33:other person?
C3 ?COLWSHYP:0/1?
C4 ?COLWSHYPSC:C5:person?
C1 ?COLWS:\1?
C2 ?COLWSSC:C988:land vehicle?
C3 ?COLWSHYP:?1?
C4 ?COLWSHYPSC:C988:land vehicle?
C1 ?COLWS:?1?
C2 ?COLWSSC:C988:land vehicle?
C3 ?COLWSHYP:2?
C4 ?COLWSHYPSC:C988:land vehicle?
Table 2: Example semantic collocation features
(SEM-Col) extracted from the word sense tagged cor-
pus and the dictionary (Lexeed and GoiTaikei) and
the ontology which have the word senses and the se-
mantic classes linked to the semantic tags. The first
column numbers the feature template corresponding
to each example.
4.2 Semantic Features
We use the semantic information (sense tags and on-
tologies) in two ways. One is to enhance the collo-
cations and the other is to enhance dependency rela-
tions.
4.2.1 Semantic Collocations
Word surface features like N-gram and BOW in-
evitably suffer from data sparseness, therefore, we
generalize them to more abstract words or concepts
and also consider words having the same mean-
ings. We used the ontology described in Sec-
tion 2.3 to get hypernyms and synonyms and the
Goi-Taikei thesaurus to abstract the words to the se-
mantic classes. The superordinate classes at level
3, 4 and 5 are also added in addition to the original
semantic class. For example, \ densha ?train?
and ? jidousha ?automobile? are both gener-
alized to the semantic class ?C988:land vehicle?
(level 7). The superordinate classes are also used:
?C706:inanimate? (level 3), ?C760:artifact?
(level 4) and ?C986:vehicle? (level 5).
4.2.2 Semantic Dependencies
The semantic dependency features are based on
a predicate and its arguments taken from the ele-
mentary dependencies. For example, consider the
semantic dependency representation for densha ya
# sample features for ?U2d1
D1 ?PRED:?U2d, ARG1:0?
D1 ?PRED:?U2d, ARG2:\?
D1 ?PRED:?U2d, ARG2:??
D2 ?PRED:?U2d, ARG1:04?
D2 ?PRED:?U2d, ARG2:\1?
D2 ?PRED:?U2d, ARG2:?1?
D3 ?PRED:?U2d, ARG1SC:C33?
D3 ?PRED:?U2d, ARG2SC:C988?
D4 ?PRED:?U2d, ARG2SYN:???1?
D5 ?PRED:?U2d, ARG1HYP:0/1?
D5 ?PRED:?U2d, ARG2HYP:?1?
D5 ?PRED:?U2d, ARG2HYP:2?
D6 ?PRED:?U2d, ARG1HYPSC:C5?
D6 ?PRED:?U2d, ARG2HYPSC:C988?
D11 ?PRED:?U2d, ARG1:0, ARG2:\?
D22 ?PRED:?U2d, ARG1:04, ARG2:\1?
D23 ?PRED:?U2d, ARG1:04, ARG2:C1460 ?
D24 ?PRED:?U2d, ARG1:04, ARG2SYN:???1?
D32 ?PRED:?U2d, ARG1:C5, ARG2:\1?
D33 ?PRED:?U2d, ARG1:C5, ARG2:C988?
D55 ?PRED:?U2d, ARG1HYP:0/4, ARG2HYP:?1?
D56 ?PRED:?U2d, ARG1HYP:0/4, ARG2HYPSC:C988?
D65 ?PRED:?U2d, ARG1HYPSC:C5 , ARG2HYP:?1?
D322 ?PRED:C2003, ARG1:04, ARG2:\1?
Table 3: Example semantic features extracted from
the dependency tree in Figure 4. The first column
numbers the feature template corresponding to each
example.
jidousha-wo unten suru hito ?a person who drives a
train or car? given in Figure 4. The predicate un-
ten ?drive?, has two arguments: ARG1 hito ?person?
and ARG2 ya ?or?. The coordinate conjunction is
expanded out into its children, giving ARG2 densha
?train? and jidousha ?automobile?.
From these, we produce several features, a sam-
ple of them are shown in Table 3. One has all argu-
ments and their labels (D11). We also produce var-
ious back offs, for example the predicate with only
one argument at a time (D1-D3). Each combination
of predicate and its related argument(s) becomes a
feature.
For the next class of features, we used the sense
information from the corpus combined with the se-
mantic classes in the dictionary to replace each pred-
481
icate by its disambiguated sense, its hypernym, its
synonym (if any) and its semantic class. The seman-
tic classes for\1 and?1 are both ?988:land
vehicle?, while ?U1 is ?2003:motion? and 04
is ?4:human?. We also expand ?1 into its syn-
onym ???1 mo?ta?ka? ?motor car?.
The semantic class features provide a seman-
tic smoothing, as words are binned into the 2,700
classes. The hypernym/synonym features provide
even more smoothing. Both have the effect of mak-
ing more training data available for the disambigua-
tor.
4.3 Domain
Domain information is a simple and sometimes
strong cue for disambiguating the target words
(Gliozzo et al, 2005). For instance, the sense of
the word ?record? is likey to be different in the mu-
sical context, which is recalled by domain-specific
words like ?orchestra?, ?guitar?, than in the sport-
ing context. We use 12 domain categories like ?cul-
ture/art?, ?sport?, etc. which are similar to ones used
in directory search web sites. About 6,000 words
are automatically classified into one of 12 domain
categories by distributions in web sites (Hashimoto
and Kurohashi, 2007) and 10% of them are manually
checked. Polysemous words which belong to multi-
ple domains and neutral words are not classified into
any domain.
5 Search Algorithm
The conditional probability of the word sense for
each word is given by the word sense selection
model described in Section 4. In the initial state,
some of the semantic features, e.g. semantic col-
locations (SEM-Col) and word sense extensions for
semantic dependencies (SEM-Dep) are not available,
since no word senses for polysemous words have
been determined. It is not practical to count all com-
binations of word senses for target words, therefore,
we first try to decide the sense for that word which
is most plausible among all the ambiguous words,
then, disambiguate the next word by using the sense.
We use the beam search algorithm, which is sim-
ilar to that used for decoder in statistical machine
translation (Watanabe, 2004), for finding the plausi-
ble combination of word sense tags.
The algorithm is described as follows. For a pol-
ysemous word set in an input sentence {w1, . . . ,wn},
twik is the k-th word sense of word wi, W is a set
having words to be disambiguated, T is a list of re-
solved word senses. A search node N is defined as
[W,T ] and a score of a node N, s(N) is defined as
the probability that the word sense set T occurs in
the context. The beam search can be done as fol-
lows (beam width is b):
1. Create an initial node N0 = [T0,W0] (T0 = {},
W0 = {}) and insert the node into an initial
queue Q0.
2. For each node N in the queue Q, do the follow-
ing steps.
? For each wi (? W ), create W ?i by picking
out wi from W
? Create new lists T ?1, . . . ,T ?l by adding one
of word sense candidates twi1,. . . ,twil for wi
to T
? Create new nodes [W ?i ,T ?0 ], . . . ,[W ?i ,T ?l ] and
insert them into the queue Q?
3. Sort the nodes in Q? by the score s(N)
4. If the top node W in the queue Q? is empty,
adopt T as the combination of word senses and
terminate. Otherwise, pick out the top b nodes
from Q? and insert them into new queue Q, then
go back to 2
6 Evaluation
We trained and tested on the Lexeed Dictionary Def-
inition (LXD-DEF) and Example sections (LXD-EX) of
the Hinoki corpus (Bond et al, 2007). These have
about 75,000 definition and 46,000 example sen-
tences respectively. Some 54,000 and 36,000 sen-
tences of them are treebanked, i.e., they have the
syntactic trees and structural semantic information.
We used these sentences with the complete informa-
tion and selected 1,000 sentences out of each sen-
tence class as test sets (LXD-DEFtest, LXD-EXtest), and
the remainder is combined and used as a training
set (LXD-ALL). We also tested 1,000 sentences from
the Kyoto Corpus of newspaper text (KYOTOtest).
These sentences have between 3.4 (LXD-EXtest) ? 5.2
(KYOTOtest) polysemous words per sentence on av-
erage.
482
We use a maximum entropy / minimum diver-
gence (MEMD) modeler to train the word sense se-
lection model. We use the open-source Maximun
Entropy Modeling Toolkit2 for training, determining
best-performing convergence thresholds and prior
sizes experimentally. The models for five differ-
ent POSs were trained with each training sets: the
base model is word collocation model (WORD-Col),
and the semantic models built by semantic colloca-
tion (SEM-Col), semantic dependency (SEM-Dep) or
domain with WORD-Col (+SEM-Col, +SEM-Dep and
+DOMAIN).
Figure 5: Learning Curve
7 Results and Discussion
Table 4 shows the precision as the results of the word
sense disambiguation on the combination of LXD-
DEF and LXD-EX (LXD-ALL). The baseline method
selects the senses occurring most frequently in the
training corpus. Each row indicates the results us-
ing the baseline, word collocation (WORD-Col), the
combinations of WORD-Col and one of the seman-
tic features (+SEM-Col, +SEM-Dep and +DOMAIN),
e.g, +SEM-Col gives the results using WORD-Col and
SEM-Col, and all features (FULL).
There are significant improvements over the base-
line and the other results on all corpora. Basic word
2http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
collocation features (WORD-Col) give a vast improve-
ment. Extending this by using the ontological in-
formation (+SEM-Col) gives a further improvement
over the WORD-Col. Adding the predicate-argument
relationships (+SEM-Dep) improves the results even
more.
Table 6 shows the statistics of the target corpora.
The best result of LXD-DEFtest (80.7%) surpasses the
inter-annotator agreement (78.7%) in building the
Hinoki Sensebank. However, there is a wide gap
between the best results of KYOTOtest (60.4%) and
the inter-annotator agreement (83.3%), this suggests
other information such as the semantic classes for
named entities (including proper nouns and multi-
word expressions (MWE)) and broader contexts are
required. However, a model built on dictionary sen-
tences lacks these features. Even, so there is some
improvement.
The domain features (+DOMAIN) give small con-
tribution to the precision, since only intra-sentence
context is counted in this experiment. Unfortunately
dictiory definition and example sentences do not re-
ally have a useful context. We expect broader con-
text should make the domain features more effective
for the newspaper text (e.g. as in Stevenson (2003)),
Table 5 shows comparison of results of different
POSs. The semantic features (+SEM-Col and +SEM-
Dep) are particularly effective for verb and also give
moderate improvements on the results of the other
POSs.
Figure 5 shows the precisions of LXD-DEFtest in
changing the size of a training corpus, which is di-
vided into five partitions. The precision is saturated
in using four partitions (264,000 tokens).
These results of the dictionary sentences are close
to the best published results for the SENSEVAL-2
task (79.3% by Murata et al (2003) using a com-
bination of simple Bayes learners). However, we
are using a different sense inventory (Lexeed not
Iwanami (Nishio et al, 1994)) and testing over a dif-
ferent corpus, so the results are not directly compa-
rable. In future work, we will test over SENSEVAL-
2 data so that we can compare directly.
None of the SENSEVAL-2 systems used onto-
logical information, despite the fact that the dic-
tionary definition sentences were made available,
and there are several algorithms describing how to
extract such information from MRDs (Tsurumaru
483
Model Test Baseline WORD-Col +SEM-Col +SEM-Dep +DOMAIN FULL
LXD-ALL LXD-DEFtest 72.8 78.4 79.8 80.2 78.1 80.7
LXD-EXtest 70.4 75.6 78.7 77.9 76.0 78.8
KYOTOtest 55.6 58.5 60.0 58.8 59.8 60.4
Table 4: The Precision of WSD
POS Baseline WORD-Col +SEM-Col +SEM-Dep +DOMAIN FULL
Noun 65.5 68.7 69.6 69.4 68.9 69.8
Verb 60.3 66.9 71.0 70.6 67.7 72.6
VN 72.6 76.2 77.7 74.6 77.6 77.5
Adj 59.9 67.2 69.5 68.9 68.9 69.5
Adv 74.4 78.6 79.8 79.2 78.6 79.8
Table 5: The Precision of WSD (per Part-of-Speech)
et al, 1991; Wilkes et al, 1996; Nichols et al, 2005).
We hypothesize that this is partly due to the way the
task is presented: there was not enough time to ex-
tract and debug an ontology as well as build a dis-
ambiguation system, and there was no ontology dis-
tributed. The CRL system (Murata et al, 2003) used
a syntactic dependency parser as one source of fea-
tures (KNP: Kurohashi and Nagao (2003)), remov-
ing it decreased performance by around 0.6%.
8 Conclusions
We used the Hinoki corpus to test the importance of
lexical and structural information in word sense dis-
ambiguation. We found that basic n-gram features
and collocations provided a great deal of useful in-
formation, but that better results could be gained by
using ontological information and semantic depen-
dencies.
Acknowledgements
We would like to thank the other members of the
NTT Natural Language Research Group NTT Com-
munication Science laboratories for their support.
We would also like to express gratitude to the re-
viewers for their valuable comments and Professor
Zeng Guangping, Wang Daliang and Shen Bin of
the University of Science and Technology Beijing
(USTB) for building the demo system.
References
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki
treebank: A treebank for text understanding. In Proceed-
ings of the First International Joint Conference on Natural
Language Processing (IJCNLP-04), pages 554?559. Hainan
Island.
Francis Bond, Sanae Fujita, and Takaaki Tanaka. 2007. The Hi-
noki syntactic and semantic treebank of Japanese. Language
Resources and Evaluation. (Special issue on Asian language
technology).
Ulrich Callmeier. 2000. PET - a platform for experimentation
with efficient HPSG processing techniques. Natural Lan-
guage Engineering, 6(1):99?108.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag.
2005. Minimal Recursion Semantics. An introduction. Re-
search on Language and Computation, 3(4):281?332.
Christine Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki
Tanaka. 2007. Exploiting semantic information for HPSG
parse selection. In ACL 2007 Workshop on Deep Linguistic
Processing, pages 25?32. Prague, Czech Republic.
Alfio Massimiliano Gliozzo, Claudio Giuliano, and Carlo
Strapparava. 2005. Domain kernels for word sense disam-
biguation. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL 2005). Ann
Arbor, U.S.
Chikara Hashimoto and Sadao Kurohashi. 2007. Construction
of domain dictionary for fundamental vocaburalry. In Pro-
ceedings of the ACL 2007 Main Conference Poster Sessions.
Association for Computational Linguistics, Prague, Czech
Republic.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lexicon:
Lexeed. In IPSG SIG: 2004-NLC-159, pages 75?82. Tokyo.
(in Japanese).
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Erhard Hinrichs and Dan Roth, edi-
tors, Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 423?430. URL
http://www.aclweb.org/anthology/P03-1054.pdf.
484
Corpus
Annotated
Tokens #WS
Agreement
token (type) %Other Sense %Homonym %MWE %Proper Noun
LXD-DEF 199,268 5.18 .787 (.850) 4.2 0.084 1.5 0.046
LXD-EX 126,966 5.00 .820 (.871) 2.3 0.035 0.4 0.0018
KYOTO 268,597 3.93 .833 (.828) 9.8 3.3 7.9 5.5
Table 6: Corpus Statistics
Sadao Kurohashi and Makoto Nagao. 2003. Building a
Japanese parsed corpus ? while improving the parsing sys-
tem. In Anne Abeille?, editor, Treebanks: Building and Using
Parsed Corpora, chapter 14, pages 249?260. Kluwer Aca-
demic Publishers.
Robert Malouf and Gertjan van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars. In
IJCNLP-04 Workshop: Beyond shallow analyses - For-
malisms and statistical modeling for deep analyses. JST
CREST. URL http://www-tsujii.is.s.u-tokyo.ac.
jp/bsa/papers/malouf.pdf.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computational Linguistics,
29(4):639?654.
Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing
Ma, and HItoshi Isahara. 2003. CRL at Japanese dictionary-
based task of SENSEVAL-2. Journal of Natural Language
Processing, 10(3):115?143. (in Japanese).
Eric Nichols and Francis Bond. 2005. Acquiring ontologies
using deep and shallow processing. In 11th Annual Meeting
of the Association for Natural Language Processing, pages
494?498. Takamatsu.
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictionar-
ies. In Proceedings of the International Joint Conference on
Artificial Intelligence IJCAI-2005, pages 1111?1116. Edin-
burgh.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani. 1994.
Iwanami Kokugo Jiten Dai Go Han [Iwanami Japanese Dic-
tionary Edition 5]. Iwanami Shoten, Tokyo. (in Japanese).
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christoper D. Manning, Dan Flickinger, and Thorsten
Brant. 2002. The LinGO redwoods treebank: Motivation
and preliminary applications. In 19th International Confer-
ence on Computational Linguistics: COLING-2002, pages
1253?7. Taipei, Taiwan.
Carl Pollard and Ivan A. Sag. 1994. Head Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing
the Wall Street Journal using a Lexical-Functional Grammar
and discriminative estimation techniques. In 41st Annual
Meeting of the Association for Computational Linguistics:
ACL-2003, pages 271?278.
Melanie Siegel and Emily M. Bender. 2002. Efficient deep pro-
cessing of Japanese. In Proceedings of the 3rd Workshop on
Asian Language Resources and International Standardiza-
tion at the 19th International Conference on Computational
Linguistics, pages 1?8. Taipei.
Mark Stevenson. 2003. Word Sense Disambiguation. CSLI Pub-
lications.
Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The Hi-
noki sensebank ? a large-scale word sense tagged corpus of
Japanese ?. In Proceedings of the Workshop on Frontiers in
Linguistically Annotated Corpora 2006, pages 62?69. Syd-
ney. URL http://www.aclweb.org/anthology/W/W06/
W06-0608, (ACL Workshop).
Hiroaki Tsurumaru, Katsunori Takesita, Itami Katsuki, Toshi-
hide Yanagawa, and Sho Yoshida. 1991. An approach to
thesaurus construction from Japanese language dictionary.
In IPSJ SIGNotes Natural Language, volume 83-16, pages
121?128. (in Japanese).
Taro Watanabe. 2004. Example-based Statistical Machine
Translation. Ph.D. thesis, Kyoto University.
Yorick A. Wilkes, Brian M. Slator, and Louise M. Guthrie.
1996. Electric Words. MIT Press.
Deyi Xiong, Qun Liu Shuanglong Li and, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese treebank
with semantic knowledge. In Robert Dale, Jian Su Kam-Fai
Wong and, and Oi Yee Kwong, editors, Natural Language
Processing ? IJCNLP 005: Second International Joint Con-
ference Proceedings, pages 70?81. Springer-Verlag.
485
