USER MODELS: THE PROBLEM OF DISPARITY 
Sandra Carberry 
I)epartment of Computer and Information Sciences 
University of Delaware 
Newark, Delaware 19716, U.S.A. 
ABSTRACT 
A signific~mt component of a user raodel in au 
infornration-seeking dialogue is tim task-related plazl 
motivating the information-seeker's queries. A number 
of researchers have Irmdeled the plan inference process 
and used these models to design more robust natural 
language interfaces. However in each case, it has been 
assumed that the system's context model and the plan 
under construction by the information-seeker are never 
at variance. This paper addre~es the problem of 
disparate plans. It presents a four phase approach and 
argues that hmldling disparate plans requires an 
enriched context model. This model nmst permit tile 
addition of companents suggested by the information- 
,'~eeker but not fully supported by the system's domain 
knowledge, and must differentiate mnong its com- 
ponents according to the kind of support accorded each 
component as a correct part of the information-seeker's 
overall plan. It is shown how a component's support 
should affect the system's hypothesis about the source 
of error once plan disparity is suggested. 
I .  INTRODUCTION 
Corranunication as we know it involves more thml ~inlply 
answering isolated queries. When two individuals participate in 
an iuformation-seeking dialogue, tile information-provider uses the 
context within which each query occurs to interpret the query, 
determine tile desired information, and formulate an appropriate 
response. This context consists of more than mere knowledge of 
the previous questions and answers. A cooperative participant 
uses the information exchanged during the dialogue and 
knowledge of the domain to hypothesize a model of the speaker; 
this model is adjusted and expanded as the dialogue progresses 
and is called a user model. 
Perhaps the most significant component of a user model is 
the listener's belief about the underlying task motivating the 
information-seeker's queries and his partially developed plan for 
accomplishing this task. A number of researchers have modeled 
the plan inference process \[Allen 1980\], \[Cacberry 1983\], \[Grosz 
1977\], \[Litman 1984\], \[Perrault 19801, \[Robinson 1981\], \[Sidner 
1983\], and these models have been used to understand indirect 
speech acts \[Perranlt 1980\], provide helpful responses \[Allen 1980\], 
interpret pragmatically ill-formed queries \[Carberry 1986\], under- 
stand intersentential ellipsis \[Allen 1980, Carberry 1985\], and iden- 
tify the kind of response intended by a speaker \[Sidner 1983\]. 
However in each case, four critical assmnptions have been 
magic: 
\[1\] Tile inforroation-seeker's knowledge about the task domain 
may be lacking but is not erroneous. 
\[2\] The infornmtion-seeker's queries never address aspects of the 
task outside tile system's knowledge. Such systems maintain 
the closed world assumption \[Reiter 1978\]. 
\[3\] The information provided by the information-seeker is 
correct a~ld not misleading. 
\[4\] The underlying plan inferred by the system prior to analysis 
of a new utterance is a partially instantiated version of the 
plan under consideration by the information-seeker. 
These assumptions eliminate the possibility that tile information- 
seeker might ask queries irrelevant o the task at hand, that the 
information..seeker might ask about details outside tile system's 
limited knowledge, that the information-seeker might accidentally 
provide misleading information, and that the system might have 
made erroneous inferences from previous queries. The end result 
is that tbe system believes that the underlying task-related plan 
inferred by the system and the task-related plan under construc- 
tion by the information-seeker a e never at variance with one 
another. 
If we want systems capable of understanding and appropri- 
ately responding to naturally occurring dialogue, natural anguage 
interfaces must be able to deal with situations where those 
assumptions are not true. Our analysis of transcripts of naturally 
occurring information-seeking dialogues indicates that human par- 
ticipants attempt to detect inconsistencies in the models and 
repair them whenever possible. We claim that natural language 
systenm must do likewise; othe~'ise they will be unable to respond 
appropriately and cooperatively to dialogue that humans regard as 
natural. 
This paper presents a taxonomy of disparate plan models, 
according to how the model inferrod by the information-provider 
reflects the information-seeker's model of his task. We claim that 
plan inference must be extended to include a four phase approach 
to handling disparate plans ~md that this approach requires a 
richer model than maintained by current systems. We show how 
the support that an information-provider accords a component as 
a correct past of the model affects her hypothesis about the source 
of error once plan disparity is suggested. 
2. TYPES OF MODELS 
An information-seeking dialogue contains two participauts, 
one seeking hfformation and the other attempting to provide that 
information. Underlying such a dialogue is a task which the 
information-seeker wants to perform, generally at some time in 
the future. The information-seeker poses queries in order to 
obtain the information ecessary to construct a plan for accom- 
plishing this task. Examples of such tasks include pursuing a pro- 
gram of study in a university domain, treating a patient in a med- 
ical domain, and taking a vacation in a travel domain. 
A cooperative natural hmguage system must attempt to 
infer the underlying task-related plan motivating the 
information-seeker's queries mad use this plan to provide coopera- 
tive, helpful responses \[Carberry 1983, 1985\]. We call the system's 
model of this plan a context model. A context model is one com- 
ponent of a user model. 
29 
We are concerned here with cases in which the system's con- 
text model fails to mirror the plan under construction by the 
information-seeker. Disparate plan models may be classified 
according to how the model inferred by the system differs from 
the information-seeker's model of his task: 
\[1\] erroneous models, representing eases in which the model 
inferred by the system is inconsistent with the information- 
seeker's model. If the information-seeker were to examine 
the system's model in such cases, he would regard it as con- 
taining errors. 
\[2\] overly-speclalized models, representing cases in which the 
model inferred by the system is more restricted than that 
intended by the information-seeker. 
\[3\] overly-generalized models, representing cases in which the 
model inferred by the system is less specific than that 
intended by the information-seeker. 
\[4\] knowledge-liraited models~ representing cases in which the 
model inferred by the system fails to mirror the plan under 
construction by the information-seeker, due to the system's 
limited domain knowledge. 
The use of default inferencing rules may produce erroneous or 
overly-specialized models. Erroneous models may also result if the 
informatlon-seeker's statements are inaccurate or misleading or if 
the system uses focusing heuristics to relate new utterances to the 
existing plan context. Overly-generalized models may result if the 
information-seeker fails to adequately communicate his intentions 
(or the system fails to recognize these intentions). Knowledge- 
limited models may result if the information-seeker's domain 
knowledge xceeds that of the system. 
A fifth category, partial models, represents cases in which 
the system has inferred only part of the information-seeker's plan; 
subsequent dialogue will enable the system to further expand and 
refine this context model as more of the information-seeker's 
intentions are communicated. We do not regard partial models as 
disparate structures: were the informatlon-seeker to examine the 
system's inferred partial plan, he would regard it as correctly 
modeling his intentions as communicated in the dialogue thus far. 
3. RELATED WORK 
Several research efforts have addressed problems related to 
plan disparity. Kaplan\[1982\] and McCoy\[1986\] investigated 
misconceptions about domain knowledge and proposed responses 
intended to remove the misconception. However such misconcep- 
tions may not be exhibited when they first influence the 
information-seeker's plan construction; in such cases, disparate 
plans may result and correction will entail both a response 
correcting the misconception and further processing to bring the 
system's context model and the plan under construction by the 
information-seeker back into alignment. 
Pollack\[1986\] is studying removal of what she terms the 
"appropriate query assumption" of previous planning systems; she 
proposes a richer model of planning that explicitly reasons about 
the information-seeker's possible beliefs and intentions. Her 
overall goal is to develop a better model of plan inference. She 
addresses the problem of queries that indicate the information- 
seeker's plan is inappropriate to his overall goal., and attempts to 
isolate the erroneous beliefs that led to the inappropriate query. 
This is a subclass of "erroneous plans", since upon hearing the 
query, the system should detect that its context model no longer 
agrees with that of the information-seeker. However, queries 
deemed inappropriate by the system may signal phenomena other 
than inappropriate user plans. For example, the information- 
seeker may have shifted focus to another aspect of the overall 
task without successfully conveying this to the system, the 
30 
information-seeker may be addressing aspects of the task outside 
the system's limited knowledge, or the system's context model 
may have been in error prior to the query. 
Pollack is concerned with issues that arise when the 
information-s.~eker's plan is incorrect due to a misconception. She 
assumes Chat, immediately prior to the user making the "prob~ 
lematic" q~ery, the system's partial model of the user's plan is 
correct. We argue that since the system's inference mechanisms 
are not infallible and communication itself is imperfect, the sys- 
tem must contend with the possibility that its inferred model does 
not accurately reflect the user's plan. Previous research as failed 
to address this problem. 
4. PROBLEM POSED BY  D ISPARATE MODELS 
Grosz\[1981\] claimed that communication can proceed 
smoothly only if both dialogue participants are focused on the 
same subset of knowledge. Extending this to inferred plans, we 
claim that communication is most successful when the 
informatlon-provider's and information-seeker's models mirror one 
another. But clearly i t  is unrealistic to expect that these models 
will never diverge, given the different knowledge bases of the two 
participants and the imperfections of communication via dialogue. 
Thus the information-provider (IP) and the information-seeker 
(IS) must be able to detect inconsistencies in the models whenever 
possible and repair them. Clearly a natural anguage system must 
do the same. 
This view is supported by the work of Pollack, Hirsehberg, 
and Webber\[1982\]. They conducted a study of naturally occurring 
expert-novice dialogues and suggested that such interaction could 
be viewed as a negotiation process, during which not only an 
acceptable solution is negotiated but also understanding of the 
terminology and the beliefs of the participants. The context 
model is one component of IP's beliefs, as is her belief that it 
accurately reflects the plan under construction by IS. 
5. AN APPROACH TO D ISPARATE MODELS 
A study of transcripts of naturally occurring information- 
seeking dialogues indicates that humans often employ a four 
phase approach in detecting and recovering from disparate plan 
structures. Therefore a natural language interface that pursues 
the same strategy will be viewed as acting naturally by human 
users. The next sections discuss each of these phases. 
5.1. DETECT ION AND HYPOTHESIS  FORMATION 
As claimed earlier, since IP is presumed to be a cooperative 
dialogue participant, IP must be on the lookout for plan disparity. 
We have identified three sources of clues to the existence of such 
disparity: 
\[1\] the discourse goals of IS, such as expressing surprise or con- 
fusion 
\[2\] relevance of ISis current utterence to IP's inferred model 
\[31 focus of attention in the model 
IS can express surprise or confusion about IP's response, 
thereby cuing the possibility of plan disparity. Consider for 
example the dialogue presented in Figure 1. This dialogue was 
transcribed from a radio talk show on investments~and will be 
referred to as the "IRA example"; utterances are numbered for 
later reference. Plan disparity is suggested when IS, in utterance 
\[5\], expresses confusion at IP's previous response. 
On the other hand, IS's query may contradict or appear 
irrelevant o what IP believes is IS's overall task, leading IP to 
suspect hat her context model may not reflect IS's plan. Or IS's 
~-~:r~;;~\[,tfo-f-ih~;:-d\]~\[o-gu-es were provided by the Depart- 
ment of Computer Science of the University of Pennsylvania 
\[1\] IS: 
\[2\] IP: 
\[3\] IS: 
\[41 IP: 
\[5\] IS: 
\[6\] IP: 
\[7\] IS: 
\[81 IP: 
"I 'm ~ retired government employee but I 'm still 
working. I'd like to start out an IRA for myself 
mid my wife --- she doesn't work." 
"Did you work outside of the government last year?" 
"Yes I did." 
"There's no reason why you shouldn't have an IRA 
for last year." 
"I thought hey just started this year." 
"Oh no. IRA's were available as long as you are 
not a participant in an existing pension." 
"Well, I do work for a company that has a pension." 
"Ahh. Then you're not eligible for 81." 
Figure 1. Individual Retirement Account Dialogue ~t 
query may require so sharp an unsignaled shift in focus as to 
cause IP to be suspicious; the strongest expectations are for 
speakers to address aspects of the task closely related to the 
current focus of attention \[Sidner 1981, McKeown 1985, Carberry 
1983\]. The dialogue presented in Figure 2, and henceforth 
referred to as the "Kennit example", illustrates a toque in which 
plan disparity is suggested by an abrupt shift in focus of atten- 
tion. Upon completion of utterance \[4\], IP's model of IS's plan 
might be represented as 
Goal: Tnmsfer-Files(IS,KERMIT,VAX,PC) 
Precox~dltion: Have(IS,KERMIT) 
oo\[o:::7::;:: 5:=:; 
Precondition: Have(IS,<x>) 
Both humans mid machines have limited knowledge. Sup- 
pose that IP does not know how to purchase floppy disks. Then 
from IP's limited knowledge, IS's next query, 
"How late is the University Bookstore open?" 
will not appear to address an aspect of the plan inferred for IS, or 
any expansion of it. IP could just respond by 
\[1\] answering the direct question~ if possible, ignoring its 
ramifications 
\[2\] responding "I don~t know", if the direct answer is not avail- 
able 
However cooperative human information-providers a e expected to 
try to understand the import of a query and provide as coopera- 
tive a response ~ they can. 
Griee's maxim of relation \[Grice 1975\] suggests that IS 
believes the query to be relevant o the overall dialogue. Several 
possibilities exist. IS may be shifting focus to some aspect of a 
higher-level task that incindes transferring files as a subaction. 
One such higher-level task might be to compose a document using 
the SCRIBE text formatting system~ and the aspect queried by 
the new uttere~me might be the purchase of a SCRIBE manual 
from the univemity bookstore; in this ease, the subtask of the 
overall task represented by the existing context model might be 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Minor alterations have been made to the dialogue to remove 
restarts and extraneous phrasing. 
\[!\] IS: "I wish I could transfer files between the 
Vax and my PC." 
I2\] IP: "Kermit lets you do that." 
\[3\] IS: "How do I get Kemfit?" 
\[4\] IP: "The computing center will give you a copy 
if you bring them a floppy disk." 
\[5\] IS: "How late is the University Bookstore open?" 
Figure 2. File Transfer Via Kermlt Dialogue 
the transfer of files containing the document so that they can be 
modified using a PC editor. 
On the other hand~ focusing heuristics and the absence of 
discourse rrmrkers \[Sidner 1985\] suggest hat the new query is 
most likely to be relevant o the current focus of attention. So IP 
should begin trying to determine how IS's utterance mght relate 
to the currently focused subtask in tim context model, and con- 
sider the possibility that IS's domain knowledge might exceed IP's 
or irfight be erroneous. 
5.2. RESPONSE PHASE 
Webber\[1986\] distinguishes between answers and responses. 
She defines an answer as the production of the information or exe- 
cution of the action requested by the speaker but a response ~s 
"tile rcspondent's complete informative and performs- 
tire reaction to the question which can include ... addi- 
tional information provided or actions performed that 
are salient o this substitute for an answer." 
Our analysis of naturally oecurring dialogue indicates that 
humans respond, rather than answer, once disparate models are 
detected. Ttmse responses often entail additional actions, includ- 
ing a negotiation dialogue to ascertain the cause of the 
discrepancy and enable the models to be modified so that they are 
once again in alignment. A robust natural language interface 
must do the same, since the system must have an accurate model 
of the information-seeker's plan in order for cooperative behavior 
to resume. 
The appropriate response depends on the cause of the 
discrepancies. In the case of a knowledge-limited model, IP should 
attempt to understand IS's uttermme in terms of IP'8 limited 
knowledge ~ld provide any pertinent helpful information, but 
inform IS of these limitations in order to avoid misleading IS by 
appearing to implicitly support his task-related plan. 
Consider again our exmnple of file transfer via Kermit, 
presented in Figure 2. We assume that, in addition to a domain- 
dependent set of plans, IP's knowledge base contains a generaliza- 
tion hierarchy of actions and entities. 
Suppose that IP's knowledge base contains the plans 
To: Have(<agent>:PERSON, <x>:BOOK) 
Action: Purchase(<agent>, <x>) 
To: Purchase(<agent>:PERSON, <x>:TEXTBOOK) 
Action: GoTo(<agant>, <p>:BOOKSTORE, <t>:TIME) 
where Sells(<p>, <x>) 
Between(<t>~ <tl>:TIME, <t2>:TIME) 
Opens(<p>~ <t l>)  
Closes(<p>, <t2>) 
IP can reason that IS's last query is relevant o a plan for pur- 
chasing a textbook at the bookstore. This is simple plan inference 
31 
books 
coxmc light craft/hobby 
books books books 
fiction non-fiction 
educational-use 
items 
educational computer 
books supplies 
textbooks technical non-technlcal disks 
books books 
Figure 3. Object Taxonomy for Kermit Example 
as embodied in our TRACK system \[Carberry 1983\]. However IP 
cannot connect purchasing a book with her model of IS. So IP 
may begin trying to expand on her knowledge. Suppose that IP's 
taxonomy of objects is as shown in Figure 3 and that IP's domain 
knowledge includes the existence of many instances of <u>,  <v>, 
<w>, and <x> such that 
Seils(UDEL-BOOKSTORE, <u>:NOVEL) 
Selis(UDEL-BOOKSTORE~ <v>:TECHBOOK) 
Sells(UDEL-BOOKSTORE, <w>:NONTECHBOOK) 
Sells(.UDEL- BOOKSTORE, <x>:TEXTBOOK) 
Novels are a subclass of light-books and~ technical-books, non- 
technical-books, and textbooks are subclasses of educational- 
books. But educational-books are a subclass of educatlonal-use- 
items, as are floppy disks. Thus IP can generalize textbooks to 
educational-use-ltems, note that this class also contains disks, and 
then hypothesize that perhaps IS thinks that the bookstore sells 
floppy disks~ since it sells other educational-use it ms. This rea- 
soning might be represented by the rule 
If Clo.ss-I is a subclass of Class-2, and for many of the 
other subclasses of Class-2 there exist many members 
<y> such that 
V(...,<y>) ,
then one can hypothesize that perhaps there exists 
<z> such that 
P(...,<z>:Class-1) 
This rule can be applied in the absence of contradictory domain 
knowledge. Having thus hypothesized that perhaps 
Sells(UDEL-BOOKSTORE, <z>:DISK) 
from 
Sells(UDEL-BOOKSTORE, <v>:TECHBOOK) 
Sells(UDEL-BOOKSTORE, <w>:NONTECHBOOK) 
Sells(UDEL-BOOKSTORE, <x>:TEXTBOOK) 
IP can hypothesize the higher-level goaLs 
Purchase(IS, <z>:DISK) 
Have(IS, <z>:DISK) 
the last of which is a component of IP's model of IS. 
Since IP has constructed a plan that may reasonably be 
ascribed to IS, is relevant o the current focus of attention~ and 
about which IP's knowledge is neutral, IP can hypothesize that 
the cause of the plan disparity may be that IS has more extensivc 
domain knowledge. IP can now respond to IS. This reply should 
of course contain a direct answer to IS's posited question. But 
this alone is insufficient. In a cooperative information-seeklng 
dialogue, IS expects IP to assimilate the dialogue and relate utter- 
ances to IS's inferred underlying task in order to provide the most 
helpful information. If IP limits herself to a direct response, IS 
may infer that IP has related IS's current utterance to this task 
and  that IP~, knowledge supports it --- that is, that IP also 
believes IS can purchase a floppy disk at the bookstore. Joshi's 
revised maxim of quality \[Joehl 1983\] asserts that IP's response 
must block false inferences. In addition, as a helpful participant, 
IP should include whatever evidence IP has for or against he pla~x 
component proposed by IS. An appropriate response wouhl be: 
"The University Bookstore is open until 4:30 PM. But 
I don't know whether it sells floppy disks. However it 
does sell many other items of an educational nature, so 
it is perhaps a good place to try." 
The above example concerned a knowledge-limited model 
caused by IP's limited domain knowledge. Other kinds of models 
suggest different reasoning and response strategies. If IP has 
failed to nm~e the inferences IS assumed would be made, then 
subsequent utter*races by IS may appear appropriate to a more 
specific model than IP's current modeh Earlier, we referred to 
this class as overly-generalized models. In these cases, IP amy 
enter a clarification dialogue to ~certaln what IS intends. 
In other cases, such as when overly-specialized or erroneous 
models are detected, a negotiation dialogue must be initiated to 
"square away" \[Joshi 1983\] the modeis; otherwise, IS will lack 
confidence in the responds provided by IP (and therefore should 
not continue the dialogue), and IP will lack confidence in her abil- 
ity to provide useful replies (and therefore cannot continue as a 
cooperative participant). As with any negotiation, this is a two- 
way process: 
\[1\] IP may select portions of the context model that she feels 
are suspect and justify them~ in an attempt o convince IS 
that IS's plan needs adjustment, not IP's inferred model of 
that plan. 
\[2\] IP may formulate queries to IS in order to ascertain why the 
task models diverge and where IP's model might be in error. 
The IRA example illustrates a negotiation dialogue. In utterance 
\[6\], IP selects a suspect component of her context model and pro- 
vides justification for it. IS's next utterance informs IP that the 
assumption on which this component was based is incorrect; IP 
then notifies IS that IP recognizes the error and that her context 
model has been repaired. The information-seeking dialogue then 
resumes .  
5.3. MODEL l tECONSTRUCTION 
Once the cause of model disparity is identified, IP and IS 
must a~ljust heir models to remove the disparities. Once again, 
32 
this depends o~ the cause of the disagreement. In the case of a 
knowledge-limited model, IP should hmorporate the components 
she believes to be part of IS's plan structure into her context 
model, noting however that her own knowledge oilers only liafited 
support for thr.m. In this way, IP's model reflects IS's, enables IP 
to understand (within her limited knowh!dge) how IS plazm to 
accomplish is objectives, and permits IP to use this knowledge to 
understand subsequent utterances and provide helpful informa- 
tion. 
If IP's m(~lel is in error~ she must alter her context model, as 
determined through the negotiation dialogue. She may also com- 
municate to IS the changes that she is making, so that IS can 
assure himself that the models now agree. On the other hand, if 
IS's model is in error, IP may inform IS of any information eee~ 
sary for 1S to construct an appropriate plan and achieve his goals. 
g.4. SUMMAI t?  
The argunmnts in the preceding sections are based on an 
analysis of transcripts of hunm~l information-seeking dialogues 
and indicate that au appropriate approach for hazldling the plan 
disparity problem entails four phases: 
\[1\] detection of disparate mc)dels 
\[2\] hypothesis for:marion as to the cause of the disparities 
\[3\] extended response, often including a negotiation dialogue to 
identify the cause of the disparities 
\[4\] model modification, to "square away" the plm~ structures. 
Since this appre~mh is representative of that employed by human 
dialogue partlcipants, a natural language interface that pursues 
the s~nne strugegy will be viewed as acting naturally by its human 
users .  
O. ENRICHED CONTEXT MODEL 
The knowledge acquired from the dialogue and how it was 
used to constrt~ct he context model are important factors in 
detecting, responding to, and recovering from disparate models. 
l\[tumazl dialogue participants typically employ various teclmiques 
such as focusing strategies and default rules for understanding 
a~xd relating dialogue, but they appear to have greater confidence 
in some parts of the resultant model than others. Natural 
language systems mnst employ similar mechanisms in order to do 
the kind of inferencing expected by humans and provide the most 
helpful responses. We claim that the representation of the 
inferred plan must differentiate among its components according 
to the support which the system accords each component as a 
correct and intended part of the inferred plan. This view parallels 
Doyle's Truth Maintenance System \[Doyle 1979\], in which atti- 
tudes are associated with reasons justifying them. 
We see font kinds of support for plan components: 
\[1\] whether the system has inferred the component directly 
from what IS said. 
\[2\] whether the system has inferred the component on the basis 
of its own domain knowledge, which the system eamlot be 
cerLain IS i~s aware of. 
\[3\] the kinds of k~mehanismu used to add each component to the 
model, (for example, default rules that select one component 
from among several possibilities, or heuristics that suggest a
shift in f(~:us of attention), and the evidence for applying 
the mechar~ism. 
\[41 whether the system's domain knowledge supports, contrad- 
icts, or is :neutral regarding inclusion of the component as 
part of a correct overall plan. 
The first three are importmlt factors in formulating a 
hypothesis regarding the source of disparity between the system's 
model and IS's plmL If the system believes that IS intends the 
system to recognize from IS's utterance that G is a component of 
IS's plan, then the system can add G to its context model and 
have the greatest faith that it really is a component of IS's plan. 
Therefore such components are unlikely sources of disparity 
between the system's context model and IS's plan. 
Components that the system adds to the context model on 
the basis of its donmin knowledge will be strongly believed by the 
system to be part of IS's plan, bnt not as much as if IS had 
directly coatmunicated them. Ttmse components resemble 
"keyhole recognition" rather thml "intended recognition" \[Sidner 
1985, 1983\]. Since IS amy not have intended to eonnnunieate 
them, they are more likely r~ources of error tha~l components 
which IS intended IP to recognize. 
Consider for example a student advisement system. If only 
BA degrees have a foreign lar~guage r quirement, the query 
"What course must I take to satisfy the foreign 
language requirement in French?" 
may lead the system to infer that IS is pursuing a Bachelor of 
Arts degree. If only BS degrees require a senior project, then a 
subsequent query such as 
"Ilow many credits of senior project are required?" 
suggests plan disparity. Either the second query is inappropriate 
to IS's overall goal \[Pollack 1986\] or the system's context model is 
already in error. Since the component 
Obtain-Degree(IS, BACHELOR-OF-ARTS) 
was inferred on the basis of the system's domain knowledge rather 
titan directly from IS's utterance, it is suspect as the source of 
er ror .  
The mechanisms u2~ed to add a component to the context 
model affect IP's faith in that component as part of ISis overall 
plan. Consider again the IRA example in Figure 1. in utterance 
I4\], IP has applied the default assumption that IS was not covered 
by a pension progrmn during the year in question (at that tim% 
rules on IRAs were different). IS's next utterance xpresses con- 
fusion at IP's response, thereby cuing the possibility of plan 
disparity. In utterance \[61, IP selects the component added to the 
context model via. the default assumption as a possible source of 
the disparity, tells IS that it is part of IP's context model, and 
attempts to justify its inclusion. 
Analysis of naturally occurring dialogues uch as that in Fig- 
ure 1 indicate that humans use mechanisms such as defanlt infer- 
cnee rules and focusing heuristics to expand the context model 
and provide a more detailed and tractable arena in which to 
understand and respond to subsequent utterances. Natural 
language systems must use similar mechanisms in order to 
cooperatively and naturally engage in dialogue with humans. 
IIowever these rules select from among multiple possibilities and 
therefore produce components that are more likely sources of 
error than components added as a result of IS's direct statements 
or inferences made on the basis of the system's domain 
knowledge. 
The fourth kind of differentiation among components --- 
whether the system's domain knowledge supports, contradicts, or 
is neutral regarding inclusion of the component as part of a 
correct overall plan - -  is important in recovering from disparate 
plans. Even an expert system has limited domain knowledge. 
Furthermore, in a rapidly eh~mging world, knowledgeable users 
may have more accurate information about some aspects of the 
domain than does the system. For example, a student advisement 
system may not be altered intmediately upon changing the teacher 
of a course. Thus we believe that the context model must allow 
for inclusion of components suggested by the informatiomseeker, 
including whether the system's domain knowledge contradicts, 
supports, or is neutral regarding the component. 
33 
For example, upon determining that IS's domain knowledge 
may exceed the system's in the Kermit dialogue, the system 
should expand its existing model to incorporate the acquired 
knowledge about how IS believes floppy disks can be obtained. 
The plan components creatively constructed can be added to the 
system's model, but as components proposed by IS and not fully 
supported by the system's knowledge. In this manner, the system 
can assimilate new utterances that exceed or contradict i s limited 
domain knowledge and develop an expanded context model which 
serves as "knowledge" that can be referred back to in the ensuing 
dialogue. 
7. SUMMARY 
This paper has addressed the problem of disparity between 
the context model inferred by a natural anguage system and the 
plan under construction by an information-seeker. We have 
presented a four phase approach and have argued that handling 
disparate plans requires an enriched context model. This model 
must permit the addition of components uggested by the 
information-seeker but not fully supported by the system's 
domain knowledge and must differentiate among its components 
according to the kind of support accorded each component as a 
correct part of the information-seeker's overall plan. We have 
further argued that support for a component should affect the 
system's hypothesis about the source of error once plan disparity 
is suggested. 
8. ACKNOWLEDGEMENTS 
I want to thank Joe Brady, Kathy Cebulka, Dan Chester, 
Kathy McCoy, Martha Pollack, and Ralph Weiscbedel for their 
many helpful discussions and coxxmaents on this work, and Dan 
Chester and Kathy McCoy for their comments and suggestions on 
this paper. 
9. REFERENCES 
Allen, James F., "Analyzing Intention in Utterances", Artificial 
Intelligence 15(3), 1980 
Carberry, Sandra, "Pragmatic Modeling in Information System 
Interfaces", Ph.D. Dissertation, Department of Computer Science, 
University of Delaware, 1985 
Carberry, Sandra, "Tracking User Goals in an Information- 
Seeking Environment", Proceedings of the National Conference 
on Artificial Intelligence, 1983 
Carberry, Sandra, "Using Inferred Knowledge to Understand 
Pragmatically Ill-Formed Queries", to appear in Communication 
Failure in Dialogue, Ronan Reilly editor, North Holland, 1986 
Doyle, Jon, "A Truth Maintenance System", Artificial Intelligence 
12(3), 1979 
Grice, H. P., "Logic and Conversation", In Syntax and Semantics, 
Cole and Morgan, editors, Academic Press, 1975 
Grice, H. P., "Utterer's Meaning and Intentions", Philosophical 
Review 68, 1969 
Grice, H. P., "Meaning", Philosophical Review 56, 1957 
Grosz, Barbara, "Focusing and Description in Natural Language 
Dialogues", in Elements of Discourse Understanding, Joshi, A., 
Webber, B., and Sag, I., editors, Cambridge University Press, 
1981 
Grosz, Barbara, "The Representation a d Use of Focus in a Sys= 
tem for Understanding Dialogs", Proceedings of the International 
Joint Conference on Artificial Intelligence, 1977 
Joshi, Aravind K., "Mutual Beliefs in Question-Answer Systems", 
Mutual Knowledge, Academic Press, 1983 
Kaplan, S. Jerroid, "Cooperative Responses from a Portable 
Natural Language Query System", Artificial Intelligence 19j 1982 
Litmus, Diane J. and Alien, James F., "A Plan Recognition Model 
for Clarification Subdialogues", Proceedings of the International 
Conference on Computational Linguistics, 1984 
McCoy, Kathleen F., "Generating Responses to Property Miscon- 
ceptions Using Perspective"~ to appear in Communication Failure 
in Dialogue, Ronan Reilly, Editor, 1986 
McKeown, Kathleen R., Te~t Generation, Cambridge University 
Press, 1985 
Perrault, C. 1L and Allen, J. F., "A Plan-Based Analysis of 
Indirect Speech Acts", American Journal of Computational 
Linguistics, 1980 
Pollack, Martha, "Inferring Domain Plans in Question- 
Answering", forthcoming Ph.D. Dissertation, University of 
Pennsylvania, 1986 
Pollack, Martha, "Some Requirements for a Model of the Plan- 
Inference Process in Conversation", to appear in Communication 
Failure in Dialogue, Ronan Reilly, editor, North Holland, 1986 
Pollack, Martha, Hirsehberg, Julia, and Webber, Bonnie, "User 
Participation in the Reasoning Processes of Expert Systems", 
Proceedings of the National Conference on Artificial Intelligence, 
1982 
Reiter, P~y, "On Closed World Data Bases", Logic and Data 
Bases, Gallaire, It. and Minker, J.~ editors, Plenum Pre~s, 1978 
Robinson, Ann E., "Determining Verb Phrase Referents in Dia- 
logs", American Journal of Computational Linguis$ics, 1981 
Sidner, Candace L., "Plan Parsing for Intended Response Recogo 
nitlon in Discourse", Computational Intelligence 1(1), 1985 
Sidner, Candace L., "What the Speaker Means: The Recognition 
of Speakers' Plans in Discourse", Computers and Mathematics 
With Applications 9(1), 1983 
Sidner, Candace L., "Focusing for Interpretation of Pronouns", 
American Journal of Computational Linguistics, 1981 
Webber, Bonnie L., "Questions, Answers, and Responses: 
Interact\[ag with Knowledge Base Systems", to appear in On 
Knowledge Base Management Systems, M. Brodie and J. Mylo~ 
poulos, editors, Springer-Verlag, 1986 
34 
