Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 83?90,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
A Non-negative Tensor Factorization Model for
Selectional Preference Induction
Tim Van de Cruys
University of Groningen
The Netherlands
t.van.de.cruys@rug.nl
Abstract
Distributional similarity methods have
proven to be a valuable tool for the in-
duction of semantic similarity. Up till
now, most algorithms use two-way co-
occurrence data to compute the mean-
ing of words. Co-occurrence frequencies,
however, need not be pairwise. One can
easily imagine situations where it is desir-
able to investigate co-occurrence frequen-
cies of three modes and beyond. This pa-
per will investigate a tensor factorization
method called non-negative tensor factor-
ization to build a model of three-way co-
occurrences. The approach is applied to
the problem of selectional preference in-
duction, and automatically evaluated in a
pseudo-disambiguation task. The results
show that non-negative tensor factoriza-
tion is a promising tool for NLP.
1 Introduction
Distributional similarity methods have proven to
be a valuable tool for the induction of semantic
similarity. The aggregate of a word?s contexts gen-
erally provides enough information to compute its
meaning, viz. its semantic similarity or related-
ness to other words.
Up till now, most algorithms use two-way co-
occurrence data to compute the meaning of words.
A word?s meaning might for example be computed
by looking at:
? the various documents that the word appears
in (words ? documents);
? a bag of words context window around the
word (words ? context words);
? the dependency relations that the word ap-
pears with (words ? dependency relations).
The extracted data ? representing the co-
occurrence frequencies of two different entities
? is encoded in a matrix. Co-occurrence fre-
quencies, however, need not be pairwise. One
can easily imagine situations where it is desirable
to investigate co-occurrence frequencies of three
modes and beyond. In an information retrieval
context, one such situation might be the investiga-
tion of words ? documents ? authors. In an NLP
context, one might want to investigatewords? de-
pendency relations ? bag of word context words,
or verbs ? subjects ? direct objects.
Note that it is not possible to investigate the
three-way co-occurrences in a matrix represen-
tation form. It is possible to capture the co-
occurrence frequencies of a verb with its sub-
jects and its direct objects, but one cannot cap-
ture the co-occurrence frequencies of the verb ap-
pearing with the subject and the direct object at
the same time. When the actual three-way co-
occurrence data is ?matricized?, valuable informa-
tion is thrown-away. To be able to capture the mu-
tual dependencies among the three modes, we will
make use of a generalized tensor representation.
Two-way co-occurrence models (such as la-
tent semantic analysis) have often been augmented
with some form of dimensionality reduction in or-
der to counter noise and overcome data sparseness.
We will also make use of a dimensionality reduc-
tion algorithm appropriate for tensor representa-
tions.
2 Previous Work
2.1 Selectional Preferences & Verb
Clustering
Selectional preferences have been a popular re-
search subject in the NLP community. One of
the first to automatically induce selectional pref-
erences from corpora was Resnik (1996). Resnik
generalizes among nouns by using WordNet noun
83
synsets as clusters. He then calculates the se-
lectional preference strength of a specific verb in
a particular relation by computing the Kullback-
Leibler divergence between the cluster distribu-
tion of the verb and the aggregate cluster distri-
bution. The selectional association is then the
contribution of the cluster to the verb?s prefer-
ence strength. The model?s generalization relies
entirely on WordNet; there is no generalization
among the verbs.
The research in this paper is related to previous
work on clustering. Pereira et al (1993) use an
information-theoretic based clustering approach,
clustering nouns according to their distribution as
direct objects among verbs. Their model is a one-
sided clustering model: only the direct objects are
clustered, there is no clustering among the verbs.
Rooth et al (1999) use an EM-based cluster-
ing technique to induce a clustering based on the
co-occurrence frequencies of verbs with their sub-
jects and direct objects. As opposed to the method
of Pereira et al (1993), their model is two-sided:
the verbs as well as the subjects/direct objects are
clustered. We will use a similar model for evalua-
tion purposes.
Recent approaches using distributional similar-
ity methods for the induction of selectional pref-
erences are the ones by Erk (2007), Bhagat et al
(2007) and Basili et al (2007).
This research differs from the approaches men-
tioned above by its use of multi-way data: where
the approaches above limit themselves to two-way
co-occurrences, this research will focus on co-
occurrences for multi-way data.
2.2 Factorization Algorithms
2.2.1 Two-way Factorizations
One of the best known factorization algorithms
is principal component analysis (PCA, Pearson
(1901)). PCA transforms the data into a new co-
ordinate system, yielding the best possible fit in a
least square sense given a limited number of di-
mensions. Singular value decomposition (SVD)
is the generalization of the eigenvalue decompo-
sition used in PCA (Wall et al, 2003).
In information retrieval, singular value decom-
position has been applied in latent semantic analy-
sis (LSA, Landauer and Dumais (1997), Landauer
et al (1998)). In LSA, a term-document matrix
is created, containing the frequency of each word
in a specific document. This matrix is then de-
composed into three other matrices with SVD. The
most important dimensions that come out of the
SVD allegedly represent ?latent semantic dimen-
sions?, according to which nouns and documents
can be represented more efficiently.
LSA has been criticized for a number of rea-
sons, one of them being the fact that the factor-
ization contains negative numbers. It is not clear
what negativity on a semantic scale should des-
ignate. Subsequent methods such as probabilistic
latent semantic analysis (PLSA, Hofmann (1999))
and non-negative matrix factorization (NMF, Lee
and Seung (2000)) remedy these problems, and
indeed get much more clear-cut semantic dimen-
sions.
2.2.2 Three-way Factorizations
To be able to cope with three-way data, sev-
eral algorithms have been developed as multilin-
ear generalizations of the SVD. In statistics, three-
way component analysis has been extensively in-
vestigated (for an overview, see Kiers and van
Mechelen (2001)). The two most popular methods
are parallel factor analysis (PARAFAC, Harshman
(1970), Carroll and Chang (1970)) and three-mode
principal component analysis (3MPCA, Tucker
(1966)), also called higher order singular value
decomposition (HOSVD, De Lathauwer et al
(2000)). Three-way factorizations have been ap-
plied in various domains, such as psychometry
and image recognition (Vasilescu and Terzopou-
los, 2002). In information retrieval, three-way fac-
torizations have been applied to the problem of
link analysis (Kolda and Bader, 2006).
One last important method dealing with multi-
way data is non-negative tensor factorization
(NTF, Shashua and Hazan (2005)). NTF is a gener-
alization of non-negative matrix factorization, and
can be considered an extension of the PARAFAC
model with the constraint of non-negativity (cfr.
infra).
One of the few papers that has investigated the
application of tensor factorization for NLP is Tur-
ney (2007), in which a three-mode tensor is used
to compute the semantic similarity of words. The
method achieves 83.75% accuracy on the TOEFL
synonym questions.
84
3 Methodology
3.1 Tensors
Distributional similarity methods usually repre-
sent co-occurrence data in the form of a matrix.
This form is perfectly suited to represent two-way
co-occurrence data, but for co-occurrence data be-
yond two modes, we need a more general repre-
sentation. The generalization of a matrix is called
a tensor. A tensor is able to encode co-occurrence
data of any n modes. Figure 1 shows a graphi-
cal comparison of a matrix and a tensor with three
modes ? although a tensor can easily be general-
ized to more than three modes.
Figure 1: Matrix representation vs. tensor repre-
sentation
3.2 Non-negative Tensor Factorization
In order to create a succinct and generalized model
of the extracted data, a statistical dimensional-
ity reduction technique called non-negative tensor
factorization (NTF) is applied to the data. The NTF
model is similar to the PARAFAC analysis ? popu-
lar in areas such as psychology and bio-chemistry
? with the constraint that all data needs to be non-
negative (i.e. ? 0).
Parallel factor analysis (PARAFAC) is a multi-
linear analogue of the singular value decomposi-
tion (SVD) used in latent semantic analysis. The
key idea is to minimize the sum of squares be-
tween the original tensor and the factorized model
of the tensor. For the three mode case of a tensor
T ? RD1?D2?D3 this gives equation 1, where k is
the number of dimensions in the factorized model
and ? denotes the outer product.
min
xi?RD1,yi?RD2,zi?RD3
? T ?
k
?
i=1
xi ? yi ? zi ?
2
F (1)
With non-negative tensor factorization, the non-
negativity constraint is enforced, yielding a model
like the one in equation 2:
min
xi?RD1?0,yi?R
D2
?0,zi?R
D3
?0
? T ?
k
?
i=1
xi ? yi ? zi ?
2
F (2)
The algorithm results in three matrices, indicat-
ing the loadings of each mode on the factorized
dimensions. The model is represented graphically
in figure 2, visualizing the fact that the PARAFAC
decomposition consists of the summation over the
outer products of n (in this case three) vectors.
Figure 2: Graphical representation of the NTF as
the sum of outer products
Computationally, the non-negative tensor fac-
torization model is fitted by applying an alternat-
ing least-squares algorithm. In each iteration, two
of the modes are fixed and the third one is fitted
in a least squares sense. This process is repeated
until convergence.1
3.3 Applied to Language Data
The model can straightforwardly be applied to lan-
guage data. In this part, we describe the fac-
torization of verbs ? subjects ? direct objects
co-occurrences, but the example can easily be
substituted with other co-occurrence information.
Moreover, the model need not be restricted to 3
modes; it is very well possible to go to 4 modes
and beyond ? as long as the computations remain
feasible.
The NTF decomposition for the verbs ? sub-
jects? direct objects co-occurrences into the three
loadings matrices is represented graphically in fig-
ure 3. By applying the NTF model to three-way
(s,v,o) co-occurrences, we want to extract a gen-
eralized selectional preference model, and eventu-
ally even induce some kind of frame semantics (in
the broad sense of the word).
In the resulting factorization, each verb, subject
and direct object gets a loading value for each fac-
tor dimension in the corresponding loadings ma-
trix. The original value for a particular (s,v,o)
1The algorithm has been implemented in MATLAB, using
the Tensor Toolbox for sparse tensor calculations (Bader and
Kolda, 2007).
85
Figure 3: Graphical representation of the NTF for
language data
triple xsvo can then be reconstructed with equa-
tion 3.
xsvo =
k
?
i=1
ssivviooi (3)
To reconstruct the selectional preference value
for the triple (man,bite,dog), for example, we
look up the subject vector for man, the verb vector
for bite and the direct object vector for dog. Then,
for each dimension i in the model, we multiply the
ith value of the three vectors. The sum of these
values is the final preference value.
4 Results
4.1 Setup
The approach described in the previous section has
been applied to Dutch, using the Twente Nieuws
Corpus (Ordelman, 2002), a 500M words corpus
of Dutch newspaper texts. The corpus has been
parsed with the Dutch dependency parser Alpino
(van Noord, 2006), and three-way co-occurrences
of verbs with their respective subject and direct
object relations have been extracted. As dimen-
sion sizes, the 1K most frequent verbs were used,
together with the 10K most frequent subjects and
10K most frequent direct objects, yielding a ten-
sor of 1K ? 10K ? 10K. The resulting tensor is
very sparse, with only 0.0002% of the values be-
ing non-zero.
The tensor has been adapted with a straight-
forward extension of pointwise mutual informa-
tion (Church and Hanks, 1990) for three-way co-
occurrences, following equation 4. Negative val-
ues are set to zero.2
2This is not just an ad hoc conversion to enforce non-
negativity. Negative values indicate a smaller co-occurrence
probability than the expected number of co-occurrences. Set-
ting those values to zero proves beneficial for similarity cal-
culations (see e.g. Bullinaria and Levy (2007)).
MI3(x,y,z) = log
p(x,y,z)
p(x)p(y)p(z)
(4)
The resulting matrix has been factorized into k
dimensions (varying between 50 and 300) with the
NTF algorithm described in section 3.2.
4.2 Examples
Table 1, 2 and 3 show example dimensions that
have been found by the algorithm with k = 100.
Each example gives the top 10 subjects, verbs
and direct objects for a particular dimension, to-
gether with the score for that particular dimension.
Table 1 shows the induction of a ?police action?
frame, with police authorities as subjects, police
actions as verbs and patients of the police actions
as direct objects.
In table 2, a legislation dimension is induced,
with legislative bodies as subjects3, legislative ac-
tions as verbs, and mostly law (proposals) as direct
objects. Note that some direct objects (e.g. ?min-
ister?) also designate persons that can be the object
of a legislative act.
Table 3, finally, is clearly an exhibition dimen-
sion, with verbs describing actions of display and
trade that art institutions (subjects) can do with
works of art (objects).
These are not the only sensible dimensions that
have been found by the algorithm. A quick qual-
itative evaluation indicates that about 44 dimen-
sions contain similar, framelike semantics. In an-
other 43 dimensions, the semantics are less clear-
cut (single verbs account for one dimension, or
different senses of a verb get mixed up). 13 dimen-
sions are not so much based on semantic character-
istics, but rather on syntax (e.g. fixed expressions
and pronomina).
4.3 Evaluation
The results of the NTF model have been quantita-
tively evaluated in a pseudo-disambiguation task,
similar to the one used by Rooth et al (1999). It is
used to evaluate the generalization capabilities of
the algorithm. The task is to judge which subject
(s or s?) and direct object (o or o?) is more likely
for a particular verb v, where (s,v,o) is a combi-
nation drawn from the corpus, and s? and o? are a
subject and direct object randomly drawn from the
corpus. A triple is considered correct if the algo-
rithm prefers both s and o over their counterparts
3Note that VVD, D66, PvdA and CDA are Dutch political
parties.
86
subjects sus verbs vs objects ob js
politie ?police? .99 houd aan ?arrest? .64 verdachte ?suspect? .16
agent ?policeman? .07 arresteer ?arrest? .63 man ?man? .16
autoriteit ?authority? .05 pak op ?run in? .41 betoger ?demonstrator? .14
Justitie ?Justice? .05 schiet dood ?shoot? .08 relschopper ?rioter? .13
recherche ?detective force? .04 verdenk ?suspect? .07 raddraaiers ?instigator? .13
marechaussee ?military police? .04 tref aan ?find? .06 overvaller ?raider? .13
justitie ?justice? .04 achterhaal ?overtake? .05 Roemeen ?Romanian? .13
arrestatieteam ?special squad? .03 verwijder ?remove? .05 actievoerder ?campaigner? .13
leger ?army? .03 zoek ?search? .04 hooligan ?hooligan? .13
douane ?customs? .02 spoor op ?track? .03 Algerijn ?Algerian? .13
Table 1: Top 10 subjects, verbs and direct objects for the ?police action? dimension
subjects sus verbs vs objects ob js
meerderheid ?majority? .33 steun ?support? .83 motie ?motion? .63
VVD .28 dien in ?submit? .44 voorstel ?proposal? .53
D66 .25 neem aan ?pass? .23 plan ?plan? .28
Kamermeerderheid ?Chamber majority? .25 wijs af ?reject? .17 wetsvoorstel ?bill? .19
fractie ?party? .24 verwerp ?reject? .14 hem ?him? .18
PvdA .23 vind ?think? .08 kabinet ?cabinet? .16
CDA .23 aanvaard ?accepts? .05 minister ?minister? .16
Tweede Kamer ?Second Chamber? .21 behandel ?treat? .05 beleid ?policy? .13
partij ?party? .20 doe ?do? .04 kandidatuur ?candidature? .11
Kamer ?Chamber? .20 keur goed ?pass? .03 amendement ?amendment? .09
Table 2: Top 10 subjects, verbs and direct objects for the ?legislation? dimension
s? and o? (so the (s,v,o) triple ? that appears in the
test corpus ? is preferred over the triples (s?,v,o?),
(s?,v,o) and (s,v,o?)). Table 4 shows three exam-
ples from the pseudo-disambiguation task.
s v o s? o?
jongere drink bier coalitie aandeel
?youngster? ?drink? ?beer? ?coalition? ?share?
werkgever riskeer boete doel kopzorg
?employer? ?risk? ?fine? ?goal? ?worry?
directeur zwaai scepter informateur vodka
?manager? ?sway? ?sceptre? ?informer? ?wodka?
Table 4: Three examples from the pseudo-
disambiguation evaluation task?s test set
Four different models have been evaluated. The
first two models are tensor factorization models.
The first model is the NTF model, as described
in section 3.2. The second model is the original
PARAFAC model, without the non-negativity con-
straints.
The other two models are matrix factorization
models. The third model is the non-negative ma-
trix factorization (NMF) model, and the fourth
model is the singular value decomposition (SVD).
For these models, a matrix has been constructed
that contains the pairwise co-occurrence frequen-
cies of verbs by subjects as well as direct objects.
This gives a matrix of 1K verbs by 10K subjects
+ 10K direct objects (1K ? 20K). The matrix has
been adapted with pointwise mutual information.
The models have been evaluated with 10-fold
cross-validation. The corpus contains 298,540 dif-
ferent (s,v,o) co-occurrences. Those have been
randomly divided into 10 equal parts. So in each
fold, 268,686 co-occurrences have been used for
training, and 29,854 have been used for testing.
The accuracy results of the evaluation are given in
table 5.
The results clearly indicate that the NTF model
outperforms all the other models. The model
achieves the best result with 300 dimensions, but
the differences between the different NTF models
are not very large ? all attaining scores around
90%.
87
subjects sus verbs vs objects ob js
tentoonstelling ?exhibition? .50 toon ?display? .72 schilderij ?painting? .47
expositie ?exposition? .49 omvat ?cover? .63 werk ?work? .46
galerie ?gallery? .36 bevat ?contain? .18 tekening ?drawing? .36
collectie ?collection? .29 presenteer ?present? .17 foto ?picture? .33
museum ?museum? .27 laat ?let? .07 sculptuur ?sculpture? .25
oeuvre ?oeuvre? .22 koop ?buy? .07 aquarel ?aquarelle? .20
Kunsthal .19 bezit ?own? .06 object ?object? .19
kunstenaar ?artist? .15 zie ?see? .05 beeld ?statue? .12
dat ?that? .12 koop aan ?acquire? .05 overzicht ?overview? .12
hij ?he? .10 in huis heb ?own? .04 portret ?portrait? .11
Table 3: Top 10 subjects, verbs and direct objects for the ?exhibition? dimension
dimensions
50 (%) 100 (%) 300 (%)
NTF 89.52 ? 0.18 90.43 ? 0.14 90.89 ? 0.16
PARAFAC 85.57 ? 0.25 83.58 ? 0.59 80.12 ? 0.76
NMF 81.79 ? 0.15 78.83 ? 0.40 75.74 ? 0.63
SVD 69.60 ? 0.41 62.84 ? 1.30 45.22 ? 1.01
Table 5: Results of the 10-fold cross-validation for
the NTF, PARAFAC, NMF and SVD model for 50,
100 and 300 dimensions (averages and standard
deviation)
The PARAFAC results indicate the fitness of ten-
sor factorization for the induction of three-way se-
lectional preferences. Even without the constraint
of non-negativity, the model outperforms the ma-
trix factorization models, reaching a score of about
85%. The model deteriorates when more dimen-
sions are used.
Both matrix factorization models perform
worse than their tensor factorization counterparts.
The NMF still scores reasonably well, indicating
the positive effect of the non-negativity constraint.
The simple SVD model performs worst, reaching a
score of about 70% with 50 dimensions.
5 Conclusion and Future Work
This paper has presented a novel method that
is able to investigate three-way co-occurrences.
Other distributional methods deal almost exclu-
sively with pairwise co-occurrences. The ability
to keep track of multi-way co-occurrences opens
up new possibilities and brings about interesting
results. The method uses a factorization model ?
non-negative tensor factorization ? that is suitable
for three way data. The model is able to generalize
among the data and overcome data sparseness.
The method has been applied to the problem
of selectional preference induction. The results
indicate that the algorithm is able to induce se-
lectional preferences, leading to a broad kind
of frame semantics. The quantitative evaluation
shows that use of three-way data is clearly benefi-
cial for the induction of three-way selectional pref-
erences. The tensor models outperform the sim-
ple matrix models in the pseudo-disambiguation
task. The results also indicate the positive ef-
fect of the non-negativity constraint: both mod-
els with non-negative constraints outperform their
non-constrained counterparts.
The results as well as the evaluation indicate
that the method presented here is a promising tool
for the investigation of NLP topics, although more
research and thorough evaluation are desirable.
There is quite some room for future work. First
of all, we want to further investigate the useful-
ness of the method for selectional preference in-
duction. This includes a deeper quantitative eval-
uation and a comparison to other methods for se-
lectional preference induction. We also want to
include other dependency relations in our model,
apart from subjects and direct objects.
Secondly, there is room for improvement and
further research with regard to the tensor factor-
ization model. The model presented here min-
imizes the sum of squared distance. This is,
however, not the only objective function possi-
ble. Another possibility is the minimization of the
Kullback-Leibler divergence. Minimizing the sum
of squared distance assumes normally distributed
data, and language phenomena are rarely normally
distributed. Other objective functions ? such as the
minimization of the Kullback-Leibler divergence
? might be able to capture the language structures
88
much more adequately. We specifically want to
stress this second line of future research as one of
the most promising and exciting ones.
Finally, the model presented here is not
only suitable for selectional preference induction.
There are many problems in NLP that involve
three-way co-occurrences. In future work, we
want to apply the NTF model presented here to
other problems in NLP, the most important one be-
ing word sense discrimination.
Acknowledgements
Brett Bader kindly provided his implementation of
non-negative tensor factorization for sparse ma-
trices, from which this research has substantially
benefited. The three anonymous reviewers pro-
vided fruitful comments and remarks, which con-
siderably improved the quality of this paper.
References
Brett W. Bader and Tamara G. Kolda. 2006. Efficient
MATLAB computations with sparse and factored
tensors. Technical Report SAND2006-7592, Sandia
National Laboratories, Albuquerque, NM and Liver-
more, CA, December.
Brett W. Bader and Tamara G. Kolda. 2007. Mat-
lab tensor toolbox version 2.2. http://csmr.ca.
sandia.gov/?tgkolda/TensorToolbox/, Jan-
uary.
Roberto Basili, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
Proceedings of RANLP 2007, Borovets, Bulgaria.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-07), pages 161?170,
Prague, Czech Republic.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39:510?526.
J. D. Carroll and J.-J. Chang. 1970. Analysis of in-
dividual differences in multidimensional scaling via
an n-way generalization of ?eckart-young? decom-
position. Psychometrika, 35:283?319.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information & lexicogra-
phy. Computational Linguistics, 16(1):22?29.
Lieven De Lathauwer, Bart De Moor, and Joos Vande-
walle. 2000. A multilinear singular value decompo-
sition. SIAM Journal on Matrix Analysis and Appli-
cations, 21(4):1253?1278.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of ACL
2007, Prague, Czech Republic.
R.A. Harshman. 1970. Foundations of the parafac pro-
cedure: models and conditions for an ?explanatory?
multi-mode factor analysis. In UCLA Working Pa-
pers in Phonetics, volume 16, pages 1?84, Los An-
geles. University of California.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of Uncertainty in Artificial Intelli-
gence, UAI?99, Stockholm.
H.A.L Kiers and I. van Mechelen. 2001. Three-way
component analysis: Principles and illustrative ap-
plication. Psychological Methods, 6:84?110.
Tamara Kolda and Brett Bader. 2006. The TOPHITS
model for higher-order web link analysis. In Work-
shop on Link Analysis, Counterterrorism and Secu-
rity.
Thomas Landauer and Susan Dumais. 1997. A so-
lution to Plato?s problem: The Latent Semantic
Analysis theory of the acquisition, induction, and
representation of knowledge. Psychology Review,
104:211?240.
Thomas Landauer, Peter Foltz, and Darrell Laham.
1998. An Introduction to Latent Semantic Analysis.
Discourse Processes, 25:295?284.
Daniel D. Lee and H. Sebastian Seung. 2000. Al-
gorithms for non-negative matrix factorization. In
NIPS, pages 556?562.
R.J.F. Ordelman. 2002. Twente Nieuws Corpus
(TwNC), August. Parlevink Language Technology
Group. University of Twente.
K. Pearson. 1901. On lines and planes of closest fit to
systems of points in space. Philosophical Magazine,
2(6):559?572.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
31st Annual Meeting of the ACL, pages 183?190.
Philip Resnik. 1996. Selectional Constraints: An
Information-Theoretic Model and its Computational
Realization. Cognition, 61:127?159, November.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
37th Annual Meeting of the ACL.
Amnon Shashua and Tamir Hazan. 2005. Non-
negative tensor factorization with applications to
statistics and computer vision. In ICML ?05: Pro-
ceedings of the 22nd international conference on
89
Machine learning, pages 792?799, New York, NY,
USA. ACM.
L.R. Tucker. 1966. Some mathematical notes on three-
mode factor analysis. Psychometrika, 31:279?311.
Peter D. Turney. 2007. Empirical evaluation of four
tensor decomposition algorithms. Technical Report
ERB-1152, National Research Council, Institute for
Information Technology.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In Piet Mertens, Cedrick Fairon, Anne
Dister, and Patrick Watrin, editors, TALN06. Verbum
Ex Machina. Actes de la 13e conference sur le traite-
ment automatique des langues naturelles, pages 20?
42, Leuven.
M. Alex O. Vasilescu and Demetri Terzopoulos. 2002.
Multilinear analysis of image ensembles: Tensor-
faces. In ECCV, pages 447?460.
Michael E. Wall, Andreas Rechtsteiner, and Luis M.
Rocha, 2003. Singular Value Decomposition and
Principal Component Analysis, chapter 5, pages 91?
109. Kluwel, Norwell, MA, Mar.
90
