Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 94?103,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Incremental Grammar Induction from Child-Directed
Dialogue Utterances?
Arash Eshghi
Interaction Lab
Heriot-Watt University
Edinburgh, United Kingdom
eshghi.a@gmail.com
Julian Hough and Matthew Purver
Cognitive Science Research Group
Queen Mary University of London
London, United Kingdom
{julian.hough, mpurver}@eecs.qmul.ac.uk
Abstract
We describe a method for learning an in-
cremental semantic grammar from data in
which utterances are paired with logical
forms representing their meaning. Work-
ing in an inherently incremental frame-
work, Dynamic Syntax, we show how
words can be associated with probabilistic
procedures for the incremental projection
of meaning, providing a grammar which
can be used directly in incremental prob-
abilistic parsing and generation. We test
this on child-directed utterances from the
CHILDES corpus, and show that it results
in good coverage and semantic accuracy,
without requiring annotation at the word
level or any independent notion of syntax.
1 Introduction
Human language processing has long been
thought to function incrementally, both in pars-
ing and production (Crocker et al, 2000; Fer-
reira, 1996). This incrementality gives rise to
many characteristic phenomena in conversational
dialogue, including unfinished utterances, inter-
ruptions and compound contributions constructed
by more than one participant, which pose prob-
lems for standard grammar formalisms (Howes et
al., 2012). In particular, examples such as (1) sug-
gest that a suitable formalism would be one which
defines grammaticality not in terms of licensing
strings, but in terms of constraints on the semantic
construction process, and which ensures this pro-
cess is common between parsing and generation.
(1) A: I burnt the toast.
? We are grateful to Ruth Kempson for her support and
helpful discussions throughout this work. We also thank
the CMCL?2013 anonymous reviewers for their constructive
criticism. This work was supported by the EPSRC, RISER
project (Ref: EP/J010383/1), and in part by the EU, FP7
project, SpaceBook (Grant agreement no: 270019).
B: But did you burn . . .
A: Myself? Fortunately not.
[where ?did you burn myself?? if uttered by
the same speaker is ungrammatical]
One such formalism is Dynamic Syntax (DS)
(Kempson et al, 2001; Cann et al, 2005); it
recognises no intermediate layer of syntax, but
instead reflects grammatical constraints via con-
straints on the word-by-word incremental con-
struction of meaning, underpinned by attendant
concepts of underspecification and update.
Eshghi et al (2013) describe a method for in-
ducing a probabilistic DS lexicon from sentences
paired with DS semantic trees (see below) repre-
senting not only their meaning, but their function-
argument structure with fine-grained typing infor-
mation. They apply their method only to an ar-
tificial corpus generated using a known lexicon.
Here, we build on that work to induce a lexi-
con from real child-directed utterances paired with
less structured Logical Forms in the form of TTR
Record Types (Cooper, 2005), thus providing less
supervision. By assuming only the availability of a
small set of general compositional semantic opera-
tions, reflecting the properties of the lambda calcu-
lus and the logic of finite trees, we ensure that the
lexical entries learnt include the grammatical con-
straints and corresponding compositional seman-
tic structure of the language. Our method exhibits
incrementality in two senses: incremental learn-
ing, with the grammar being extended and refined
as each new sentence becomes available; resulting
in an inherently incremental, probabilistic gram-
mar for parsing and production, suitable for use
in state-of-the-art incremental dialogue systems
(Purver et al, 2011) and for modelling human-
human dialogue.
94
?Ty(t)
?Ty(e),
?
?Ty(e ? t)
??
?john?
?Ty(t)
Ty(e),
john
?Ty(e ? t),
?
??
?upset?
?Ty(t)
Ty(e),
john ?Ty(e ? t)
?Ty(e),
?
Ty(e ? (e ? t)),
?y?x.upset?(x)(y)
??
?mary?
Ty(t),?,
upset?(john?)(mary?)
Ty(e),
john
Ty(e ? t),
?x.upset?(x)(mary?)
Ty(e),
mary?
Ty(e ? (e ? t)),
?y?x.upset?(x)(y)
Figure 1: Incremental parsing in DS producing semantic trees: ?John upset Mary?
2 Background
2.1 Grammar Induction and Semantics
We can view existing grammar induction meth-
ods along a spectrum from supervised to unsu-
pervised. Fully supervised methods take a parsed
corpus as input, pairing sentences with syntactic
trees and words with their syntactic categories, and
generalise over the phrase structure rules to learn
a grammar which can be applied to a new set of
data. Probabilities for production rules sharing a
LHS category can be estimated, producing a gram-
mar suitable for probabilistic parsing and disam-
biguation e.g. a PCFG (Charniak, 1996). While
such methods have shown great success, they pre-
suppose detailed prior linguistic information and
are thus inadequate as human grammar learning
models. Fully unsupervised methods, on the other
hand, proceed from unannotated raw data; they
are thus closer to the human language acquisition
setting, but have seen less success. In its pure
form ?positive data only, without bias? unsu-
pervised learning is computationally too complex
(?unlearnable?) in the worst case (Gold, 1967).
Successful approaches involve some prior learning
or bias (see (Clark and Lappin, 2011)) e.g. a set
of known lexical categories, a probability distri-
bution bias (Klein and Manning, 2005) or a semi-
supervised method with shallower (e.g. POS-tag)
annotation (Pereira and Schabes, 1992).
Another point on the spectrum is lightly su-
pervised learning: providing information which
constrains learning but with little or no lexico-
syntactic detail. One possibility is the use of se-
mantic annotation, using sentence-level proposi-
tional Logical Forms (LF). It seems more cogni-
tively plausible, as the learner can be said to be
able to understand, at least in part, the meaning
of what she hears from evidence gathered from
(1) her perception of her local, immediate environ-
ment given appropriate biases on different patterns
of individuation of entities and relationships be-
tween them, and (2) helpful interaction, and joint
focus of attention with an adult (see e.g. (Saxton,
1997)). Given this, the problem she is faced with
is one of separating out the contribution of each
individual linguistic token to the overall meaning
of an uttered linguistic expression (i.e. decompo-
sition), while maintaining and generalising over
several such hypotheses acquired through time as
she is exposed to more utterances involving each
token.
This has been successfully applied in Combi-
natorial Categorial Grammar (CCG) (Steedman,
2000), as it tightly couples compositional seman-
tics with syntax (Zettlemoyer and Collins, 2007;
Kwiatkowski et al, 2010; Kwiatkowski et al,
2012); as CCG is a lexicalist framework, grammar
learning involves inducing a lexicon assigning to
each word its syntactic and semantic contribution.
Moreover, the grammar is learnt incrementally, in
the sense that the learner collects data over time
and does the learning sentence by sentence.
Following this approach, Eshghi et al (2013)
outline a method for inducing a DS grammar
from semantic LFs. This brings an added di-
mension of incrementality: not only is learning
sentence-by-sentence incremental, but the gram-
mar learned is inherently word-by-word incre-
mental (see section 2.2 below). However, their
method requires a higher degree of supervision
than (Kwiatkowski et al, 2012): the LFs assumed
are not simply flat semantic formulae, but full DS
semantic trees (see e.g. Fig. 1) containing infor-
mation about the function-argument structure re-
95
quired for their composition, in addition to fine
grained type and formula annotations. Further,
they test their method only on artificial data cre-
ated using a known, manually-specified DS gram-
mar. In contrast, in this paper we provide an
approach which can learn from LFs without any
compositional structure information, and test it on
real language data; thus providing the first prac-
tical learning system for an explicitly incremental
grammar that we are aware of.
2.2 Dynamic Syntax (DS)
Dynamic Syntax (Kempson et al, 2001; Cann et
al., 2005) is a parsing-directed grammar formal-
ism, which models the word-by-word incremental
processing of linguistic input. Unlike many other
formalisms, DS models the incremental building
up of interpretations without presupposing or in-
deed recognising an independent level of syntactic
processing. Thus, the output for any given string
of words is a purely semantic tree representing
its predicate-argument structure; tree nodes cor-
respond to terms in the lambda calculus, deco-
rated with labels expressing their semantic type
(e.g. Ty(e)) and formula, with beta-reduction de-
termining the type and formula at a mother node
from those at its daughters (Figure 1).
These trees can be partial, containing unsatis-
fied requirements for node labels (e.g. ?Ty(e) is a
requirement for future development to Ty(e)), and
contain a pointer ? labelling the node currently
under development. Grammaticality is defined as
parsability: the successful incremental construc-
tion of a tree with no outstanding requirements (a
complete tree) using all information given by the
words in a sentence. The complete sentential LF
is then the formula decorating the root node ? see
Figure 1. Note that in these trees, leaf nodes do
not necessarily correspond to words, and may not
be in linear sentence order; syntactic structure is
not explicitly represented, only the structure of se-
mantic predicate-argument combination.
2.2.1 Actions in DS
The parsing process is defined in terms of condi-
tional actions: procedural specifications for mono-
tonic tree growth. These include general structure-
building principles (computational actions), puta-
tively independent of any particular natural lan-
guage, and language-specific actions associated
with particular lexical items (lexical actions). The
latter are what we learn from data here.
Computational actions These form a small,
fixed set, which we assume as given here. Some
merely encode the properties of the lambda cal-
culus and the logical tree formalism itself, LoFT
(Blackburn and Meyer-Viol, 1994) ? these we
term inferential actions. Examples include THIN-
NING (removal of satisfied requirements) and
ELIMINATION (beta-reduction of daughter nodes
at the mother). These actions are language-
independent, cause no ambiguity, and add no new
information to the tree; as such, they apply non-
optionally whenever their preconditions are met.
Other computational actions reflect the fun-
damental predictivity and dynamics of the DS
framework. For example, *-ADJUNCTION in-
troduces a single unfixed node with underspec-
ified tree position (replacing feature-passing or
type-raising concepts for e.g. long-distance depen-
dency); and LINK-ADJUNCTION builds a paired
(?linked?) tree corresponding to semantic con-
junction (licensing relative clauses, apposition and
more). These actions represent possible parsing
strategies and can apply optionally whenever their
preconditions are met. While largely language-
independent, some are specific to language type
(e.g. INTRODUCTION-PREDICTION in the form
used here applies only to SVO languages).
Lexical actions The lexicon associates words
with lexical actions; like computational actions,
these are sequences of tree-update actions in an
IF..THEN..ELSE format, and composed of ex-
plicitly procedural atomic tree-building actions
such as make (creates a new daughter node),
go (moves the pointer), and put (decorates the
pointed node with a label). Figure 2 shows an ex-
ample for a proper noun, John. The action checks
whether the pointed node (marked as ?) has a re-
quirement for type e; if so, it decorates it with type
e (thus satisfying the requirement), formula John?
and the bottom restriction ???? (meaning that the
node cannot have any daughters). Otherwise the
action aborts, i.e. the word ?John? cannot be parsed
in the context of the current tree.
Graph-based Parsing & Generation These ac-
tions define the parsing process. Given a sequence
of words (w1, w2, ..., wn), the parser starts from
the axiom tree T0 (a requirement to construct a
complete propositional tree, ?Ty(t)), and applies
the corresponding lexical actions (a1, a2, . . . , an),
optionally interspersing computational actions.
96
Action Input tree Output tree
John
IF ?Ty(e)
THEN put(Ty(e))
put(Fo(John?)
put(????)
ELSE ABORT
?Ty(t)
?Ty(e),
?
?Ty(e ? t)
?John??? ?Ty(t)
Ty(e), ?Ty(e)
John?, ????,?
?Ty(e ? t)
Figure 2: Lexical action for the word ?John?
T0
T1intro
T2pred
T3
link-adj
T4*-adj
T5
john
abort
T6
john
?john?
T7
thin
T8
comp
T9
pred
T10
link-adj
T11
thin
T12
comp
T13
likes
abort
abort
?likes?
Figure 3: DS parsing as a graph: actions (edges) are transitions between partial trees (nodes).
This parsing process can be modelled as a di-
rected acyclic graph (DAG) rooted at T0, with par-
tial trees as nodes, and computational and lexi-
cal actions as edges (i.e. transitions between trees)
(Sato, 2011). Figure 3 shows an example: here,
intro, pred and *adj correspond to the computa-
tional actions INTRODUCTION, PREDICTION and
*-ADJUNCTION respectively; and ?john? is a lex-
ical action. Different DAG paths represent dif-
ferent parsing strategies, which may succeed or
fail depending on how the utterance is continued.
Here, the path T0?T3 will succeed if ?John? is the
subject of an upcoming verb (?John upset Mary?);
T0 ? T4 will succeed if ?John? turns out to be a
left-dislocated object (?John, Mary upset?).
This incrementally constructed DAG makes up
the entire parse state at any point. The right-
most nodes (i.e. partial trees) make up the current
maximal semantic information; these nodes with
their paths back to the root (tree-transition actions)
make up the linguistic context for ellipsis and
pronominal construal (Purver et al, 2011). Given
a conditional probability distribution P (a|w, T )
over possible actions a given a word w and (some
set of features of) the current partial tree T , we can
parse probabilistically, constructing the DAG in a
best-first, breadth-first or beam parsing manner.
Generation uses exactly the same actions and
structures, and can be modelled on the same DAG
with the addition only of a goal tree; partial
trees are checked for subsumption of the goal
at each stage. The framework therefore inher-
ently provides both parsing and generation that
are word-by-word incremental and interchange-
able, commensurate with psycholinguistic results
(Lombardo and Sturt, 1997; Ferreira and Swets,
2002) and suitable for modelling dialogue (Howes
et al, 2012). While standard grammar formalisms
can of course also be used with incremental pars-
ing or generation algorithms (Hale, 2001; Collins
and Roark, 2004; Clark and Curran, 2007), their
string-based grammaticality and lack of inherent
parsing-generation interoperability means exam-
ples such as (1) remain problematic.
3 Method
Our task here is to learn an incremental DS gram-
mar; following Kwiatkowski et al (2012), we
assume as input a set of sentences paired with
their semantic LFs. Eshghi et al (2013) outline a
method for inducing DS grammars from semantic
DS trees (e.g. Fig. 1), in which possible lexical en-
tries are incrementally hypothesized, constrained
by subsumption of the target tree for the sentence.
Here, however, this structured tree information is
not available to us; our method must therefore con-
strain hypotheses via compatibility with the sen-
tential LF, represented as Record Types of Type
Theory with Records (TTR).
3.1 Type Theory with Records (TTR)
Type Theory with Records (TTR) is an exten-
sion of standard type theory shown useful in se-
mantics and dialogue modelling (Cooper, 2005;
Ginzburg, 2012). It is also used for representing
97
non-linguistic context such as the visual percep-
tion of objects (Dobnik et al, 2012), suggesting
potential for embodied learning in future work.
Some DS variants have incorporated TTR as the
semantic LF representation (Purver et al, 2011;
Hough and Purver, 2012; Eshghi et al, 2012).
Here, it can provide us with the mechanism we
need to constrain hypotheses in induction by re-
stricting them to those which lead to subtypes of
the known sentential LF.
In TTR, logical forms are specified as record
types (RTs), sequences of fields of the form [ l : T ]
containing a label l and a type T . RTs can be wit-
nessed (i.e. judged true) by records of that type,
where a record is a sequence of label-value pairs
[ l = v ], and [ l = v ] is of type [ l : T ] just in case
v is of type T .
R1 :
?
?
l1 : T1
l2=a : T2
l3=p(l2) : T3
?
? R2 :
[
l1 : T1
l2 : T2?
]
R3 : []
Figure 4: Example TTR record types
Fields can be manifest, i.e. given a singleton
type e.g. [ l : Ta ] where Ta is the type of which
only a is a member; here, we write this using the
syntactic sugar [ l=a : T ]. Fields can also be de-
pendent on fields preceding them (i.e. higher) in
the record type ? see R1 in Figure 4. Importantly
for us here, the standard subtyping relation ? can
be defined for record types: R1 ? R2 if for all
fields [ l : T2 ] in R2, R1 contains [ l : T1 ] where
T1 ? T2. In Figure 4, R1 ? R2 if T2 ? T2? , and
both R1 and R2 are subtypes of R3.
Following Purver et al (2011), we assume
that DS tree nodes are decorated not with simple
atomic formulae but with RTs, and correspond-
ing lambda abstracts representing functions from
RT to RT (e.g. ?r : [ l1 : T1 ].[ l2=r.l1 : T1 ] where
r.l1 is a path expression referring to the label l1
in r) ? see Figure 5. The equivalent of conjunc-
tion for linked trees is now RT extension (concate-
nation modulo relabelling ? see (Cooper, 2005;
Ferna?ndez, 2006)). TTR?s subtyping relation now
allows a record type at the root node to be in-
ferred for any partial tree, and incrementally fur-
ther specified via subtyping as parsing proceeds
(Hough and Purver, 2012).
We assume a field head in all record types, with
this corresponding to the DS tree node type. We
also assume a neo-Davidsonian representation of
?, T y(t),
?
?
?
x=john : e
e=arrive : es
p=subj(e,x) : t
head=p : t
?
?
?
Ty(e),
[
x=john : e
head=x : e
]
Ty(e ? t),
?r :
[
head : e
]
.
?
?
?
x=r.head : e
e=arrive : es
p=subj(e,x) : t
head=p : t
?
?
?
Figure 5: DS-TTR tree
predicates, with fields corresponding to the event
and to each semantic role; this allows all available
semantic information to be specified incrementally
via strict subtyping (e.g. providing the subj() field
when subject but not object has been parsed) ? see
Figure 5 for an example.
3.2 Problem Statement
Our induction procedure now assumes as input:
? a known set of DS computational actions.
? a set of training examples of the form
?Si, RTi?, where Si = ?w1 . . . wn? is a sen-
tence of the language and RTi ? henceforth
referred to as the target RT ? is the record
type representing the meaning of Si.
The output is a grammar specifying the possi-
ble lexical actions for each word in the corpus.
Given our data-driven approach, we take a prob-
abilistic view: we take this grammar as associat-
ing each word w with a probability distribution ?w
over lexical actions. In principle, for use in pars-
ing, this distribution should specify the posterior
probability p(a|w, T ) of using a particular action
a to parse a word w in the context of a particular
partial tree T . However, here we make the sim-
plifying assumption that actions are conditioned
solely on one feature of a tree, the semantic type
Ty of the currently pointed node; and that actions
apply exclusively to one such type (i.e. ambiguity
of type implies multiple actions). This simplifies
our problem to specifying the probability p(a|w).
In traditional DS terms, this is equivalent to as-
suming that all lexical actions have a simple IF
clause of the form IF ?Ty(X); this is true of
most lexical actions in existing DS grammars (see
Fig. 2), but not all. Our assumption may there-
fore lead to over-generation ? inducing actions
which can parse some ungrammatical strings ? we
must rely on the probabilities learned to make such
98
parses unlikely, and evaluate this in Section 4.
Given this, our focus here is on learning the THEN
clauses of lexical actions: sequences of DS atomic
actions such as go, make, and put (Fig. 2), but now
with attendant posterior probabilities. We will
henceforth refer to these sequences as lexical hy-
potheses. We first describe how we construct lexi-
cal hypotheses from individual training examples;
we then show how to generalise over these, while
incrementally estimating corresponding probabil-
ity distributions.
3.3 Hypothesis construction
DS is strictly monotonic: actions can only extend
the current (partial) tree Tcur, deleting nothing ex-
cept satisfied requirements. Thus, we can hypoth-
esise lexical actions by incrementally exploring
the space of all monotonic, well-formed exten-
sions T of Tcur, whose maximal semantics R is
a supertype of (extendible to) the target RT (i.e.
R ? RT ). This gives a bounded space described
by a DAG equivalent to that of section 2.2.1: nodes
are trees; edges are possible extensions; paths start
from Tcur and end at any tree with LF RT . Edges
may be either known computational actions or
new lexical hypotheses. The space is further con-
strained by the properties of the lambda-calculus
and the modal tree logic LoFT (not all possible
trees and extensions are well-formed).1
Hypothesising increments In purely semantic
terms, the hypothesis space at any point is the pos-
sible set of TTR increments from the current LF
R to the target RT . We can efficiently compute
and represent these possible increments using a
type lattice (see Figure 6),2 which can be con-
structed for the whole sentence before processing
each training example. Each edge is a RTR repre-
senting an increment from one RT, Rj , to another,
Rj+1, such that Rj ? RI = Rj+1 (where ? rep-
resents record type intersection (Cooper, 2005));
possible parse DAG paths must correspond to
some path through this lattice.
Hypothesising tree structure These DAG paths
can now be hypothesised with the lattice as a con-
straint: hypothesising possible sequences of ac-
1We also prevent arbitrary type-raising by restricting the
types allowed, taking the standard DS assumption that noun
phrases have semantic type e (rather than a higher type as in
Generalized Quantifier theory) and common nouns their own
type cn, see Cann et al (2005), chapter 3 for details.
2Clark (2011) similarly use a concept lattice relating
strings to their contexts in syntactic grammar induction.
Ri : []
R11 :
[
a : b
]
R12 :
[
c : d
]
R12 :
[
e : f
]
R21 :
[
a : b
c : d
]
R22 :
[
a : b
e : f
]
R22 :
[
c : d
e : f
]
RT :
?
?
a : b
c : d
e : f
?
?
Figure 6: RT extension hypothesis lattice
tions which extend the tree to produce the required
semantic increment, while the increments them-
selves constitute a search space of their own which
we explore by traversing the lattice.
The lexical hypotheses comprising these DAG
paths are divide into two general classes: (1) tree-
building hypotheses, which hypothesise appropri-
ately typed daughters to compose a given node;
and (2) content hypotheses, which decorate leaf
nodes with appropriate formulae from Ri (non-
leaf nodes then receive their content via beta-
reduction/extension of daughters).
Tree-building can be divided into two general
options: functional decomposition (corresponding
to the addition of daughter nodes with appropri-
ate types and formulae which will form a suitable
mother node by beta-reduction); and type exten-
sion (corresponding to the adjunction of a linked
tree whose LF will extend that of the current tree,
see Sec. 3.1 above). The availability of the former
is constrained by the presence of suitable depen-
dent types in the LF (e.g. in Fig. 5, p = subj(e, x)
depends on the fields with labels x and e, and
could therefore be hypothesised as the body of a
function with x and/or e as argument). The latter is
more generally available, but constrained by shar-
ing of a label between the resulting linked trees.
Figure 7 shows an example: a template for
functional decomposition hypotheses, extending a
node with some type requirement ?Ty(X) with
daughter nodes which can combine to satisfy that
requirement ? here, of types Y and Y ? X.
Specific instantiations are limited to a finite set of
types: e.g. X = e ? t and Y = e is allowed,
but higher types for Y are not. We implement
these constraints by packaging together permitted
sequences of tree updates as macros, and using
these macros to hypothesise DAG paths commen-
surate with the lattice.
Finally, semantic content decorations (as se-
99
IF ?Ty(X)
THEN make(??0?); go(??0?)
put(?Ty(Y )); go(???)
make(??1?); go(??1?)
put(?Ty(Y ? X)); go(?)
ELSE ABORT
Figure 7: Tree-building hypothesis
quences of put operations) are hypothesised for
the leaf nodes of the tree thus constructed; these
are now determined entirely by the tree structure
so far hypothesised and the target LF RT .
3.4 Probabilistic Grammar Estimation
This procedure produces, for each training sen-
tence ?w1 . . . wn?, all possible sequences of ac-
tions that lead from the axiom tree T0 to a tree
with the target RT as its semantics. These must
now be split into n sub-sequences, hypothesising
a set of word boundaries to form discrete word hy-
potheses; and a probability distribution estimated
over this (large) word hypothesis space to provide
a grammar that can be useful in parsing. For this,
we apply the procedure of Eshghi et al (2013).
For each training sentence S = ?w1 . . . wn?,
we have a set HT of possible Hypothesis Tuples
(sequences of word hypotheses), each of the form
HTj = ?hj1 . . . h
j
n?, where hji is the word hypoth-
esis for wi in HTj . We must estimate a prob-
ability distribution ?w over hypotheses for each
word w, where ?w(h) is the posterior probability
p(h|w) of a given word hypothesis h being used to
parse w. Eshghi et al (2013) define an incremen-
tal version of Expectation-Maximisation (Demp-
ster et al, 1977) for use in this setting.
Re-estimation At any point, the Expectation
step assigns each hypothesis tuple HTj a proba-
bility based on the current estimate ??w:
p(HTj|S) =
n
?
i=1
p(hji |wi) =
n
?
i=1
??wi(h
j
i ) (2)
The Maximisation step then re-estimates
p(h|w) as the normalised sum of the probabilities
of all observed tuples HTj which contain h,w:
???w(h) =
1
Z
?
{j|h,w?HTj}
n
?
i=1
??wi(h
j
i ) (3)
where Z is the appropriate normalising constant
summed over all the HTj?s.
Incremental update The estimate of ?w is now
updated incrementally at each training example:
the new estimate ?Nw is a weighted average of the
previous estimate ?N?1w and the new value from
the current example ???w from equation (3):
?Nw (h) =
N ? 1
N ?
N?1
w (h) +
1
N ?
??
w(h) (4)
?e.not(aux|do(v|have(pro|he, det|a(x,n|hat(x)), e), e), e)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
e=have : es
p3=not(e) : t
p2=do-aux(e) : t
r :
?
?
x : e
p=hat(x) : t
head=x : e
?
?
x2=?(r.head,r) : e
x1=he : e
p1=object(e,x2) : t
p=subject(e,x1) : t
head=e : es
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 8: Conversion of LFs from FOL to TTR.
For the first training example, a uniform distribu-
tion is assumed; when subsequent examples pro-
duce new previously unseen hypotheses these are
assigned probabilities uniformly distributed over a
held-out probability mass.
4 Experimental Setup
Corpus We tested our approach on a section
of the Eve corpus within CHILDES (MacWhin-
ney, 2000), a series of English child-directed ut-
terances, annotated with LFs by Kwiatkowski et
al. (2012) following Sagae et al (2004)?s syntactic
annotation. We convert these LFs into semanti-
cally equivalent RTs; e.g. Fig 8 shows the conver-
sion to a record type for ?He doesn?t have a hat?.
Importantly, our representations remove all
part-of-speech or syntactic information; e.g. the
subject, object and indirect object predicates func-
tion as purely semantic role information express-
ing an event?s participants. This includes e.g.
do-aux(e) in (8), which is taken merely to rep-
resent temporal/aspectual information about the
event, and could be part of any word hypothesis.
From this corpus we selected 500 short
utterance-record type pairs. The minimum utter-
ance length in this set is 1 word, maximum 7,
mean 3.7; it contains 1481 word tokens of 246
types, giving a type:token ratio of 6.0). We use the
first 400 for training and 100 for testing; the test
set alo has a mean utterance length of 3.7 words,
and contains only words seen in training.
Evaluation We evaluate our learner by compar-
ing the record type semantic LFs produced using
the induced lexicon against the gold standard LFs,
calculating precision, recall and f-score using a
method similar to Allen et al (2008).
100
Coverage % Precision Recall F-Score
Top-1 59 0.548 0.549 0.548
Top-2 85 0.786 0.782 0.782
Top-3 92 0.854 0.851 0.851
Table 1: Results: parse coverage & accuracy using
the top N hypotheses induced in training.
Each field has a potential score in the range
[0,1]. A method maxMapping(R1, R2) con-
structs a mapping from fields in R1 to those in R2
to maximise alignment, with fields that map com-
pletely scoring a full 1, and partially mapped fields
receiving less, depending on the proportion of the
R1 field?s representation that subsumes its mapped
R2 field;e.g. a unary predicate field in RT2 such
as
[
p=there(e) : t
]
could score a maximum of
3 - 1 for correct type t, 1 for correct predicate
there and 1 for the subsumption of its argument
e; we use the total to normalise the final score.
The potential maximum for any pair is therefore
the number of fields in R1 (including those in em-
bedded record types). So, for hypothesis H and
goal record type G, with NH and NG fields re-
spectively:
(5) precision = maxMapping(H,G)/NH
recall = maxMapping(H,G)/NG
5 Results
Table 1 shows that the grammar learned achieves
both good parsing coverage and semantic accu-
racy. Using the top 3 lexical hypotheses induced
from training, 92% of test set utterances receive a
parse, and average LF f-score reaches 0.851.
We manually inspected the learned lexicon for
instances of ambiguous words to assess the sys-
tem?s ability to disambiguate (e.g. the word ??s?
(is) has three different senses in our corpus: (1)
auxiliary, e.g. ?the coffee?s coming?; (2) verb
predicating NP identity, e.g. ?that?s a girl?; and
(3) verb predicating location, e.g. ?where?s the
pencil?). From these the first two were in the top
3 hypotheses (probabilities p=0.227 and p=0.068).
For example, the lexical entry learned for (2) is
shown in Fig. 9.
However, less common words fared worse: e.g.
the double object verb ?put?, with only 3 tokens,
had no correct hypothesis in the top 5. Given suffi-
cient frequency and variation in the token distribu-
tions, our method appears successful in inducing
the correct incremental grammar. However, the
complexity of the search space also limits the pos-
sibility of learning from larger record types, as the
space of possible subtypes used for hypothesising
IF ?Ty(e ? t)
THEN make(??0?); go(??0?)
put(?Ty(e))
go(??0?)
make(??1?); go(??1?)
put(Ty(e ? (e ? t)))
put(Fo(
?r1 :
[
head : e
]
?r2 :
[
head : e
]
.
?
?
?
?
?
?
?
?
x1=r1.head : e
x2=r2.head : e
e=eq : es
p1=subj(e,x2) : t
p2=obj(e,x1) : t
head=e : t
?
?
?
?
?
?
?
?
))
put(????)
ELSE ABORT
Figure 9: Action learned for second sense of ?is?
tree structure grows exponentially with the num-
ber of fields in the type. Therefore, when learning
from longer, more complicated sentences, we may
need to bring in further sources of bias to constrain
our hypothesis process further (e.g. learning from
shorter sentences first).
6 Conclusions
We have outlined a novel method for the induc-
tion of a probabilistic grammar in an inherently in-
cremental and semantic formalism, Dynamic Syn-
tax, compatible with dialogue phenomena such
as compound contributions and with no indepen-
dent level of syntactic phrase structure. Assum-
ing only general compositional mechanisms, our
method learns from utterances paired with their
logical forms represented as TTR record types.
Evaluation on a portion of the CHILDES corpus
of child-directed dialogue utterances shows good
coverage and semantic accuracy, which lends sup-
port to viewing it as a plausible, yet idealised, lan-
guage acquisition model.
Future work planned includes refining the
method outlined above for learning from longer
utterances, and then from larger corpora e.g. the
Groningen Meaning Bank (Basile et al, 2012),
which includes more complex structures. This will
in turn enable progress towards large-scale incre-
mental semantic parsers and allow further investi-
gation into semantically driven language learning.
101
References
James F. Allen, Mary Swift, and Will de Beaumont.
2008. Deep Semantic Analysis of Text. In Johan
Bos and Rodolfo Delmonte, editors, Semantics in
Text Processing. STEP 2008 Conference Proceed-
ings, volume 1 of Research in Computational Se-
mantics, pages 343?354. College Publications.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation (LREC 2012), pages 3196?3200, Istan-
bul, Turkey.
Patrick Blackburn and Wilfried Meyer-Viol. 1994.
Linguistics, logic and finite trees. Logic Journal
of the Interest Group of Pure and Applied Logics,
2(1):3?29.
Ronnie Cann, Ruth Kempson, and Lutz Marten. 2005.
The Dynamics of Language. Elsevier, Oxford.
Eugene Charniak. 1996. Statistical Language Learn-
ing. MIT Press.
Stephen Clark and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493?552.
Alexander Clark and Shalom Lappin. 2011. Linguistic
Nativism and the Poverty of the Stimulus. Wiley-
Blackwell.
Alexander Clark. 2011. A learnable representation for
syntax using residuated lattices. In Philippe Groote,
Markus Egg, and Laura Kallmeyer, editors, Formal
Grammar, volume 5591 of Lecture Notes in Com-
puter Science, pages 183?198. Springer Berlin Hei-
delberg.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42ndMeeting of the ACL, pages 111?118,
Barcelona.
Robin Cooper. 2005. Records and record types in se-
mantic theory. Journal of Logic and Computation,
15(2):99?112.
Matthew Crocker, Martin Pickering, and Charles
Clifton, editors. 2000. Architectures and Mecha-
nisms in Sentence Comprehension. Cambridge Uni-
versity Press.
A.P. Dempster, N.M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety. Series B (Methodological), 39(1):1?38.
Simon Dobnik, Robin Cooper, and Staffan Larsson.
2012. Modelling language, action, and perception in
type theory with records. In Proceedings of the 7th
International Workshop on Constraint Solving and
Language Processing (CSLP12), pages 51?63.
Arash Eshghi, Julian Hough, Matthew Purver, Ruth
Kempson, and Eleni Gregoromichelaki. 2012. Con-
versational interactions: Capturing dialogue dynam-
ics. In S. Larsson and L. Borin, editors, From Quan-
tification to Conversation: Festschrift for Robin
Cooper on the occasion of his 65th birthday, vol-
ume 19 of Tributes, pages 325?349. College Publi-
cations, London.
Arash Eshghi, Matthew Purver, and Julian Hough.
2013. Probabilistic induction for an incremental se-
mantic grammar. In Proceedings of the 10th In-
ternational Conference on Computational Seman-
tics (IWCS 2013) ? Long Papers, pages 107?118,
Potsdam, Germany, March. Association for Compu-
tational Linguistics.
Raquel Ferna?ndez. 2006. Non-Sentential Utterances
in Dialogue: Classification, Resolution and Use.
Ph.D. thesis, King?s College London, University of
London.
Fernanda Ferreira and Benjamin Swets. 2002. How
incremental is language production? evidence from
the production of utterances requiring the compu-
tation of arithmetic sums. Journal of Memory and
Language, 46:57?84.
Victor Ferreira. 1996. Is it better to give than to do-
nate? Syntactic flexibility in language production.
Journal of Memory and Language, 35:724?755.
Jonathan Ginzburg. 2012. The Interactive Stance:
Meaning for Conversation. Oxford University
Press.
E. Mark Gold. 1967. Language identification in the
limit. Information and Control, 10(5):447?474.
John Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Proceedings of the
2nd Conference of the North American Chapter of
the Association for Computational Linguistics, Pitts-
burgh, PA.
Julian Hough and Matthew Purver. 2012. Process-
ing self-repairs in an incremental type-theoretic di-
alogue system. In Proceedings of the 16th SemDial
Workshop on the Semantics and Pragmatics of Di-
alogue (SeineDial), pages 136?144, Paris, France,
September.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL 2012 Confer-
ence), pages 79?83, Seoul, South Korea, July. Asso-
ciation for Computational Linguistics.
Ruth Kempson,WilfriedMeyer-Viol, and Dov Gabbay.
2001. Dynamic Syntax: The Flow of Language Un-
derstanding. Blackwell.
102
Dan Klein and Christopher D. Manning. 2005. Nat-
ural language grammar induction with a genera-
tive constituent-context mode. Pattern Recognition,
38(9):1407?1419.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, andMark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1223?1233, Cambridge, MA, Oc-
tober. Association for Computational Linguistics.
Tom Kwiatkowski, Sharon Goldwater, Luke Zettle-
moyer, and Mark Steedman. 2012. A proba-
bilistic model of syntactic and semantic acquisition
from child-directed utterances and their meanings.
In Proceedings of the Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL).
Vincenzo Lombardo and Patrick Sturt. 1997. Incre-
mental processing and infinite local ambiguity. In
Proceedings of the 1997 Cognitive Science Confer-
ence.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Lawrence Erlbaum As-
sociates, Mahwah, New Jersey, third edition.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting
of the Association for Computational Linguistics,
pages 128?135, Newark, Delaware, USA, June. As-
sociation for Computational Linguistics.
Matthew Purver, Arash Eshghi, and Julian Hough.
2011. Incremental semantic construction in a di-
alogue system. In J. Bos and S. Pulman, editors,
Proceedings of the 9th International Conference on
Computational Semantics, pages 365?369, Oxford,
UK, January.
Kenji Sagae, Brian MacWhinney, and Alon Lavie.
2004. Adding syntactic annotations to transcripts of
parent-child dialogs. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation (LREC), pages 1815?1818, Lisbon.
Yo Sato. 2011. Local ambiguity, search strate-
gies and parsing in Dynamic Syntax. In E. Gre-
goromichelaki, R. Kempson, and C. Howes, editors,
The Dynamics of Lexical Interfaces. CSLI Publica-
tions.
Matthew Saxton. 1997. The contrast theory of nega-
tive input. Journal of Child Language, 24(1):139?
161.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
103
