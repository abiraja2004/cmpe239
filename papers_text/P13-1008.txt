Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 73?82,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Joint Event Extraction via Structured Prediction with Global Features
Qi Li Heng Ji Liang Huang
Departments of Computer Science and Linguistics
The Graduate Center and Queens College
City University of New York
New York, NY 10016, USA
{liqiearth, hengjicuny, liang.huang.sh}@gmail.com
Abstract
Traditional approaches to the task of ACE
event extraction usually rely on sequential
pipelines with multiple stages, which suf-
fer from error propagation since event trig-
gers and arguments are predicted in isola-
tion by independent local classifiers. By
contrast, we propose a joint framework
based on structured prediction which ex-
tracts triggers and arguments together so
that the local predictions can be mutu-
ally improved. In addition, we propose
to incorporate global features which ex-
plicitly capture the dependencies of multi-
ple triggers and arguments. Experimental
results show that our joint approach with
local features outperforms the pipelined
baseline, and adding global features fur-
ther improves the performance signifi-
cantly. Our approach advances state-of-
the-art sentence-level event extraction, and
even outperforms previous argument la-
beling methods which use external knowl-
edge from other sentences and documents.
1 Introduction
Event extraction is an important and challeng-
ing task in Information Extraction (IE), which
aims to discover event triggers with specific types
and their arguments. Most state-of-the-art ap-
proaches (Ji and Grishman, 2008; Liao and Gr-
ishman, 2010; Hong et al, 2011) use sequential
pipelines as building blocks, which break down
the whole task into separate subtasks, such as
trigger identification/classification and argument
identification/classification. As a common draw-
back of the staged architecture, errors in upstream
component are often compounded and propagated
to the downstream classifiers. The downstream
components, however, cannot impact earlier deci-
sions. For example, consider the following sen-
tences with an ambiguous word ?fired?:
(1) In Baghdad, a cameraman died when an
American tank fired on the Palestine Hotel.
(2) He has fired his air defense chief .
In sentence (1), ?fired? is a trigger of type Attack.
Because of the ambiguity, a local classifier may
miss it or mislabel it as a trigger of End-Position.
However, knowing that ?tank? is very likely to be
an Instrument argument of Attack events, the cor-
rect event subtype assignment of ?fired? is obvi-
ously Attack. Likewise, in sentence (2), ?air de-
fense chief? is a job title, hence the argument clas-
sifier is likely to label it as an Entity argument for
End-Position trigger.
In addition, the local classifiers are incapable
of capturing inter-dependencies among multiple
event triggers and arguments. Consider sentence
(1) again. Figure 1 depicts the corresponding
event triggers and arguments. The dependency be-
tween ?fired? and ?died? cannot be captured by the
local classifiers, which may fail to attach ?camera-
man? to ?fired? as a Target argument. By using
global features, we can propagate the Victim ar-
gument of the Die event to the Target argument
of the Attack event. As another example, know-
ing that an Attack event usually only has one At-
tacker argument, we could penalize assignments
in which one trigger has more than one Attacker.
Such global features cannot be easily exploited by
a local classifier.
Therefore, we take a fresh look at this prob-
lem and formulate it, for the first time, as a struc-
tured learning problem. We propose a novel joint
event extraction algorithm to predict the triggers
and arguments simultaneously, and use the struc-
tured perceptron (Collins, 2002) to train the joint
model. This way we can capture the dependencies
between triggers and argument as well as explore
73
In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel.
AttackDie
Instrument
Place
Victim
Target
Instrument
Target
Place
Figure 1: Event mentions of example (1). There are two event mentions that share three arguments,
namely the Die event mention triggered by ?died?, and the Attack event mention triggered by ?fired?.
arbitrary global features over multiple local pre-
dictions. However, different from easier tasks such
as part-of-speech tagging or noun phrase chunking
where efficient dynamic programming decoding is
feasible, here exact joint inference is intractable.
Therefore we employ beam search in decoding,
and train the model using the early-update percep-
tron variant tailored for beam search (Collins and
Roark, 2004; Huang et al, 2012).
We make the following contributions:
1. Different from traditional pipeline approach,
we present a novel framework for sentence-
level event extraction, which predicts triggers
and their arguments jointly (Section 3).
2. We develop a rich set of features for event
extraction which yield promising perfor-
mance even with the traditional pipeline
(Section 3.4.1). In this paper we refer to them
as local features.
3. We introduce various global features to ex-
ploit dependencies among multiple triggers
and arguments (Section 3.4.2). Experi-
ments show that our approach outperforms
the pipelined approach with the same set of
local features, and significantly advances the
state-of-the-art with the addition of global
features which brings a notable further im-
provement (Section 4).
2 Event Extraction Task
In this paper we focus on the event extraction task
defined in Automatic Content Extraction (ACE)
evaluation.1 The task defines 8 event types and
33 subtypes such as Attack, End-Position etc. We
introduce the terminology of the ACE event ex-
traction that we used in this paper:
1http://projects.ldc.upenn.edu/ace/
? Event mention: an occurrence of an event
with a particular type and subtype.
? Event trigger: the word most clearly ex-
presses the event mention.
? Event argument: an entity mention, tempo-
ral expression or value (e.g. Job-Title) that
serves as a participant or attribute with a spe-
cific role in an event mention.
? Event mention: an instance that includes one
event trigger and some arguments that appear
within the same sentence.
Given an English text document, an event ex-
traction system should predict event triggers with
specific subtypes and their arguments from each
sentence. Figure 1 depicts the event triggers and
their arguments of sentence (1) in Section 1. The
outcome of the entire sentence can be considered a
graph in which each argument role is represented
as a typed edge from a trigger to its argument.
In this work, we assume that argument candi-
dates such as entities are part of the input to the
event extraction, and can be from either gold stan-
dard or IE system output.
3 Joint Framework for Event Extraction
Based on the hypothesis that facts are inter-
dependent, we propose to use structured percep-
tron with inexact search to jointly extract triggers
and arguments that co-occur in the same sentence.
In this section, we will describe the training and
decoding algorithms for this model.
3.1 Structured perceptron with beam search
Structured perceptron is an extension to the stan-
dard linear perceptron for structured prediction,
which was proposed in (Collins, 2002). Given a
sentence instance x ? X , which in our case is a
sentence with argument candidates, the structured
perceptron involves the following decoding prob-
74
lem which finds the best configuration z ? Y ac-
cording to the current model w:
z = argmax
y??Y(x)
w ? f(x, y?) (1)
where f(x, y?) represents the feature vector for in-
stance x along with configuration y?.
The perceptron learns the model w in an on-
line fashion. Let D = {(x(j), y(j))}nj=1 be the set
of training instances (with j indexing the current
training instance). In each iteration, the algorithm
finds the best configuration z for x under the cur-
rent model (Eq. 1). If z is incorrect, the weights
are updated as follows:
w = w + f(x, y)? f(x, z) (2)
The key step of the training and test is the de-
coding procedure, which aims to search for the
best configuration under the current parameters. In
simpler tasks such as part-of-speech tagging and
noun phrase chunking, efficient dynamic program-
ming algorithms can be employed to perform ex-
act inference. Unfortunately, it is intractable to
perform the exact search in our framework be-
cause: (1) by jointly modeling the trigger labeling
and argument labeling, the search space becomes
much more complex. (2) we propose to make use
of arbitrary global features, which makes it infea-
sible to perform exact inference efficiently.
To address this problem, we apply beam-search
along with early-update strategy to perform inex-
act decoding. Collins and Roark (2004) proposed
the early-update idea, and Huang et al (2012) later
proved its convergence and formalized a general
framework which includes it as a special case. Fig-
ure 2 describes the skeleton of perceptron train-
ing algorithm with beam search. In each step of
the beam search, if the prefix of oracle assign-
ment y falls out from the beam, then the top re-
sult in the beam is returned for early update. One
could also use the standard-update for inference,
however, with highly inexact search the standard-
update generally does not work very well because
of ?invalid updates?, i.e., updates that do not fix a
violation (Huang et al, 2012). In Section 4.5 we
will show that the standard perceptron introduces
many invalid updates especially with smaller beam
sizes, also observed by Huang et al (2012).
To reduce overfitting, we used averaged param-
eters after training to decode test instances in our
experiments. The resulting model is called aver-
aged perceptron (Collins, 2002).
Input: Training set D = {(x(j), y(j))}ni=1,
maximum iteration number T
Output: Model parameters w
1 Initialization: Set w = 0;
2 for t? 1...T do
3 foreach (x, y) ? D do
4 z ? beamSearch (x, y,w)
5 if z 6= y then
6 w? w + f(x, y[1:|z|])? f(x, z)
Figure 2: Perceptron training with beam-
search (Huang et al, 2012). Here y[1:i] de-
notes the prefix of y that has length i, e.g.,
y[1:3] = (y1, y2, y3).
3.2 Label sets
Here we introduce the label sets for trigger and ar-
gument in the model. We use L ? {?} to denote
the trigger label alphabet, where L represents the
33 event subtypes, and ? indicates that the token
is not a trigger. Similarly, R ? {?} denotes the
argument label sets, whereR is the set of possible
argument roles, and ? means that the argument
candidate is not an argument for the current trig-
ger. It is worth to note that the set R of each par-
ticular event subtype is subject to the entity type
constraints defined in the official ACE annotation
guideline2. For example, the Attacker argument
for an Attack event can only be one of PER, ORG
and GPE (Geo-political Entity).
3.3 Decoding
Let x = ?(x1, x2, ..., xs), E? denote the sentence
instance, where xi represents the i-th token in the
sentence and E = {ek}mk=1 is the set of argument
candidates. We use
y = (t1, a1,1, . . . , a1,m, . . . , ts, as,1, . . . , as,m)
to denote the corresponding gold standard struc-
ture, where ti represents the trigger assignment for
the token xi, and ai,k represents the argument role
label for the edge between xi and argument candi-
date ek.
2http://projects.ldc.upenn.edu/ace/docs/English-Events-
Guidelines v5.4.3.pdf
75
y = (t1, a1,1, a1,2, t2, a2,1, a2,2,| {z }
arguments for x2
t3, a3,1, a3,2)
g(1) g(2) h(2, 1) h(3, 2)
Figure 3: Example notation with s = 3,m = 2.
For simplicity, throughout this paper we use
yg(i) and yh(i,k) to represent ti and ai,k, respec-
tively. Figure 3 demonstrates the notation with
s = 3 and m = 2. The variables for the toy sen-
tence ?Jobs founded Apple? are as follows:
x = ?(Jobs,
x2? ?? ?
founded, Apple),
E? ?? ?
{JobsPER,AppleORG}?
y = (?,?,?, Start Org? ?? ?
t2
, Agent, Org? ?? ?
args for founded
,?,?,?)
Figure 4 describes the beam-search procedure
with early-update for event extraction. During
each step with token i, there are two sub-steps:
? Trigger labeling We enumerate all possible
trigger labels for the current token. The linear
model defined in Eq. (1) is used to score each
partial configuration. Then the K-best par-
tial configurations are selected to the beam,
assuming the beam size is K.
? Argument labeling After the trigger label-
ing step, we traverse all configurations in the
beam. Once a trigger label for xi is found in
the beam, the decoder searches through the
argument candidates E to label the edges be-
tween each argument candidate and the trig-
ger. After labeling each argument candidate,
we again score each partial assignment and
select the K-best results to the beam.
After the second step, the rank of different trigger
assignments can be changed because of the argu-
ment edges. Likewise, the decision on later argu-
ment candidates may be affected by earlier argu-
ment assignments.
The overall time complexity for decoding is
O(K ? s ?m).
3.4 Features
In this framework, we define two types of fea-
tures, namely local features and global features.
We first introduce the definition of local and global
features in this paper, and then describe the im-
plementation details later. Recall that in the lin-
ear model defined in Eq. (1), f(x, y) denotes the
features extracted from the input instance x along
Input: Instance x = ?(x1, x2, ..., xs), E? and
the oracle output y if for training.
K: Beam size.
L ? {?}: trigger label alphabet.
R? {?}: argument label alphabet.
Output: 1-best prediction z for x
1 Set beam B ? [] /*empty configuration*/
2 for i? 1...s do
3 buf ? {z? ? l | z? ? B, l ? L ? {?}}
B ?K-best(buf )
4 if y[1:g(i)] 6? B then
5 return B[0] /*for early-update*/
6 for ek ? E do /*search for arguments*/
7 buf ? ?
8 for z? ? B do
9 buf ? buf ? {z? ? ?}
10 if z?g(i) 6= ? then /*xi is a trigger*/
11 buf ? buf ? {z? ? r | r ? R}
12 B ?K-best(buf )
13 if y[1:h(i,k)] 6? B then
14 return B[0] /*for early-update*/
15 return B[0]
Figure 4: Decoding algorithm for event extrac-
tion. z?l means appending label l to the end of
z. During test, lines 4-5 & 13-14 are omitted.
with configuration y. In general, each feature in-
stance f in f is a function f : X ? Y ? R, which
maps x and y to a feature value. Local features are
only related to predictions on individual trigger or
argument. In the case of unigram tagging for trig-
ger labeling, each local feature takes the form of
f(x, i, yg(i)), where i denotes the index of the cur-
rent token, and yg(i) is its trigger label. In practice,
it is convenient to define the local feature function
as an indicator function, for example:
f1(x, i, yg(i)) =
{
1 if yg(i) = Attack and xi = ?fire?
0 otherwise
The global features, by contrast, involve longer
range of the output structure. Formally,
each global feature function takes the form of
f(x, i, k, y), where i and k denote the indices
of the current token and argument candidate in
decoding, respectively. The following indicator
function is a simple example of global features:
f101(x, i, k, y) =
?
??
??
1 if yg(i) = Attack and
y has only one ?Attacker?
0 otherwise
76
Category Type Feature Description
Trigger
Lexical
1. unigrams/bigrams of the current and context words within the window of size 2
2. unigrams/bigrams of part-of-speech tags of the current and context words within the
window of size 2
3. lemma and synonyms of the current token
4. base form of the current token extracted from Nomlex (Macleod et al, 1998)
5. Brown clusters that are learned from ACE English corpus (Brown et al, 1992; Miller et
al., 2004; Sun et al, 2011). We used the clusters with prefixes of length 13, 16 and 20 for
each token.
Syntactic
6. dependent and governor words of the current token
7. dependency types associated the current token
8. whether the current token is a modifier of job title
9. whether the current token is a non-referential pronoun
Entity
Information
10. unigrams/bigrams normalized by entity types
11. dependency features normalized by entity types
12. nearest entity type and string in the sentence/clause
Argument
Basic
1. context words of the entity mention
2. trigger word and subtype
3. entity type, subtype and entity role if it is a geo-political entity mention
4. entity mention head, and head of any other name mention from co-reference chain
5. lexical distance between the argument candidate and the trigger
6. the relative position between the argument candidate and the trigger: {before, after,
overlap, or separated by punctuation}
7. whether it is the nearest argument candidate with the same type
8. whether it is the only mention of the same entity type in the sentence
Syntactic
9. dependency path between the argument candidate and the trigger
10. path from the argument candidate and the trigger in constituent parse tree
11. length of the path between the argument candidate and the trigger in dependency graph
12. common root node and its depth of the argument candidate and parse tree
13. whether the argument candidate and the trigger appear in the same clause
Table 1: Local features.
3.4.1 Local features
In general there are two kinds of local features:
Trigger features The local feature func-
tion for trigger labeling can be factorized as
f(x, i, yg(i)) = p(x, i) ? q(yg(i)), where p(x, i) is
a predicate about the input, which we call text fea-
ture, and q(yg(i)) is a predicate on the trigger label.
In practice, we define two versions of q(yg(i)):
q0(yg(i)) = yg(i) (event subtype)
q1(yg(i)) = event type of yg(i)
q1(yg(i)) is a backoff version of the standard un-
igram feature. Some text features for the same
event type may share a certain distributional sim-
ilarity regardless of the subtypes. For example,
if the nearest entity mention is ?Company?, the
current token is likely to be Personnel no matter
whether it is End-Postion or Start-Position.
Argument features Similarly, the local fea-
ture function for argument labeling can be rep-
resented as f(x, i, k, yg(i), yh(i,k)) = p(x, i, k) ?
q(yg(i), yh(i,k)), where yh(i,k) denotes the argu-
ment assignment for the edge between trigger
word i and argument candidate ek. We define two
versions of q(yg(i), yh(i,k)):
q0(yg(i), yh(i,k)) =
?
??
??
yh(i,k) if yh(i,k) is Place,
Time or None
yg(i) ? yh(i,k) otherwise
q1(yg(i), yh(i,k)) =
{
1 if yh(i,k) 6=None
0 otherwise
It is notable that Place and Time arguments are
applicable and behave similarly to all event sub-
types. Therefore features for these arguments are
not conjuncted with trigger labels. q1(yh(i,k)) can
be considered as a backoff version of q0(yh(i,k)),
which does not discriminate different argument
roles but only focuses on argument identification.
Table 1 summarizes the text features about the in-
put for trigger and argument labeling. In our ex-
periments, we used the Stanford parser (De Marn-
effe et al, 2006) to create dependency parses.
3.4.2 Global features
Table 2 summarizes the 8 types of global features
we developed in this work. They can be roughly
divided into the following two categories:
77
Category Feature Description
Trigger
1. bigram of trigger types occur in the same sentence or the same clause
2. binary feature indicating whether synonyms in the same sentence have the same trigger label
3. context and dependency paths between two triggers conjuncted with their types
Argument
4. context and dependency features about two argument candidates which share the same role within the
same event mention
5. features about one argument candidate which plays as arguments in two event mentions in the same
sentence
6. features about two arguments of an event mention which are overlapping
7. the number of arguments with each role type of an event mention conjuncted with the event subtype
8. the pairs of time arguments within an event mention conjuncted with the event subtype
Table 2: Global features.
Transport
(transport)
Entity
(women)
Entity
(children)
Art
ifac
t Artifact
conj and
(a)
Entity
(cameramen)
Die
(died)
Attack
(fired)
Vic
tim
Target
advcl
(b)
End-Position
(resigned)
Entity Entity
[co-chief executive of [Vivendi Universal Entertainment]]
Pos
itio
n Entity
Overlapping
(c)
Figure 5: Illustration of global features (4-6) in Table 2.
Event Probability
Attack 0.34
Die 0.14
Transport 0.08
Injure 0.04
Meet 0.02
Table 3: Top 5 event subtypes that co-occur with
Attack event in the same sentence.
Trigger global feature This type of feature
captures the dependencies between two triggers
within the same sentence. For instance: feature (1)
captures the co-occurrence of trigger types. This
kind of feature is motivated by the fact that two
event mentions in the same sentence tend to be se-
mantically coherent. As an example, from Table 3
we can see that Attack event often co-occur with
Die event in the same sentence, but rarely co-occur
with Start-Position event. Feature (2) encourages
synonyms or identical tokens to have the same la-
bel. Feature (3) exploits the lexical and syntactic
relation between two triggers. A simple example
is whether an Attack trigger and a Die trigger are
linked by the dependency relation conj and.
Argument global feature This type of feature
is defined over multiple arguments for the same
or different triggers. Consider the following sen-
tence:
(3) Trains running to southern Sudan were used
to transport abducted women and children.
The Transport event mention ?transport? has
two Artifact arguments, ?women? and ?chil-
dren?. The dependency edge conj and be-
tween ?women? and ?children? indicates that
they should play the same role in the event men-
tion. The triangle structure in Figure 5(a) is an ex-
ample of feature (4) for the above example. This
feature encourages entities that are linked by de-
pendency relation conj and to play the same role
Artifact in any Transport event.
Similarly, Figure 5(b) depicts an example of
feature (5) for sentence (1) in Section 1. In this ex-
ample, an entity mention is Victim argument to Die
event and Target argument to Attack event, and the
two event triggers are connected by the typed de-
pendency advcl. Here advcl means that the word
?fired? is an adverbial clause modier of ?died?.
Figure 5(c) shows an example of feature (6) for
the following sentence:
(4) Barry Diller resigned as co-chief executive of
Vivendi Universal Entertainment.
The job title ?co-chief executive of Vivendi Uni-
versal Entertainment? overlaps with the Orga-
nization mention ?Vivendi Universal Entertain-
ment?. The feature in the triangle shape can be
considered as a soft constraint such that if a Job-
Title mention is a Position argument to an End-
Position trigger, then the Organization mention
78
which appears at the end of it should be labeled
as Entity argument for the same trigger.
Feature (7-8) are based on the statistics about
different arguments for the same trigger. For in-
stance, in many cases, a trigger can only have one
Place argument. If a partial configuration mis-
takenly classifies more than one entity mention as
Place arguments for the same trigger, then it will
be penalized.
4 Experiments
4.1 Data set and evaluation metric
We utilized the ACE 2005 corpus as our testbed.
For comparison, we used the same test set with 40
newswire articles (672 sentences) as in (Ji and Gr-
ishman, 2008; Liao and Grishman, 2010) for the
experiments, and randomly selected 30 other doc-
uments (863 sentences) from different genres as
the development set. The rest 529 documents (14,
840 sentences) are used for training.
Following previous work (Ji and Grishman,
2008; Liao and Grishman, 2010; Hong et al,
2011), we use the following criteria to determine
the correctness of an predicted event mention:
? A trigger is correct if its event subtype and
offsets match those of a reference trigger.
? An argument is correctly identified if its event
subtype and offsets match those of any of the
reference argument mentions.
? An argument is correctly identified and clas-
sified if its event subtype, offsets and argu-
ment role match those of any of the reference
argument mentions.
Finally we use Precision (P), Recall (R) and F-
measure (F1) to evaluate the overall performance.
4.2 Baseline system
Chen and Ng (2012) have proven that perform-
ing identification and classification in one step is
better than two steps. To compare our proposed
method with the previous pipelined approaches,
we implemented two Maximum Entropy (Max-
Ent) classifiers for trigger labeling and argument
labeling respectively. To make a fair comparison,
the feature sets in the baseline are identical to the
local text features we developed in our framework
(see Figure 1).
4.3 Training curves
We use the harmonic mean of the trigger?s F1
measure and argument?s F1 measure to measure
the performance on the development set.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21# of training iteration0.44
0.46
0.48
0.50
0.52
0.54
0.56
0.58
0.60
Harm
onic
 me
an
local+globallocal
Figure 6: Training curves on dev set.
Figure 6 shows the training curves of the aver-
aged perceptron with respect to the performance
on the development set when the beam size is 4.
As we can see both curves converge around itera-
tion 20 and the global features improve the over-
all performance, compared to its counterpart with
only local features. Therefore we set the number
of iterations as 20 in the remaining experiments.
4.4 Impact of beam size
The beam size is an important hyper parameter in
both training and test. Larger beam size will in-
crease the computational cost while smaller beam
size may reduce the performance. Table 4 shows
the performance on the development set with sev-
eral different beam sizes. When beam size = 4, the
algorithm achieved the highest performance on the
development set with trigger F1 = 67.9, argument
F1 = 51.5, and harmonic mean = 58.6. When
the size is increased to 32, the accuracy was not
improved. Based on this observation, we chose
beam size as 4 for the remaining experiments.
4.5 Early-update vs. standard-update
Huang et al (2012) define ?invalid update? to be
an update that does not fix a violation (and instead
reinforces the error), and show that it strongly
(anti-)correlates with search quality and learning
quality. Figure 7 depicts the percentage of in-
valid updates in standard-update with and with-
out global features, respectively. With global fea-
tures, there are numerous invalid updates when the
79
Beam size 1 2 4 8 16 32
Training time (sec) 993 2,034 3,982 8,036 15,878 33,026
Harmonic mean 57.6 57.7 58.6 58.0 57.8 57.8
Table 4: Comparison of training time and accuracy on the dev set.
1 2 4 8 16 32beam size0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
% of
 inva
lid u
pdat
es
local+globallocal
Figure 7: Percentage of the so-called ?invalid up-
dates? (Huang et al, 2012) in standard perceptron.
Strategy F1 on Dev F1 on TestTrigger Arg Trigger Arg
Standard (b = 1) 68.3 47.4 64.4 49.8
Early (b = 1) 68.9 49.5 65.2 52.1
Standard (b = 4) 68.4 50.5 67.1 51.4
Early (b = 4) 67.9 51.5 67.5 52.7
Table 5: Comparison between the performance
(%) of standard-update and early-update with
global features. Here b stands for beam size.
beam size is small. The ratio decreases mono-
tonically as beam size increases. The model with
only local features made much smaller numbers
of invalid updates, which suggests that the use of
global features makes the search problem much
harder. This observation justify the application of
early-update in this work. To further investigate
the difference between early-update and standard-
update, we tested the performance of both strate-
gies, which is summarized in Table 5. As we can
see the performance of standard-update is gener-
ally worse than early-update. When the beam size
is increased (b = 4), the gap becomes smaller as
the ratio of invalid updates is reduced.
4.6 Overall performance
Table 6 shows the overall performance on the blind
test set. In addition to our baseline, we compare
against the sentence-level system reported in Hong
et al (2011), which, to the best of our knowledge,
is the best-reported system in the literature based
on gold standard argument candidates. The pro-
posed joint framework with local features achieves
comparable performance for triggers and outper-
forms the staged baseline especially on arguments.
By adding global features, the overall performance
is further improved significantly. Compared to
the staged baseline, it gains 1.6% improvement
on trigger?s F-measure and 8.8% improvement on
argument?s F-measure. Remarkably, compared to
the cross-entity approach reported in (Hong et al,
2011), which attained 68.3% F1 for triggers and
48.3% for arguments, our approach with global
features achieves even better performance on ar-
gument labeling although we only used sentence-
level information.
We also tested the performance with argument
candidates automatically extracted by a high-
performing name tagger (Li et al, 2012b) and an
IE system (Grishman et al, 2005). The results
are summarized in Table 7. The joint approach
with global features significantly outperforms the
baseline and the model with only local features.
We also show that it outperforms the sentence-
level baseline reported in (Ji and Grishman, 2008;
Liao and Grishman, 2010), both of which at-
tained 59.7% F1 for triggers and 36.6% for argu-
ments. Our approach aims to tackle the problem of
sentence-level event extraction, thereby only used
intra-sentential evidence. Nevertheless, the perfor-
mance of our approach is still comparable with the
best-reported methods based on cross-document
and cross-event inference (Ji and Grishman, 2008;
Liao and Grishman, 2010).
5 Related Work
Most recent studies about ACE event extraction
rely on staged pipeline which consists of separate
local classifiers for trigger labeling and argument
labeling (Grishman et al, 2005; Ahn, 2006; Ji and
Grishman, 2008; Chen and Ji, 2009; Liao and Gr-
ishman, 2010; Hong et al, 2011; Li et al, 2012a;
Chen and Ng, 2012). To the best of our knowl-
edge, our work is the first attempt to jointly model
these two ACE event subtasks.
80
Methods
Trigger
Identification (%)
Trigger Identification
+ classification (%)
Argument
Identification (%) Argument Role (%)P R F1 P R F1 P R F1 P R F1
Sentence-level in Hong et al (2011) N/A 67.6 53.5 59.7 46.5 37.15 41.3 41.0 32.8 36.5
Staged MaxEnt classifiers 76.2 60.5 67.4 74.5 59.1 65.9 74.1 37.4 49.7 65.4 33.1 43.9
Joint w/ local features 77.4 62.3 69.0 73.7 59.3 65.7 69.7 39.6 50.5 64.1 36.5 46.5
Joint w/ local + global features 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7
Cross-entity in Hong et al (2011)? N/A 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3
Table 6: Overall performance with gold-standard entities, timex, and values. ?beyond sentence level.
Methods Trigger F1 Arg F1
Ji and Grishman (2008)
cross-doc Inference
67.3 42.6
Ji and Grishman (2008)
sentence-level
59.7 36.6
MaxEnt classifiers 64.7 (?1.2) 33.7 (?10.2)
Joint w/ local 63.7 (?2.0) 35.8 (?10.7)
Joint w/ local + global 65.6 (?1.9) 41.8 (?10.9)
Table 7: Overall performance (%) with predicted
entities, timex, and values. ? indicates the perfor-
mance drop from experiments with gold-standard
argument candidates (see Table 6).
For the Message Understanding Conference
(MUC) and FAS Program for Monitoring Emerg-
ing Diseases (ProMED) event extraction tasks,
Patwardhan and Riloff (2009) proposed a proba-
bilistic framework to extract event role fillers con-
ditioned on the sentential event occurrence. Be-
sides having different task definitions, the key
difference from our approach is that their role
filler recognizer and sentential event recognizer
are trained independently but combined in the test
stage. Our experiments, however, have demon-
strated that it is more advantageous to do both
training and testing with joint inference.
There has been some previous work on joint
modeling for biomedical events (Riedel and Mc-
Callum, 2011a; Riedel et al, 2009; McClosky et
al., 2011; Riedel and McCallum, 2011b). (Mc-
Closky et al, 2011) is most closely related to our
approach. They casted the problem of biomedi-
cal event extraction as a dependency parsing prob-
lem. The key assumption that event structure can
be considered as trees is incompatible with ACE
event extraction. In addition, they used a separate
classifier to predict the event triggers before ap-
plying the parser, while we extract the triggers and
argument jointly. Finally, the features in the parser
are edge-factorized. To exploit global features,
they applied a MaxEnt-based global re-ranker. In
comparison, our approach is a unified framework
based on beam search, which allows us to exploit
arbitrary global features efficiently.
6 Conclusions and Future Work
We presented a joint framework for ACE event ex-
traction based on structured perceptron with inex-
act search. As opposed to traditional pipelined
approaches, we re-defined the task as a struc-
tured prediction problem. The experiments proved
that the perceptron with local features outperforms
the staged baseline and the global features further
improve the performance significantly, surpassing
the current state-of-the-art by a large margin.
As shown in Table 7, the overall performance
drops substantially when using predicted argu-
ment candidates. To improve the accuracy of end-
to-end IE system, we plan to develop a complete
joint framework to recognize entities together with
event mentions for future work. Also we are inter-
ested in applying this framework to other IE tasks
such as relation extraction.
Acknowledgments
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), U.S. NSF
CAREER Award under Grant IIS-0953149,
U.S. NSF EAGER Award under Grant No. IIS-
1144111, U.S. DARPA Award No. FA8750-13-2-
0041 in the ?Deep Exploration and Filtering of
Text? (DEFT) Program, a CUNY Junior Faculty
Award, and Queens College equipment funds. The
views and conclusions contained in this document
are those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
81
References
David Ahn. 2006. The stages of event extraction.
In Proceedings of the Workshop on Annotating and
Reasoning about Time and Events, pages 1?8.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Zheng Chen and Heng Ji. 2009. Language specific
issue and feature exploration in chinese event ex-
traction. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Short Pa-
pers, pages 209?212.
Chen Chen and Vincent Ng. 2012. Joint modeling for
chinese event extraction with rich linguistic features.
In COLING, pages 529?544.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 111.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, pages 449?454.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu?s english ace 2005 system description.
In Proceedings of ACE 2005 Evaluation Workshop.
Washington.
Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao,
Guodong Zhou, and Qiaoming Zhu. 2011. Using
cross-entity inference to improve event extraction.
In Proceedings of ACL, pages 1127?1136.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of ACL, pages 254?262.
Peifeng Li, Guodong Zhou, Qiaoming Zhu, and Li-
bin Hou. 2012a. Employing compositional seman-
tics and discourse consistency in chinese event ex-
traction. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 1006?1016.
Qi Li, Haibo Li, Heng Ji, Wen Wang, Jing Zheng, and
Fei Huang. 2012b. Joint bilingual name tagging for
parallel corpora. In Proceedings of the 21st ACM
international conference on Information and knowl-
edge management, pages 1727?1731.
Shasha Liao and Ralph Grishman. 2010. Using doc-
ument level cross-event inference to improve event
extraction. In Proceedings of ACL, pages 789?797.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, and Ruth Reeves. 1998. Nomlex: A
lexicon of nominalizations. In Proceedings of EU-
RALEX, volume 98, pages 187?193.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of ACL, pages 1626?1635.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of HLT-NAACL,
volume 4, pages 337?342.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 1-Volume 1, pages 151?
160.
Sebastian Riedel and Andrew McCallum. 2011a. Fast
and robust joint models for biomedical event extrac-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 1?
12.
Sebastian Riedel and Andrew McCallum. 2011b. Ro-
bust biomedical event extraction with dual decom-
position and minimal domain adaptation. In Pro-
ceedings of the BioNLP Shared Task 2011 Work-
shop, pages 46?50.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proceedings
of the Workshop on Current Trends in Biomedical
Natural Language Processing: Shared Task, pages
41?49.
Ang Sun, Ralph Grishman, and Satoshi Sekine. 2011.
Semi-supervised relation extraction with large-scale
word clustering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
521?529.
82
