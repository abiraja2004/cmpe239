A New Strategy for Providing Definitions 
In Task-Oriented Dialogues 
Margaret H. SARNER 
Department of Computer Science 
University of Delaware 
Newark, Delaware 19716 U.S.A. 
Sandra CARBERRY 
Department of Computer Science 
University of Delaware 
Newark, Delaware 19716 U.S.A. 
Abstract 
Definitions may be made up of one or more components, 
which correspond to strategic predicates. The selection of which 
components o use in giving a definition in a task-oriented dialogue 
depends heavily on the needs of the user. The selection strategy 
we present involves weighting possible strategic predicates and the 
propositiomL used to fill them at multiple points throughout an 
ongoing dialogue and at the actual time of giving the definition. 
Weighting will he influenced by a model of the user's domain kimwl- 
edge~ task-related plans and goals, and receptivity to the different 
kinds of intormation that could he presented. An utteraalce can 
then be produced that incorporates the most important informa- 
tion while adhering to common rhetorical practices. 
1 Introduction 
In the course of ongoing task-oriented expert-consultation 
dialogues, many occasions arise in which the expert must provide 
a definition. In this paper we will present a new strategy for a 
computer expert to use in giving definitions in a way that is most 
helpful to the individual user. 
The strategy relies on a dynamically inferred model of the 
user's dom~dn knowledge, task-related plans and goals, and recep- 
tivity to different kinds of information. It constructs a definition by 
weighting both the strategic predicates that might comprise a def- 
inition and the propositions that might be used to fill the strategic 
predicates. These weights are used to order what migh t be said ac- 
cording to its anticipated usefulness to the user. Rules can then be 
used to produce an utterance that incorporates the most important 
informatlo~t while adhering to common rhetorical practices. This 
strategy rellects our overall hypothesis that beliefs about the appro- 
priate content of a definition should guide selection of a rhetorical 
strategy, instead of the choice of a rhetorical strategy determining 
content. 
Section 2 describes ituations in task-oriented dialogues in 
which definitions are called for. Section 3 identifies three charac- 
teristics that differentiate definitions provided by experts during 
task-oriented dialogues from those provided in response to isolated 
requests for definitions, and argues that the choice of a rhetorical 
strategy should be made on the basis of being able to include in 
the definition those features deemed most important. Section 4 
proposes a Principle of Usefulness as a guideline for selecting infor- 
mation to include in definitions. Section 5 discusses strategic pred- 
icates. Section 6 presents an overview of our strategy for weighting 
predicates and propositions and then ranking what might be said 
according to its usefulness to the user. 
2 Definition Situations 
In its simplest form, a definition-giving dialogue consists of 
an information-seeker asking "What is an Xf" and an information- 
provider saying "An X /s  a . . . .  " In actual practice, however, there 
are many ways a definition can he requested and many ways the 
request cmt be responded to by the information-provider. In or- 
der to identify the characteristics of definitign-giving dialogues, we 
have analyzed transcripts of novice-expert dialogues from a variety 
of domains, including student/advisor dialogues, recipe-providing 
dialogues, taxpayer/tax-agent dialogues, and radio talk shows in 
!which callers 8ought expert advice on investments and real estate. 1 
This section describes definition-glving situations identified in this 
study. 
An expert may give a definition either in response to a user's 
request or spontaneously. Occasions for providing definitions arise 
most obviously when the user asks a question of the form "What is 
... ?" or "What is the significance of . . . ? "  The question doesn't 
have to be explicit, however, as illustrated in the exchange below, 
which is an excerpt from a money-management talk show tran- 
script: 
E: "I'd llke to see you put that into two different South- 
ern utilities." 
U: "Southern utilities?" 
As shown in \[Carberry 1985\], such elliptical fragments are often 
intended to elicit clarification and explanation of the repeated term. 
In addition to giving definitions in response to a request by 
the user, the expert may provide a definition as part of correcting 
a user misconception \[McCoy 1986\], or may generate definitions 
spontaneously. There are several reasons an expert may give spon- 
taneous definitions. He may provide additional definitional infor- 
mation to justify use of a concept, tie may think it likely that the 
user doesn't know about the entity being introduced. The expert 
may want to ensure that he and the user are working with the same 
definition. The statement below is an example of a spontaneous 
definition from a recipe-giving dialogue: 
E: "You use a spring-form pan - -  the kind that ,allows 
you to separate the bottom and the sides once you 
have prepared your dish." 
3 Def in i t ions  in  Task -Or iented  D ia logues  
McKeown \[McKeown 1985\] studied definitions in the cou- 
text of requests for information about the objects modeled by a 
database system. She claimed that humans have mutually known 
conventions for organizing information and providing definitions, 
and that a natural language system should make use of these strate* 
gies in producing explanations. Given a definition request, her 
TEXT system selected a rhetorical strategy based on the infor- 
mation available. The rhetorical strategy was then responsible for 
selecting the information to he incorporated into the definition. 
TEXT treated requests for definitions as isolated queries, whereas 
we are interested in definitions generated in the course of ongoing 
task-oriented dialogues. 
Our analysis of transcripts of naturally occurring interac- 
tions indicates that definitions generated in task-oriented dialogues 
differ significantly .from those generated statically or as a result of 
isolated efinition requests. The differences appear to be the result 
of several factors: 
tThese trmascrlpts were provided by the Computer Science Departments of 
the University of Pen~ylvania and the University of Delaware. 
567 
1. In task-oriented dialogues, the information-provider knows 
something about what the information-seeker is trying to ac- 
complish, and will generate definitions that help the informa- 
tion-seeker achieve his goals. For example, the first response 
below would be an appropriate definition of baking soda if 
the information-seeker is baking a cake, whereas the second 
would be appropriate if he is trying to relieve indigestion. 
E: "Baking soda is an ingredient hat, when 
heated, releases carbon dioxide, thereby caus- 
ing the mixture to expand in size." 
E: "Baking soda is a substance that, when dis- 
solved in water,produces a chemically basic so- 
lution that will counteract acidity." 
2. Whereas tatic definitions or responses to one-shot requests 
for defiuit|ons must assume a generic model for the informa- 
tion-seeker, esponses to definition requests during an ongo- 
ing dialogue can take into account acquired beliefs about the 
information-seeker's specific domain knowledge. For exam- 
ple, the information-provider might include an analogy to an 
entity that the information-seeker is already familiar with, as 
in the following definition of the course CS106: 
E: "CS106 is like CS105, except hat it uses For- 
tran instead of Pascal and emphasizes scientific 
applications of computing." 
3. Whereas tatic definitions and responses to one-shot requests 
for definitions must be generated all at once, dialogue allows 
the information-provider to produce what he thinks will be 
an acceptable definition and analyze the information-seeker's 
response to determine whether to elaborate on the definition. 
For example, in the following dialogue with a veterinarian 
about treating a cat with a hyperthyroid condition, the vet- 
erinarian (E) provides a definition that he believes will Sat- 
isfy the information-seeker's needs, then must elaborate on it 
when the information-seeker's response reveals multiple goals: 
to improve the condition of the cat and to have medication 
that is easy to administer. 
E: "Tapazole is a drug that decreases the function 
of the thyroid." 
U: "How large are the pills?" " 
H a system carrying on a task-oriented dialogue is te be 
viewed by the information-seeker as cooperative, intelligent, and 
natural, it must take the above factors into account. Otherwise, it
will not appear to be directed toward the user's goals (uncoopera- 
tive), will not appear to make use of what the user already knows 
(unintelligent), and will not appear to take advantage of the fact 
that the interaction is ongoing, as opposed to one-shot (unnatural). 
Our hypothesis that, instead of using a rhetorical strategy 
to determine the content of a definition, the system should reason 
about the user's plans and goals and speclli? domain knowledge to 
decide the importance of incorporating individual propositions into 
the final definition. For this purpose a user model, preferably a
dynamically constructed user model, is essential. The  choice of a 
rhetorical strategy should be made on the basis of being able to 
include into the definition those features deemed most important. 
Thus beliefs about the appropiiate content of the definition should 
guide selection of a rhetorical strategy, instead of the choice of a 
rhetorical strategy determining content. 
McKeown, Wish, and Matthews \[McKeown et al 1985\] ad- 
dressed some of these issues in their work on an expert lystem 
that could provide explanations tailored to users. They described 
a method for using a model of the user's goals along with p~bui l t  
perspectives on the knowledge base to generate appropriate expla- 
nations. While they touched on some of the issues that concern 
us, they took a different approach from the one we are proposing. 
568 
Their perspectives were built into the domain knowledge base, and 
their system did not make much use of informaticm available from 
the system's model of the user's plans and goals. Also, they were 
concerned with answering can and should questions, whereas we are 
interested in definition explanations. 
4 Appropr iate Content  of a Definition 
Our analysis of naturally occurring consultation dialogues 
indicates that definitions can take many forms. They may be 
made up of one or more of a set of components, which correspond 
to rhetorical predicates described in \[Grimes 1975, Williams 1893, 
McKeown 1985\]. These predicates will be discussed further in Sec- 
tion 5. 
Since we are studying cooperative dialogues in which the ex- 
pert's goal is to help the information-seeker solve his problem, we 
hypothesize that the expert's overriding concern in selecting the 
information to include is that the response be as useful as possi- 
ble to the individual user. Intuitively, to be truly useful to the 
user, the information must be something he doesn't already know 
but something relevant hat he can understand. Our hypothesis, 
which appears to explain the definitions occurring in our dialogue 
transcripts, uggests the following Principle of Usefulness: 
Pr inciple of  Usefulness 
1. The response should be made at a high enough level that it 
is meaningful to the user. 
(a) Don't say something the user won't understand. 
(b) Don't give information that addresses more detailed as- 
pects of the user's task-related plan than is appropriate 
for his current focus of attention. 
2. The response should be made at a low enough level that it is 
helpful to the user. 
(a) Don't inform the user of something he already knows. 
(b) Don't give information that is unrelated to the user's 
goals and task-related plan, or is too general for his cur- 
rent focus of attention in the plan. 
Grice \[(\]rice 1975\] stated that contributions should be as 
informative as required for the exchange, but not more informative 
than required. Paris \[Paris 1988\] suggested that an answer to a 
question should be both informative and understandable to the 
user, based on the user's level of knowledge about the domain of 
discourse. The Principle of Usefulness formalizes and extends these 
guidelines for definitions by selecting the appropriate l vel both in 
knowledge-related issues (la, 2a) and in plans and goals (lb, 2b). 
This Principle will be used whenever a selection of appropriate l vel 
of information to fill a predicate is called for. 
For example, consider a plant classification hierarchy. 
THING 
\[ isa 
PLANT 
\] isa 
FLOWERING PLANT 
\[ isa 
ARUM 
\[ isa 
CUCKOOPINT 
To descrlbe a Cuckoopint as an arum would have no meantm84fo an 
information-seeker who has never heard of an arnm, while defining 
it as a thing is too general. The useful evel of explanation for the  
information-seeker with no special knowledge of botany is defining 
a cuckoopint as a flowering plant. In task-odanted dialogues, ad- 
dltional care must be taken to avoid providing extra information 
that is unrelated t0; or too detailed for, the user's current needs. 
Otherwise, the extra information may lead the user to erroneously 
assume that the system believes the distinguishing characteristics 
are important or that the system has mls-identified the aspect of 
his task on which the user is currently focused. 
The term rhetorical predicate has taken on several mean 
lugs in the literature of linguistics and coutputationM linguistics. 
It ha.s been used to describe relationships ranting from structural 
to conceptual in uature. Grinms \[Grimes 1.975\] described rhetorical 
predicates i.hat "relate ~he kinds of informatio~t communica*ed i~t 
discourse with each other." One of his predicates was ~he Attribu 
tive predica.te which "adds qualities or color to sa~other predicaie 
as center." Ilobbs \[tIobbs 1979\] chose to use the term coherence *~. 
lution in pn;ference to rhetorical predicate to place tile emphasis on 
the coherence between sentential units. McKeown's description of 
rhetoricM vredicatcs \[McKeown 1985\] imtdied ~ut association with 
sentential s~ructure, but ia practice the predicates he used, such 
a~ Constitsency, dealt more with conceptuM relationships. 
Wc :n'e using predicates to chara?terize the componeni;s of 
defiuitio~s i~a terms of relationships between conceptual uuits. Our 
predicates relate information M)out the entity being defined to the 
entity itself. This relationship is datuMs-independent mid content- 
independent. For exarnple, our Identification predicate is instanti- 
axed by fiuding iu a generalization hierarchy an entity which is art 
ancestor of the entity being defined. This usage is close to MeKe. 
own's, but because of the apparent ambiguity of the term rhctori.. 
cal pmdicales, we prefer to call the predicates strttte#ic predicates, 
putting emghasis on the motivation of g~fining ant end (in this case, 
conveying useful information to the user) ratber than on style. 
l,?om our study of definitions occurring in actual dialogues, 
we have identified fourteen distiuct predicates that relate to deft-- 
nixies content. Each predicate corresponds to a different type of 
iah)rntatio~ that can be put into a definition. Although we do 
not claim lltat the list is complete or unique, we do believe it is 
sutllcient to generate appropriate definitions in an expert consul- 
tatlon system. Some of our predicates (ldeutification, Properties, 
Analogy, Components) are similar to McKeowu's. Others (Effect, 
Prerequisites) are paxticular to a task--orieuted environment. 
Associated witit each predicate alie semantics ~hat indicate 
how to inst ~utiate it. Foc example, efl~ct information will i~e tbund 
iu the system's library of plans ~ud ,,~o'ds, aud property information 
will be f~a,~d ill the generalization bieliarchy. \[a either case, the 
~;ystem m;t *'casaba about, the paFt icH\]ar_  ' usea++s plans mid goals in 
,~rder to deternfinc a propositiou's relewntce to what the user is 
~xyiug h) a.:contplish. When au occasion !br a definition a~iscs, a
given predicate laity be lilled one or c, tore times. The propositiotm 
tiros prod,~ced at'e caudidates for inclusion in the detluitio~,. Siuce 
our goal i.~ to selecl; !~he informatiou thai; i~; l,tost important to th, 
'user, we as~;ociate a me'tsnrc at" signiftcauce with each proposition~ 
The sigailia:aa:ce metrics will be described in Section 6. 
In the rent,-finder o\[" this sectiou we will look at three, types 
of definitio:,t components in some detail to illustrate how the user 
model influences election. 
,%1 :tfde~tifieation 
Many naturally occurring definitions contain au Identifi- 
cation component, identification consists (ff ideutifying the entity 
beiug described as a member (d a generic class in a hierarchicMly 
structured knowledge base ~- for example, 
E: "Amaretto is a liqueur." 
Th~ system's model of the user dictates what superclass from 
the generalizaLioa hierarchy to use ia au identification. In order tbr 
identificati,m to I)e h@fful to the user, it is necessary that the 
user have knowledge of the pareut category used in making the 
identitication. Ttds condition corresponds to the first part of tile 
Prirtcipk ~, ,,; Usefldness. Knowledge of a parent category may not be 
suhici,mt, however, to cause that parent category to be given in the 
definition. If the systemh beliefs indicate that the pareut category 
is w.lated ~,.~ the u~er's pin, Is end goals, then there is stronger reason 
to mention it. In t\],e cane iu which the entity ha8 severM parents 
that the n~:e~" haJ; kuowledge of, plans and goals should be u.qed to 
~elect he one (or ones) most appropriate lo mention. Suppose, 
lbr exampl% that a digital systems course is cross-listed as both a 
Cmapu*er Science and an Elec~ricM Engineering course. 
U: "What is Digital Systems?" 
E: "It is a Computer Science course . . ."  
or  
F,: "It is an F, lectrieal Engineering course . . ."  
The choice of answer depends on whether tim user model indicates 
that the user is trying to satisfy Coolputer Science or Eleetricefl 
Fingineering requirements. A third ~dternative is
F: "It is both a Computer Scieu(:e course mid an Electrical 
Engineering course . . . "  
This response might be given if the model indicates laoth parent 
categories play a role in tim user's plans and goals. 
Following tile Principle of Usefulness, the appropriate super. 
class is the lowest level parent category that would have meaning to 
the user and be relevant to what the system believes al.Ie the user's 
plans and goals. 
I 
D 
I 
X 
The user knows what A, B, C are 
Tim user doesn't know about i) 
Tim user asks "What i.~ X ?" 
\]n the cm;e illustrated above, the expert's taleutification a:n.sw~r 
migllt be "X  is a C." The efl'eci; of an.uwering "X  is a D" wo01d 
be to caaJse the user to ask "What is a D?" . r  give up withottt get- 
ting meaningful info~'mation. The an~Jwm' "X i'.~ a 11" would miss 
tile distinguishing features hared lay C :uld ;( but not lay B. If the~e 
distinguishing features ~Lre not important to the tJ.~el ~ii\[1 wol.l!d \[;i-V(! 
the false impression that tlle system believes they are. a<W(~v:;~i t
tile user's task, however~ a higher hwel thnu C shouhi b.'~ selected. 
5.2 P roper t ies  
A Properties response consists o! naminv~ characteristics 
of tile entity. These are often expl~ssed i, descriptions Kiwm by 
humans a~q "adjectival phrases attached to the ldentitlcati(m of the 
entity. 
E: "A no-load fired is a mutual fired with no sales charge." 
E: "Amaretti are crisp Italian almond-flavored macaroons." 
In the TEXT systenl \[McKeown 1985\], attributes whose v;A- 
ues distinguish one sub-type from another axe marked in ~;he knowl- 
edge base. In task-oriented dialogues, however, an entity's mo~qt im 
portant distinguishing attributes are not always static but inul.ead 
may vary depending on tile inhxrmation..seeker'.q plans and goals. 
For example, the coarse Computer, Ethics and Society may have 
several distinguishing properties, including its content, its sub:;tan- 
tial writing component, its lack of programmiug projects, and itt+ 
scheduling at night through continuing education. An information. 
seeker whose objective is to earn a\]IA degree at night while holding 
a full-time job would consider its schedtding property of interest ili 
differentiating it from other computer science courses, whereas aa~ 
electrical engineering major seeking a technical elective would prob- 
ably consider its lack of programming projects of particular siguif. 
icance. Titus, although the properties of an entity are found in the 
generalization hierarchy, the system's beliefs about the user's plaJls 
and goals should play a major role in determining which properties 
of the entity are most appropriate to iuclude in a (lefiuititm. 
569 
5.3 Operat ion  
An Operation response consists of a description of how 
something works. Paris \[Paris 1988\] has demonstrated that expla- 
nations given novices in a domain often take the form of process 
traces. An Operation definition may take the form of process infor- 
mation or steps in implementation. The difference between the two 
is that the process information is essentially a chain of cause-and- 
effect occurrences, while the steps in implementation are sequential, 
but not necessarily causal, as is shown in the example: 
U: "Can yon tell me what the money market is?" 
E: " A money market fund is a group of people getting 
together - -  put their money together in a pool and it is 
invested by professional investors." 
As with the Properties predicate, the system's beliefs about 
the user's plans and goals must be taken into consideration. The 
expert might identify the need for an Operation explanation i  a 
task-oriented dialogue when the entity being explained appears in a 
step in a plan the user must carry out to meet a goal. For example, 
if the user is a traveler asking the expert for help planning a car 
trip and the expert advises the user to follow a "Trip Tik," the 
expert should explain how a Trip Tik works if the model of the user 
indicates lack of familiarity with it. The definitions of baking soda 
given earlier illustrate a case in which the appropriate Operation 
explanation depends on the use to which the entity will be put by 
the information-seeker. 
6 Se lect ing  Def in i t ion  Content  
Our strategy assumes a knowledge base consisting of a gen- 
eralization hierarchy containing domain knowledge, a plan library, 
and a lexicon. The user model has three components: 
1. a model of the user's domain knowledge in the form of mark- 
ings on the knowledge base showing the pieces with which the 
user is familiar \[Kass 1987\], 
2. a model of the user's underlying t/ak-related plan and cur- 
rent focus of attention in the plan, given by a context ree 
\[Carberry 1988\], 
3. a model of how receptive the user is to various kinds of infor- 
mation, given by weightings on strategic predicates. 
The first two components will be dynamically updated uring the 
dialogue as shown in \[Kass 1987\] and \[Carberry 1988\]. The third 
component will also be updated dynamically in response to the 
user's receptivity to types of definitions and his own usage of strate- 
gic predicates. 
6.1 Weight ing  Pred icates  
When a definition occasion arises, a local predicate recep- 
tivity model is created. Starting with a copy of the current global 
weights representing the user's general receptivity to the kinds of in- 
formation represented by the strategic predicates, as inferred from 
the preceding dialogue, further adjustments may be made to reflect 
the appropriateness of the predicates in the particular situation. 
The question itself and the level of local domain expertise 
may cause further weighting of predicates. For example, if the user 
asks "What is XP' where X is an object, the Identification predicate 
would be more heavily weighted. If X is an action, the Operation 
predicate would be more heavily weighted. The level of local do- 
main expertise can be ascertained when a definition is requested by 
looking at the parts of the plan library and generalization hierarchy 
that contain references to the entity in question. If they are heavily 
marked with things the user knows, the user can be considered to 
have a high level of expertise; otherwise, the user will be considered 
to be a novice. The weights for predicates that have been deter- 
mined to be appropriate for expert and novice users will then be 
increased \[Paris 1988\]. 
hO. 
,8 -  
,6 -  
,4 -  
I 
13 " '2 d " ''+ 
Figure 1: Graph of Relevance Formula 
6.2 we ight ing  Propos i t ions  
After predicate weighting has been determined, predicates 
are filled with information from the knowledge base (generaliza- 
tion hierarchy, lexicon, plans and goals) relevant o the concept 
being defined. The semantics of each individual predicate dictate 
where to find the information to fill the predicate. For instance, the 
Identification and Properties predicates are filled with information 
found in the generalization hierarchy, and Necessity propositions 
are drawn from the plans of the user. Some predicates may pro- 
duce several propositions. For example, an entity may have several 
properties. For others there might not be any corresponding propo- 
sitions available. 
Selection of propositions depends on both the weights of the 
possible predicates and a measure of significance of the informa- 
tion that could be used to fill them. Significance reflects where 
the proposition fits into the system's model of the user's goals and 
possible plans for accomplishing them (relevance) and what infor- 
mation in the generalization hierarchy has been marked as known 
by the user (familiarity). 
The system's beliefs about the user's underlying task-related 
plan, as dynamically inferred from the preceding dialogue, are rep+ 
resented in a tree structure called a context model \[Carberry 1988\]. 
Each node in this tree represents a goal that the user has investi- 
gated achieving. Except for the root, each goal in the context model 
is a descendant of a higher-level goal whose associated plan, found 
in the system's plan library, contains the lower-level goal. One node 
in the tree is marked as the current focus of attention and indicates 
that aspect of the task on which the user's attention is currently 
centered. The context model may be expanded to arbitrarily ma~y 
levels of detail by repeatedly replacing non-prlmitive suhgoals with 
associated plans which themselves contain constituent subgoals. 
If pursuing a subgoal in a plan represents a significant shift 
in focus, it is marked in the plan library as introducing a new focus 
domain~;~, Within the context model, a focus domain of subgoals 
that are at approximately the same level of focus is generated by 
expanding'the plan associated with a subgoai that introduces the 
focus domain. As long as this plan is expanded by substituting 
plans for just those subgoals that do not introduce another new 
focus domain, the subgoals appearing in the expanded plan are 
part of the same focus domain. 
Our estimate of relevance is based on distance of the part of 
the context model in which the definition information is found from 
the current focus of attention in the context model. This distance 
is measured as the number of shifts in focus domains. If the plan is 
at the focus of attention, the information derived from it is of very 
high relevance. If it is in the immediately surrounding focus domain 
(one shift), the information is still of high relevance. As the number 
of focus domain shifts increases, the relevance of information i  the 
plans begins to fall off, but as long as a plan has been activated 
the information found in it is of some relevance. This situation in 
which relevance remains high close to the focus of attention, but 
drops off more rapidly as the distance increases, is modeled by an 
inverse xponential function, as shown in Figure 1. The equation 
d2 r = e - ( , )  , 
where r is the relevance rating and d is the number of shifts from 
the current focus of attention, captures the desired features. 
570 
i ..... 
13 
Figure 2: Graph of Familiarity Formula 
Currently, our relevance metric treats all shifts ~xaong focus 
domains equally. It may be the case, however, that information i  a 
higber-level plan h that led to the current focus of attention is more 
appropriate to include in a'defiuition than is information extracted 
from a subplan s appearing in an expansion of the current focused 
plan, even if the two plans, h and s, represent the same number of 
shifts from the current focus of attention in the context model. The 
current fecund plan is part of an expansion of h, so we know that 
the user is concerned with accomplishing h; therefore, information 
relevant o h may be more significant to the user than information 
relevant o details of carrying out the current focused plan. This is 
an issue that we plan to investigate further. 
Our measure of familiarity is based on the knowledge the 
expert believes the user has about the objects, properties, or con- 
cepts that could be used in a definition. We are assuming a variant 
of the user modeling system described by Kass \[Kass&Fiuin 1987\], 
modified so that each node in the knowledge base is marked with a 
bellef factor~ ranging in value from O to 1, giving the system's level 
of belief that the user is familiar with the entity. Because of the 
importance of giving a definition in terms of something the person 
receiving the. definition will understand, an entity known to have 
meaning to the user (belief factor = 1) should be treated as poten- 
tially useful to include, even if it is not germane to the hypothesized 
goals. If it is not believed strongly that the person is fandllar with 
the entity, however, it is less useful to tie the definition to that en- 
tity. Note that since the dialogues under consideration are ongoing, 
as opposed to one-shot, a definition can include items that the sys- 
tem believes the user is probably familiar with, mad the system can 
wait for the user's response to decide whether the definition was 
successful. The heuristic described here is modeled by the function 
shown in Figure 2. The formula 
e 6b(2-b) - -  1 
f= e e -  1 ' 
where f is the familiarity rating and b is the belief factor, exhibits 
an appropriate amount of curvature to reflect the rapid drop-off in 
usefulness a~ the belief factor decreases. 
The \]ast step in computing a measure of significance for a 
piece of information is to form a weighted combination of the rele- 
vance rating and the familiarity rating. Since our primary goal is to 
provide information that will help the user accomplish a task, our 
ibrmula for combining the two measures weights ignificance twice 
as heavily ~ familiarity. Our significance metric, then, is 
2r + f 
.3  
where S is significance, r is the relevance rating, and f is the famil- 
iarity rating. 
The following example from a hypothetical travel domain 
ifiustrates how propo~itions are weighted according to significance. 
The dialogue pertains to planning a trip abroad. 
U: "I need to have enough money with me to pay for 
anything I buy." 
E: "You can carry as much as you like in travelers 
checks." 
U: "Travelers checks?" 
The first statement causes the have-money plants beinfocas. The 
have-moneyplan has subgoals 
have-convartlble-funda ((_agent: person) 
(_amountl: funds)) 
hart_currency ((_agent:  person) 
(_country: country) 
(_amount2: funds)). 
Suppose that the user's elliptical fragment is interpreted as a re- 
quest for a definition. Figure 3 shows part of the context model. As 
a result of the expert's preceding response, the focus of attention is 
now on the have-convertible-funds plan. Suppose further that the 
other plans shown are in a focus domain at a distance of 1 from the 
focus of attention. 
Figure 3: A Portion of the Context Model 
The Operation predicate produces the candidate proposition 
formed from components of the use-travelers-checks subplazt (not 
shown) equivalent to the statement 
"You can buy travelers checks at a bank here and cash them 
in the currency of the country." 
The information comes from the body of the use*travelers-checks 
subplan, which is at distance d=l  from the focus of attention. As- 
suming that the expert believes that the user is familiar with the 
concepts of buying, banks, currency, and cashing things in, we have 
r = e-(}) 2 = e-('z) 2 -- .939 
e ~(2-b)  - -  1 e s(1)  - -  1 
J - e s _ ~ -  eS_ l  =1  
S = - -=2r+f  .959 
3 
571 
The Analogy predicate is filled by a reference to a sibling 
with similar properties, equivalent to 
"Travelers checks are like personal checks." 
Suppose the belief factor for personal checks is .9 - -  that is, the 
expert believes it very likely but is not absolutely certain that the 
user knows about personal checks. Suppose further that the prop- 
erties of travelers checks that are similar to those of personal checks 
appear in plans at a distance of two shifts of focus domain from the 
focus of attention. Iu this case we compute 
r = e-(~) 2 - -  e-(~) 2 = .779 
e sb(2-b) -- 1 e s'4(l't) -- 1 
f = e 6 -- -1 -- e 6 -- 1 .942 
S - 2 r+f_ .833  
3 
The fact that the first definition component has higher com- 
puted significance than the second oes not necessarily mean that it 
will be preferred, however. Recall that weights of candidate propo- 
sitions must reflect both significance of the information and predi- 
cate receptivity. 
Once weights have been assigned to the candidate proposi- 
tions, they are then ranked according to weight and put into cate- 
gories. There are four categories: 
Must Say 
Say if Convenient 
Say if Needed for Coherence 
Do Not Say 
The higher weight categories receive the higher-weighted propo- 
sitions; the lower-weighted propositions go into the lower weight 
categories. Some categories may be empty. 
When all category assignments have been made, the result- 
ing four groups of propositions axe passed to an answer generator. 
Construction of this answer generator is a future project. The gen- 
erator will take the classes of propositions, find a way to say all of 
the Must Say propositions a~ld as many as possible of the Say if 
Convenient propositions, using Say if Needed for Coherence propo- 
sitions whenever they help the construction of the response. We 
propose to do this task using rules of combination developed to 
produce an utterance that adheres to common rhetorical practices 
that people appear to follow. 
7 A Comparison 
Our strategy will produce different responses tban would 
current definition systems. For example, consider a request for a 
definition of amaretti. McKeown's TEXT system would identify the 
entity and include all based database and distinguishing database 
attributes, and would produce a definition resembling 
"Amaretti are macaroons. They are made from apricot ker- 
nels, have ahnond flavor, are of Italian origin, and have crisp 
texture. The most popular brand is made by Lazzaroni and 
Company." 
Our definition module would attempt o pick information appro- 
priate to the individual user. If the user is selecting food items to 
sell in an international bazaar, it would say 
"Amaretti are Italian macaroons. The most popular brand 
is made by Lazzaxoni and Company." 
If the user is making Amaretti Amaretto Chocolate Cheesecake, for 
which amaretti are an ingredient, however, it would say 
"Amaretti are crisp almond-flauored macaroons." 
8 .Future Work  
Our continuing research will work out additional details of 
our strategy for providing definitions in task-oriented dialogues. We 
need to investigate a strategy for dynamically weighting strategic 
predicates according to the user's perceived receptivity to different 
kinds of information, and putting this weighting together with ore' 
measure of significance for propositions. An answer generator that 
combines propositions, giving emphasis to including those proposi.. 
tions deemed most important o say, must be designed. This task 
includes ranking the candidate propositions by weight and combin- 
ing the most heavily weighted ones in a way that will produce a 
coherent utterance. Finally, the system must be implemented to 
test and demonstrate he utility of our definition strategy. 
9 Summary 
We claim that determining the most important hings to 
say for the individual user is the most significant task in providing 
definitions in task-oriented dialogues. In thls paper we prasent a 
new strategy for generating definitions, using a weighting strategy 
that draws on a dynamically inferred model of the user's domain 
knowledge, task-related plans, and receptivity to different kinds 
of information. This strategy reflects our over-all hypothesis that 
beliefs about the appropriate content of a definition should guide 
selection of a rhetorical strategy, instead of the choice of a rhetor- 
ical strategy determining content. This approach will produce a 
system that exhibits cooperative, intelligent behavior by providing 
definitions tailored to the needs of the individual user. 
References  
Carberry, Sa~dra. 1985. A Pragmatics Based Approach to 
Understanding Intersentential El ipsis. In: t'roceedings of the 
23rd Annual Meeting of the Association for Computation Lin.. 
gaistics, 188'-197. 
Carberry, Sandra. 1988. Modeling the User's Plans and Coals. 
Computational Linguistics Journal, To Appear. 
Grice, H. Paul. 1975. Logic and Conversation. In: P. Cole 
and J. L. Morgan, Eds., Syntax and Semantics II\[: Speech 
Acts, Academic Press, N.Y.: 41-58. 
Grimes, J. E. 1975. The Thread of Discourse. Mouton. 
lIobbs, Jerry R. 1979. Coherence and Coreferenee. Cognitive 
Science, 3:67-90. 
Kass, Robert. 1987. Implicit Acquisition of User Models in 
Cooperative Advisory Systems. Technical Report MS-CIS-87- 
05, Department of Computer and Information Science, Uni- 
versity of Pennsylvania, Philadelphia, PA. 
Kass, Robert and Finin, Tim. 1987. Rules for the Implicit 
Acquisition of Knowledge About the User. Proceedings of the 
Sixth National Conference on Artificial Intelligencc, 295-30{}. 
McCoy, Kathleen F. 1986. The ROMPEI~ System: Respond- 
ing to Object-Related Misconceptions Using Perspective. Pro~. 
ceedings of the 24th Annual Meeting of the Association for 
Computational Linguistics, 97-105. 
McKeown, Kathleen IL 1985. Text Generation. Cambridge 
University Press. 
McKeown, K., Wish, M., and Matthews, K. 1985. l~lorirlg 
Explanations for the User. In: Proceedings of the 1985 Con.- 
ference, Int'l Joint Conference on Artificial Intelligence, Los 
Angeles CA. 
Paris, Cecile L. 1988. Tailoring Object Descriptions to a 
User's Level of Expertise. Computational Linguistics Journal. 
Williams, W. 1893. Composition and Rhetoric. Heath and 
Company. 
572 
