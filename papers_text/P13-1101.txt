Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023?1032,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Subtree Extractive Summarization via Submodular Maximization
Hajime Morita
Tokyo Institute of Technology, Japan
morita@lr.pi.titech.ac.jp
Hiroya Takamura
Tokyo Institute of Technology, Japan
takamura@pi.titech.ac.jp
Ryohei Sasano
Tokyo Institute of Technology, Japan
sasano@pi.titech.ac.jp
Manabu Okumura
Tokyo Institute of Technology, Japan
oku@pi.titech.ac.jp
Abstract
This study proposes a text summarization
model that simultaneously performs sen-
tence extraction and compression. We
translate the text summarization task into
a problem of extracting a set of depen-
dency subtrees in the document cluster.
We also encode obligatory case constraints
as must-link dependency constraints in or-
der to guarantee the readability of the gen-
erated summary. In order to handle the
subtree extraction problem, we investigate
a new class of submodular maximization
problem, and a new algorithm that has
the approximation ratio 12(1 ? e?1). Ourexperiments with the NTCIR ACLIA test
collections show that our approach outper-
forms a state-of-the-art algorithm.
1 Introduction
Text summarization is often addressed as a task
of simultaneously performing sentence extraction
and sentence compression (Berg-Kirkpatrick et
al., 2011; Martins and Smith, 2009). Joint mod-
els of sentence extraction and compression have
a great benefit in that they have a large degree of
freedom as far as controlling redundancy goes. In
contrast, conventional two-stage approaches (Za-
jic et al, 2006), which first generate candidate
compressed sentences and then use them to gen-
erate a summary, have less computational com-
plexity than joint models. However, two-stage ap-
proaches are suboptimal for text summarization.
For example, when we compress sentences first,
the compressed sentences may fail to contain im-
portant pieces of information due to the length
limit imposed on each sentence. On the other
hand, when we extract sentences first, an impor-
tant sentence may fail to be selected, simply be-
cause it is long. Enumerating a huge number
of compressed sentences is also infeasible. Joint
models can prune unimportant or redundant de-
scriptions without resorting to enumeration.
Meanwhile, submodular maximization has re-
cently been applied to the text summarization task,
and the methods thereof have performed very well
(Lin and Bilmes, 2010; Lin and Bilmes, 2011;
Morita et al, 2011). Formalizing summarization
as a submodular maximization problem has an im-
portant benefit inthat the problem can be solved by
using a greedy algorithm with a performance guar-
antee.
We therefore decided to formalize the task of si-
multaneously performing sentence extraction and
compression as a submodular maximization prob-
lem. That is, we extract subsentences for mak-
ing the summary directly from all available sub-
sentences in the documents and not in a stepwise
fashion. However, there is a difficulty with such
a formalization. In the past, the resulting maxi-
mization problem has been often accompanied by
thousands of linear constraints representing logi-
cal relations between words. The existing greedy
algorithm for solving submodular maximization
problems cannot work in the presence of such nu-
merous constraints although monotone and non-
monotone submodular maximization with con-
straints other than budget constraints have been
studied (Lee et al, 2009; Kulik et al, 2009; Gupta
et al, 2010). In this study, we avoid this difficulty
by reducing the task to one of extracting depen-
dency subtrees from sentences in the source doc-
uments. The reduction replaces the difficulty of
numerous linear constraints with another difficulty
wherein two subtrees can share the same word to-
1023
ken when they are selected from the same sen-
tence, and as a result, the cost of the union of the
two subtrees is not always the mere sum of their
costs. We can overcome this difficulty by tackling
a new class of submodular maximization prob-
lem: a budgeted monotone nondecreasing sub-
modular function maximization with a cost func-
tion, where the cost of an extraction unit varies
depending on what other extraction units are se-
lected. By formalizing the subtree extraction prob-
lem as this new maximization problem, we can
treat the constraints regarding the grammaticality
of the compressed sentences in a straightforward
way and use an arbitrary monotone submodular
word score function for words including our word
score function (shown later). We also propose a
new greedy algorithm that solves this new class of
maximization problem with a performance guar-
antee 12(1? e?1).
We evaluated our method on by using it to per-
form query-oriented summarization (Tang et al,
2009). Experimental results show that it is supe-
rior to state-of-the-art methods.
2 Related Work
Submodularity is formally defined as a property of
a set function for a finite universe V . The function
f : 2V ? R maps a subset S ? V to a real value.
If for any S, T ? V , f(S ? T ) + f(S ? T ) ?
f(S)+f(T ), f is called submodular. This defini-
tion is equivalent to that of diminishing returns,
which is well known in the field of economics:
f(S ?{u})? f(S) ? f(T ?{u})? f(T ), where
T ? S ? V and u is an element of V . Di-
minishing returns means that the value of an el-
ement u remains the same or decreases as S be-
comes larger. This property is suitable for sum-
marization purposes, because the gain of adding a
new sentence to a summary that already contains
sufficient information should be small. Therefore,
many studies have formalized text summarization
as a submodular maximization problem (Lin and
Bilmes, 2010; Lin and Bilmes, 2011; Morita et
al., 2011). Their approaches, however, have been
based on sentence extraction. To our knowledge,
there is no study that addresses the joint task of
simultaneously performing compression and ex-
traction through an approximate submodular max-
imization with a performance guarantee.
In the field of constrained maximization prob-
lems, Kulik et al (2009) proposed an algorithm
that solves the submodular maximization problem
under multiple linear constraints with a perfor-
mance guarantee 1? e?1 in polynomial time. Al-
though their approach can represent more flexible
constraints, we cannot use their algorithm to solve
our problem, because their algorithm needs to enu-
merate many combinations of elements. Integer
linear programming (ILP) formulations can repre-
sent such flexible constraints, and they are com-
monly used to model text summarization (McDon-
ald, 2007). Berg-Kirkpatrick et al (2011) formu-
lated a unified task of sentence extraction and sen-
tence compression as an ILP. However, it is hard to
solve large-scale ILP problems exactly in a practi-
cal amount of time.
3 Budgeted Submodular Maximization
with Cost Function
3.1 Problem Definition
Let V be the finite set of all valid subtrees in
the source documents, where valid subtrees are
defined to be the ones that can be regarded as
grammatical sentences. In this paper, we regard
subtrees containing the root node of the sentence
as valid. Accordingly, V denotes a set of all
rooted subtrees in all sentences. A subtree con-
tains a set of elements that are units in a de-
pendency structure (e.g., morphemes, words or
clauses). Let us consider the following problem
of budgeted monotone nondecreasing submodu-
lar function maximization with a cost function:
maxS?V {f(S) : c (S) ? L} , where S is a sum-
mary represented as a set of subtrees, c(?) is the
cost function for the set of subtrees, L is our bud-
get, and the submodular function f(?) scores the
summary quality. The cost function is not always
the sum of the costs of the covered subtrees, but
depends on the set of the covered elements by the
subtrees. Here, we will assume that the generated
summary has to be as long as or shorter than the
given summary length limit, as measured by the
number of characters. This means the cost of a
subtree is the integer number of characters it con-
tains.
V is partitioned into exclusive subsetsB of valid
subtrees, and each subset corresponds to the orig-
inal sentence from which the valid subtrees de-
rived. However, the cost of a union of subtrees
from different sentences is simply the sum of the
costs of subtrees, while the cost of a union of sub-
trees from the same sentence is smaller than the
sum of the costs. Therefore, the problem can be
represented as follows:
1024
max
S?V
{
f(S) :
?
B?B
c (B ? S) ? L
}
. (1)
For example, if we add a subtree t containing
words {wa,wb,wc} to a summary that already
covers words {wa, wb, wd} from the same sen-
tence, the additional cost of t is only c({wc}) be-
cause wa and wb are already covered1.
The problem has two requirements. The first
requirement is that the union of valid subtrees is
also a valid subtree. The second requirement is
that the union of subtrees and a single valid sub-
tree have the same score and the same cost if they
cover the same elements. We will refer to the sin-
gle valid subtree as the equivalent subtree of the
union of subtrees. These requirements enable us
to represent sentence compression as the extrac-
tion of subtrees from a sentence. This is because
the requirements guarantee that the extracted sub-
trees represent a sentence.
3.2 Greedy Algorithm
We propose Algorithm 1 that solves the maximiza-
tion problem (Eq.1). The algorithm is based on
ones proposed by Khuller et al (1999) and Krause
et al (2005). Instead of enumerating all candidate
subtrees, we use a local search to extract the ele-
ment that has the highest gain per cost. In the al-
gorithm, Gi indicates a summary set obtained by
adding element si to Gi?1. U means the set of
subtrees that are not extracted. The algorithm it-
eratively adds to the current summary the element
si that has the largest ratio of the objective func-
tion gain to the additional cost, unless adding it
violates the budget constraint. We set a parame-
ter r that is the scaling factor proposed by Lin and
Bilmes (2010). After the loop, the algorithm com-
pares Gi with the {s?} that has the largest value of
the objective function among all subtrees that are
under the budget, and it outputs the summary can-
didate with the largest value.
Let us analyze the performance guarantee of Al-
gorithm 12.
1Each subset B corresponds to a kind of greedoid con-
straint. V implicitly constrains the model such that it can
only select valid subtrees from a set of nodes and edges.
2Our performance guarantee is lower than that reported
by Lin and Bilmes (2010). However, their proof is er-
roneous. In their proof of Lemma 2, they derive ?u ?
S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)
Crvi
, for any i(1 ? i ? |G|),
from line 4 of their Algorithm 1, which selects the densest
element out of all available elements. However, the inequal-
ity does not hold for i, for which element u selected on line
4 is discarded on line 5 of their algorithm. The performance
guarantee of their algorithm is actually the same as ours, since
Algorithm 1 Modified greedy algorithm for budgeted
submodular function maximization with a cost function .
1: G0 ? ?
2: U ? V
3: i? 1
4: while U 6= ? do
5: si ? argmaxs?U f(Gi?1?{s})?f(Gi?1)(c(Gi?1?{s})?c(Gi?1))r
6: if c({si} ?Gi?1) ? L then
7: Gi ? Gi?1 ? {si}
8: i? i + 1
9: end if
10: U ? U\{si}
11: end while
12: s?? argmaxs?V,c(s)?L f({s})
13: return Gf = argmaxS?{{s?},Gi} f(S)
Theorem 1 For a normalized monotone submod-
ular function f(?), Algorithm 1 has a constant
approximation factor when r = 1 as follows:
f(Gf ) ?
(1
2(1? e
?1)
)
f(S?), (2)
where S? is the optimal solution and, Gf is the
solution obtained by Greedy Algorithm 1.
Proof. See appendix.
3.3 Relation with Discrete Optimization
We argue that our optimization problem can be
regarded as an extraction of subtrees rooted at a
given node from a directed graph, instead of from
a tree. Let D be the set of edges of the directed
graph, F be a subset of D that is a subtree. In the
field of combinatorial optimization, a pair (D, F)
is a kind of greedoid: directed branching greedoid
(Schmidt, 1991). A greedoid is a generalization of
the matroid concept. However, while matroids are
often used to represent constraints on submodular
maximization problems (Conforti and Cornue?jols,
1984; Calinescu et al, 2011), greedoids have not
been used for that purpose, in spite of their high
representation ability. To our knowledge, this is
the first study that gives a constant performance
guarantee for the submodular maximization under
greedoid (non-matroid) constraints.
the guarantee 12 (1? e?1) was already proved by Krause andGuestrin (2005). We show a counterexample. Suppose that
V is { e1(density 4:cost 6), e2(density 2:cost 4), e3(density
3:cost 1), e4(density 1:cost 1) }, and cost limit K is 10. The
optimal solution is S? = {e1, e2}. Their algorithm selects
e1, e3, e4 in this order. However the algorithm selects e2 on
line 4 after selecting e3, and it drops e2 on line 5. As a result,
e4 selected by the algorithm does not satisfy the inequality
?u ? S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)
Crvi
.
1025
4 Joint Model of Extraction and
Compression
We will formalize the unified task of sentence
compression and extraction as a budgeted mono-
tone nondecreasing submodular function maxi-
mization with a cost function. In this formaliza-
tion, a valid subtree of a sentence represents a
candidate of a compressed sentence. We will re-
fer to all valid subtrees of a given sentence as a
valid set. A valid set corresponds to all candi-
dates of the compression of a sentence. Note that
although we use the valid set in the formaliza-
tion, we do not have to enumerate all the candi-
dates for each sentence. Since, from the require-
ments, the union of valid subtrees is also a valid
subtree in the valid set, the model can extract one
or more subtrees from one sentence, and generate
a compressed sentence by merging those subtrees
to generate an equivalent subtree. Therefore, the
joint model can extract an arbitrarily compressed
sentence as a subtree without enumerating all can-
didates. The joint model can remove the redundant
part as well as the irrelevant part of a sentence, be-
cause the model simultaneously extracts and com-
presses sentences. We can approximately solve the
subtree extraction problem by using Algorithm 1.
On line 5 of the algorithm, the subtree extraction
is performed as a local search that finds maximal
density subtrees from the whole documents. The
maximal density subtree is a subtree that has the
highest score per cost of subtree. We use a cost
function to represent the cost, which indicates the
length of word tokens in the subtree.
In this paper, we address the task of summariza-
tion of Japanese text by means of sentence com-
pression and extraction. In Japanese, syntactic
subtrees that contain the root of the dependency
tree of the original sentence often make gram-
matical sentences. This means that the require-
ments mentioned in Section 3.1 that a union of
valid subtrees is a valid and equivalent tree is of-
ten true for Japanese. The root indicates the pred-
icate of a sentence, and it is syntactically modi-
fied by other prior words. Some modifying words
can be pruned. Therefore, sentence compression
can be represented as edge pruning. The linguis-
tic units we extract are bunsetsu phrases, which
are syntactic chunks often containing a functional
word after one or more content words. We will re-
fer to bunsetsu phrases as phrases for simplicity.
Since Japanese syntactic dependency is generally
defined between two phrases, we use the phrases
as the nodes of subtrees.
In this joint model, we generate a compressed
sentence by extracting an arbitrary subtree from a
dependency tree of a sentence. However, not all
subtrees are always valid. The sentence generated
by a subtree can be unnatural even though the sub-
tree contains the root node of the sentence. To
avoid generating such ungrammatical sentences,
we need to detect and retain the obligatory de-
pendency relations in the dependency tree. We
address this problem by imposing must-link con-
straints if a phrase corresponds to an obligatory
case of the main predicate. We merge obligatory
phrases with the predicate beforehand so that the
merged nodes make a single large node.
Although we focus on Japanese in this pa-
per, our approach can be applied to English and
other languages if certain conditions are satisfied.
First, we need a dependency parser of the lan-
guage in order to represent sentence compression
as dependency tree pruning. Moreover, although,
in Japanese, obligatory cases distinguish which
edges of the dependency tree can be pruned or not,
we need another technique to distinguish them in
other languages. For example we can distinguish
obligatory phrases from optional ones by using se-
mantic role labeling to detect arguments of predi-
cates. The adaptation to other languages is left for
future work.
4.1 Objective Function
We extract subtrees from sentences in order to
solve the query-oriented summarization problem
as a unified one consisting of sentence compres-
sion and extraction. We thus need to allocate a
query relevance score to each node. Off-the-shelf
similarity measures such as the cosine similarity of
bag-of-words vectors with query terms would al-
locate scores to the terms that appear in the query,
but would give no scores to terms that do not ap-
pear in it. With such a similarity, sentence com-
pression extracts nearly only the query terms and
fails to contain important information. Instead,
we used Query SnowBall (QSB) (Morita et al,
2011) to calculate the query relevance score of
each phrase. QSB is a method for query-oriented
summarization, which calculates the similarity be-
tween query terms and each word by using co-
occurrences within the source documents. Al-
though the authors of QSB also provided scores
of word pairs to avoid putting excessive penalties
1026
on word overlaps, we do not score word pairs. The
score function is supermodular as a score function
of subtree extraction3, because the union of two
subtrees can have extra word pairs that are not in-
cluded in either subtree. If the extra pair has a pos-
itive score, the score of the union is greater than
the sum of the score of the subtrees. This violates
the definition of submodularity, and invalidates the
performance guarantee of our algorithms.
We designed our objective function by combin-
ing this relevance score with a penalty for redun-
dancy and too-compressed sentences. Important
words that describe the main topic should occur
multiple times in a good summary. However, ex-
cessive overlap undermines the quality of a sum-
mary, as do irrelevant words. Therefore, the scores
of overlapping words should be lower than thoseof
new words. The behavior can be represented by a
submodular objective function that reduces word
scores depending on those already included in the
summary. Furthermore, a summary consisting of
many too-compressed sentences would lack read-
ability. We thus gives a positive reward to long
sentences. The positive reward leads to a natu-
ral summary being generated with fewer sentences
and indirectly penalizes too short sentences. Our
positive reward for long sentences is represented
as
reward(S) = c(S)? |S|, (3)
where c(S) is the cost of summary S, and |S| is the
number of sentences in S. Since a sentence must
contain more than one character, the reward con-
sistently gives a positive score, and gives a higher
score to a summary that consists of fewer sen-
tences.
Let d be the damping rate, countS(w) be the
number of sentences containing word w in sum-
mary S, words(S) be the set of words included in
summary S, qsb(w) be the query relevance score
of word w, and ? be a parameter that adjusts the
rate of sentence compression. Our score function
for a summary S is as follows:
f(S) =
?
w?words(S)
?
?
?
countS(w)?1?
i=0
qsb(w)di
?
?
?+ ? reward(S).
(4)
An optimization problem with this objective
function cannot be regarded as an ILP problem be-
cause it contains non-linear terms. It is also ad-
3The score is still submodular for the purpose of sentence
extraction.
vantageous that the submodular maximization can
deal with such objective functions. Note that the
objective function is such that it can be calculated
according to the type of word. Due to the na-
ture of the objective function, we can use dynamic
programming to effectively search for the subtree
with the maximal density.
4.2 Local Search for Maximal Density
Subtree
Let us now discuss the local search used on line
5 of Algorithm 1. We will use a fast algorithm to
find the maximal density subtree (MDS) of a given
sentence for each cost in Algorithm 1.
Consider the objective function Eq. 4, We can
ignore the second term of the reward function
while looking for the MDS in a sentence because
the number of sentences is the same for every
MDS in a sentence. That is, the gain function of
adding a subtree to a summary can be represented
as the sum of gains for words:
g(t) =
?
w?t
{gainS(w) + freqt(w)c(w)?},
gainS(w) = qsb(w)dcountS(w),
where freqt(w) is the number of ws in subtree
t, and gainS(w) is the gain of adding the word
w to the summary S. Our algorithm is based on
dynamic programming, and it selects a subtree that
maximizes the gain function per cost.
When the word gain is a constant, the algorithm
proposed by Hsieh et al (2010) can be used to
find the MDS. We extended this algorithm to work
for submodular word gain functions that are not
constant. Note that the gain of a word that oc-
curs only once in the sentence, can be treated as
a constant. In what follows, we will describe an
extended algorithm to find the MDS even if there
is word overlap.
For example, let us describe how to obtain the
MDS in the case of a binary tree. First let us tackle
the case in which the gain is always constant. Let
n be a node in the tree, a and b be child nodes of n,
c(n) be the cost of n, mdsca be the MDS rooted at
a and have cost c. mdsn = {mdsc(n)n , . . . ,mdsLn}
denotes the set of MDSs for each cost and its root
node n. The valid subtrees rooted at n can be ob-
tained by taking unions of n with one or both of
t1 ? mdsa and t2 ? mdsb. mdscn is the union that
has the largest gain over the union with the cost of
c (by enumerating all the unions). The MDS for
1027
the sentence root can be found by calculating each
mdscn from the bottom of the tree to the top.
Next, let us consider the objective function that
returns the sum of values of submodular word gain
functions. When there is no word overlap within
the union, we can obtain mdscn in the same man-
ner as for the constant gain. In contrast, if the
union includes word overlap, the gain is less than
the sum of gains: g(mdscn) ? g(n) + g(mdska) +
g(mdsc?k?c(n)b ), where k and c are variables. Thescore reduction can change the order of the gains
of the union. That is, it is possible that another
union without word overlaps will have a larger
gain. Therefore, the algorithm needs to know
whether each t ? mdsn has the potential to have
word overlaps with other MDSs. Let O be the set
of words that occur twice or more in the sentence
on which the local seach focuses. The algorithm
stores MDS for each o ? O, as well as each cost.
By storing MDS for each o and cost as shown
in Fig. 1, the algorithm can find MDS with the
largest gain over the combinations of subtrees.
Algorithm 2 shows the procedure. In it, t andm
denote subtrees, words(t) returns a set of words
in the subtree, g(t) returns the gain of t, tree(n)
means a tree consisting of node n, and t ?m de-
notes the union of subtrees: t and m. subt in-
dicates a set of current maximal density subtrees
among the combinations calculated before. newt
indicates a set of temporary maximal density sub-
trees for the combinations calculated from line 4
to 8. subt[cost,ws] indicates a element of subt that
has a cost cost and contains a set of words ws.
newt[cost,ws] is defined similarly. Line 1 sets subt
to a set consisting of a subtree that indicates node
n itself. The algorithm calculates maximal den-
sity subtrees within combinations of the root node
n and MDSs rooted at child nodes of n. Line 3
iteratively adds MDSs rooted at a next child node
to the combinations; the algorithm then calculates
MDSs newt between subt and the MDSs of the
child node. The procedure from line 6 to 8 selects
a subtree that has a larger gain from the tempo-
rary maximal subtree and the union of t and m.
The computational complexity of this algorithm is
O(NC2) when there is no word overlap within the
sentence, where C denotes the cost of the whole
sentence, and N denotes the number of nodes in
the sentence. The complexity order is the same
as that of the algorithm of Hsieh et al (2010).
When we treat word overlaps, we need to count
Algorithm 2 Algorithm for finding maximal density
subtree for each cost: MDSs.
Function: MDSs
Require: root node n
1: subt[c(n),words(n)?O] = tree(n)
2: newt = ?
3: for i ? child node of n do
4: for t ?MDSs(i) do
5: for m ? subt do
6: index = [c(t ?m), words(t ?m) ? O]
7: newtindex = argmaxj?{newtindex,t?m} g(j)8: end for
9: end for
10: subt = newt
11: end for
12: return subt
Figure 1: Maximal density subtree extraction. The
right table enumerates the subtrees rooted at w2 in
the left tree for all indices. The number in each
tree node is the score of the word.
all unions of combinations of the stored MDSs.
There are at most (C2|O|) MDSs that the algo-
rithm needs to store at each node. Therefore the
total computational complexity is O(NC222|O|).
Since it is unlikely that a sentence contains many
word tokens of one type, the computational cost
may not be so large in practical situations.
5 Experimental Settings
We evaluate our method on Japanese QA test
collections from NTCIR-7 ACLIA1 and NTCIR-
8 ACLIA2 (Mitamura et al, 2008; Mitamura et
al., 2010). The collections contain questions and
weighted answer nuggets. Our experimental set-
tings followed the settings of (Morita et al, 2011),
except for the maximum summary length. We
generated summaries consisting of 140 Japanese
characters or less, with the question as the query
terms. We did this because our aim is to use our
method in mobile situations. We used ?ACLIA1
test data? to tune the parameters, and evaluated our
method on ?ACLIA2 test? data.
We used JUMAN (Kurohashi and Kawahara,
2009a) for word segmentation and part-of-speech
tagging, and we calculated idf over Mainichi
newspaper articles from 1991 to 2005. For the de-
1028
POURPRE Precision Recall F1 F3
Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174
Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190
Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183
Table 1: Results on ACLIA2 test data.
pendency parsing, we used KNP (Kurohashi and
Kawahara, 2009b). Since KNP internally has a
flag that indicates either an ?obligatory case? or an
?adjacent case?, we regarded dependency relations
flagged by KNP as obligatory in the sentence com-
pression. KNP utilizes Kyoto University?s case
frames (Kawahara and Kurohashi, 2006) as the re-
source for detecting obligatory or adjacent cases.
To evaluate the summaries, we followed the
practices of the TAC summarization tasks (Dang,
2008) and NTCIR ACLIA tasks, and computed
pyramid-based precision with the allowance pa-
rameter, recall, and F? (where ? is 1 or 3)
scores. The allowance parameter was determined
from the average nugget length for each question
type of the ACLIA2 collection (Mitamura et al,
2010). Precision and recall are computed from the
nuggets that the summary covered along with their
weights. One of the authors of this paper man-
ually evaluated whether each nugget matched the
summary. We also used the automatic evaluation
measure, POURPRE (Lin and Demner-Fushman,
2006). POURPRE is based on word matching
of reference nuggets and system outputs. We re-
garded as stopwords the most frequent 100 words
in Mainichi articles from 1991 to 2005 (the doc-
ument frequency was used to measure the fre-
quency). We also set the threshold of nugget
matching as 0.5 and binarized the nugget match-
ing, following the previous study (Mitamura et al,
2010). We tuned the parameters by using POUR-
PRE on the development dataset.
Lin and Bilmes (2011) designed a monotone
submodular function for query-oriented summa-
rization. Their succinct method performed well
in DUC from 2004 to 2007. They proposed a
positive diversity reward function in order to de-
fine a monotone submodular objective function for
generating a non-redundant summary. The diver-
sity reward gives a smaller gain for a biased sum-
mary, because it consists of gains based on three
clusters and calculates a square root score with
respect to each sentence. The reward also con-
tains a score for the similarity of a sentence to
the query, for purposes of query-oriented summa-
Recall Length # of nuggets
Subtree extraction 0.213 11,143 100
Reconstructed (RC) 0.228 13,797 108
Table 2: Effect of sentence compression.
rization. Their objective function also includes a
coverage function based on the similarity wi,j be-
tween sentences. In the coverage function min
function limits the maximum gain ??i?V wi,j ,
which is a small fraction ? of the similarity be-
tween a sentence j and the all source documents.
The objective function is the sum of the positive
reward R and the coverage function L over the
source documents V , as follows:
F(S) = L(S) +
3?
k=1
?kRQ,k(S),
L(S) = ?
i?V
min
??
?
?
j?S
wi,j , ?
?
k?V
wi,k
??
? ,
RQ,k =
?
c?Ck
???? ?
j?S?c
( ?N
?
i?V
wi,j + (1? ?)rj,Q),
where ?, ? and ?k are parameters, and rj,Q repre-
sents the similarity between sentence j and query
Q. We tuned the parameters on the development
dataset. Lin and Bilmes (2011) used three clusters
Ck with different granularities, which were calcu-
lated in advance. We set the granularity to (0.2N ,
0.15N , 0.05N ) according to the settings of them,
where N is the number of sentences in a docu-
ment.
We also regarded as stopwords ???? (tell),?
??? (know),? ?? (what)? and their conjugated
forms, which are excessively common in ques-
tions. For the query expansion in the baseline, we
used Japanese WordNet to obtain synonyms and
hypernyms of query terms.
6 Results
Table 1 summarizes our results. ?Subtree ex-
traction (SbE)? is our method, and ?Sentence ex-
traction (NC)? is a version of our method with-
out compression. The NC has the same objec-
tive function but only extracts sentences. The F1-
measure and F3-measure of our method are 0.159
and 0.190 respectively, while those of the state-of-
1029
the-art baseline are 0.135 and 0.174 respectively.
Unfortunately, since the document set is small, the
difference is not statistically significant. Compar-
ing our method with the one without compression,
we can see that there are improvements in the F1
and F3 scores of the human evaluation, whereas
the POURPRE score of the version of our method
without compression is higher than that of our
method with compression. The compression im-
proved the precision of our method, but slightly
decreased the recall.
For the error analyses, we reconstructed the
original sentences from which our method ex-
tracted the subtrees. Table 2 shows the statistics
of the summaries of SbE and reconstructed sum-
maries (RC). The original sentences covered 108
answer nuggets in total, and 8 of these answer
nuggets were dropped by the sentence compres-
sion. Comparing the results of SbE and RC, we
can see that the sentence compression caused the
recall of SbE to be 7% lower than that of RC.
However, the drop is relatively small in light of
the fact that the sentence compression can discard
19% of the original character length with SbE.
This suggests that the compression can efficiently
prune words while avoiding pruning informative
content.
Since the summary length is short, we can select
only two or three sentences for a summary. As
Morita et al (2011) mentioned, answer nuggets
overlap each other. The baseline objective func-
tion R tends to extract sentences from various
clusters. If the answer nuggets are present in the
same cluster, the objective function does not fit the
situation. However, our methods (SbE and NC)
have a parameter d that can directly adjust overlap
penalty with respect to word importance as well
as query relevance. This may help our methods to
cover similar answer nuggets. In fact, the develop-
ment data resulted in a relatively high parameter d
(0.8) for NC compared with 0.2 for SbE.
7 Conclusions and Future Work
We formalized a query-oriented summarization,
which is a task in which one simultaneously per-
forms sentence compression and extraction, as a
new optimization problem: budgeted monotone
nondecreasing submodular function maximization
with a cost function. We devised an approximate
algorithm to solve the problem in a reasonable
computational time and proved that its approxima-
tion rate is 12(1 ? e?1). Our approach achieved
an F3-measure of 0.19 on the ACLIA2 Japanese
test collection, which is 9.2 % improvement over
a state-of-the-art method using a submodular ob-
jective function.
Since our algorithm requires that the objective
function is the sum of word score functions, our
proposed method has a restriction that we cannot
use an arbitrary monotone submodular function as
the objective function for the summary. Our fu-
ture work will improve the local search algorithm
to remove this restriction. As mentioned before,
we also plan to adapt of our system to other lan-
guages.
Appendix
Here, we analyze the performance guarantee of
Algorithm 1. We use the following notation. S? is
the optimal solution, cu(S) is the residual cost of
subtree u when S is already covered, and i? is the
last step before the algorithm discards a subtree
s ? S? or a part of the subtree s. This is because
the subtree does not belong to either the approxi-
mate solution or the optimal solution. We can re-
move the subtree s? from V without changing the
approximate rate. si is the i-th subtree obtained by
line 5 of Algorithm 1. Gi is the set obtained after
adding subtree si to Gi?1 from the valid set Bi.
Gf is the final solution obtained by Algorithm 1.
f(?) : 2V ? R is a monotone submodular func-
tion.
We assume that there is an equivalent sub-
tree with any union of subtrees in a valid set B:
?t1, t2,?te, te ? {t1, t2}. Note that for any or-
der of the set, the cost or profit of the set is fixed:?
ui?S={u1,...,u|S|} cui(Si?1) = c(S).
Lemma 1 ?X,Y ? V, f(X) ? f(Y ) +?
u?X\Y ?u(Y ), where ?u(S) = f(S ? {u}) ?
f(S).
The inequality can be derived from the definition
of submodularity. 2
Lemma 2 For i = 1, . . . , i?+1, when 0 ? r ? 1,
f(S?)?f(Gi?1)?L
r |S?|1?r
csi (Gi?1)
(f(Gi?1?{si})?f(Gi?1)),
where cu(S)=c(S?{u})?c(S).
Proof. From line 5 of Algorithm 1, we have
?u ? S?\Gi?1,
?u(Gi?1)
cu(Gi?1)r
? ?si(Gi?1)csi(Gi?1)r
.
Let B be a valid set, and union be a func-
tion that returns the union of subtrees. We have
1030
?T ? B, ?b ? B, b = union(T ), because we
have an equivalent tree b ? B for each union
of trees T in a valid set B. That is, for any
set of subtrees, we have an equivalent set of sub-
trees, where bi ? Bi. Without loss of generality,
we can replace the difference set S?\Gi?1 with
a set T ?i?1 = {b0, . . . , b|T ?i?1|} that does not con-tain any two elements extracted from the same
valid set. Thus when 0 ? r ? 1 and 0 ?
i ? i? + 1, ?s?\Gi?1 (Gi?1)cS?\Gi?1 (Gi?1)r =
?T ?i?1 (Gi?1)
cT ?i?1 (Gi?1)
r , and
?bj ? T ?i?1,
?bj (Gi?1)
cbj (Gi?1)r
? ?si (Gi?1)csi (Gi?1)r . Thus,
?T ?i?1 (Gi?1) =
?
u?T ?i?1
?u(Gi?1)
? ?si (Gi?1)csi (Gi?1)r
?
u?T ?i?1
cu(Gi?1)r
? ?si (Gi?1)csi (Gi?1)r |T
?
i?1|
(?
u?T ?i?1
cu(Gi?1)
|T ?i?1|
)r
? ?si (Gi?1)csi (Gi?1)r |T
?
i?1|1?r
(?
u?T ?i?1
cu(?)
)r
? ?si (Gi?1)csi (Gi?1)r |S
?|1?rLr,
where the second inequality is from Ho?lder?s in-
equality. The third inequality uses the submodu-
larity of the cost function,
cu(Gi?1) = c({u} ?Gi?1)? c(Gi?1) ? cu(?)
and the fact that |S?| ? |S?\Gi?1| ? |T ?i?1|, and?
u?T ?i?1 cu(?) = c(T
?
i?1) ? L .
As a result, we have
?s?\Gi?1(Gi?1) = ?T ?i?1(Gi?1)
? ?si(Gi?1)csi(Gi?1)r
|S?|1?rLr.
Let X = S? and Y = Gi?1. Applying Lemma
1 yields
f(S?) ? f(Gi?1) + ?u?S?\Gi?1(Gi?1).
? f(Gi?1) +
?si(Gi?1)
csi(Gi?1)
|S?|1?rLr.
The lemma follows as a result.
Lemma 3 For a normalized monotone submodu-
lar f(?), for i = 1, . . . , i? + 1 and 0 ? r ? 1 and
letting si be the i-th unit added into G and Gi be
the set after adding si, we have
f(Gi) ?
(
1?
i?
k=1
(
1? csk(Gk?1)
r
Lr|S?|1?r
))
f(S?).
Proof. This is proved similarly to Lemma 3 of
(Krause and Guestrin, 2005) using Lemma 2.
Proof of Theorem 1. This is proved similarly to
Theorem 1 of (Krause and Guestrin, 2005) using
Lemma 3.
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages
481?490, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Calinescu Calinescu, Chandra Chekuri, Martin Pa?l,
and Jan Vondra?k. 2011. Maximizing a monotone
submodular function subject to a matroid constraint.
SIAM Journal on Computing, 40(6):1740?1766.
Michele Conforti and Ge?rard Cornue?jols. 1984. Sub-
modular set functions, matroids and the greedy al-
gorithm: Tight worst-case bounds and some gener-
alizations of the rado-edmonds theorem. Discrete
Applied Mathematics, 7(3):251 ? 274.
Hoa Trang Dang. 2008. Overview of the tac
2008 opinion question answering and summariza-
tion tasks. In Proceedings of Text Analysis Confer-
ence.
Anupam Gupta, Aaron Roth, Grant Schoenebeck, and
Kunal Talwar. 2010. Constrained non-monotone
submodular maximization: offline and secretary
algorithms. In Proceedings of the 6th interna-
tional conference on Internet and network eco-
nomics, WINE?10, pages 246?257, Berlin, Heidel-
berg. Springer-Verlag.
Sun-Yuan Hsieh and Ting-Yu Chou. 2010. The
weight-constrained maximum-density subtree prob-
lem and related problems in trees. The Journal of
Supercomputing, 54(3):366?380, December.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
HLT-NAACL ?06, pages 176?183, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Infor-
mation Processing Letters, 70(1):39?45.
Andreas Krause and Carlos Guestrin. 2005. A
note on the budgeted maximization on submodular
functions. Technical Report CMU-CALD-05-103,
Carnegie Mellon University.
1031
Ariel Kulik, Hadas Shachnai, and Tami Tamir. 2009.
Maximizing submodular set functions subject to
multiple linear constraints. In Proceedings of
the twentieth Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA ?09, pages 545?554,
Philadelphia, PA, USA. Society for Industrial and
Applied Mathematics.
Sadao Kurohashi and Daisuke Kawahara, 2009a.
Japanese Morphological Analysis System JUMAN
6.0 Users Manual. http://nlp.ist.i.
kyoto-u.ac.jp/EN/index.php?JUMAN.
Sadao Kurohashi and Daisuke Kawahara, 2009b. KN
parser (Kurohashi-Nagao parser) 3.0 Users Man-
ual. http://nlp.ist.i.kyoto-u.ac.jp/
EN/index.php?KNP.
Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and
Maxim Sviridenko. 2009. Non-monotone submod-
ular maximization under matroid and knapsack con-
straints. In Proceedings of the 41st annual ACM
symposium on Theory of computing, STOC ?09,
pages 323?332, New York, NY, USA. ACM.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 912?920, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 510?520,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jimmy Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to com-
plex questions. Information Retrieval, 9(5):565?
587, November.
Andre? F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, ILP ?09, pages 1?9, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of the 29th European conference on IR
research, ECIR?07, pages 557?564, Berlin, Heidel-
berg. Springer-Verlag.
Teruko Mitamura, Eric Nyberg, Hideki Shima,
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Rui-
hua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong
Ji, and Noriko Kando. 2008. Overview of the
NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual
Information Access. In Proceedings of the 7th NT-
CIR Workshop.
Teruko Mitamura, Hideki Shima, Tetsuya Sakai,
Noriko Kando, Tatsunori Mori, Koichi Takeda,
Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, and
Cheng-Wei Lee. 2010. Overview of the ntcir-8 aclia
tasks: Advanced cross-lingual information access.
In Proceedings of the 8th NTCIR Workshop.
Hajime Morita, Tetsuya Sakai, and Manabu Okumura.
2011. Query snowball: a co-occurrence-based ap-
proach to multi-document summarization for ques-
tion answering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 223?229, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Wolfgang Schmidt. 1991. Greedoids and searches in
directed graphs. Discrete Mathmatics, 93(1):75?88,
November.
Jie Tang, Limin Yao, and Dewei Chen. 2009. Multi-
topic based query-oriented summarization. In Pro-
ceedings of 2009 SIAM International Conference
Data Mining (SDM?2009), pages 1147?1158.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and
Richard Schwartz. 2006. Sentence compression
as a component of a multi-document summariza-
tion system. In Proceedings of the 2006 Doc-
ument Understanding Conference (DUC 2006) at
NLT/NAACL 2006.
1032
