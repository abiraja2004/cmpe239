Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 112?115,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Timed Annotations ? Enhancing MUC7 Metadata
by the Time It Takes to Annotate Named Entities
Katrin Tomanek and Udo Hahn
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena, Germany
{katrin.tomanek|udo.hahn}@uni-jena.de
Abstract
We report on the re-annotation of selected
types of named entities from the MUC7
corpus where our focus lies on record-
ing the time it takes to annotate these
entities given two basic annotation units
? sentences vs. complex noun phrases.
Such information may be helpful to lay
the empirical foundations for the develop-
ment of cost measures for annotation pro-
cesses based on the investment in time for
decision-making per entity mention.
1 Introduction
Manually supplied annotation metadata is at the
heart of (semi)supervised machine learning tech-
niques which have become very popular in NLP
research. At their flipside, they create an enor-
mous bottleneck because major shifts in the do-
main of discourse, the basic entities of interest, or
the text genre often require new annotation efforts.
But annotations are costly in terms of getting well-
trained and intelligible human resources involved.
Surprisingly, cost awareness has not been a pri-
mary concern in most of the past linguistic anno-
tation initiatives. Only recently, annotation strate-
gies (such as Active Learning (Cohn et al, 1996))
which strive for minimizing the annotation load
have gained increasing attention. Still, when it
comes to the empirically plausible assessment of
annotation costs even proponents of Active Learn-
ing make overly simplistic and empirically ques-
tionable assumptions, e.g., the uniformity of an-
notation costs over the number of linguistic units
(e.g., tokens) to be annotated.
We here consider the time it takes to annotate
a particular entity mention as a natural indicator
of effort for named entity annotations. In order to
lay the empirical foundations for experimentally
grounded annotation cost models we couple com-
mon named entity annotation metadata with a time
stamp reflecting the time measured for decision
making.1
Previously, two studies ? one dealing with POS
annotation (Haertel et al, 2008), the other with
named entity and relation annotation (Settles et al,
2008) ? have measured the time needed to anno-
tate sentences on small data sets and attempted to
learn predictive models of annotation cost. How-
ever, these data sets do not meet our requirements
as we envisage a large, coherent, and also well-
known newspaper entity corpus extended by an-
notation costs on a fine-grained level. Especially
size and coherence of such a corpus are not only
essential for building accurate cost models but also
as a reference baseline for cost-sensitive annota-
tion strategies. Moreover, the annotation level for
which cost information is available is crucial be-
cause document- or sentence-level data might be
too coarse for several applications. Accordingly,
this paper introduces MUC7T , our extension to the
entity annotations of the MUC7 corpus (Linguis-
tic Data Consortium, 2001) where time stamps are
added to two levels of annotation granularity, viz.
sentences and complex noun phrases.
2 Corpus Annotation
2.1 Annotation Task
Our annotation initiative constitutes an extension
to the named entity annotations (ENAMEX) of the
English part of the MUC7 corpus covering New
York Times articles from 1996. ENAMEX annota-
tions cover three types of named entities, viz. PER-
SONS, LOCATIONS, and ORGANIZATIONS. We in-
structed two human annotators, both advanced stu-
dents of linguistics with good English language
skills, to re-annotate the MUC7 corpus for the
ENAMEX subtask. To be as consistent as possi-
1These time stamps should not be confounded with the an-
notation of temporal expressions (TIMEX in MUC7, or even
more advanced metadata using TIMEML for the creation of
the TIMEBANK (Pustejovsky et al, 2003)).
112
ble with the existing MUC7 annotations, the an-
notators had to follow the original guidelines of
the MUC7 named entity task. For ease of re-
annotation, we intentionally ignored temporal and
number expressions (TIMEX and NUMEX).
MUC7 covers three distinct document sets for
the named entity task. We used one of these sets
to train the annotators and develop the annotation
design, and another one for our actual annotation
initiative which consists of 100 articles reporting
on airplane crashes. We split lengthy documents
(27 out of 100) into halves to fit on the annota-
tion screen without the need for scrolling. Further-
more, we excluded two documents due to over-
length which would have required overly many
splits. Our final corpus contains 3,113 sentences
(76,900 tokens) (see Section 3.1 for more details).
Time-stamped ENAMEX annotation of this cor-
pus constitutes MUC7T , our extension of MUC7.
Annotation time measurements were taken on two
syntactically different annotation units of single
documents: (a) complete sentences and (b) com-
plex noun phrases. The annotation task was de-
fined such as to assign an entity type label to each
token of an annotation unit. Sentence-level anno-
tation units where derived by the OPENNLP2 sen-
tence splitter. The use of complex noun phrases
(CNPs) as an alternative annotation unit is mo-
tivated by the fact that in MUC7 the syntactic
encoding of named entity mentions basically oc-
curs through nominal phrases. CNPs were derived
from the sentences? constituency structure using
the OPENNLP parser (trained on PENNTREE-
BANK data) to determine top-level noun phrases.
To avoid overly long phrases, CNPs dominating
special syntactic structures, such as coordinations,
appositions, or relative clauses, were split up at
discriminative functional elements (e.g., a relative
pronoun) and these elements were eliminated. An
evaluation of our CNP extractor on ENAMEX an-
notations in MUC7 showed that 98.95% of all en-
tities where completely covered by automatically
identified CNPs. For the remaining 1.05% of the
entity mentions, parsing errors were the most com-
mon source of incomplete coverage.
2.2 Annotation and Time Measurement
While the annotation task itself was ?officially?
declared to yield only annotations of named en-
tity mentions within the different annotation units,
2http://opennlp.sourceforge.net
we were primarily interested in the time needed
for these annotations. For precise time measure-
ments, single annotation examples were shown to
the annotators, one at a time. An annotation exam-
ple consists of the chosen MUC7 document with
one annotation unit (sentence or CNP) selected
and highlighted. Only the highlighted part of the
document could be annotated and the annotators
were asked to read only as much of the context sur-
rounding the annotation unit as necessary to make
a proper annotation decision. To present the an-
notation examples to annotators and allow for an-
notation without extra time overhead for the ?me-
chanical? assignment of entity types, our annota-
tion GUI is controlled by keyboard shortcuts. This
minimizes annotation time compared to mouse-
controlled annotation such that the measured time
reflects only the amount of time needed for taking
an annotation decision.
In order to avoid learning effects at the annota-
tors? side on originally consecutive syntactic sub-
units, we randomly shuffled all annotation exam-
ples so that subsequent annotation examples were
not drawn from the same document. Hence, an-
notation times were not biased by the order of ap-
pearance of the annotation examples.
Annotators were given blocks of either 500
CNP- or 100 sentence-level annotation examples.
They were asked to annotate each block in a single
run under noise-free conditions, without breaks
and disruptions. They were also instructed not to
annotate for too long stretches of time to avoid tir-
ing effects making time measurements unreliable.
All documents were first annotated with respect
to CNP-level examples within 2-3 weeks, with
only very few hours per day of concrete annota-
tion work. After completion of the CNP-level an-
notation, the same documents had to be annotated
on the sentence level as well. Due to randomiza-
tion and rare access to surrounding context during
the CNP-level annotation, annotators credibly re-
ported that they had indeed not remembered the
sentences from the CNP-level round. Thus, the
time measurements taken on the sentence level do
not seem to exhibit any human memory bias.
Both annotators went through all annotation ex-
amples so that we have double annotations of the
complete data set. Prior to coding, they indepen-
dently got used to the annotation guidelines and
were trained on several hundred examples. For the
annotators? performance see Section 3.2.
113
3 Analysis
3.1 Corpus Statistics
Table 1 summarizes statistics on the time-stamped
MUC7 corpus. About 60% of all tokens are cov-
ered by CNPs (45,097 out of 76,900 tokens) show-
ing that sentences are made up from CNPs to a
large extent. Still, removing the non-CNP to-
kens markedly reduces the amount of tokens to
be considered for entity annotation. CNPs cover
slightly less entities (3,937) than complete sen-
tences (3,971), a marginal loss only.
sentences 3,113
sentence tokens 76,900
chunks 15,203
chunk tokens 45,097
entity mentions in sentences 3,971
entity mentions in CNPs 3,937
sentences with entity mentions 63%
CNPs with entity mentions 23%
Table 1: Descriptive statistics of time-stamped MUC7 corpus
On the average, sentences have a length of
24.7 tokens, while CNPs are rather short with
3.0 tokens, on the average. However, CNPs vary
tremendously in their length, with the shortest
ones having only one token and the longest ones
(mostly due to parsing errors) spanning over 30
(and more) tokens. Figure 1 depicts the length
distribution of sentences and CNPs showing that
a reasonable portion of CNPs have less than five
tokens, while the distribution of sentence lengths
almost follows a normal distribution in the interval
[0, 50]. While 63% of all sentences contain at least
one entity mention, only 23% of CNPs contain en-
tity mentions. These statistics show that CNPs are
generally rather short and a large fraction of CNPs
does not contain entity mentions at all. We may
hypothesize that this observation will be reflected
by annotation times.
sentence length
tokens
freq
uenc
y
0 20 40 60 80
0
100
300
500
CNP length
tokens
freq
uenc
y
0 5 10 15 20
0
200
0
600
0
Figure 1: Length distribution of sentences and CNPs
3.2 Annotation Performance
To test the validity of the guidelines and the gen-
eral performance of our annotators A and B, we
compared their annotation results on 5 blocks of
sentence-level annotation examples created dur-
ing training. Annotation performance was mea-
sured in terms of Cohen?s kappa coefficient ? on
the token level and entity-segment F -score against
MUC7 annotations. The annotators achieved
?A = 0.95 and ?B = 0.96, and FA = 0.92
and FB = 0.94, respectively.3 Moreover, they ex-
hibit an inter-annotator agreement of ?A,B = 0.94
and an averaged mutual F-score of FA,B = 0.90.
These numbers reveal that the task is well-defined
and the annotators have sufficiently internalized
the annotation guidelines to produce valid results.
Figure 2 shows the annotators? scores against
the original MUC7 annotations for the 31 blocks
of sentence-level annotations (3,113 sentences)
which range from ? = 0.89 to ? = 0.98. Largely,
annotation performance is similar for both anno-
tators and shows that they consistently found a
block either rather hard or easy to annotate. More-
over, annotation performance seems stationary ?
no general trend in annotation performance over
time can be observed.
l l l l
l
l
l
l
l
l
l
l
l
l
l l
l
l l
l
l l
l
l l
l l
l l
l l
0 5 10 15 20 25 300
.80
0.85
0.90
0.95
1.00
sentence?level annotation
blocks
kapp
a l
l l l l l l
l
l l l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l annotator Aannotator B
Figure 2: Average kappa coefficient per block
3.3 Time Measurements
Figure 3 shows the average annotation time per
block (CNPs and sentences). Considering the
CNP-level annotations, there is a learning effect
for annotator B during the first 9 blocks. Af-
ter that, both annotators are approximately on a
par regarding the annotation time. For sentence-
level annotations, both annotators again yield sim-
ilar annotation times per block, without any learn-
ing effects. Similar to annotation performance,
3Entity-specific F-scores against MUC7 annotations for
A and B are 0.90 and 0.92 for LOCATION, 0.92 and 0.93
for ORGANIZATION, and 0.96 and 0.98 for PERSON, respec-
tively.
114
l l l l l
l l l
l l
l
l l l l l l
l
l l
l l l l
l
l l l
l
l
0 5 10 15 20 25 30
1.0
1.2
1.4
1.6
1.8
2.0
CNP?level annotation
blocks
sec
onds
l
l l
l
l l
l
l l
l l
l l l l l l
l l l l
l
l
l
l
l
l annotator Aannotator B
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l
l l
l
l l
l
l
0 5 10 15 20 25 30
4.0
4.5
5.0
5.5
6.0
sentence?level annotation
blocks
sec
onds
l
l l l
l
l
l
l
l
l
l
l
l l
l
l
l l l l
l
l
l l
l l
l
l
l
l
l annotator Aannotator B
Figure 3: Average annotation times per block
analysis of annotation time shows that the annota-
tion behavior is largely stationary (excluding first
rounds of CNP-level annotation) which allows sin-
gle time measurements to be interpreted indepen-
dently of previous time measurements. Both, time
and performance plots exhibit that there are blocks
which were generally harder or easier than other
ones because both annotators operated in tandem.
3.4 Easy and Hard Annotation Examples
As we have shown, inter-annotator variation of
annotation performance is moderate. Intra-block
performance, in contrast, is subject to high vari-
ance. Figure 4 shows the distribution of annota-
tor A?s CNP-level annotation times for block 20.
A?s average annotation time on this block amounts
to 1.37 seconds per CNP, the shortest time be-
ing 0.54, the longest one amounting 10.2 seconds.
The figure provides ample evidence for an ex-
tremely skewed time investment for coding CNPs.
A preliminary manual analysis revealed CNPs
with very low annotation times are mostly short
and consist of stop words and pronouns only, or
CNP?level annotation
annotation time
freq
uenc
y
2 4 6 8 10
05
0
150
250
Figure 4: Distribution of annotation times in one block
are otherwise simple noun phrases with a sur-
face structure incompatible with entity mentions
(e.g., all tokens are lower-cased). Here, humans
can quickly exclude the occurrence of entity men-
tions which results in low annotation times. CNPs
which took desparately long (more than 6 seconds)
were outliers indicating distraction or loss of con-
centration. Times between 3 and 5 seconds were
basically caused by semantically complex CNPs.
4 Conclusions
We have created a time-stamped version of MUC7
entity annotations, MUC7T , on two levels of anno-
tation granularity ? sentences and complex noun
phrases. Especially the phrase-level annotations
allow for fine-grained time measurement. We will
use this corpus for studies on (time) cost-sensitive
Active Learning. MUC7T can also be used to de-
rive or learn accurate annotation cost models al-
lowing to predict annotation time on new data. We
are currently investigating causal factors of anno-
tation complexity for named entity annotation on
the basis of MUC7T .
Acknowledgements
This work was funded by the EC within the
BOOTStrep (FP6-028099) and CALBC (FP7-
231727) projects. We want to thank Oleg Lichten-
wald (JULIE Lab) for implementing the noun
phrase extractor for our experiments.
References
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research,
4:129?145.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
costs of sampling methods in active learning for an-
notation. In Proceedings of the ACL-08: HLT, Short
Papers, pages 65?68.
Linguistic Data Consortium. 2001. Message Under-
standing Conference 7. LDC2001T02. FTP file.
James Pustejovsky, Patrick Hanks, Roser Saur??, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
corpus. In Proceedings of the Corpus Linguistics
2003 Conference, pages 647?656.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Pro-
ceedings of the NIPS?08 Workshop on Cost Sensitive
Learning, pages 1?10.
115
