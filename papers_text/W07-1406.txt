Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 36?41,
Prague, June 2007. c?2007 Association for Computational Linguistics
Recognizing Textual Entailment Using Sentence Similarity based on 
Dependency Tree Skeletons 
 Rui Wang and G?nter Neumann 
LT-lab, DFKI 
Stuhlsatzenhausweg 3, 66123 Saarbr?cken, Germany 
{wang.rui,Neumann}@dfki.de 
 
 
Abstract 
We present a novel approach to RTE that 
exploits a structure-oriented sentence rep-
resentation followed by a similarity func-
tion. The structural features are automati-
cally acquired from tree skeletons that are 
extracted and generalized from dependency 
trees. Our method makes use of a limited 
size of training data without any external 
knowledge bases (e.g. WordNet) or hand-
crafted inference rules. We have achieved 
an accuracy of 71.1% on the RTE-3 devel-
opment set performing a 10-fold cross 
validation and 66.9% on the RTE-3 test 
data. 
1 Introduction 
Textual entailment has been introduced as a rela-
tion between text expressions, capturing the fact 
that the meaning of one expression can be inferred 
from the other (Dagan and Glickman, 2004). More 
precisely, textual entailment is defined as ?? a 
relationship between a coherent text T and a lan-
guage expression, which is considered as a hy-
pothesis, H. We say that T entails H (H is a conse-
quent of T), denoted by T ? H, if the meaning of 
H, as interpreted in the context of T, can be in-
ferred from the meaning of T.?  
Table 1 displays several examples from the 
RTE-3 development set. For the third pair (id=410) 
the key knowledge needed to decide whether the 
entailment relation holds is that ?[PN1]?s wife, 
[PN2]? entails ?The name of [PN1]?s wife is 
[PN2]?, although T contains much more (irrelevant) 
information. On the other hand, the first pair (id=1) 
requires an understanding of concepts with oppo-
site meanings (i.e. ?buy? and ?sell?), which is a 
case of semantic entailment. 
The different sources of possible entailments 
motivated us to consider the development of spe-
cialized entailment strategies for different NLP 
tasks. In particular, we want to find out the poten-
tial connections between entailment relations be-
longing to different linguistic layers for different 
applications. 
In this paper, we propose a novel approach to-
wards structure-oriented entailment based on our 
empirical discoveries from the RTE corpora: 1) H 
is usually textually shorter than T; 2) not all infor-
mation in T is relevant to make decisions for the 
entailment; 3) the dissimilarity of relations among 
the same topics between T and H are of great im-
portance.  
Based on the observations, our primary method 
starts from H to T (i.e. in the opposite direction of 
the entailment relation) so as to exclude irrelevant 
information from T. Then corresponding key top-
ics and predicates of both elements are extracted. 
We then represent the structural differences be-
tween T and H by means of a set of Closed-Class 
Symbols. Finally, these acquired representations 
(named Entailment Patterns - EPs) are classified by 
means of a subsequence kernel. 
The Structure Similarity Function is combined 
with two robust backup strategies, which are re-
sponsible for cases that are not handled by the EPs. 
One is a Triple Similarity Function applied on top 
of the local dependency relations of T and H; the 
other is a simple Bag-of-Words (BoW) approach 
that calculates the overlapping ratio of H and T. 
Together, these three methods deal with different 
entailment cases in practice. 
36
2 Related Work 
Conventional methods for RTE define measures for 
the similarity between T and H either by assuming 
an independence between words (Corley and Mi-
halcea, 2005) in a BoW fashion or by exploiting 
syntactic interpretations. (Kouylekov and Magnini, 
2006) explore a syntactic tree editing distance to 
detect entailment relations. Since they calculate the 
similarity between the two dependency trees of T 
and H directly, the noisy information may decrease 
accuracy. This observation actually motivated us to 
start from H towards the most relevant information 
in T. 
Logic rules (as proposed by (Bos and Markert, 
2005)) or sequences of allowed rewrite rules (as in 
(de Salvo Braz et al, 2005)) are another fashion of 
tackling RTE. One the best two teams in RTE-2 
(Tatu et al, 2006) proposed a knowledge represen-
tation model which achieved about 10% better per-
formance than the third (Zanzotto and Moschitti, 
2006) based on their logic prover. The other best 
team in RTE-2 (Hickl et al, 2006) automatically 
acquired extra training data, enabling them to 
achieve about 10% better accuracy than the third as 
well. Consequently, obtaining more training data 
and embedding deeper knowledge were expected 
to be the two main directions pointed out for future 
research in the RTE-2 summary statement. How-
ever, except for the positive cases of SUM, T-H 
pairs are normally not very easy to collect auto-
matically. Multi-annotator agreement is difficult to 
reach on most of the cases as well. The knowledge-
based approach also has its caveats since logical 
rules are usually implemented manually and there-
fore require a high amount of specialized human 
expertise in different NLP areas. 
 Another group (Zanzotto and Moschitti, 2006) 
utilized a tree kernel method for cross-pair similar-
ity, which showed an improvement, and this has 
motivated us to investigate kernel-based methods. 
The main difference in our method is that we apply 
subsequence kernels on patterns extracted from the 
dependency trees of T and H, instead of applying 
tree kernels on complete parsing trees. On the one 
hand, this allows us to discover essential parts in-
dicating an entailment relationship, and on the 
other hand, computational complexity is reduced. 
3 An Overview of RTE 
Figure 1 shows the different processing techniques 
and depths applied to the RTE task. Our work fo-
cuses on constructing a similarity function operat-
ing between sentences. In detail, it consists of sev-
eral similarity scores with different domains of 
locality on top of the dependency structure. Figure 
2 gives out the workflow of our system. The main 
part of the sentence similarity function is the Struc-
ture Similarity Function; two other similarity 
scores are calculated by our backup strategies. The 
first backup strategy is a straightforward BoW 
method that we will not present in this paper (see 
more details in (Corley and Mihalcea, 2005)); 
Id Ta s k Te x t H y po th e s i s En ta i l s ?
1 IE
Th e sa le wa s m a d e to  p a y Yu ko s' U S $  2 7 .5  b illio n  ta x b ill, Yu g a n skn efteg a z 
wa s o rig in a lly so ld  fo r U S $  9 .4  b illio n  to  a  little  kn o wn  co m p a n y 
B a ika lfin a n sg ro u p  wh ich  wa s la ter b o ught  b y th e R u ssia n  sta te-o wn ed  o il 
B a ika lfin a n sg ro u p  
wa s so ld  to  
R o sn eft.
Y E S
3 9 0 IR
T yp ho o n  X a n g sa n e la shed  th e P hilip p ine  ca p ita l o n  Th u rsd a y, 
g ro u n d in g  flig h ts, h a ltin g  vessels a n d  clo sin g  sch o o ls a n d  m a rkets a fter 
trig g erin g  fa ta l fla sh  flo o d s in  th e cen tre o f th e  co u n try.
A  typ ho o n  b a tters 
th e  P hilip p ines . Y E S
4 1 0 Q A
(S en ten ce 1  ...) . A lo n g  with  th e first la d y's m o th er, Jen n a  Welch , th e 
weeken d  g a th erin g  in clu d es th e p resid en t's p a ren ts, fo rm er P resid en t 
G eo rge H .W . Bush a nd  his wife, Ba rb a ra ;  h is sister D o ro  Ko ch  a n d  h er 
h u sb a n d , B o b b y;  a n d  h is b ro th er, M a rvin , a n d  h is wife , M a rg a ret.
Th e n a m e o f 
G eo rg e H .W. 
B u sh 's wife is 
B a rb a ra .
Y E S
7 3 9 SU M
Th e FD A  wo u ld  n o t sa y in  wh ich  sta tes  th e p ills  h a d  b een  so ld , b u t 
in stea d  reco m m en d ed  th a t cu sto m ers d eterm in e wh eth er p ro d u cts th ey 
b o u g h t a re b ein g  reca lled  b y ch eckin g  th e sto re list  o n  th e  FD A  Web  site , 
a n d  th e b a tch  list .  Th e b a tch  n u m b ers a p p ea r o n  th e co n ta in er's la b el.
Th e FD A 
p ro vid ed  a  list  o f 
sta tes  in  wh ich  th e 
p ills  h a ve b een  
N O
Table 1 Examples from RTE-3 
Figure 1 Overview of RTE 
37
while the second one is based on a triple set repre-
sentation of sentences that expresses the local de-
pendency relations found by a parser1. 
A dependency structure consists of a set of triple 
relations (TRs). A TR is of the form <node1, rela-
tion, node2>, where node1 represents the head, 
node2 the modifier and relation the dependency 
relation. Chief requirements for the backup system 
are robustness and simplicity. Accordingly, we 
construct a similarity function, the Triple Similar-
ity Function (TSF), which operates on two triple 
sets and determines how many triples of H2 are 
contained in T. The core assumption here is that 
the higher the number of matching triple elements, 
the more similar both sets are, and the more likely 
it is that T entails H. 
TSF uses an approximate matching function. 
Different cases (i.e. ignoring either the parent node 
or the child node, or the relation between nodes) 
might provide different indications for the similar-
ity of T and H. In all cases, a successful match be-
tween two nodes means that they have the same 
lemma and POS. We then sum them up using dif-
ferent weights and divide the result by the cardinal-
ity of H for normalization. The different weights 
learned from the corpus indicate that the ?amount 
of missing linguistic information? affect entailment 
decisions differently. 
4 Workflow of the Main Approach 
Our Structure Similarity Function is based on the 
hypothesis that some particular differences be-
tween T and H will block or change the entailment 
relationship. Initially we assume when judging the 
entailment relation that it holds for each T-H pair 
                                                 
1
 We are using Minipar (Lin, 1998) and Stanford Parser (Klein 
and Manning, 2003) as preprocessors, see also sec. 5.2. 
2
 Note that henceforth T and H will represent either the origi-
nal texts or the dependency structures. 
(using the default value ?YES?). The major steps 
are as follows (see also Figure 2): 
4.1 Tree Skeleton Extractor 
Since we assume that H indicates how to extract 
relevant parts in T for the entailment relation, we 
start from the Tree Skeleton of H (TSH). First, we 
construct a set of keyword pairs using all the nouns 
that appear in both T and H. In order to increase 
the hits of keyword pairs, we have applied a partial 
search using stemming and some word variation 
techniques on the substring level. For instance, the 
pair (id=390) in Table 1 has the following list of 
keyword pairs, 
<Typhoon_Xangsane ## typhoon, 
Philippine ## Philippines> 
Then we mark the keywords in the dependency 
trees of T and H and extract the sub-trees by ignor-
ing the inner yields. Usually, the Root Node of H 
(RNH) is the main verb; all the keywords are con-
tained in the two spines of TSH (see Figure 3). 
Note that in the Tree Skeleton of T (TST), 1) the 
Root Node (RNT) can either be a verb, a noun or 
even a dependency relation, and 2) if the two Foot 
Nodes (FNs) belong to two sentences, a dummy 
node is created that connects the two spines. 
Thus, the prerequisite for this algorithm is that 
TSH has two spines containing all keywords in H, 
and T satisfies this as well. For the RTE-3 devel-
opment set, we successfully extracted tree skele-
Figure 3 Example of a Tree Skeleton 
Figure 2 Workflow of the System 
38
?
=
?
=
+?
=
?
=
=
><><
||
1
|'|
1'
)
'
,(
||
1
|'|
1'
)
'
,(
)',',,(
H
j
H
j j
CCSjCCSCCSK
T
i
T
i i
CCSiCCSCCSK
HTHT
esubsequencK
tons from 254 pairs, i.e., 32% of the data is cov-
ered by this step, see also sec. 5.2. 
Next, we collapse some of the dependency rela-
tion names from the parsers to more generalized 
tag names, e.g., collapsing <OBJ2> and <DESC> 
to <OBJ>. We group together all nodes that have 
relation labels like <CONJ> or <NN>, since they 
are assumed to refer to the same entity or belong to 
one class of entities sharing some common charac-
teristics. Lemmas are removed except for the key-
words. Finally, we add all the tags to the CCS set. 
Since a tree skeleton TS consists of spines con-
nected via the same root node, TS can be trans-
formed into a sequence. Figure 4 displays an ex-
ample corresponding to the second pair (id=390) of 
Table 1. Thus, the general form of a sequential rep-
resentation of a tree skeleton is: 
LSP #RN# RSP 
where LSP represents the Left Spine, RSP repre-
sents the Right Spine, and RN is the Root Node. 
On basis of this representation, a comparison of the 
two tree skeletons is straightforward: 1) merge the 
two LSPs by excluding the longest common prefix, 
and 2) merge the two RSPs by excluding the long-
est common suffix. Then the Spine Difference (SD) 
is defined as the remaining infixes, which consists 
of two parts, SDT and SDH. Each part can be either 
empty (i.e. ?) or a CCS sequence. For instance, the 
two SDs of the example in Figure 4 (id=390) are 
(LSD ? Left SD; RSD ? Right SD; ## is a separa-
tor sign): 
LSDT(N) ## LSDH(?) 
RSDT(?) ## RSDH(?) 
We have observed that two neighboring depend-
ency relations of the root node of a tree skeleton 
(<SUBJ> or <OBJ>) can play important roles in 
predicting the entailment relation as well. There-
fore, we assign them two extra features named 
Verb Consistence (VC) and Verb Relation Con-
sistence (VRC). The former indicates whether two 
root nodes have a similar meaning, and the latter 
indicates whether the relations are contradictive 
(e.g. <SUBJ> and <OBJ> are contradictive). 
We represent the differences between TST and 
TSH by means of an Entailment Pattern (EP), 
which is a quadruple <LSD, RSD, VC, VRC>. VC 
is either true or false, meaning that the two RNs 
are either consistent or not. VRC has ternary value, 
whereby 1 means that both relations are consistent, 
-1 means at least one pair of corresponding rela-
tions is inconsistent, and 0 means RNT is not a 
verb.3 The set of EPs defines the feature space for 
the subsequence kernels in our Structure Similarity 
Function. 
4.2 Structure Similarity Function 
We define the function by constructing two basic 
kernels to process the LSD and RSD part of an EP, 
and two trivial kernels for VC and VRC. The four 
kernels are combined linearly by a composite ker-
nel that performs binary classification on them. 
Since all spine differences SDs are either empty 
or CCS sequences, we can utilize subsequence 
kernel methods to represent features implicitly, cf. 
(Bunescu and Mooney, 2006). Our subsequence 
kernel function is: 
whereby T and H refers to all spine differences 
SDs from T and H, and |T| and |H| represent the 
cardinalities of SDs. The function KCCS(CCS,CCS?) 
checks whether its arguments are equal. 
Since the RTE task checks the relationship be-
tween T and H, we need to consider collocations 
of some CCS subsequences between T and H as 
well. Essentially, this kernel evaluates the similar-
ity of T and H by means of those CCS subse-
quences appearing in both elements. The kernel 
function is as follows: 
On top of the two simple kernels, KVC, and KVRC, 
we use a composite kernel to combine them line-
arly with different weights: 
VRCVCncollocatioesubsequenccomposite KKKKK ???? +++= , 
                                                 
3
 Note that RNH is guaranteed to be a verb, because otherwise 
the pair would have been delegated to the backup strategies. 
????
= = = =
?=
><><
||
1
'||
1'
||
1
'||
1'
''
),(),(
)',',,(
T
i
T
i
H
j
H
j
jjCCSiiCCS
ncollocatio
CCSCCSKCCSCCSK
HTHTK
Figure 4 Spine Merging 
39
where ? and ? are learned from the training corpus; 
?=?=1. 
5 Evaluation 
We have evaluated four methods: the two backup 
systems as baselines (BoW and TSM, the Triple 
Set Matcher) and the kernel method combined with 
the backup strategies using different parsers, Mini-
par (Mi+SK+BS) and the Stanford Parser 
(SP+SK+BS). The experiments are based on RTE-
3 Data 4 . For the kernel-based classification, we 
used the classifier SMO from the WEKA toolkit 
(Witten and Frank, 1999).  
5.1 Experiment Results 
RTE-3 data include the Dev Data (800 T-H pairs, 
each task has 200 pairs) and the Test Data (same 
size). Experiment A performs a 10-fold cross-
validation on Dev Data; Experiment B uses Dev 
Data for training and Test Data for testing cf. Table 
2 (the numbers denote accuracies): 
Systems\Tasks IE IR QA SUM All 
Exp A: 10-fold Cross Validation on RTE-3 Dev Data 
BoW 54.5 70 76.5 68.5 67.4 
TSM 53.5 60 68 62.5 61.0 
Mi+SK+BS 63 74 79 68.5 71.1 
SP+SK+BS 60.5 70 81.5 68.5 70.1 
Exp B: Train: Dev Data; Test: Test Data 
BoW 54.5 66.5 76.5 56 63.4 
TSM 54.5 62.5 66 54.5 59.4 
Mi+SP+SK+BS 58.5 70.5 79.5 59 66.9 
Table 2 Results on RTE-3 Data 
For the IE task, Mi+SK+BS obtained the highest 
improvement over the baseline systems, suggesting 
that the kernel method seems to be more appropri-
ate if the underlying task conveys a more ?rela-
tional nature.? Improvements in the other tasks are 
less convincing as compared to the baselines. Nev-
ertheless, the overall result obtained in experiment 
B would have been among the top 3 of the RTE-2 
challenge. We utilize the system description table 
of (Bar-Haim et al, 2006) to compare our system 
with the best two systems of RTE-2 in Table 35: 
                                                 
4
 See (Wang and Neumann, 2007) for details concerning the 
experiments of our method on RTE-2 data. 
5
 Following the notation in  (Bar-Haim et al, 2006): Lx: Lexi-
cal Relation DB; Ng: N-Gram / Subsequence overlap; Sy: 
Syntactic Matching / Alignment; Se: Semantic Role Labeling; 
LI: Logical Inference; C: Corpus/Web; M: ML Classification; 
B: Paraphrase Technology / Background Knowledge; L: Ac-
quisition of Entailment Corpora. 
Systems Lx Ng Sy Se LI C M B L 
Hickl et al X X X X  X X  X 
Tatu et al X    X   X  
Ours  X X    X   
Table 3 Comparison with the top 2 systems in 
RTE-2. 
Note that the best system (Hickl et al, 2006) ap-
plies both shallow and deep techniques, especially 
in acquiring extra entailment corpora. The second 
best system (Tatu et al, 2006) contains many 
manually designed logical inference rules and 
background knowledge. On the contrary, we ex-
ploit no additional knowledge sources besides the 
dependency trees computed by the parsers, nor any 
extra training corpora. 
5.2 Discussions 
Table 4 shows how our method performs for the 
task-specific pairs matched by our patterns: 
Tasks IE IR QA SUM ALL 
ExpA:Matched 53 19 23.5 31.5 31.8 
ExpA:Accuracy 67.9 78.9 91.5 71.4 74.8 
ExpB:Matched 58.5 16 27.5 42 36 
ExpB:Accuracy 57.2 81.5 90.9 65.5 68.8 
Table 4 Performances of our method 
For IE pairs, we find good coverage, whereas 
for IR and QA pairs the coverage is low, though it 
achieves good accuracy. According to the experi-
ments, BoW has already achieved the best per-
formance for SUM pairs cf. Table 2. 
As a whole, developing task specific entailment 
operators is a promising direction. As we men-
tioned in the first section, the RTE task is neither a 
one-level nor a one-case task. The experimental 
results uncovered differences among pairs of dif-
ferent tasks with respect to accuracy and coverage. 
On the one hand, our method works successfully 
on structure-oriented T-H pairs, most of which are 
from IE. If both TST and TSH can be transformed 
into CCS sequences, the comparison performs well, 
as in the case of the last example (id=410) in Table 
1. Here, the relation between ?wife?, ?name?, and 
?Barbara? is conveyed by the punctuation ?,?, the 
verb ?is?, and the preposition ?of?. Other cases like 
the ?work for? relation of a person and a company 
or the ?is located in? relation between two location 
names are normally conveyed by the preposition 
?of?. Based on these findings, taking into account 
more carefully the lexical semantics based on in-
ference rules of functional words might be helpful 
in improving RTE. 
40
On the other hand, accuracy varies with T-H 
pairs from different tasks. Since our method is 
mainly structure-oriented, differences in modifiers 
may change the results and would not be caught 
under the current version of our tree skeleton. For 
instance, ?a commercial company? will not entail 
?a military company?, even though they are struc-
turally equivalent. 
Most IE pairs are constructed from a binary rela-
tion, and so meet the prerequisite of our algorithm 
(see sec. 4.1). However, our method still has rather 
low coverage. T-H pairs from other tasks, for ex-
ample like IR and SUM, usually contain more in-
formation, i.e. more nouns, the dependency trees of 
which are more complex. For instance, the pair 
(id=739) in Table 1 contains four keyword pairs 
which we cannot handle by our current method. 
This is one reason why we have constructed extra 
T-H pairs from MUC, TREC, and news articles 
following the methods of (Bar-Haim et al, 2006). 
Still, the overall performance does not improve. 
All extra training data only serves to improve the 
matched pairs (about 32% of the data set) for 
which we already have high accuracy (see Table 4). 
Thus, extending coverage by machine learning 
methods for lexical semantics will be the main fo-
cus of our future work. 
6 Conclusions and Future Work 
Applying different RTE strategies for different 
NLP tasks is a reasonable solution. We have util-
ized a structure similarity function to deal with the 
structure-oriented pairs, and applied backup strate-
gies for the rest. The results show the advantage of 
our method and direct our future work as well. In 
particular, we will extend the tree skeleton extrac-
tion by integrating lexical semantics based on in-
ference rules for functional words in order to get 
larger domains of locality. 
Acknowledgements 
The work presented here was partially supported 
by a research grant from BMBF to the DFKI pro-
ject HyLaP (FKZ: 01 IW F02) and the EC-funded 
project QALL-ME. 
References 
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampic-
colo, D., Magnini, B. and Szpektor, I. 2006. The Sec-
ond PASCAL Recognising Textual Entailment Chal-
lenge. In Proc. of the PASCAL RTE-2 Challenge. 
Bos, J. and Markert, K. 2005. Combining Shallow and 
Deep NLP Methods for Recognizing Textual Entail-
ment. In Proc. of the PASCAL RTE Challenge. 
Bunescu, R. and Mooney, R. 2006. Subsequence Ker-
nels for Relation Extraction. In Advances in Neural 
Information Processing Systems 18. MIT Press. 
Corley, C. and Mihalcea, R. 2005. Measuring the Se-
mantic Similarity of Texts. In Proc. of the ACL 
Workshop on Empirical Modeling of Semantic 
Equivalence and Entailment. 
Dagan, R., Glickman, O. 2004. Probabilistic textual 
entailment: Generic applied modelling of language 
variability. In PASCAL Workshop on Text Under-
standing and Mining. 
de Salvo Braz, R., Girju, R., Punyaka-nok, V., Roth, D., 
and Sammons, M. 2005. An Inference Model for Se-
mantic Entailment in Natural Language. In Proc. of 
the PASCAL RTE Challenge. 
Hickl, A., Williams, J., Bensley, J., Roberts, K., Rink, 
B. and Shi, Y. 2006. Recognizing Textual Entailment 
with LCC?s GROUNDHOG System. In Proc. of the 
PASCAL RTE-2 Challenge. 
Klein, D. and Manning, C. 2003. Accurate Unlexical-
ized Parsing. In Proc. of ACL 2003. 
Kouylekov, M. and Magnini, B. 2006. Tree Edit Dis-
tance for Recognizing Textual Entailment: Estimat-
ing the Cost of Insertion. In Proc. of the PASCAL 
RTE-2 Challenge. 
Lin, D. 1998. Dependency-based Evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Sys-
tems. 
Tatu, M., Iles, B., Slavik, J., Novischi, A. and Moldo-
van, D. 2006. COGEX at the Second Recognizing 
Textual Entailment Challenge. In Proc. of the PAS-
CAL RTE-2 Challenge. 
Wang, R. and Neumann, G. 2007. Recognizing Textual 
Entailment Using a Subsequence Kernel Method. In 
Proc. of AAAI 2007. 
Witten, I. H. and Frank, E. Weka: Practical Machine 
Learning Tools and Techniques with Java Implemen-
tations. Morgan Kaufmann, 1999. 
Zanzotto, F.M. and Moschitti, A. 2006. Automatic 
Learning of Textual Entailments with Cross-pair 
Similarities. In Proc. of ACL 2006. 
41
