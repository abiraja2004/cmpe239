ALGORITHMS FOR GENERATION IN  LAMBEK 
THEOREM PROVING 
Erik-Jan van der Linden * 
Guido Minnen 
Institute for Language Technology and Artificial Intelligence 
Tilburg University 
PO Box 90153, 5000 LE Tilburg, The Netherlands 
E-maih vdlindenOkub.nl 
ABSTRACT 
We discuss algorithms for generation within the 
Lambek Theorem Proving Framework. Efficient 
algorithms for generation i this framework take 
a semantics-driven strategy. This strategy can 
be modeled by means of rules in the calculus that 
are geared to generation, or by means of an al- 
gorithm for the Theorem Prover. The latter pos- 
sibility enables processing of a bidirectional cal- 
culus. Therefore Lambek Theorem Proving is a 
natural candidate for a 'uniform' architecture for 
natural anguage parsing and generation. 
Keywords: generation algorithm; natural lan- 
guage generation; theorem proving; bidirection- 
ality; categorial grammar. 
1 INTRODUCTION 
Algorithms for tactical generation are becoming 
an increasingly important subject of research in 
computational linguistics (Shieber, 1988; Shieber 
et al, 1989; Calder et al, 1989). In this pa- 
per, we will discuss generation algorithms within 
the Lambek Theorem Proving (LTP) framework 
(Moortgat, 1988; Lambek, 1958; van Benthem, 
1988). In section (2) we give an introduction to a 
categorial calculus that is extended towards bidi- 
rectionality. The naive top-down control strategy 
in this section does not suit the needs of efficient 
generation. Next, we discuss two ways to imple- 
ment a semantics-driven strategy. Firstly, we add 
inference rules and cut rules geared to generation 
to the calculus (3). Secondly, since these changes 
in the calculus do not support bidirectionality, we 
*We would llke to thank Gosse Bouma, Wietske 
Si~tsma and Marianne Sanders for their comments on an 
earlier draft of the paper. 
220 
introduce a second implementation: a bottom-up 
algorithm for the theorem prover (4). 
2 EXTENDING THE CAL-  
CULUS 
Natura l  Language Processing as deduct ion 
The architectures in this paper resemble the uni- 
form architecture in Shieber (1988) because lan- 
guage processing is viewed as logical deduction, in 
analysis and generation: 
"The generation of strings matching some crite- 
ria can equally well be thought of as a deductive 
process, namely a process of constructive proof of 
the existence of a string that matches the crite- 
ria." (Shieber, 1988, p. 614). 
In the LTP framework a categorial reduction sys- 
tem is viewed as a logical calculus where parsing 
a syntagm is an attempt o show that it follows 
from a set of axioms and inference rules. These 
inference rules describe what he processor does in 
assembling a semantic representation (representa- 
tional non-autonomy: Crain and Steedman, 1982; 
Ades and Steedman, 1982). Derivation trees rep- 
resent a particular parse process (Bouma, 1989). 
These rules thus seem to be nondeclarative, and 
this raises the question whether they can be used 
for generation. The answer to this question will 
emerge throughout this paper. 
Lexical information As in any categorial 
grammar, linguistic information i  LTP is for the 
larger part represented with the signs in the lex- 
icon and not with the rules of the calculus (signs 
are denoted by prosody:syntax:semantlcs).  A 
generator using a categorial grammar needs lex- 
ical information about the syntactic form of a 
functor that is connected to some semantic func- 
tot in order to syntactically correctly generate the 
semantic arguments of this functor. For a parser, 
the reverse is true. In order to fulfil both needs, 
lexical information is made available to the the- 
orem prover in the form of in~t6aces of o~ionu. I 
Axioms then truely represent what should be ax- 
iomatic in a lexicalist description of a language: 
the \]exical items, the connections between form 
and meaning. 2 
I* s l i a inat ionru les  */ 
(U,\[Pros_Fu:X/Y:Functor\],\[TIR\],V)=>\[Z\] <- 
\[Pros_Fu:X/Y:Functor\] => 
\[Pros_Fu:X/Y:Functor\] k 
\[TIR\] => \[Pros Arg:Y:Ar~ k 
(U,\[(Pros_Fu*l~os_Arg):X:Functor@Arg\],V) => 
\[z\]. 
(U,\[T\[R\],\[Pros_Fu:Y\X:Functor\],V) => \[Z\] <- 
\[Pros_Fu:Y\X:Functor\] => 
\[Pros_Fu:Y\X:Functor\] k 
\[TIR\] => \[Pros_arg:Y:krg\] k 
(U,\[(Pros_krg*Pros_Fu):X:FunctorQArg\],V) => 
\[z\]. 
Ru les  Whenever inference rules are applied, an 
attempt is made to axiomatize the functor that 
participates in the inference by the first subse- 
quent of the elimination rules. This way, lexical 
information is retrieved from the lexicon. 
/* introduction ru lss  */ 
\[T\[R\]=>\[Pros:Y\X:Var_Y'Tsra_X\] <- 
nogsnvar(Y\X) k 
(\[id:Y:Var_Y\],\[T\[R\]) => 
\[(id*Pros):X:Tarm_X\]. 
A prosodic operator connects prosodic ele- 
ments. A prosodic identity element, id, is neces- 
sary because introduction rules are prosodical\]y 
vacuous. In order to avoid unwanted matching 
between axioms and id-elements, one special ax- 
iota is added for id-elements. Meta-logical checks 
are included in the rules in order to avoid vsri- 
ables occuring in the final derivation, nogenv,2r 
reeursively checks whether any part of an expres- 
sion is a variable. 
A sequent in the calculus is denoted with 
P => T, where P, called the antecedent, and T, 
the succedent, are finite sequences of signs. The 
calculus is presented in (1) . In what follows, X 
and ? are categories; T and Z, are signs; R, U 
and V are possibly empty sequences of signs; @ 
denotes functional application, a caret denotes ~- 
abstraction, s 
(i) 
/* axioms */ 
\[Pros:X:?\] => \[Pros:X:Y\] <- 
\[Pros:l:Y\] =i> \[Pros:X:Y\] k
true.  
\[Pros:X:Y\] => \[Pros:X:Y\] <- 
(nossnvar(X), nonvar(Y)) k 
1;rue. 
\[TIR\] => \[Pros:X/Y:Var_Y'Tsrm_X\] <- 
nogsnvar(X/Y) k 
(\[T\[R\],Cid:Y:Var_Y\]) -> 
\[(Pros*id):l:Term_X\]. 
/* axiom for prosodic id-element */ 
\[id:X:Y\] =i> \[id:X:Y\] <- 
isvs.r(Y). 
/* lexicon, lexioms */ 
\[john:np:john\] =1> \[john:np:john\]. 
\[mary:np:mexy\] =1> \[maxy:np:maxy\]. 
\ [ loves : (np \s ) /np : lovn \ ]  =1> 
\ [ loves : (np \s ) /np : lows \ ] .  
In order to initiate analysis, the theorem prover is 
presented with sequents like (2). Inference rules 
are applied recursively to the antecedent of the 
sequent until axioms are found. This regime can 
be called top-down from the point of view ofprob- 
\]em solving and bottom-up from a "parsing" point 
of view. For generation, a sequent like (3) is pre- 
sented to the theorem prover. Both analysis and 
generation result in a derivation like (4). Note 
that generation ot only results in a sequence of 
lexical signs, but also in a peosodic pl~rasing that 
could be helpful for speech generation. 
(2) 
lVem der Linden and Minnen (submitted) contains a 
more elaborate comparison ofthe extended cedcu\]tm with 
the origins\] calculus as proposed in Moortgat (1988). 
2A suggestion similar to this proposal was made by 
K~nig (1989) who stated that lexicsI items are to be seen 
as axioms, but did not include them as such in her de- 
scription of the L-calculus. 
SThroughout this paper we will use a Prolog notation 
because the architectures presented here depend partly on 
the Prolog un\[i~cstlon mechanism. 
221 
\[john:A:B,lovss:C:D,msxy:E:F\] => \[Pros:s:Ssm\] 
(3) 
U => \[Pros:s:loves@maryQjohn\] 
Although both (2) and (3) result (4), in the 
case of generation, (4) does not represent he 
(4) 
j ohn :np : john  1or*s :  (np \s ) /np : loves  ma~ry:np:mary => john*( loves*mary) : s : lovesQaary@john <- 
loves :  (np \s ) /np : loves  => loves :  (np \s ) /np :1oves  <- 
loves :  (np \s ) /np : loves  =I> loves : (np \s ) /np :1oves  <- t rue  
aary :np :aary  => aary :np :aary  <- 
ms.ry:np:aa~ry =I> aary :np :aary  <- t rue  
john:  np:  J olm loves*mary  : np \s  : lovea@aary => j ohn* ( loves*mary)  : s : loves@aary@j olm <- 
loves*aary  :np \s  : loves@mary => loves*aary  :np \s  : loves@mary <- t rue  
john:np : john  => john:np : john  <- 
john:np : john  -1> john:np : john  <- t rue  
john*  ( loves*aary)  :s  : lovss@aaryQj  ohn => john*  ( loves*mary)  : s : loves@aary@j ohn: <- t rue  
exact proceedings of the theorem prover. It 
starts applying rules, matching them with the an- 
tecedent, without making use of the original se- 
mantic information, and thus resulting in an in- 
efficient and nondeterministic generation process: 
all possible derivations including all hxical items 
are generated until some derivation is found that 
results in the succedent. 4 We conclude that the 
algorithm normally used for parsing in LTP is in- 
efficient with respect o generation. 
3 CALCUL I  DES IGNED 
FOR GENERATION 
A solution to the ei~ciency problem raised in 
the previous section is to start from the origi- 
hal semantics. In this section we discuss calculi 
that make explicit use of the original semantics. 
Firstly, we present Lambek-like rules especially 
designed for generation. Secondly, we introduce 
a Cut-rule for generation with sets of categorial 
reduction rules. Both entail a variant of the cru- 
cial starting-point ofthe semantic-he~d-driven al- 
gorithms described in Calder et al (1989) and 
Shieber et al (1989): if the functor of a semantic 
representation can be identified, and can be re- 
fated to a lexical representation containing syn- 
tactic information, it is possible to generate the 
arguments yntactically. The efficiency of this 
strategy stems from the fact that it is guided by 
the known semantic and syntactic information, 
and lexical information is retrieved as soon as pos- 
sible. 
In contrast o the semantic-head-driven al>- 
proach, our semantic representations do not al- 
low for immediate recognition of semantic heads: 
these can only be identified after all arguments 
4ef. Shleber et el. (1989) on top-down generation 
algorithms. 2 2 2 
have been stripped of the functor recursively 
(loves@mary@john =:> loves@mary => loves). 
Calder et al conjecture that their algorithm 
"(...) extends naturally to the rules of compo- 
sition, division and permutation of Combinatory 
Categorial Grammar (Steedman, 1987) and the 
Lambek Calculus (1958)" (Calder et al, 1989, p. 
23 ). 
This conjecture should be handled with care. As 
we have stated before, inference rules in LTP de- 
scribe ho~ a processor operates. An important 
difference with the categorial reduction rules of 
Calder et al is that inference.rules in LTP implic- 
itly initiate the recursion of the parsing and gen- 
eration process. Technically speaking, Lambek 
rules cannot be arguments of the rule-predicate 
of Calder et al (1989, p. 237). The gist of our 
strategy is similar to theirs, but the algorithms 
dilTer. 
Lambek-l lke generat ion Rules are presented 
in (5) that explicitly start from the known infor- 
mation during generation: the syntax and seman- 
tics of the succedent. Literally, the inference rule 
states that a sequent consisting of an antecedent 
that unifies with two sequences of signs U and 
Y, and a succedent that unifies with a sign with 
semantics Sem_FuQSem_Arg is a theorem of 
the calculus if Y reduces to a syntactic functor 
looking for an argument on its left side with the 
functor-meaning of the original semantics, and U 
reduces to its argument. This rule is an equiva- 
lent of the second elimination rule in (I). 
(5) 
/* e l~ inat ionru le  */ 
~,v \ ]  => 
\[(Pros_krg*Pros_Fu):X:Sem_Fu@Sea_krg\] <- 
V =>\[Pros_Fu:Y\X:Sen_Fu\] t 
U =>\[Pros_Arg:Y:Sen_krg\]. 
/* introduction-rule */
\[T\[R\] => \[Pros:Y\l:Var_Y'Tera_X\] <- 
nogenvsr(Y\X) k
(CCid:Y:Vnur_Y\]\],CTIR\]) => 
\[(id*Pros):X:Tora_l\]. 
4 A COMBINED BOT-  
TOM-UP/TOP-DOWN 
REGIME 
In this section, we describe an algorithm for 
the theorem prover that proceeds in a combined 
bottom-up/top-down fashion from the problem 
solving point of view. It maintains the same 
semantics-driven strategy, and enables efficient 
generation with the bidirectional calculus in (I). 
The algorithm results in derivations like (4), in 
the same theorem prover architecture, be it along 
another path. 
A Cut - ru le  for generat ion  A Cut-rule is a 
structural rule that can be used within the L- 
calculus to include partial proofs derived with 
categorial reduction rules into other proofs. In 
(6) a generation Cut-rule is presented together 
with the AB-system. 
(6) 
/* Cut-rule for generation */ 
\[U.V\] => \[Pros_Z:Z:Su_Z\] <-
\[Pros_X:X:Sem_X, Pros_Y:Y:Sem_Y\] =*> 
\[Pros_g:z:sem_Z\] 
U => \[Pros_Z:X:Sem_Z\] 
V ffi> \[Proe_Y:Y:Sem_Y\]. 
/* reduction rules,  system AB */ 
\[Pros_Fu:X/Y:Functor. lhcos_Arg:Y:lrg\] =*> 
(Pros_FU*Pros_Arg):X:Functor@Arg\]. 
\[Pros_Arg:Y:Arg, Pros_Fu:Y\l:Functor\] =*> 
(Pros.Arg*Pros_Fu):X:Functor@ArS\]. 
The generator regimes presented in this section 
are semantics-driven: they start from a seman- 
tic representation, assume that it is part of the 
uppermost sequent within a derivation, and work 
towards the lexical items, axioms, with the recur- 
sive application of inference rules. From the point 
of view of theorem proving, this process should 
be described as a top-down problem solving strat- 
egy. The rules in this section are, however, geared 
towards generation. Use of these rules for pars- 
ing would result in massive non-determinism. El- 
ficient parsing and generation require different 
rules: the calculus is not bidirectioaal. 223 
B id i rect iona l i ty  There are two reasons to 
avoid duplication of grammars for generation and 
interpretation. Firstly, it is theoretically more el- 
egant and simple to make use of one grammar. 
Secondly, for any language processing system, hu- 
man or machine, it is more economic (Bunt, 1987, 
p. 333). Scholars in the area of language gen- 
eration have therefore pleaded in favour of the 
bidirectionalit~ of linguistic descriptions (Appelt, 
1987). 
Bidirectionality might in the first place be im- 
plemented by using one grammar and two sepa- 
rate algorithms for analysis and generation (Ja- 
cobs, 1985; Calder et el., 1989). However, apart 
from the desirability to make use of one and the 
same grammar for generation and analysis, it 
would be attractive to have one and the same 
processiag architecture for both analysis and gen- 
eration. Although attempts to find such architec- 
tures (Shieber, 1988) have been termed "looking 
for the fountain of youth',  s it is a stimulating 
question to what extent it is possible to use the 
same architecture for both tasks. 
Example  An example will illustrate how our 
algorithm proceeds. In order to generate from 
a sign, the theorem prover assumes that it is 
the succedent of one of the subsequeats of one 
of the inference rules (7-1/2). (In case of an 
introduction rule the sign is matched with the 
succedent of the headseq~en~; this implies a top- 
down step.) If unification with one of these subse- 
quents can be established, the other subsequents 
and the headsequent can be partly instantiated. 
These sequents can then serve as starting points 
for further bottom-up processing. Firstly, the 
headsequent is subjected to bottom-up rocess- 
SRon Kaplan during discussion of the $hieber presen- 
tation at Coling 1988. 
Generation of  nounphrase ~he ~abie. Start with sequent 
P => \[Pros :np: the@table\] 
l- Assume suecedent is part of  an axiom: 
\[Pros : np: the0t able\] => \[Pros :np: the@table\] 
2- Match axiom with last subsequent of an inference rule: 
(U, \[Pros_Fu:X/Y:Functor\],  \[T\[I~,V) => \[Z\] <- 
\[Pros_Fu:X/Y:Functor\] => \[Pros_Fu:X/Y:Functor\] & 
\[T \[ R\] => \[Pros_krg : Y : Arg\] & 
(U, \[ (Pros_Fu*Pros_Arg) : X: Functor@~g\],  V) => \[Z\]. 
Z = Pros:np:the@table; Functor : the; Arg = table; X = np; U = \[ \]; V = \[ \]. 
3- Derive instantiated head sequent: 
\[Pros_Fu: np/Y: the\ ] ,  \[T \[ R\] => \[Pros :rip: the0table\]  
4- No more applications in head sequent: Prove (bottom-up) first instantiated subsequent: 
\[Pros_Fu: np/Y: the\] ,,> \[Pros_Fu :np/Y : the\] 
Unifies with the axiom for "the": Pros_Fu = the; Y = n. 
5- Prove (bottom-up) second instantiated subsequent: 
\[T\[ R\] => \[Pros_Arg: n: "~ able\] 
Unifies with axiom for "table": Pros_Arg = table; T = table:n:table; R = \[ \] 
6- Prove (bottum-up) last subsequent: is a nonlexical ax/om. 
\[ ( the*t able) :np : the@table\] => \[ ( the*table)  : np: theQtable\] .  
7- Final derivation: 
the :np /n : the  tab le :n : tab le  => the*table:np.the@table <- 
the :np /n : the  => the :np /n : the  <- 
the :np /n : the  =1> the :np /n : the  <- t rue 
tab le :n : tab le  => tab le :n : tab le  <- 
tab le :n : tab le  =i> tabls:n:table <- t rue 
the*table :np:the@table => the*table :np:the@table <- true 
224  
ing (7-3), in order to axiomatize the head functor 
as soon as possible. Bottom-up rocessing stops 
when no more application operators can be elim- 
insted from the head sequent (7-4). Secondly, 
working top-down, the other subsequents (7-4/5) 
are made subject o bottom-up rocessing, and at 
last the last subsequent (7-6). (7) presents gen- 
eration of a nounphrsse, the ~able. 
Non-determinism A source for non-determin- 
ism in the semantics-driven strategy is the fact 
that the theorem prover forms hypotheses about 
the direction a functor seeks its arguments, and 
then checks these against he lexicon. A possibil- 
ity here would be to use a calculus where dora- 
inance and precedence are taken apart. We will 
pursue rids suggestion i  future research. 
5 CONCLUDING 
REMARKS 
Implementat ion The algorithms and calculi 
presented here have been implemented with the 
use of modified versions of the categorial calculi 
interpreter described in Moortgat (1988). 
Conclusion Efl\]cient, bidirectional use of cat- 
egorial calculi is possible if extensions are made 
with respect o the calculus, and if s combined 
bottom-up/top-down algorithm is used for gener- 
ation. Analysis and generation take place within 
the same processing architecture, with the same 
linguistics descriptions, be it with the use of dif- 
ferent algorithms. LTP thus serves as a natural 
candidate for a uniform architecture of parsing 
and generation. 
Semantic non-monotonie i ty  A constraint on 
grammar formalisms that can be dealt with in 
current generation systems is semantic mono- 
tonicity (Shieber, 1988; but cf. Shieber et al, 
1989). The algorithm in Calder et al (1989) re- 
quires an even stricter constaint. Firstly, in van 
der Linden and Minnen (submitted) we describe 
how the addition of a unification-based semantics 
to the calculus described here enables process- 
ing of non-monotonic phenomena such as non- 
compositional verb particles and idioms. Identity 
semantics (cf. Calder et al p. 235) should be 
no problem in this respect. Secondly, unary rules 
and type-raising (ibid.) are part of the L-calculus, 
and are neither fundamental problems. 
Inverse E-reduct ion A problem that exists for 
all generation systems that include some form of 
~-semantics is that generation ecessitates the in- 
verse operation of~-reduction. Although we have 
implemented algorithms for inverse E-reduction, 
these are not computationally tractable,  A way 
out could be the inclusion of a unification based 
semantics. 7 
SBunt (1987) states that  an expression with n constants  
results in 2 n - 1 possible inverse ~-reduct lons.  
7As proposed in van der L inden and  Minnen (submit -  
ted) for the calculus in (2). 225  
6 REFERENCES 
Ades, A., and Steedman, M., 1982 On the order 
of words. Linguistics and Pkilosoph~/, 4, pp. 517- 
558. 
Appelt, D.E., 1987 Bidirectional Grammars and 
the Design of Natural Language Systems. In 
Wilks, Y. (Ed.), Theoretical Issues in Natural 
Language Processing. Las Cruces, New Mexico: 
New Mexico State University, January 7-9, pp. 
185-191. 
Van Benthem, J., 1988 Categorial Grammar. 
Chapter 7 in Van Benthem, J., Essays in Logi- 
cal Semantics. Reidel, Dordrecht. 
Boums, G., 1989 Emcient Processing of Flexi- 
ble Categorial Grammar. In Proceedings of the 
EACL 1989, Manchester. pp. 19-26. 
Bunt, H., 1987 Utterance generation from seman- 
tic representations augmented with pragmatic in- 
formation. In Kempen 1987. 
Calder, J., Reape M., and Zeevat, H., 1989 An 
algorithm for generation in Unification Catego- 
rial Grammar. In Proceedings of the EACL 1989, 
Manchester. pp. 233-240. 
Crain, S., and Steedman, M., 1982 On not being 
led up the garden path. In Dowry, Karttunen and 
Zwicky (Eds.) Natu~l language pQrsing. Cam- 
bridge: Cambridge University Press. 
Jacobs, P., 1985 PHRED, A generator for Natural 
Language Interfaces. Computational Linguistics 
11, 4, pp. 219-242. 
Kempen , G., (Ed.) 1987 Natural language gen- 
eration: new results in artificial intelligence, pay. 
cttology and linouiatics. Dordrecht: Nijhoff. 
K6nig, E., 1989 Parsing as natured eduction. In 
Proceedings of the ACL 1989, Vancouver. 
Lsmbek, J., 1958 The mathematics of sentence 
structure. Am. Math Monthly, 85, 154-169. 
Linden, E. van der, and Minnen, G., (submit- 
ted) An account of Non-monotonous phenomena 
in bidirectional Lambek Theorem Proving. 
Moortgat, M., 1988 Categorial Inueatigetions. 
Logical and Hnguistic ?apects of the Lambek cal- 
culus. Disseration, University of Amsterdam. 
Shieber, S., 1988 A uniform architecture for Pars- 
ing and Generation. In Proceedings of Coling 
1988, Budapest, pp. 614-619. 
Shieber, S., van Noord, G., Moore, R., and 
Pereira, P., 1989 A semantic-Head-Driven Gen- 
eration Algorithm for Unification-Based For- 
mallsms. In Proceedings of ACL 1989 Vancouver. 
Steedman, M., 1987 Combinatory Grammars and 
Parasitic Gaps Natural Language and Linguistic 
Theory, 5, pp. 403-439. 
226 
