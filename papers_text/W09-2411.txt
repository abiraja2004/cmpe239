Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 70?75,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages 
 
 
Marta Recasens, Toni Mart?, Mariona Taul? Llu?s M?rquez, Emili Sapena 
Centre de Llenguatge i Computaci? (CLiC) TALP Research Center,  
University of Barcelona Technical University of Catalonia 
Gran Via de les Corts Catalanes 585 
08007 Barcelona 
Jordi Girona Salgado 1-3 
08034 Barcelona 
{mrecasens, amarti, mtaule} 
@ub.edu 
{lluism, esapena} 
@lsi.upc.edu 
 
 
 
Abstract 
This paper presents the task ?Coreference 
Resolution in Multiple Languages? to be run 
in SemEval-2010 (5th International Workshop 
on Semantic Evaluations). This task aims to 
evaluate and compare automatic coreference 
resolution systems for three different lan-
guages (Catalan, English, and Spanish) by 
means of two alternative evaluation metrics, 
thus providing an insight into (i) the portabil-
ity of coreference resolution systems across 
languages, and (ii) the effect of different scor-
ing metrics on ranking the output of the par-
ticipant systems. 
1 Introduction 
Coreference information has been shown to be 
beneficial in many NLP applications such as In-
formation Extraction (McCarthy and Lehnert, 
1995), Text Summarization (Steinberger et al, 
2007), Question Answering (Morton, 2000), and 
Machine Translation. In these systems, there is a 
need to identify the different pieces of information 
that refer to the same discourse entity in order to 
produce coherent and fluent summaries, disam-
biguate the references to an entity, and solve ana-
phoric pronouns.  
Coreference is an inherently complex phenome-
non. Some of the limitations of the traditional rule-
based approaches (Mitkov, 1998) could be over-
come by machine learning techniques, which allow 
automating the acquisition of knowledge from an-
notated corpora. 
 
This task will promote the development of lin-
guistic resources ?annotated corpora1? and ma-
chine-learning techniques oriented to coreference 
resolution. In particular, we aim to evaluate and 
compare coreference resolution systems in a multi-
lingual context, including Catalan, English, and 
Spanish languages, and by means of two different 
evaluation metrics.  
By setting up a multilingual scenario, we can 
explore to what extent it is possible to implement a 
general system that is portable to the three lan-
guages, how much language-specific tuning is nec-
essary, and the significant differences between 
Romance languages and English, as well as those 
between two closely related languages such as 
Spanish and Catalan. Besides, we expect to gain 
some useful insight into the development of multi-
lingual NLP applications.  
As far as the evaluation is concerned, by em-
ploying B-cubed (Bagga and Baldwin, 1998) and 
CEAF (Luo, 2005) algorithms we can consider 
both the advantages and drawbacks of using one or 
the other scoring metric. For comparison purposes, 
the MUC score will also be reported. Among oth-
ers, we are interested in the following questions: 
Which evaluation metric provides a more accurate 
picture of the accuracy of the system performance? 
Is there a strong correlation between them? Can 
                                                           
1 Corpora annotated with coreference are scarce, especially for 
languages other than English.  
70
statistical systems be optimized under both metrics 
at the same time? 
The rest of the paper is organized as follows. 
Section 2 describes the overall task. The corpora 
and the annotation scheme are presented in Section 
3. Conclusions and final remarks are given in Sec-
tion 4. 
 
2 Task description  
The SemEval-2010 task ?Coreference Resolution 
in Multiple Languages? is concerned with auto-
matic coreference resolution for three different 
languages: Catalan, English, and Spanish.  
2.1 Specific tasks  
Given the complexity of the coreference phenom-
ena, we will concentrate only in two tractable as-
pects, which lead to the two following subtasks for 
each of the languages: 
i) Detection of full coreference chains, com-
posed by named entities, pronouns, and full 
noun phrases (NPs). 
ii) Pronominal resolution, i.e. finding the antece-
dents of the pronouns in the text.  
 
 
The example in Figure 1 illustrates the two sub-
tasks.2 Given a text in which NPs are identified and 
indexed (including elliptical subjects, represented 
as ?), the goal of (i) is to extract all coreference 
chains: 1?5?6?30?36, 9?11, and 7?18; while the 
goal of (ii) is to identify the antecedents of pro-
nouns 5 and 6, which are 1 and 5 (or 1), respec-
tively. Note that (b) is a simpler subtask of (a) and 
that for a given pronoun there can be multiple an-
tecedents (e.g. both 1 and 5 are correct antecedents 
for 6).  
We restrict the task to solving ?identity? rela-
tions between NPs (coreference chains), and be-
tween pronouns and antecedents. Nominal 
predicates and appositions as well as NPs with a 
non-nominal antecedent (discourse deixis) will not 
been taken into consideration in the recognition of 
coreference chains (see Section 3.1 for more in-
formation about decisions concerning the annota-
tion scheme). 
Although we target at general systems address-
ing the full multilingual task, we will allow taking 
part on any subtask of any language in order to 
promote participation. 
 
 
Figure 1.  NPs in a sample from the Catalan training 
data (left) and the English translation (right). 
                                                           
2 The example in Figure 1 is a simplified version of the anno-
tated format. See Section 2.2 for more details. 
[The beneficiaries of [[spouse?s]3 pensions]2]1 will 
be able to keep [the payment]4 even if [they]5 re-
marry provided that [they]6 fulfill [a series of [con-
ditions]8]7, according to [the royal decree approved 
yesterday by [the Council of Ministers]10]9.  
[The new rule]11 affects [the recipients of [a 
[spouse?s]13 pension]12 [that]14 get married after 
[January_1_,_2002]16]17. 
[The first of [the conditions]18]19 is being older 
[than 61 years old]20 or having [an officially rec-
ognized permanent disability [that]22 makes one 
disabled for [any [profession]24 or [job]25]23]21. 
[The second one]26 requires that [the pension]27 be 
[the main or only source of [the [pensioner?s]30 in-
come]29]28, and provided that [the annual amount 
of [the pension]32]31 represents, at least, [75% of 
[the total [yearly income of [the pen-
sioner]36]35]34]33. 
[Els beneficiaris de [pensions de [viudetat]3]2]1 po-
dran conservar [la paga]4 encara_que [?]5 es tornin 
a casar si [?]6 compleixen [una s?rie de [condi-
cions]8]7 , segons [el reial decret aprovat ahir pel 
[Consell_de_Ministres]10]9 .  
[La nova norma]11 afecta [els perceptors d' [una 
pensi? de [viudetat]13]12 [que]14 contreguin [matri-
moni]15 a_partir_de [l' 1_de_gener_del_2002]16]17 .  
[La primera de [les condicions]18]19 ?s tenir [m?s 
de 61 anys]20 o tenir reconeguda [una incapacitat 
permanent [que]22 inhabiliti per a [tota [professi?]24 
o [ofici]25]23]21. 
[La segona]26 ?s que [la pensi?]27 sigui [la principal 
o ?nica font d' [ingressos del [pensionista]30]29]28 , i 
sempre_que [l' import anual de [la mateixa pen-
si?]32]31 representi , com_a_m?nim , [el 75% del 
[total dels [ingressos anuals del [pensionis-
ta]36]35]34]33.  
 
71
2.2 Evaluation  
2.1.1 Input information 
The input information for the task will consist of: 
word forms, lemmas, POS, full syntax, and seman-
tic role labeling. Two different scenarios will be 
considered regarding the source of the input infor-
mation: 
 
i) In the first one, gold standard annotation will 
be provided to participants. This input annota-
tion will correctly identify all NPs that are part 
of coreference chains. This scenario will be 
only available for Catalan and Spanish. 
ii) In the second, state-of-the-art automatic lin-
guistic analyzers for the three languages will 
be used to generate the input annotation of the 
data. The matching between the automatically 
generated structure and the real NPs interven-
ing in the chains does not need to be perfect in 
this setting. 
  
By defining these two experimental settings, we 
will be able to check the performance of corefer-
ence systems when working with perfect linguistic 
(syntactic/semantic) information, and the degrada-
tion in performance when moving to a more realis-
tic scenario with noisy input annotation.  
2.1.2 Closed/open challenges 
In parallel, we will also consider the possibility of 
differentiating between closed and open chal-
lenges, that is, when participants are allowed to use 
strictly the information contained in the training 
data (closed) and when they make use of some ex-
ternal resources/tools (open). 
2.1.3 Scoring measures 
Regarding evaluation measures, we will have spe-
cific metrics for each of the subtasks, which will be 
computed by language and overall.  
Several metrics have been proposed for the task 
of coreference resolution, and each of them pre-
sents advantages and drawbacks. For the purpose 
of the current task, we have selected two of them ? 
B-cubed and CEAF ? as the most appropriate ones. 
In what follows we justify our choice.  
The MUC scoring algorithm (Vilain et al, 1995) 
has been the most widely used for at least two rea-
sons. Firstly, the MUC corpora and the MUC 
scorer were the first available systems. Secondly, 
the MUC scorer is easy to understand and imple-
ment. However, this metric has two major weak-
nesses: (i) it does not give any credit to the correct 
identification of singleton entities (chains consist-
ing of one single mention), and (ii) it intrinsically 
favors systems that produce fewer coreference 
chains, which may result in higher F-measures for 
worse systems. 
A second well-known scoring algorithm, the 
ACE value (NIST, 2003), owes its popularity to 
the ACE evaluation campaign. Each error (a miss-
ing element, a misclassification of a coreference 
chain, a mention in the response not included in the 
key) made by the response has an associated cost, 
which depends on the type of entity (e.g. person, 
location, organization) and on the kind of mention 
(e.g. name, nominal, pronoun). The fact that this 
metric is entity-type and mention-type dependent, 
and that it relies on ACE-type entities makes this 
measure inappropriate for the current task. 
The two measures that we are interested in com-
paring are B-cubed (Bagga and Baldwin, 1998) 
and CEAF (Luo, 2005). The former does not look 
at the links produced by a system as the MUC al-
gorithm does, but looks at the presence/absence of 
mentions for each entity in the system output. Pre-
cision and recall numbers are computed for each 
mention, and the average gives the final precision 
and recall numbers.  
CEAF (Luo, 2005) is a novel metric for evaluat-
ing coreference resolution that has already been 
used in some published papers (Ng, 2008; Denis 
and Baldridge, 2008). It mainly differs from B-
cubed in that it finds the best one-to-one entity 
alignment between the gold and system responses 
before computing precision and recall. The best 
mapping is that which maximizes the similarity 
over pairs of chains. The CEAF measure has two 
variants: a mention-based, and an entity-based one. 
While the former scores the similarity of two 
chains as the absolute number of common men-
tions between them, the latter scores the relative 
number of common mentions. 
Luo (2005) criticizes the fact that a response 
with all mentions in the same chain obtains 100% 
B-cubed recall, whereas a response with each men-
tion in a different chain obtains 100% B-cubed 
72
precision. However, precision will be penalized in 
the first case, and recall in the second case, each 
captured by the corresponding F-measure. Luo?s 
entity alignment might cause that a correctly iden-
tified link between two mentions is ignored by the 
scoring metric if that entity is not aligned. Finally, 
as far as the two CEAF metrics are concerned, the 
entity-based measure rewards alike a correctly 
identified one-mention entity and a correctly iden-
tified five-mention entity, while the mention-based 
measure takes into account the size of the entity. 
Given this series of advantages and drawbacks, 
we opted for including both B-cubed and CEAF 
measures in the final evaluation of the systems. In 
this way we will be able to perform a meta-
evaluation study, i.e. to evaluate and compare the 
performance of metrics with respect to the task 
objectives and system rankings. It might be inter-
esting to break B-cubed and CEAF into partial re-
sults across different kinds of mentions in order to 
get a better understanding of the sources of errors 
made by each system. Additionally, the MUC met-
ric will also be included for comparison purposes 
with previous results.  
Finally, for the setting with automatically gener-
ated input information (second scenario in Section 
2.1.1), it might be desirable to devise metric vari-
ants accounting for partial matches of NPs. In this 
case, capturing the correct NP head would give 
most of the credit. We plan to work in this research 
line in the near future.  
Official scorers will be developed in advance 
and made available to participants when posting 
the trial datasets. The period in between the release 
of trial datasets and the start of the full evaluation 
will serve as a test for the evaluation metrics. De-
pending on the feedback obtained from the partici-
pants we might consider introducing some 
improvements in the evaluation setting.  
3 AnCora-CO corpora  
The corpora used in the task are AnCora-CO, 
which are the result of enriching the AnCora cor-
pora (Taul? et al, 2008) with coreference informa-
tion. AnCora-CO is a multilingual corpus 
annotated at different linguistic levels consisting of 
400K words in Catalan3, 400K words in Spanish2, 
                                                           
3 Freely available for research purposes from the following 
URL: http://clic.ub.edu/ancora 
and 120K words in English. For the purpose of the 
task, the corpora are split into a training (85%) and 
test (15%) set. Each file corresponds to one news-
paper text.  
AnCora-CO consists mainly of newspaper and 
newswire articles: 200K words from the Spanish 
and Catalan versions of El Peri?dico newspaper, 
and 200K words from the EFE newswire agency in 
the Spanish corpus, and from the ACN newswire 
agency in the Catalan corpus. The source corpora 
for Spanish and Catalan are the AnCora corpora, 
which were annotated by hand with full syntax 
(constituents and functions) as well as with seman-
tic information (argument structure with thematic 
roles, semantic verb classes, named entities, and 
WordNet nominal senses). The annotation of 
coreference constitutes an additional layer on top 
of the previous syntactic-semantic information. 
The English part of AnCora-CO consists of a se-
ries of documents of the Reuters newswire corpus 
(RCV1 version).4 The RCV1 corpus does not come 
with any syntactic nor semantic annotation. This is 
why we only count with automatic linguistic anno-
tation produced by statistical taggers and parsers 
on this corpus. 
Although the Catalan, English, and Spanish cor-
pora used in the task all belong to the domain of 
newspaper texts, they do not form a three-way par-
allel corpus. 
3.1 Coreference annotation 
The annotation of a corpus with coreference in-
formation is highly complex due to (i) the lack of 
information in descriptive grammars about this 
topic, and (ii) the difficulty in generalizing the in-
sights from one language to another. Regarding (i), 
a wide range of units and relations occur for which 
it is not straightforward to determine whether they 
are or not coreferent. Although there are theoretical 
studies for English, they cannot always be ex-
tended to Spanish or Catalan since coreference is a 
very language-specific phenomenon, which ac-
counts for (ii). 
In the following we present some of the linguis-
tic issues more problematic in relation to corefer-
ence annotation, and how we decided to deal with 
them in AnCora-CO (Recasens, 2008). Some of 
them are language dependent (1); others concern 
                                                           
4 Reuters Corpus RCV1 is distributed by NIST at the follow-
ing URL: http://trec.nist.gov/data/reuters/reuters.html 
73
the internal structure of the mentions (2), or the 
type of coreference link (3). Finally, we present 
those NPs that were left out from the annotation 
for not being referential (4). 
 
1. Language-specific issues 
- Since Spanish and Catalan are pro-drop 
languages, elliptical subjects were intro-
duced in the syntactic annotation, and they 
are also annotated with coreference.  
- Expletive it pronouns, which are frequent 
in English and to a lesser extent in Spanish 
and Catalan are not referential, and so they 
do not participate in coreference links. 
- In Spanish, clitic forms for pronouns can 
merge into a single word with the verb; in 
these cases the whole verbal node is anno-
tated for coreference. 
2. Issues concerning the mention structure  
- In possessive NPs, only the reference of 
the thing possessed (not the possessor) is 
taken into account. For instance, su libro 
?his book? is linked with a previous refer-
ence of the same book; the possessive de-
terminer su ?his? does not constitute an NP 
on its own. 
- In the case of conjoined NPs, three (or 
more) links can be encoded: one between 
the entire NPs, and additional ones for 
each of the constituent NPs. AnCora-CO 
captures links at these different levels. 
3. Issues concerning types of coreference links 
- Plural NPs can refer to two or more ante-
cedents that appear separately in the text. 
In these cases an entity resulting from the 
addition of two or more entities is created.  
- Discourse deixis is kept under a specific 
link tag because not all coreference resolu-
tion systems can handle such relations. 
- Metonymy is annotated as a case of iden-
tity because both mentions pragmatically 
corefer. 
4. Non-referential NPs 
- In order to be linguistically accurate (van 
Deemter and Kibble, 2000), we distinguish 
between referring and attributive NPs: 
while the first point to an entity, the latter 
express some of its properties. Thus, at-
tributive NPs like apposition and predica-
tive phrases are not treated as identity 
coreference in AnCora-CO (they are kept 
distinct under the ?predicative link? tag).  
- Bound anaphora and bridging reference go 
beyond coreference and so are left out 
from consideration. 
The annotation process of the corpora is outlined in 
the next section. 
3.2 Annotation process 
The Ancora coreference annotation process in-
volves: (a) marking of mentions, and (b) marking 
of coreference chains (entities). 
(a) Referential full NPs (including proper nouns) 
and pronouns (including elliptical and clitic pro-
nouns) are the potential mentions of a coreference 
chain.  
(b) In the current task only identity relations 
(coreftype=?ident?) will be considered, which link 
referential NPs that point to the same discourse 
entity. Coreferent mentions are annotated with the 
attribute entity. Mentions that point to the same 
entity share the same entity number. In Figure 1, 
for instance, el reial decret aprovat ahir pel Con-
sell_de_Ministres ?the royal decree approved yes-
terday by the Council of Ministers? is 
entity=?entity9? and la nova norma ?the new rule? 
is also entity=?entity9? because they corefer. 
Hence, mentions referring to the same discourse 
entity all share the same entity number.  
The corpora were annotated by a total of seven 
annotators (qualified linguists) using the An-
CoraPipe annotation tool (Bertran et al, 2008), 
which allows different linguistic levels to be anno-
tated simultaneously and efficiently. AnCoraPipe 
supports XML in-line annotations.  
An initial reliability study was performed on a 
small portion of the Spanish AnCora-CO corpus. 
In that study, eight linguists annotated the corpus 
material in parallel. Inter-annotator agreement was 
computed with Krippendorff?s alpha, achieving a 
result above 0.8. Most of the problems detected 
were attributed either to a lack of training of the 
coders or to ambiguities that are left unresolved in 
the discourse itself. After carrying out this reliabil-
ity study, we opted for annotating the corpora in a 
two-stage process: a first pass in which all mention 
attributes and coreference links were coded, and a 
second pass in which the already annotated files 
were revised. 
 
74
4 Conclusions 
The SemEval-2010 multilingual coreference reso-
lution task has been presented for discussion.  
Firstly, we aim to promote research on coreference 
resolution from a learning-based perspective in a 
multilingual scenario in order to: (a) explore port-
ability issues; (b) analyze language-specific tuning 
requirements; (c) facilitate cross-linguistic com-
parisons between two Romance languages and be-
tween Romance languages and English; and (d) 
encourage researchers to develop linguistic re-
sources ? annotated corpora ? oriented to corefer-
ence resolution for other languages. 
Secondly, given the complexity of the corefer-
ence phenomena we split the coreference resolu-
tion task into two (full coreference chains and 
pronominal resolution), and we propose two dif-
ferent scenarios (gold standard vs. automatically 
generated input information) in order to evaluate to 
what extent the performance of a coreference reso-
lution system varies depending on the quality of 
the other levels of information. 
Finally, given that the evaluation of coreference 
resolution systems is still an open issue, we are 
interested in comparing different coreference reso-
lution metrics: B-cubed and CEAF measures. In 
this way we will be able to evaluate and compare 
the performance of these metrics with respect to 
the task objectives and system rankings. 
Acknowledgments 
This research has been supported by the projects 
Lang2World (TIN2006-15265-C06), TEXT-MESS 
(TIN2006-15265-C04), OpenMT (TIN2006-
15307-C03-02), AnCora-Nom (FFI2008-02691-E), 
and the FPU grant (AP2006-00994) from the Span-
ish Ministry of Education and Science, and the 
funding given by the government of the Generalitat 
de Catalunya.  
References  
Bagga, Amit and Breck Baldwin. 1998. Algorithms for 
scoring coreference chains. In Proceedings of Lan-
guage Resources and Evaluation Conference. 
Bertran, Manuel, Oriol Borrega, Marta Recasens, and 
B?rbara Soriano. 2008. AnCoraPipe: A tool for mul-
tilevel annotation, Procesamiento del Lenguaje Natu-
ral, n. 41: 291-292, SEPLN. 
Denis, Pascal and Jason Baldridge. 2008. Specialized 
models and ranking for coreference resolution. Pro-
ceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP 2008). 
Luo, Xiaoqiang. 2005. On coreference resolution per-
formance metrics. Proceedings of HLT/NAACL 2005. 
McCarthy Joseph and Wendy Lehnert.  1995. Using 
decision trees for coreference resolution. Proceed-
ings of the Fourteenth International Joint Conference 
on Artificial Intelligence. 
Mitkov, Ruslan. 1998. Robust pronoun resolution with 
limited knowledge. Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics, and 17th International Conference on Com-
putational Linguistics (COLING-ACL98). 
Morton, Thomas. 2000. Using coreference for question 
answering. Proceedings of the 8th Text REtrieval 
Conference (TREC-8). 
Ng, Vincent. 2008. Unsupervised models for corefer-
ence resolution. Proceedings of the Empirical Meth-
ods in Natural Language Processing (EMNLP 2008). 
NIST. 2003. In Proceedings of ACE 2003 workshop. 
Booklet, Alexandria, VA. 
Recasens, Marta. 2008. Towards Coreference Resolu-
tion for Catalan and Spanish. Master Thesis. Univer-
sity of Barcelona.  
Steinberger, Josef, Massimo Poesio, Mijail Kabadjov, 
and Karel Jezek. 2007. Two uses of anaphora resolu-
tion in summarization. Information Processing and 
Management, 43:1663?1680.  
Taul?, Mariona, Ant?nia Mart?, and Marta Recasens. 
2008. AnCora: Multilevel corpora with coreference 
information for Spanish and Catalan. In Proceedings 
of the Language Resources and Evaluation Confer-
ence (LREC 2008). 
van Deemter, Kees and Rodger Kibble. 2000. Squibs 
and Discussions: On coreferring: coreference in 
MUC and related annotation schemes. Computa-
tional Linguistics, 26(4):629-637. 
Vilain, Marc, John Burger, John Aberdeen, Dennis 
Connolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings 
of MUC-6. 
 
75
