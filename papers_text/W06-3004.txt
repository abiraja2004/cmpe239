Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 25?32,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
Enhanced Interactive Question-Answering with Conditional Random Fields
Andrew Hickl and Sanda Harabagiu
Language Computer Corporation
Richardson, Texas 75080
andy@languagecomputer.com
Abstract
This paper describes a new methodology
for enhancing the quality and relevance of
suggestions provided to users of interac-
tive Q/A systems. We show that by using
Conditional Random Fields to combine
relevance feedback gathered from users
along with information derived from dis-
course structure and coherence, we can
accurately identify irrelevant suggestions
with nearly 90% F-measure.
1 Introduction
Today?s interactive question-answering (Q/A) sys-
tems enable users to pose questions in the context
of extended dialogues in order to obtain information
relevant to complex research scenarios. When work-
ing with an interactive Q/A system, users formulate
sequences of questions which they believe will re-
turn answers that will let them reach certain infor-
mation goals.
Users need more than answers, however: while
they might be cognizant of many of the different
types of information that they need, few ? if any ?
users are capable of identifying all of the questions
that must be asked and answered for a particular sce-
nario. In order to take full advantage of the Q/A
capabilities of current systems, users need access to
sources of domain-specific knowledge that will ex-
pose them to new concepts and ideas and will allow
them to ask better questions.
In previous work (Hickl et al, 2004; Harabagiu et
al., 2005a), we have argued that interactive question-
answering systems should be based on a predictive
dialogue architecture which can be used to provide
users with both precise answers to their questions as
well as suggestions of relevant research topics that
could be explored throughout the course of an inter-
active Q/A dialogue.
Typically, the quality of interactive Q/A dialogues
has been measured in three ways: (1) efficiency, de-
fined as the number of questions that the user must
pose to find particular information, (2) effectiveness,
defined by the relevance of the answer returned, and
(3) user satisfaction (Scholtz and Morse, 2003).
In our experiments with an interactive Q/A sys-
tem, (known as FERRET), we found that perfor-
mance in each of these areas improves as users are
provided with suggestions that are relevant to their
domain of interest. In FERRET, suggestions are
made to users in the form of predictive question-
answer pairs (known as QUABs) which are either
generated automatically from the set of documents
returned for a query (using techniques first described
in (Harabagiu et al, 2005a)), or are selected from a
large database of questions-answer pairs created off-
line (prior to a dialogue) by human annotators.
Figure 1 presents an example of ten QUABs
that were returned by FERRET in response to the
question ?How are EU countries responding to the
worldwide increase of job outsourcing to India??.
While FERRET?s QUABs are intended to provide
users with relevant information about a domain of
interest, we can see from Figure 1 that users do not
always agree on which QUAB suggestions are rel-
evant. For example, while someone unfamiliar to
the notion of ?job outsourcing? could benefit from
25
Relevant?
QUAB Question
User1 User2
NO YES QUAB1: What EU countries are outsourcing jobs to India?
YES YES QUAB2: What EU countries have made public statements
against outsourcing jobs to India?
NO YES QUAB3: What is job outsourcing?
YES YES QUAB4: Why are EU companies outsourcing jobs to India?
NO NO QUAB5: What measures has the U.S. Congress taken to stem
the tide of job outsourcing to India?
YES NO QUAB6: How could the anti-globalization movements in EU
countries impact the likelihood that the EU Parliament will
take steps to prevent job outsourcing to India?
YES YES QUAB7: Which sectors of the EU economy could be most
affected by job outsourcing?
YES YES QUAB8: How has public opinion changed in the EU on job
outsourcing issues over the past 10 years?
YES YES QUAB9: What statements has French President Jacques
Chirac made about job outsourcing?
YES YES QUAB10: How has the EU been affected by anti-job outsourc-
ing sentiments in the U.S.?
Figure 1: Examples of QUABs.
a QUAB like QUAB3: ?What is job outsourcing??,
we expect that a more experienced researcher would
find this definition to be uninformative and poten-
tially irrelevant to his or her particular information
needs. In contrast, a complex QUAB like QUAB6:
?How could the anti-globalization movements in EU
countries impact the likelihood that the EU Parlia-
ment will take steps to prevent job outsourcing to
India?? could provide a domain expert with rel-
evant information, but would not provide enough
background information to satisfy a novice user who
might not be able to interpret this information in the
appropriate context.
In this paper, we present results of a new set of
experiments that seek to combine feedback gathered
from users with a relevance classifier based on con-
ditional random fields (CRF) in order to provide sug-
gestions to users that are not only related to the topic
of their interactive Q/A dialogue, but provide them
with the new types of information they need to know.
Section 2 presents the functionality of several
of FERRET?s modules and describes the NLP tech-
niques for processing questions as well as the frame-
work for acquiring domain knowledge. In Section 3
we present two case studies that highlight the impact
of user background. Section 4 describes a new class
of user interaction models for interactive Q/A and
presents details of our CRF-based classifier. Section
5 presents results from experiments which demon-
strate that user modeling can enhance the quality
of suggestions provided to both expert and novice
users. Section 6 summarizes the conclusions.
2 The FERRET Interactive
Question-Answering System
We believe that the quality of interactions produced
by an interactive Q/A system can be enhanced by
predicting the range of questions that a user might
ask while researching a particular topic. By provid-
ing suggestions from a large database of question-
answer pairs related to a user?s particular area of
interest, interactive Q/A systems can help users
gather the information they need most ? without the
need for complex, mixed-initiative clarification dia-
logues.
FERRET uses a large collection of QUAB
question-answer pairs in order to provide users with
suggestions of new research topics that could be ex-
plored over the course of a dialogue. For example,
when a user asks a question likeWhat is the result of
the European debate on outsourcing to India? (as il-
lustrated in (Q1) in Table 1), FERRET returns a set of
answers (including (A1) and proposes the questions
in (Q2), (Q3), and (Q4) as suggestions of possible
continuations of the dialogue. Users then have the
freedom to choose how the dialogue should be con-
tinued, either by (1) ignoring the suggestions made
by the system, (2) selecting one of the proposed
QUAB questions and examining its associated an-
swer, or (3) resubmitting the text of the QUAB ques-
tion to FERRET?s automatic Q/A system in order to
retrieve a brand-new set of answers.
(Q1) What is the result of the European debate on outsourcing to India?
(A1) Supporters of economic openness understand how outsourcing can
strengthen the competitiveness of European companies, as well as benefi t
jobs and growth in India.
(Q2) Has the number of customer service jobs outsourced to India in-
creased since 1990?
(Q3) How many telecom jobs were outsourced to India from EU-based
companies in the last 10 years?
(Q4) Which European Union countries have experienced the most job
losses due to outsourcing over the past 10 years?
Table 1: Sample Q/A Dialogue.
FERRET was designed to evaluate how databases
of topic-relevant suggestions could be used to en-
hance the overall quality of Q/A dialogues. Fig-
ure 2 illustrates the architecture of the FERRET sys-
tem. Questions submitted to FERRET are initially
processed by a dialogue shell which (1) decomposes
complex questions into sets of simpler questions (us-
ing techniques first described in (Harabagiu et al,
2005a)), (2) establishes discourse-level relations be-
tween the current question and the set of questions
26
Previous
Dialogue Context
Management
Management
Dialogue Act
Decomposition
Question
Topic Partitioning
and Representation
Answer
Fusion
Database (QUAB)
Question?Answer
Information
Extraction
Question
Similarity
Conversation
Scenario
Predictive
Questions
Answer
Fusion
Online Question Answering Off?line Question Answering
Answer
Question
Document
Collection
Dialogue Shell Predictive Dialogue
(PQN)
Network
Predictive
Question
Figure 2: FERRET - A Predictive Interactive Question-Answering Architecture.
already entered into the discourse, and (3) identifies
a set of basic dialogue acts that are used to manage
the overall course of the interaction with a user.
Output from FERRET?s dialogue shell is sent to
an automatic question-answering system which is
used to find answers to the user?s question(s). FER-
RET uses a version of LCC?s PALANTIR question-
answering system (Harabagiu et al, 2005b) in or-
der to provide answers to questions in documents.
Before being returned to users, answer passages are
submitted to an answer fusion module, which filters
redundant answers and combines answers with com-
patible information content into single coherent an-
swers.
Questions and relational information extracted by
the dialogue shell are also sent to a predictive dia-
logue module, which identifies the QUABs that best
meet the user?s expected information requirements.
At the core of the FERRET?s predictive dialogue
module is the Predictive Dialogue Network (PQN), a
large database of QUABs that were either generated
off-line by human annotators or created on-line by
FERRET (either during the current dialogue or dur-
ing some previous dialogue)1. In order to generate
QUABs automatically, documents identified from
FERRET?s automatic Q/A system are first submit-
ted to a Topic Representation module, which com-
putes both topic signatures (Lin and Hovy, 2000)
and enhanced topic signatures (Harabagiu, 2004) in
order to identify a set of topic-relevant passages.
Passages are then submitted to an Information Ex-
traction module, which annotates texts with a wide
1Techniques used by human annotators for creating QUABs
were fi rst described in (Hickl et al, 2004); full details of FER-
RET?s automatic QUAB generation components are provided in
(Harabagiu et al, 2005a).
range of lexical, semantic, and syntactic informa-
tion, including (1) morphological information, (2)
named entity information from LCC?s CICEROLITE
named entity recognition system, (3) semantic de-
pendencies extracted from LCC?s PropBank-style
semantic parser, and (4) syntactic parse informa-
tion. Passages are then transformed into natural lan-
guage questions using a set of question formation
heuristics; the resultant QUABs are then stored in
the PQN. Since we believe that the same set of re-
lations that hold between questions in a dialogue
should also hold between pairs of individual ques-
tions taken in isolation, discourse relations are dis-
covered between each newly-generated QUAB and
the set of QUABs stored in the PQN. FERRET?s
Question Similarity module then uses the similar-
ity function described in (Harabagiu et al, 2005a) ?
along with relational information stored in the PQN
? in order to identify the QUABs that represent the
most informative possible continuations of the dia-
logue. QUABs are then ranked in terms of their rel-
evance to the user?s submitted question and returned
to the user.
3 Two Types of Users of Interactive Q/A
Systems
In order to return answers that are responsive to
users? information needs, interactive Q/A systems
need to be sensitive to the different questioning
strategies that users employ over the course of a di-
alogue. Since users gathering information on the
same topic can have significantly different informa-
tion needs, interactive Q/A systems need to be able
to accommodate a wide range of question types in
order to help users find the specific information that
27
SQ ? How are European Union countries responding to the worldwide increase in job outsourcing to countries like India?
EQ2 ? What impact has public opposition to globalization in
EU countries had on companies to relocate EU jobs to India?
EQ4 ? What economic advantages could EU
countries realize by outsourcing jobs to India?
NQ1 ? What countries in the European Union are outsourcing jobs to India?
NQ2 ? How many jobs have been outsourced to India?
NQ3 ? What industries have been most
active in outsourcing jobs to India?
NQ4 ? Are the companies that are outsourcing
jobs to India based in EU countries?
NQ5 ? What could European Countries do to
respond to increases in job outsourcing to India?
NQ6 ? Do European Union Countries view job
outsourcing to countries like India as a problem?
EQ1 ? Is the European Union likely to implement protectionist
policies to keep EU companies from outsourcing jobs to India?
EQ3 ? What economic ties has the EU maintained historically with India?
EQ5 ? Will the EU adopt either any of the the U.S.?s or
Japan?s anti?outsourcing policies in the near future?
ease tensions over immiggration in many EU countries?
EQ6 ? Could the increasing outsourcing of jobs to India
Figure 3: Expert User Interactions Versus Novice User Interactions with a Q/A System.
they are looking for.
In past experiments with users of interactive Q/A
systems (Hickl et al, 2004), we have found that a
user?s access to sources of domain-specific knowl-
edge significantly affects the types of questions that
a user is likely to submit to a Q/A system. Users par-
ticipate in information-seeking dialogues with Q/A
systems in order to learn ?new? things ? that is, to
acquire information that they do not currently pos-
sess. Users initiate a set of speech acts which allow
them to maximize the amount of new information
they obtain from the system while simultaneously
minimizing the amount of redundant (or previously-
acquired) information they encounter. Our experi-
ments have shown that Q/A systems need to be sen-
sitive to two kinds of users: (1) expert users, who
interact with a system based on a working knowl-
edge of the conceptual structure of a domain, and
(2) novice users, who are presumed to have lim-
ited to no foreknowledge of the concepts associ-
ated with the domain. We have found that novice
users that possess little or no familiarity with a do-
main employ markedly different questioning strate-
gies than expert users who possess extensive knowl-
edge of a domain: while novices focus their atten-
tion in queries that will allow them to discover ba-
sic domain concepts, experts spend their time ask-
ing questions that enable them to evaluate their hy-
potheses in the context of a the currently available
information. The experts tend to ask questions that
refer to the more abstract domain concepts or the
complex relations between concepts. In a similar
fashion, we have discovered that users who have ac-
cess to structured sources of domain-specific knowl-
edge (e.g. knowledge bases, conceptual networks
or ontologies, or mixed-initiative dialogues) can end
up employing more ?expert-like? questioning strate-
gies, despite the amount of domain-specific knowl-
edge they possess.
In real-world settings, the knowledge that expert
users possess enables them to formulate a set of hy-
potheses ? or belief states ? that correspond to each
of their perceived information needs at a given mo-
ment in the dialogue context. As can be seen in the
dialogues presented in Figure 3, expert users gener-
ally formulate questions which seek to validate these
belief states in the context of a document collection.
Given the global information need in S1, it seems
reasonable to presume that questions like EQ1 and
EQ2 are motivated by a user?s expectation that pro-
tectionist policies or public opposition to globaliza-
tion could impact a European Union country?s will-
ingness to take steps to stem job outsourcing to In-
dia. Likewise, questions like EQ5 are designed to
provide the user with information that can decide be-
tween two competing belief states: in this case, the
user wants to know whether the European Union is
more likely to model the United States or Japan in its
policies towards job outsourcing. In contrast, with-
out a pre-existing body of domain-specific knowl-
edge to derive reasonable hypotheses from, novice
users ask questions that enable them to discover
the concepts (and the relations between concepts)
needed to formulate new, more specific hypotheses
and questions. Returning again to Figure 3, we can
see that questions like NQ1 and NQ3 are designed
to discover new knowledge that the user does not
currently possess, while questions like NQ6 try to
28
establish whether or not the user?s hypothesis (i.e.
namely, that EU countries view job outsourcing to
India as an problem) is valid and deserves further
consideration.
4 User Interaction Models for Relevance
Estimation
Unlike systems that utilize mixed initiative dia-
logues in order to determine a user?s information
needs (Small and Strzalkowski, 2004), systems (like
FERRET) which rely on interactions based on pre-
dictive questioning have traditionally not incorpo-
rated techniques that allow them to gather relevance
feedback from users. In this section, we describe
how we have used a new set of user interaction mod-
els (UIM) in conjunction with a relevance classifier
based on conditional random fields (CRF) (McCal-
lum, 2003; Sha and Pereira, 2003) in order to im-
prove the relevance of the QUAB suggestions that
FERRET returns in response to a user?s query.
We believe that systems based on predictive ques-
tioning can derive feedback from users in three
ways. First, systems can learn which suggestions
or answers are relevant to a user?s domain of inter-
est by tracking which elements users select through-
out the course of a dialogue. With FERRET, each
answer or suggestion presented to a user is associ-
ated with a hyperlink that links to the original text
that the answer or QUAB was derived from. While
users do not always follow links associated with pas-
sages they deem to be relevant to their query, we
expect that the set of selected elements are gener-
ally more likely to be relevant to the user?s interests
than unselected elements. Second, since interactive
Q/A systems are often used to gather information
for inclusion in written reports, systems can identify
relevant content by tracking the text passages that
users copy to other applications, such as text editors
or word processors. Finally, predictive Q/A systems
can gather explicit feedback from users through the
graphical user interface itself. In a recent version of
FERRET, we experimented with adding a ?relevance
checkbox? to each answer or QUAB element pre-
sented to a user; users were then asked to provide
feedback to the system by selecting the checkboxes
associated with answers that they deemed to be par-
ticularly relevant to the topic they were researching.
4.1 User Interaction Models
We have experimented with three models that we
have used to gather feedback from users of FERRET.
The models are illustrated in Figure 4.
UIM1: Under this model, the set of QUABs that users copied from were selected
as relevant; all QUABs not copied from were annotated as irrelevant.
UIM2: Under this model, QUABs that users viewed were considered to be rele-
vant; QUABs that remained unviewed were annotated as irrelevant.
UIM3: Under this model, QUABs that were either viewed or copied from were
marked as relevant; all other QUABs were annotated as irrelevant.
Figure 4: User Interaction Models.
With FERRET, users are presented with as many
as ten QUABs for every question they submit to the
system. QUABs ? whether they be generated auto-
matically by FERRET?s QUAB generation module,
or selected from FERRET?s knowledge base of over
10,000 manually-generated question/answer pairs ?
are presented in terms of their conceptual similarity
to the original question. Conceptual similarity (as
first described in (Harabagiu et al, 2005a)) is calcu-
lated using the version of the cosine similarity for-
mula presented in Figure 5.
Conceptual Similarity weights content terms in Q1 and Q2 using tfidf
(wi = w(ti) = (1 + log(tfi))
log N
dfi
), where N is the number of
questions in the QUAB collection, while dfi is equal to the number of
questions containing ti and tfi is the number of times ti appears in Q1
and Q2. The questions Q1 and Q2 can be transformed into two vectors,
vq = ?wq1 , wq2 , ..., wqm ? and vu = ?wu1 , wu2 , ..., wun ?; The sim-
ilarity between Q1 and Q2 is measured as the cosine measure between their
corresponding vectors:
cos(vq, vu) = (
?
i
wqiwui )/((
?
i
w2qi )
1
2 ? (
?
i
w2ui )
1
2 )
Figure 5: Conceptual Similarity.
In the three models from Figure 4, we allowed
users to perform research as they normally would.
Instead of requiring users to provide explicit forms
of feedback, features were derived from the set of
hyper-links that users selected and the text passages
that users copied to the system clipboard.
Following (Kristjansson et al, 2004) we analyzed
the performance of each of these three models using
a new metric derived from the number of relevant
QUABs that were predicted to be returned for each
model. We calculated this metric ? which we refer
to as the Expected Number of Irrelevant QUABs ?
using the formula:
p0(n) =
10
?
k=1
kp0(k) (1)
p1(n) = (1 ? p0(0)) +
10
?
k=1
kp1(k) (2)
29
where pm(n) is equal to the probability of finding
n irrelevant QUABs in a set of 10 suggestions re-
turned to the user given m rounds of interaction.
p0(n) (equation 1) is equal to the probability that all
QUABs are relevant initially, while p1(n) (equation
2) is equal to the probability of finding an irrelevant
QUAB after the set of QUABs has been interacted
with by a user. For the purposes of this paper, we
assumed that all of the QUABs initially returned by
FERRET were relevant, and that p0(0) = 1.0. This
enabled us to calculate p1(n) for each of the three
models provided in Figure 4.
4.2 Relevance Estimation using Conditional
Random Fields
Following work done by (Kristjansson et al, 2004),
we used the feedback gathered in Section 4.1 to es-
timate the probability that a QUAB selected from
FERRET?S PQN is, in fact, relevant to a user?s orig-
inal query. We assume that humans gauge the rel-
evance of QUAB suggestions returned by the sys-
tem by evaluating the informativeness of the QUAB
with regards to the set of queries and suggestions
that have occurred previously in the discourse. A
QUAB, then, is deemed relevant when it conveys
content that is sufficiently informative to the user,
given what the user knows (i.e. the user?s level of
expertise) and what the user expects to receive as
answers from the system.
Our approach treats a QUAB suggestion
as a single node in a sequence of questions
?Qn?1, Qn, QUAB? and classifies the QUAB as
relevant or irrelevant based on features from the
entire sequence.
We have performed relevance estimation us-
ing Conditional Random Fields (CRF). Given a
random variable x (corresponding to data points
{x1, . . . , xn}) and another random variable y (cor-
responding to a set of labels {y1, . . . , yn}), CRFs
can be used to calculate the conditional probability
p(y|x). Given a sequence {x1, . . . , xn} and set of
labels {y1, . . . , yn}, p(y|x) can be defined as:
p(y|x) =
1
z0
exp
(
N
?
n=1
?
k
?kfk(yi?1, yi, x, n)
)
(3)
where z0 is a normalization factor and ?k is a weight
learned for each feature vector fk(yi?1, yi, x, n).
We trained our CRF model in the following way.
If we assume that ? is a set of feature weights
(?0, . . . ,?k), then we expect that we can use maxi-
mum likelihood to estimate values for ? given a set
of training data pairs (x, y).
Training is accomplished by maximizing the log-
likelihood of each labeled data point as in the fol-
lowing equation:
w? =
N
?
i=1
log(p?(xi|yi)) (4)
Again, following (Kristjansson et al, 2004), we
used the CRF Viterbi algorithm to find the most
likely sequence of data points assigned to each la-
bel category using the formula:
y? = arg max
y
p?(y|x) (5)
Motivated by the types of discourse relations that
appear to exist between states in an interactive Q/A
dialogue, we introduced a large number of features
to estimate relevance for each QUAB suggestion.
The features we used are presented in Figure 6
(a) Rank of QUAB: the rank (1, ..., 10) of the QUAB in question.
(b) Similarity: similarity of QUAB, Qn and QUAB, Qn?1.
(c) Relation likelihood: equal to the likelihood of each predicate-argument
structure included in QUAB given all QUABs contained in FERRET?s QUAB;
calculated for Arg-0, Arg-1, and ArgM-TMP for each predicate found in
QUAB suggestions. (Predicate-argument relations were identifi ed using a se-
mantic parser trained on PropBank (Palmer et al, 2005) annotations.)
(d) Conditional Expected Answer Type likelihood: equal to the joint probabil-
ity p(EATQUAB |EATquestion) calculated from a corpus of dialogues
collected from human users of FERRET.
(e) Terms in common: real-valued feature equal to the number of terms in
common between the QUAB and both Qn and Qn?1.
(f) Named Entities in common: same as terms in common, but calculated for
named entities detected by LCC?s CIEROLITE named entity recognition sys-
tem.
Figure 6: Relevance Features.
In the next section, we describe how we utilized
the user interaction model described in Subsection
4.1 in conjunction with this subsection in order to
improve the relevance of QUAB suggestions re-
turned to users.
5 Experimental Results
In this section, we describe results from two experi-
ments that were conducted using data collected from
human interactions with FERRET.
In order to evaluate the effectiveness of our rel-
evance classifier, we gathered a total of 1000 ques-
tions from human dialogues with FERRET. 500 of
30
these came from interactions (41 dialogues) where
the user was a self-described ?expert? on the topic;
another selection of 500 questions came from a to-
tal of 23 dialogues resulting from interactions with
users who described themselves as ?novice? or were
otherwise unfamiliar with a topic. In order to
validate the user?s self-assessment, we selected 5
QUABs at random from the set of manually created
QUABs assembled for each topic. Users were asked
to provide written answers to those questions. Users
that were judged to have correctly answered three
out of five questions were considered ?experts? for
the purpose of our experiments. Table 2 presents the
breakdown of questions across these two conditions.
User Type Unique Topics # Dialogues Avg # of Qs/dialogue Total Qs
Expert 12 41 12.20 500
Novice 8 23 21.74 500
Total 12 64 15.63 1000
Table 2: Question Breakdown.
Each of these experiments were run using a ver-
sion of FERRET that returned the top 10 most similar
QUABs from a database that combined manually-
created QUABs with the automatically-generated
QUABs created for the user?s question. While a to-
tal of 10,000 QUABs were returned to users during
these experiments, only 3,998 of these QUABs were
unique (39.98%).
We conducted two kinds of experiments with
users. In the first set of experiments, users were
asked to mark all of the relevant QUABs that FER-
RET returned in response to questions submitted by
users. After performing research on a particular
scenario, expert and novice users were then sup-
plied with as many as 65 questions (and associ-
ated QUABs) taken from previously-completed di-
alogues on the same scenario; users were then asked
to select checkboxes associated with QUABs that
were relevant. In addition, we also had 2 linguists
(who were familiar with all of the research sce-
narios but did not research any of them) perform
the same task for all of the collected questions and
QUABs. Results from these three sets of annotations
are found in Table 3.
User Type Users # Qs # QUABs # rel. QUABs % relevant ENIQ(P1)
Expert 6 250 2500 699 27.96% 5.88
Novice 4 250 2500 953 38.12% 3.73
Linguists 2 500 5000 2240 44.80% 3.53
Table 3: User Comparison.
As expected, experts believed QUABs to be sig-
nificantly (p < 0.05) less relevant than novices, who
found approximately 38.12% of QUABs to be rel-
evant to the original question submitted by a user.
In contrast, the two linguists found 44.8% of the
QUABs to be relevant. This number may be arti-
ficially high: since the linguists did not engage in
actual Q/A dialogues for each of the scenarios they
were annotating, they may not have been appropri-
ately prepared to make a relevance assessment.
In the second set of experiments, we used the UIM
in Figure 4 to train CRF-based relevance classifiers.
We obtained training data for UIM1 (?copy-and-
paste?-based), UIM2 (?click?-based), and UIM3
(?hybrid?) from 16 different dialogue histories col-
lected from 8 different novice users. During these
dialogues, users were asked to perform research as
they normally would; no special instructions were
given to users to provide additional relevance feed-
back to the system. After the dialogues were com-
pleted, QUABs that were copied from or clicked
were annotated as ?relevant? examples (according
to each UIM); the remaining QUABs were anno-
tated as ?irrelevant?. Once features (as described
in Table 3) were extracted and the classifiers were
trained, they were evaluated on a set of 1000 QUABs
(500 ?relevant?, 500 ?irrelevant?) selected at ran-
dom from the annotations performed in the first ex-
periment. Table 4 presents results from these two
classifiers.
UIM1 P R F (? = 1)
Irrelevant 0.9523 0.9448 0.9485
Relevant 0.3137 0.3478 0.3299
UIM2 P R F (? = 1)
Irrelevant 0.8520 0.8442 0.8788
Relevant 0.3214 0.4285 0.3673
UIM3 P R F (? = 1)
Irrelevant 0.9384 0.9114 0.9247
Relevant 0.3751 0.3961 0.3853
Table 4: Experimental Results from 3 User Models.
Our results suggest that feedback gathered from a
user?s ?normal? interactions with FERRET could be
used to provide valuable input to a relevance classi-
fier for QUABsWhen ?copy-and-paste? events were
used to train the classifier, the system detected in-
stances of irrelevant QUABs with over 80% F.When
the much more frequent ?clicking? events were used
to train the classifier, irrelevant QUABs were de-
tected at over 90%F for both UIM2 and UIM3. In
each of these three cases, however, detection of rel-
31
evant QUABs lagged behind significantly: relevant
QUABs were detected at 42% F in UIM1 at nearly
33% F under UIM2 and at 39% under UIM3.
We feel that these results suggest that the detec-
tion of relevant QUABs (or the filtering of irrelevant
QUABs) may be feasible, even without requiring
users to provide additional forms of explicit feed-
back to the system. While we acknowledge that
training models on these types of events may not al-
ways provide reliable sources of training data ? es-
pecially as users copy or click on QUAB passages
that may not be relevant to their interests in the re-
search scenario, we believe the initial performance
of these suggests that accurate forms of relevance
feedback can be gathered without the use of mixed-
initiative clarification dialogues.
6 Conclusions
In this paper, we have presented a methodology that
combines feedback that was gathered from users in
conjunction with a CRF-based classifier in order to
enhance the quality of suggestions returned to users
of interactive Q/A systems. We have shown that
the irrelevant QUAB suggestions can be identified at
over 90% when systems combine information from
a user?s interaction with semantic and pragmatic fea-
tures derived from the structure and coherence of an
interactive Q/A dialogue.
7 Acknowledgments
This material is based upon work funded in whole
or in part by the U.S. Government and any opin-
ions, findings, conclusions, or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the U.S. Gov-
ernment.
References
Sanda Harabagiu, Andrew Hickl, John Lehmann, and
Dan Moldovan. 2005a. Experiments with Interac-
tive Question-Answering. In Proceedings of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL?05).
S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A.
Hickl, and P. Wang. 2005b. Employing Two Question
Answering Systems in TREC 2005. In Proceedings of
the Fourteenth Text REtrieval Conference.
Sanda Harabagiu. 2004. Incremental Topic Represen-
tations. In Proceedings of the 20th COLING Confer-
ence.
Andrew Hickl, John Lehmann, John Williams, and Sanda
Harabagiu. 2004. Experiments with Interactive
Question-Answering in Complex Scenarios. In Pro-
ceedings of the Workshop on the Pragmatics of Ques-
tion Answering at HLT-NAACL 2004.
T. Kristjansson, A. Culotta, P. Viola, and A. McCallum.
2004. Interactive information extraction with con-
strained conditional random fi elds. In Proceedings of
AAAI-2004.
Chin-Yew Lin and Eduard Hovy. 2000. The Automated
Acquisition of Topic Signatures for Text Summariza-
tion. In Proceedings of the 18th COLING Conference.
A. McCallum. 2003. Effi ciently inducing features of
conditional random fi elds. In Proceedings of the Nine-
teenth Conference on Uncertainty in Artificial Intelli-
gence (UAI03).
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An Annotated Corpus of Semantic
Roles. In Computational Linguistics, 31(1):71?106.
Jean Scholtz and Emile Morse. 2003. Using consumer
demands to bridge the gap between software engineer-
ing and usability engineering. In Software Process:
Improvement and Practice, 8(2):89?98.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL-
2003.
Sharon Small and Tomek Strzalkowski. 2004. HITIQA:
Towards analytical question answering. In Proceed-
ings of Coling 2004.
32
