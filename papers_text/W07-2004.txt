Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 19?23,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 5: Multilingual Chinese-English Lexical Sample 
Peng Jin, Yunfang Wu and Shiwen Yu 
Institute of Computational Linguistics  
 Peking University, Beijing China 
{jandp, wuyf, yusw}@pku.edu.cn 
 
 
Abstract 
The Multilingual Chinese-English lexical 
sample task at SemEval-2007 provides a 
framework to evaluate Chinese word sense 
disambiguation and to promote research. 
This paper reports on the task preparation 
and the results of six participants. 
1 Introduction 
The Multilingual Chinese-English lexical sample 
task is designed following the leading ideas of the 
Senseval-3 Multilingual English-Hindi lexical 
sample task (Chklovski et al, 2004). The ?sense 
tags? for the ambiguous Chinese target words are 
given in the form of their English translations. 
The data preparation is introduced in the second 
section. And then the participating systems are 
briefly described and their scores are listed.  
In the conclusions we bring forward some sug-
gestion for the next campaign. 
2 Chinese Word Sense Annotated Corpus 
All the training and test data come from the 
People?s Daily in January, February and March of 
2000. The People?s Daily is the most popular 
newspaper in China and is open domain. Before 
manually sense annotating, the texts have been 
word-segmented and part of speech (PoS) tagged 
according to the PoS tagging scheme of Institute of 
Computational Linguistics in Peking University 
(ICL/PKU). The corpus had been used as one of 
the gold-standard data set for the second 
international Chinese word segmentation bakeoff 
in 2005.1
2.1 Manual Annotation 
The sense annotated corpus is manually con-
structed with the help of a word sense annotating 
interface developed in Java. Three native annota-
tors, two major in Chinese linguistics and one ma-
jor in computer science took part in the construc-
tion of the sense-annotated corpus. A text generally 
is first annotated by one annotator and then veri-
fied by two checkers. Checking is of course a nec-
essary procedure to keep the consistency. Inspired 
by the observation that checking all the instances 
of a word in a specific time frame will greatly im-
prove the precision and accelerate the speed, a 
software tool is designed in Java to gather all the 
occurrences of a word in the corpus into a check-
ing file with the sense KWIC (Key Word in Con-
text) format in sense tags order. The inter-
annotator agreement gets to 84.8% according to 
Wu. et al (2006). 
The sense entries are specified in the Chinese 
Semantic Dictionary (CSD) developed by 
ICL/PKU. The sense distinctions are made mainly 
according to the Contemporary Chinese Dictionary, 
the most widely used dictionary in mandarin Chi-
nese, with necessary adjustment and improvement 
is implemented according to words usage in real 
texts. Word senses are described using the feature-
based formalism.  The features, which appear in 
the form ?Attribute =Value?, can incorporate ex-
tensive distributional information about a word 
sense. The feature set constitutes the representation 
of a sense, while the verbal definitions of meaning 
                                                 
1 http://sighan.cs.uchicago.edu/bakeoff2005/ 
19
serve only as references for human use. The Eng-
lish translation is assigned to each sense in the at-
tribute ?English translation? in CSD. 
Based on the sense-annotated corpus, a sense is 
replaced by its English translation, which might 
group different senses together under the same 
English word. 
2.2 Instances selection 
In this task together 40 Chinese ambiguous words: 
19 nouns and 21 verbs are selected for the evalua-
tion. Each sense of one word is provided at least 15 
instances and at most 40 instances, in which 
around 2/3 is used as the training data and 1/3 as 
the test data. Table 1 presents the number of words 
under each part of speech, the average number of 
senses for each PoS and the number of instances 
respectively in the training and test set. 
 
 # Average 
senses 
# training 
instances 
# test 
instances
19 
nouns 
2.58 1019 364 
21 
verbs 
3.57 1667 571 
 
Table 1: Summary of the sense inventory and 
number of training data and test set 
 
In order to escape from the sense-skewed distri-
bution that really exists in the corpus of People?s 
Daily, many instances of some senses have been 
removed from the sense annotated corpus. So the 
sense distribution of the ambiguous words in this 
task does not reflect the usages in real texts. 
3 Participating Systems 
In order to facilitate participators to select the fea-
tures, we gave a specification for the PoS-tag set. 
Both word-segmented and un-segmented context 
are provided. 
Two kinds of precisions are evaluated. One is 
micro-average: 
 
??
==
=
N
i
i
N
i
imir nmP
11
/  
 
N  is the number of all target word-types. is 
the number of labeled correctly to one specific tar-
get word-type and  is the number of all test in-
stances for this word-type. 
im
in
The other is macro-average: 
 
?
=
=
N
i
imar NpP
1
/ ,  iii nmp /=
 
All teams attempted all test instances. So the re-
call is the same with the precision. The precision 
baseline is obtained by the most frequent sense. 
Because the corpus is not reflected the real usage, 
the precision is very low. 
Six teams participated in this word sense disam-
biguation task. Four of them used supervised learn-
ing algorithms and two used un-supervised method. 
For each team two kinds of precision are given as 
in table 2.  
 
Team Micro-average Macro-average
SRCB-WSD 0.716578 0.749236 
I2R 0.712299 0.746824 
CITYU-HIF 0.710160 0.748761 
SWAT 0.657754 0.692487 
TorMd 0.375401 0.431243 
HIT 0.336898 0.395993 
baseline 0.4053 0.4618 
 
Table 2: The scores of all participating systems 
 
As follow the participating systems are briefly 
introduced. 
SRCB-WSD system exploited maximum entropy 
model as the classifier from OpenNLP2 The fol-
lowing features are used in this WSD system: 
 
? All the verbs and nouns in the context, that is, 
the words with tags ?n, nr, ns, nt, nz, v, vd, vn?  
? PoS of the left word and the right word 
?noun phrase, verb phrase, adjective phrase, 
time phrase, place phrase and quantity phrase. 
These phrases are considered as constituents of 
context, as well as words and punctuations which 
do not belong to any phrase.  
?the type of these phrases which are around the 
target phrases   
                                                 
2 http:// maxent.sourceforge.net/ 
20
? word category information comes from Chi-
nese thesaurus 
 
I2R system used a semi-supervised classification 
algorithm (label propagation algorithm) (Niu, et al, 
2005). They used three types of features: PoS of 
neighboring words with position information, un-
ordered single words in topical context, and local 
collocations.  
In the label propagation algorithm (LP) (Zhu 
and Ghahramani, 2002), label information of any 
vertex in a graph is propagated to nearby vertices 
through weighted edges until a global stable stage 
is achieved. Larger edge weights allow labels to 
travel through easier. Thus the closer the examples, 
the more likely they have similar labels (the global 
consistency assumption). In label propagation 
process, the soft label of each initial labeled exam-
ple is clamped in each iteration to replenish label 
sources from these labeled data. Thus the labeled 
data act like sources to push out labels through 
unlabeled data. With this push from labeled exam-
ples, the class boundaries will be pushed through 
edges with large weights and settle in gaps along 
edges with small weights. If the data structure fits 
the classification goal, then LP algorithm can use 
these unlabeled data to help learning classification 
plane. 
CITYU-HIF system was a fully supervised one 
based on a Na?ve Bayes classifier with simple fea-
ture selection for each target word.  The features 
used are as follows: 
 
? Local features at specified positions: 
PoS of word at w-2, w-1, w1, w2
Word at w-2, w-1, w1, w2
? Topical features within a given window: 
Content words appearing within w-10 to w10
? Syntactic features: 
PoS bi-gram at w-2w0 , w-1w0 , w0w1 , w0w2
PoS tri-gram at w-2 w-1w0 and w0w1w2
 
One characteristic of this system is the incorpo-
ration of the intrinsic nature of each target word in 
disambiguation. It is assumed that WSD is highly 
lexically sensitive and each word is best character-
ized by different lexical information. Human 
judged to consider for each target word the type of 
disambiguation information if they found useful.  
During disambiguation, they run two Na?ve Bayes 
classifiers, one on all features above, and the other 
only on the type of information deemed useful by 
the human judges. When the probability of the best 
guess from the former is under a certain threshold, 
the best guess from the latter was used instead.  
SWAT system uses a weighted vote from three 
different classifiers to make the prediction. The 
three systems are: a Na?ve Bayes classifier that 
compares similarities based on Bayes' Rule, a clas-
sifier that creates a decision list of context features, 
and a classifier that compares the angles between 
vectors of the features found most commonly with 
each sense. The features include bigrams, and tri-
grams, and unigrams are weighted by distance 
from the ambiguous word. 
TorMd used an unsupervised naive Bayes classi-
fier. They combine Chinese text and an English 
thesaurus to create a `Chinese word'--`English 
category' co-occurrence matrix. This system gener-
ated the prior-probabilities and likelihoods of a 
Na?ve Bayes word sense classifier not from sense-
annotated (in this case English translation anno-
tated) data, but from this word--category co-
occurrence matrix. They used the Macquarie The-
saurus as very coarse sense inventory. 
They asked a native speaker of Chinese to map 
the English translations of the target words to ap-
propriate thesaurus categories. Once the Na?ve 
Bayes classifier identifies a particular category as 
the intended sense, the mapping file is used to label 
the target word with the corresponding English 
translation. They rely simply on the bag of words 
that co-occur with the target word (window size of 
5 words on either side). 
HIT is a fully unsupervised WSD system, which 
puts bag of words of Chinese sentences and the 
English translations of target ambiguous word to 
search engine (Google and Baidu). Then they 
could get al kinds of statistic data. The correct 
translation was found through comparing their 
cross entropy. 
4 Conclusion 
The goal of this task is to create a framework to 
evaluate Chinese word sense disambiguation and 
to promote research. 
21
 
Scores Target 
Word 
Sen
se # 
Train
ing # 
Test 
# 
Base-
line SRCB
-WSD
I2R CITY
U-HIF
SWA
T-MP
TOR
MD 
HIT 
? 3 63 20 .50 .70 .80 .75 .75 .55 .55 
?? 3 73 27 .370 .778 .815 .741 .778 .481 .407 
? 4 69 23 .435 .696 .609 .696 .696 .174 .174 
? 9 222 77 .130 .506 .506 .481 .532 .169 .091 
? 8 197 67 .150 .567 .552 .537 .433 .119 .104 
? 4 58 20 .50 .60 .50 .55 .60 .30 .30 
?? 2 47 16 .625 .875 .875 .875 .563 .50 .438 
? 5 105 36 .278 .694 .667 .611 .889 .25 .139 
? 3 56 18 .50 .667 .722 .667 .667 .389 .333 
? 4 106 39 .256 .718 .615 .641 .538 .256 .256 
? 5 132 44 .227 .659 .75 .727 .568 .25 .114 
?? 2 56 20 .50 .90 .95 .95 .60 .50 .50 
? 4 103 34 .294 .765 .706 .765 .559 .294 .294 
?? 2 20 8 .50 .75 .75 .75 .625 .375 .50 
? 2 46 16 .625 .938 .813 .813 .875 .563 .438 
?? 2 60 18 .556 .667 .722 .778 .722 .444 .556 
? 2 40 14 .429 .571 .643 .571 .571 .143 .286 
?? 2 29 10 .60 .80 .70 .90 .80 .30 .30 
? 2 37 13 .769 .769 .769 .769 .769 .462 .462 
? 4 110 37 .270 .730 .676 .676 .541 .216 .216 
?? 2 38 14 .714 .930 1.0 .929 .786 .714 .571 
Ave. 3.5
7 
1667 571 .342/ 
.44 
.685/   
.728 
.676/   
.721 
.671/   
.723 
.618/   
.66 
.30/     
.355 
.263/   
.335 
 
 Table 3: Performance on verbs. Micro / macro average precisions are spitted by ?/? at the last row. 
 
Together six teams participate in this WSD task, 
four of them adopt supervised learning methods 
and two of them used unsupervised algorithms. All 
of the four supervised learning systems exceed ob-
viously the baseline obtained by the most frequent 
sense. It is noted that the performances of the first 
three systems are very close. Two unsupervised 
methods? scores are below the baseline. More 
unlabeled data maybe improve their performance.  
Although the SRCB-WSD system got the high-
est scores among the six participants, it does not 
perform always better than other system from table 
2 and table 3. But to each word, the four super-
vised systems always predict correctly more in-
stances than the two un-supervised systems.  
Besides the corpus, we provide a specification of 
the PoS tag set. Only SRCB-WSD system utilized 
this knowledge in feature selection. We will pro-
vide more instances in the next campaign. 
22
Scores Target 
Word 
Sen
se # 
Train
ing # 
Test 
# 
Base-
line SRCB
-WSD
I2R CITY
U-HIF
SWA
T-MP
TOR
MD 
HIT 
? 3 68 25 .40 .88 .84 .88 .76 .72 .32 
?? 2 53 18 .611 .611 .722 .722 .833 .556 .333 
? 2 56 19 .526 .842 .842 .684 .789 .474 .632 
?? 3 48 21 .476 .571 .591 .619 .619 .429 .619 
?? 2 50 17 .588 .824 .824 .824 .647 .706 .529 
? 3 53 18 .50 .778 .722 .778 .611 .50 .222 
?? 3 64 22 .455 .591 .591 .636 .545 .318 .364 
?? 2 60 20 .50 1.0 .95 1.0 1.0 .50 .50 
?? 2 38 14 .714 1.0 1.0 1.0 1.0 .643 .571 
?? 2 45 15 .533 .733 .733 .60 .467 .467 .467 
? 3 67 23 .435 .783 .783 .739 .696 .348 .696 
?? 2 44 17 .353 .529 .589 .588 .588 .353 .529 
?? 3 50 18 .556 .611 .611 .722 .722 .50 .111 
?? 2 39 14 .714 .929 .786 .714 .786 .857 .571 
?? 2 47 16 .625 .813 .813 .938 1.0 .438 .563 
?? 3 88 32 .313 .656 .563 .625 .656 .281 .344 
?? 3 65 25 .40 .88 1.0 .92 .60 .56 .44 
?? 2 41 14 .714 .786 .714 .786 .643 .714 .50 
?? 2 43 16 .625 .875 .938 1.0 .875 .438 .50 
Ave. 2.4
5 
1019 364 .506/ 
.528
.766/   
.773 
.761/ 
.769
.772/  
.778 
.72/     
.728 
.50/     
.516 
.456/   
.464 
  
Table 4: Performance on nouns. Micro / macro average precisions are spitted by ?/? at the last row. 
 
5 Acknowledgements 
This research is supported by Humanity and Social 
Science Research Project of China State Education 
Ministry (No. 06JC740001) and National Basic 
Research Program of China (No. 2004CB318102). 
We would like to thank Tao Guo and Yulai Pei 
for their hard work to guarantee the quality of the 
corpus. Huiming Duan provides us the corpus 
which has been word-segmented and PoS-tagged 
and gives some suggestions during the manual an-
notation. 
References 
Rada Mihalcea, Timothy Chklovski and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample 
task. Proceedings of SENSEVAL-3. 25-28. 
Timothy Chklovski, Rada Mihalcea, Ted Pedersen and 
Amruta Purandare. 2004. The Senseval-3 Multilin- 
 
 
gual English-Hindi lexical sample task. Proceedings of 
SENSEVAL-3. 5-8. 
Xiaojin Zhu, Zoubin Ghahramani. 2002. Learning from 
Labeled and Unlabeled Data with Label Propagation. 
CMU CALD tech report CMU-CALD-02-107. 
Yunfang Wu, Peng Jin, Yangsen Zhang, and Shiwen Yu. 
2006. A Chinese Corpus with Word Sense Annota-
tion. Proceedings of ICCPOL, Singapore, 414-421. 
Zhen-Yu Niu, Dong-Hong Ji and Chew-Lim Tan. 2005. 
Word Sense Disambiguation Using Label Propaga-
tion Based Semi Supervised Learning. Proceedings 
of the 43rd Annual Meeting of the Association for 
Computational Linguistics.395-402 
23
