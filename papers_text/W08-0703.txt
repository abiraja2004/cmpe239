Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 12?19,
Columbus, Ohio, USA June 2008. c?2008 Association for Computational Linguistics
A Bayesian model of natural language phonology:
generating alternations from underlying forms
David Ellis
de@cs.brown.edu
Brown University
Providence, RI 02912
Abstract
A stochastic approach to learning phonology.
The model presented captures 7-15% more
phonologically plausible underlying forms
than a simple majority solution, because it
prefers ?pure? alternations. It could be use-
ful in cases where an approximate solution is
needed, or as a seed for more complex mod-
els. A similar process could be involved in
some stages of child language acquisition; in
particular, early learning of phonotactics.
1 Introduction
Sound changes in natural language, such as stem
variation in inflected forms, can be described as
phonological processes. These are governed by a
constraint hierarchy as in Optimality Theory (OT),
or by a set of ordered rules. Both rely on a sin-
gle lexical representation of each morpheme (i.e., its
underlying form), and context-sensitive transforma-
tions to surface forms. Phonological changes often
affect segments near morpheme boundaries, but can
also apply over an entire prosodic word, as in vowel
harmony.
It does not seem straightforward to incorporate
context into a Bayesian model of phonology, al-
though a clever solution may yet be found. A
standard way of incorporating conditioning envi-
ronments is to treat them as factors in a Gibbs
model (Liang and Klein, 2007), but such models
require an explicit calculation of the partition func-
tion. Unless the rule contexts possess some kind of
locality, we don?t know how to compute this par-
tition function efficiently. Some context could be
captured by generating underlying phonemes from
an n-gram model, or by annotating surface forms
with neighborhood features. However, the effects of
autosegmental phonology and other long-range de-
pendencies (like vowel harmony) cannot be easily
Bayesianized.
1.1 Related Work
In the last decade, finite-state approaches to phonol-
ogy (Gildea and Jurafsky, 1996; Beesley and Kart-
tunen, 2000) have effectively brought theoretical lin-
guistic work on rewrite rules into the computational
realm. A finite-state approximation of optimality
theory (Karttunen, 1998) was later refined into a
compact treatment of gradient constraints (Gerde-
mann and van Noord, 2000).
Recent work on Bayesian models of morpholog-
ical segmentation (Johnson et al, 2007) could be
combined with phonological rule induction (Gold-
water and Johnson, 2004) in a variety of ways,
some of which will be explored in our discussion
of future work. Also, the Hierarchical Bayes Com-
piler (Daume III, 2007) could be used to generate a
model similar to the one presented here, but less con-
strained1 which makes correspondingly more ran-
dom, less accurate predictions.
1.2 Dataset
As we describe the model and its implementation in
this and subsequent sections, we will refer to a sam-
1Recent updates to HBC, inspired by discussions with the
author, have addressed some of these limitations.
12
ple dataset (in Figure 1), consisting of a paradigm2
of verb stems and person/number suffixes. The
head of each row or column is an /underlying/ form,
which in 3rd person singular is a phonologically null
segment (represented as /?/). In [surface] forms, the
realization of each morpheme is affected by phono-
logical processes. For example, in the combination
of /tieta?/ + /vat/, the result is [tieta?+va?t], where the
3rd person plural /a/ becomes [a?] due to vowel har-
mony.
1.3 Bayesian Approach
As a baseline model, we select the most frequently
occurring allophone as the underlying form. Our
goal is to outperform this baseline using a Bayesian
model. In other words, what patterns in phonologi-
cal processes can be inferred with such a statistical
model? This simple framework begins learning with
the assumption that the underlying forms are faithful
to the surface (i.e., without considering markedness
or phonotactics).
We model the generation of surface forms from
underlying ones on the segmental (character) level.
Input is an inflectional paradigm, with tokens of the
form stem+suffix. Morphology is limited to a
single suffix (no agglutination), and is already iden-
tified. Each character of an underlying stem or suf-
fix (ui) generates surface characters (sij) in an entire
row or column of the input.
To capture the phonology of a variety of lan-
guages with a single model, we need constraints
from linguistically plausible priors (universal gram-
mar). We prefer that underlying characters be pre-
served in surface forms, especially when there is no
alternation. It is also reasonable that there be fewer
underlying forms (phonemes) than surface forms
(phones, phonetic inventory), to account for allo-
phones. We expect to be able to capture a signifi-
cant subset of phonological processes using a simple
model (only faithfulness constraints).
1.4 Pure Generators
Our model has an advantage over the baseline in its
preference for ?purity? in underlying forms. Each
underlying segment should generate as few distinct
2The paradigm format lends itself to analysis of word types,
but if supplemented with surface counts, can also handle tokens.
surface segments as possible: if it generates non-
alternating (identical) segments, it will be less likely
to generate an alternation in addition. This means
that when two segments alternate, the underlying
form should be the one that appears less frequently
in other contexts, irrespective of the majority within
the alternation.
In the first stem of our Finnish verb conjugation
(Figure 1), we see a [t,d] alternation (a case of con-
sonant gradation), as well as unalternating [t]. If we
isolate three of the surface forms where /tieta?/ is in-
flected (1st person singular, and 3rd person singular
and plural), and consider only the dental segments in
the stem of each, we have two underlying segments.
Here, we use question marks to indicate unknown
underlying segments.
/??/ [dt] [tt] [tt]
In this subset of the data, the reasonable candidate
underlying forms are /t/ and /d/. These two compete
to explain the observed data (surface forms). The na-
ture of the prior probability distribution determines
whether the majority is hypothesized for each under-
lying form, so /t/ produces both alternating and unal-
ternating surface segments, or /d/ is hypothesized as
the source of the alternation (and /t/ remains ?pure?).
In a Bayesian setting, we impose a sparse prior over
underlying forms conditioned on the surface forms
they generate.
If u2 is hypothesized to be /t/, the posterior prob-
ability of u1 being /t/ goes down:
P (u1 = /t/|u2 = /t/) < P (u1 = /t/)
The probability of u1 being the competitor, /d/, cor-
respondingly increases:
P (u1 = /d/|u2 = /t/) > P (u1 = /d/)
Even though the majority in this case would be /t/,
the favored candidate for the alternating form was
/d/. This happened because of how we defined the
model?s prior, in combination with the evidence that
/t/ (assigned to u2) generated the sequence of [t]. So
selection bias prefers /d/ as the source of an ambigu-
ous segment, leaving /t/ to always generate itself.
A similar effect can occur if there are both unal-
ternating [t]?s and [d]?s on the surface, in addition to
the [t,d] alternation. The candidate (/t/ or /d/) that is
13
aaaaaa
StemSuffix /n/ (1s) /t/ (2s) /?/ (3s) /mme/ (1p) /tte/ (2p) /vat/ (3p)
/tieta?/ [tieda?+n] [tieda?+t] [tieta?+a?] [tieda?+mme] [tieda?+tte] [tieta?+va?t]
/aiko/ [ai?o+n] [ai?o+t] [aiko+o] [ai?o+mme] [ai?o+tte] [aiko+vat]
/luke/ [lu?e+n] [lu?e+t] [luke+e] [lu?e+mme] [lu?e+tte] [luke+vat]
/puhu/ [puhu+n] [puhu+t] [puhu+u] [puhu+mme] [puhu+tte] [puhu+vat]
/saa/ [saa+n] [saa+t] [saa+?] [saa+mme] [saa+tte] [saa+vat]
/tule/ [tule+n] [tule+t] [tule+e] [tule+mme] [tule+tte] [tule+vat]
/pelka?a?/ [pelka?a?+n] [pelka?a?+t] [pelka?a?+?] [pelka?a?+mme] [pelka?a?+tte] [pelka?a?+va?t]
Figure 1: Sample dataset (constructed by hand): Finnish verbs, with inflection for person and number.
generating fewer unalternating segments is preferred
to explain the alternation. For example, if there were
1000 cases of [t], 500 [d] and 500 [t,d], we would ex-
pect the following hypotheses: /t/ ? [t], /d/ ? [d]
and /d/ ? [t, d]. This is because one of the two
candidates must be responsible for both unalternat-
ing and alternating segments, but we prefer to have
as much ??purity? as possible, to minimize ambigu-
ity.
With this solution, we still have 1000 pure /t/ ?
[t], and only the 500 /d/ ? [d] are now indistinct
from /d/ ? [t, d]. If we had selected /t/ as the
source of the alternation, there would be only 500
remaining ?pure? (/d/) segments, and 1500 ambigu-
ous /t/. Our Bayesian model should prefer the less
ambiguous (?purer?) solution, given an appropriate
prior.
2 Model
We will use boldface to indicate vectors, and sub-
scripts to identify an element from a vector or ma-
trix. The variable N(u) is a vector of observed
counts with the current underlying form hypothe-
ses. The notation we use for a vector u with one
element i removed is u?i, so we can exclude the
counts associated with a particular underlying form
by indicating that in the parenthesized variable (i.e.,
N(u?4) is all the counts except those associated with
the fourth underlying form). Ni(u) is the number of
times character i is used as an underlying form, and
Nij(u) is the number of times character i generated
surface character j.
The priors over surface s and underlying u seg-
ments in Figure 2 are captured by Dirichlet priors
? and ?, which generate the multinomial distribu-
tions ? and ?, respectively (see Figure 3). The
prior over underlying form encourages sparse solu-
tions, so ?u < 1 for all u. The prior over surface
form given underlying encourages identity mapping,
/x/ ? [x], so ?xx > 1, and discourages different
segments, /x/ ? [y], so ?xy < 1 for all x 6= y.
nc
?
?
nu
mnu
s
u
?
?
Figure 2: Bayesian network: ? and ? are vectors of hy-
perparameters, and ?i (for i ? {1, . . . , nc}) and ? are
distributions. u is a vector of underlying forms, generated
from ?, and si (for i ? nu) is a set of observed surface
forms generated from the hidden variable ui according to
?i
Phones and phonemes are drawn from a set of
characters (e.g., IPA, unicode) C used to represent
them. ?i is the probability of a character (Ci for
i ? nc) being an underlying form, irrespective of
current alignments or its position in the paradigm.
?ij is the conditional probability of a surface char-
14
?c | ? ? DIR(?), c = 1, . . . , nc
? | ? ? DIR(?)
ui | ?i ? MULTI(?i), i = 1, . . . , nu
sij | ui,?ui ? MULTI(?ui), i = 1, . . . , nu,
j = 1, . . . ,mi
Figure 3: Model parameters: nc is # different segments,
nu is # underlying segments
acter (skn = Cj for j ? nc, n ? mk) given the
underlying character it is generated from (uk = Ci
for i ? nc, k ? nu), which is determined by its po-
sition in the paradigm.
In our Finnish example (Figure 1), if k = 1, we
are looking at the first underlying character, which
is /t/ (from /tieta?/), so assuming our character set is
the Finnish alphabet, of which ?t? is the 20th char-
acter, u1 = C20 = t. It generates the first character
of each inflected form (1st, 2nd, 3rd person, singu-
lar and plural) of that stem, so m1 = 6, and since
there is no alternation s1n = t (for n ? {1, . . . , 6}).
Given the phonologically plausible (gold) underly-
ing forms, the probability of /t/ is ?20 = 7/41.
On the other hand, k = 33 identifies the 3rd per-
son singular /?/, which inflects each of the seven
stems, so m33 = 7. Since we need our alpha-
bet to identify a null character, we?ll give it in-
dex zero (i.e., u33 = C0 = ?). For each of the
(underlying, surface) alignments in this alternation
(caused by vowel gemination), we can identify the
probability in ?. For 3rd person singular [tieta?+a?],
where s33,1 = C28 = a?, the conditional probability
?0,28 = 1/7.
The prior hyperparameters can be understood as
follows. As ?i gets smaller, an underlying form uk
is less likely to be Ci. As ?ij gets smaller, an un-
derlying uk = Ci is less likely to generate a surface
segment skn = Cj ?n ? mk. In our experiments,
we will vary ?i=j (prior over identity map from un-
derlying to surface) and ?i6=j .
Our implementation of this model uses Gibbs
sampling (c.f., (Bishop, 2006), pp 542-8), an algo-
rithm that produces samples from the posterior dis-
tribution. Each sample is an assignment of the hid-
den variables, u (i.e., a set of hypothesized underly-
ing forms). Our sampler initializes u from a uniform
distribution over segments in the training data, and
resamples underlying forms in a fixed order, as in-
put in the paradigm. Rather than reestimate ? and
? at each iteration before sampling from u, we can
marginalize these intermediate probability distribu-
tions in order to ease implementation and speed con-
vergence.
Our search procedure tries to sample from the
posterior probability, according to Bayes? rule.
posterior ? likelihood ? prior
P (u, s|?,?) ? P (u|?)P (s, u|?)
Each of these probabilities is drawn from a Dirichlet
distribution, which is defined in terms of the multi-
variate Beta function, C . The prior ? added to un-
derlying counts N(u) forms the posterior Dirichlet
corresponding to P (u|?). In P (s|u,?), each ?i
vector is supplemented by the observed counts of
(underlying, surface) pairs N(si).
P (u, s|?,?) = C(? + N(u))C(?)
nc
?
c=1
C(?c +
?
i:ui=c N(si))
C(?)
The collapsed update procedure consists of re-
sampling each underlying form, u, incorporating the
prior hyperparameters ?,? and counts N over the
rest of the dataset. The relevant counts for a can-
didate k being the underlying form ui are Nk(u?i)
and Nksij (u?i) for j ? mi. P (ui = k|u?i,?,?) is
proportional to the probability of generating ui = k,
given the other u?i and all sij (for j ? mi), given
s?i and u?i.
P (ui = c|u?i,?,?) ? Nc(u?i) + ?cn? 1 + ??
C(? + ?i? 6=i:ui?=c N(s
?
i) + N(si))
C(? + ?i? 6=i:ui?=c N(s
?
i))
Suppose we were updating this sampler running
on the Finnish verb inflections. If we had all seg-
ments as in Figure 1, but wanted to resample u31 (1st
person singular /n/), we would consider the counts
N excluding that form (i.e., under u?31). The prior
for /n/, ?14, is fixed, and there are no other occur-
rences, so N14(u?31) = 0. Another potential un-
derlying form, like /t/, would have higher uncondi-
tioned posterior probability, because of the counts
15
(7, in this case) added to its prior from ?. Then, we
have to multiply by the probability of each generated
surface segment (all are [n], so 7 ? P ([n]|c,?) for a
given hypothesis u31 = c).
We select a given character c ? C for u31 from a
distribution at random. Depending on the prior, /n/
will be the most likely choice, but other values are
still possible with smaller probability. The counts
used for the next resampling, N(u?31), are affected
by this choice, because the new identity of u31 has
contributed to the posterior distribution. After un-
bounded iterations, Gibbs sampling is guaranteed to
converge and produce samples from the true poste-
rior (Geman and Geman, 1984).
3 Evaluation
This model provides a language agnostic solution to
a subset of phonological problems. We will first
examine performance on the sample Finnish data
(from Figure 1), and then look more closely at the is-
sue of convergence. Finally, we present results from
larger corpora 3.
3.1 Finnish
Output from a trial run on Finnish verbs (from Fig-
ure 1) follows, with hyperparameters ?ij{100 ??
i = j, 0.05 ?? i 6= j} and ?i = {0.1}.
In the paradigm (a sample after 1000 iterations),
each [sur+face] form is followed by its hypothesized
/under/ + /lying/ morphemes.
[tieda?+n] : /tieda?/ + /n/
[tieda?+t] : /tieda?/ + /t/
[tieta?+a?] : /tieda?/ + /a?/
[tieda?+mme] : /tieda?/ + /mme/
[tieda?+tte] : /tieda?/ + /tte/
[tieta?+va?t] : /tieda?/ + /va?t/
[ai?o+n] : /ai?o/ + /n/
...
[pelka?a?+va?t] : /pelka?a?/ + /vat/
With strong enough priors (faithfulness con-
straints), our sampler often selects the most com-
mon surface form aligned with an underlying seg-
ment. Although [vat] is more common than [va?t],
we choose the latter as the purer underlying form.
So /a/ is always [a], but /a?/ can be either [a?] or [a].
32.8 million word types from Morphochallenge2007 (Ku-
rimo et al, 2007)
3.2 Convergence
Testing convergence, we run again on the sample
data (Figure 1), using ?ij = 0.1 when i 6= j and
10 when i = j and ? = 0.1, starting from different
initializations, we get the same solution.
0 10 20 30 40 50 60 70 80 90 100
2.97
2.98
2.99
3
3.01
3.02
3.03
3.04
3.05 x 10
6
Iteration
?
lo
g 
Li
ke
lih
oo
d
Figure 4: Posterior likelihood at each of the first 100 iter-
ations, from 4 runs (with different random seeds) on 10%
of the Morphochallenge dataset (?i6=j = 0.001, ?i=j =
100, ? = 0.1), indicating convergence within the first 15
iterations.
To confirm that the sampler has converged, we
output and plot trace statistics at each iteration, in-
cluding marginal probability, log likelihood, and
changes in underlying forms (i.e., variables resam-
pled). If the sampler has converged, there should no
longer be a trend (consistent slope) in any of these
statistics (as in Figure 4).
Examining the posterior probability of each se-
lected underlying form reveals interesting patterns
that also help explain the variation. In the above run,
the ambiguous segments (with surface alternations)
were drawn from the distributions (with improbable
segments elided) in Figure 5.
We expect this model to maximize the probabil-
ity of either the ?majority? solution or a solution
demonstrating selection bias. We compare likeli-
hood of the posterior sample with that of a ?phono-
logically plausible? solution (in which underlying
forms are determined by referring to formal lin-
guistic accounts of phonological derivation) and a
?majority solution? (see Figure 6 for a log-log plot,
where lower is more likely).
The posterior sample has optimal likelihood with
each parameter setting, as expected. The majority
parse is selected with ?i6=j = 0.5 With lower val-
ues of ?i6=j , the ?phonologically plausible? parse is
16
u4=/d/ s4=[d,d,t,d,d,t]
P (ui = c) ?
d 0.99968
t 0.00014
u8=/k/ s8=[?,?,k,?,?,k]
(same behavior as u12)
P (ui = c) ?
? 0.642
k 0.124
u33=/e/ s33=[a?,o,e,u,?,e,?]
P (ui = c) ?
a?,o,u 0.0029
? 0.215
a 0.0003
e 0.297
Figure 5: Resampling probabilities for alternations, after
1000 iterations.
10?2 10?1 100
104.24
104.25
104.26
alpha
?
lo
g 
lik
el
ih
oo
d
 
 
posterior sample
majority solution
phonologically plausible
Figure 6: Parse likelihood
more likely than the majority. However, the sam-
pler does not converge to this solution, because in
this [t,d] alternation, the ?phonologically plausible?
solution identifies /t/, but neither selection bias nor
majority rules would lead to that with the given data.
3.3 Morphologically segmented corpora
In our search for appropriate data for additional,
larger-scale experiments, we found several vi-
able alternatives. The correct morphological seg-
mentations for Finnish data used in Morphochal-
lenge2007 (Kurimo et al, 2007) provide a rich and
varied set of words, and are readily analyzable by
our sampler. Rather than associating each surface
form with a position in the paradigm, we use the an-
Majority Bayesian
types 50.84 69.53
tokens 65.23 72.11
Figure 7: Accuracy of underlying segment hypotheses.
notated morphemes.
For example, the word ajavalle is listed in the cor-
pus as follows:
ajavalle aja:ajaa|V va:PCP1 lle:ALL The
word is segmented into a verb stem, ?aja? (drive),
a present participle marker ?va?, and the allative suf-
fix (?for?). Each surface realization of a given mor-
pheme is identified by the same tag (e.g., PCP1).
However, in this corpus, insertion and deletion are
not explicitly marked (as they were in the paradigm,
by ?). Rather than introduce another component
to determine which segments in the form were
dropped, we ignore these cases.
The sampling algorithm proceeds as described in
section 2. To run on tokens (as opposed to types), we
incorporate another input file that contains counts
from the original text (ajavalle appeared 8 times).
The counts of each morpheme?s surface forms then
reflect the number of times that form appeared in any
word in the corpus.
3.3.1 Type or Token
In Finnish verb conjugation, 3rd person (esp. sin-
gular) forms have high frequency and tend to be un-
marked (i.e., closer to underlying). In types, un-
marked is a minority (one third), but incorporat-
ing token frequency shifts that balance, benefiting
the ?majority learner.? Among noun inflections, un-
marked has higher frequency in speech, but marked
tokens may still dominate in text. We might expect
that it is easier to learn from tokens than types, in
part because more data is often helpful.
Testing on half of the Morphochallenge 2007
Finnish data (1M word types, 5M morph types,
17.5M word tokens, 48M morph tokens), we ran
both our Bayesian model and a majority solver on
the morphological analyses, and compared against
phonologically plausible (gold) underlying forms.
Results are reported in Figure 7.
The Bayesian estimate consistently outperformed
the majority solution, and cases where the two differ
could often be ascribed to the preference for ?pure?
17
analyses.
4 Conclusion
We have described a model where surface forms
are generated from underlying representations seg-
ment by segment. Taking this approach allowed us
to investigate the properties of a Bayesian statistical
learner, and how these can be useful in the context
of sound systems, a basic component of language.
Experiments with our implementation of a collapsed
sampler have produced results largely confirming
our hypotheses.
Without context, we can often learn about 60 to 80
percent of the mapping from underlying phonemes
to surface phones. Especially with lower values of
?i6=j , closer to 0, our model does prefer pure alter-
nations. Gibbs sampling tends to select the major-
ity underlying form, particularly with ?i6=j relatively
high, closer to 1. So, a sparser prior leads us further
from the baseline, and often closer to a phonologi-
cally plausible solution.
4.1 Directions
In future research, we hope to integrate morpholog-
ical analysis into this sort of a treatment of phonol-
ogy. This is a natural approach for children learn-
ing their first language. They intuitively discover
phonotactics, and how it affects the prosodic shape
of each word, as they learn meaningful units and
compose them together. It is clear that many lay-
ers of linguistic information interact in the early
stages of child language acquisition (Demuth and
Ellis, 2005 in press), so they should also interact
in our models. As discussed above, the present
model should be applicable to analysis of language-
learners? speech errors, and this connection should
be explored in greater depth.
It might be interesting to predispose the sampler
to select underlying forms from open syllables. That
is, set ? to increase the probability of matching
one of the surface segments if its context (feature
annotations) includes a vocalic segment or a word
boundary immediately following. The probability
of phonological processes like assimilation could be
similarly modeled, with the prior higher for choos-
ing a segment that appears on the surface in a con-
trastive context (where it shares few features with
neighboring segments).
If we define a MaxEnt distribution over Optimal-
ity Theoretic constraints, we might use that to in-
form our selection of underlying forms. In (Gold-
water and Johnson, 2003), the learning algorithm
was given a set of candidate surface forms asso-
ciated with an underlying form, and tried to opti-
mize the constraint weights. In addition to the con-
straint weights, we must also optimize the underly-
ing form, since our goal is to take as input only ob-
servable data. Sampling from this type of complex
distribution is quite difficult, but some approaches
(e.g., (Murray et al, 2006)) may help reduce the in-
tractability.
References
Kenneth R. Beesley and Lauri Karttunen. 2000. Finite-
state non-concatenative morphotactics. In Lauri Kart-
tunen, Jason Eisner, and Alain The?riault, editors, SIG-
PHON2000, August 6 2000. Proceedings of the Fifth
Workshop of the ACL Special Interest Group in Com-
putational Phonology., pages 1?12.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer, August.
Hal Daume III. 2007. Hbc: Hierarchical bayes compiler.
Katherine Demuth and David Ellis, 2005 (in press). Re-
visiting the acquisition of Sesotho noun class prefixes.
Lawrence Erlbaum.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, gibbs distributions, and the bayesian restora-
tion of images. IEEE Trans. Pattern Anal. Machine
Intell., 6(6):721?741, Nov.
Dale Gerdemann and Gertjan van Noord. 2000. Approx-
imation and exactness in finite state optimality theory.
Daniel Gildea and Daniel Jurafsky. 1996. Learning bias
and phonological-rule induction. Computational Lin-
guistics, 22(4):497?530.
Sharon Goldwater and Mark Johnson. 2003. Learning ot
constraint rankings using a maximum entropy model.
Sharon Goldwater and Mark Johnson. 2004. Priors in
Bayesian learning of phonological rules. In Proceed-
ings of the Seventh Meeting of the ACL Special Inter-
est Group in Computational Phonology, pages 35?42,
Barcelona, Spain, July. Association for Computational
Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Scho?lkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641?648. MIT Press, Cambridge, MA.
18
Lauri Karttunen. 1998. The proper treatment of optimal-
ity in computational phonology. In Lauri Karttunen,
editor, FSMNLP?98: International Workshop on Fi-
nite State Methods in Natural Language Processing,
pages 1?12. Association for Computational Linguis-
tics, Somerset, New Jersey.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of morpho challenge in clef 2007.
In Working Notes for the CLEF 2007 Workshop, Bu-
dapest, Hungary.
Percy Liang and Dan Klein. 2007. Tutorial 1: Bayesian
nonparametric structured models, June.
Iain Murray, Zoubin Ghahramani, and David MacKay.
2006. MCMC for doubly-intractable distributions. In
UAI. AUAI Press.
19
