Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 801?809, Prague, June 2007. c?2007 Association for Computational Linguistics
Finding Good Sequential Model Structures
using Output Transformations
Edward Loper
Department of Computer & Information Science
University of Pennsylvania
3330 Walnut Street
Philadelphia, PA 19104
edloper@cis.upenn.edu
Abstract
In Sequential Viterbi Models, such as
HMMs, MEMMs, and Linear Chain CRFs,
the type of patterns over output sequences
that can be learned by the model depend di-
rectly on the model?s structure: any pattern
that spans more output tags than are covered
by the models? order will be very difficult
to learn. However, increasing a model?s or-
der can lead to an increase in the number of
model parameters, making the model more
susceptible to sparse data problems.
This paper shows how the notion of output
transformation can be used to explore a va-
riety of alternative model structures. Us-
ing output transformations, we can selec-
tively increase the amount of contextual in-
formation available for some conditions, but
not for others, thus allowing us to capture
longer-distance consistencies while avoid-
ing unnecessary increases to the model?s pa-
rameter space. The appropriate output trans-
formation for a given task can be selected by
applying a hill-climbing approach to held-
out data. On the NP Chunking task, our
hill-climbing system finds a model structure
that outperforms both first-order and second-
order models with the same input feature set.
1 Sequence Prediction
A sequence prediction task is a task whose input is
a sequence and whose output is a corresponding se-
quence. Examples of sequence prediction tasks in-
clude part-of-speech tagging, where a sequence of
words is mapped to a sequence of part-of-speech
tags; and IOB noun phrase chunking, where a se-
quence of words is mapped to a sequence of labels,
I, O, and B, indicating whether each word is inside a
chunk, outside a chunk, or at the boundary between
two chunks, respectively.
In sequence prediction tasks, we are interested in
finding the most likely output sequence for a given
input. In order to be considered likely, an output
value must be consistent with the input value, but it
must also be internally consistent. For example, in
part-of-speech tagging, the sequence ?preposition-
verb? is highly unlikely; so we should reject an out-
put value that contains that sequence, even if the
individual tags are good candidates for describing
their respective words.
2 Sequential Viterbi Models
This intuition is captured in many sequence learning
models, including HiddenMarkovModels (HMMs),
Maximum Entropy Markov Models (MEMMs), and
Linear Chain Conditional Random Fields (LC-
CRFs), by including terms corresponding to pieces
of output structure in their scoring functions. (Sha
and Pereira, 2003; Sutton andMcCallum, 2006; Mc-
Callum et al, 2000; Alpaydin, 2004)
Each of these Sequential Viterbi Models defines
a set of scoring functions that evaluate fixed-size
pieces of the output sequence based on fixed-size
pieces of the input sequence.1 The overall score for
1For HMMs and MEMMs, the local scores are negative log
probabilities. For LC-CRFs, the local scores do not have any
direct probabilistic interpretation.
801
(a)
(b)
(c)
(d)
Figure 1: Common Model Structures. (a) Simple
first order. (b) Extended first order. (c) Simple sec-
ond order. (d) Extended second order.
an output value is then computed by summing the
scores for all its fixed-size pieces. Sequence predic-
tion models can differ from one another along two
dimensions:
1. Model Structure: The set of output pieces and
input pieces for which local scoring functions
are defined.
2. Model Type: The set of parametrized equa-
tions used to define those local scoring func-
tions, and the procedures used to determine
their parameters.
In this paper, we focus on model structure. In par-
ticular, we are interested in finding a suitable model
structure for a given task and training corpus.
2.1 Common Model Structures
The model structure used by classical HMMs is the
?simple first order? structure. This model structure
defines two local scoring functions. The first scoring
function evaluates an output value in the context of
the corresponding input value; and the second scor-
ing function evaluates adjacent pairs of output val-
ues. Simple LC-CRFs often extend this structure by
adding a third local scoring function, which evalu-
ates adjacent pairs of output values in the context of
the input value corresponding to one of those out-
puts. These model structures are illustrated in Fig-
ure 1.
Because these first order structures include scor-
ing functions for adjacent pairs of output items,
they can identify and reject output values that con-
tain improbable subsequences of length two. For
example, in part-of-speech tagging, the sequence
?preposition-verb? is highly unlikely; and such
models will easily learn to reject outputs contain-
ing that sequence. However, it is much more dif-
ficult for first order models to identify improbable
subsequences of length three or more. For example,
in English texts, the sequence ?verb-noun-verb? is
much less likely than one would predict based just
on the subsequences ?verb-noun? and ?noun-verb.?
But first order models are incapable of learning that
fact.
Thus, in order to improve performance, it is of-
ten necessary to include scoring functions that span
over larger sequences. In the ?simple second order?
model structure, the local scoring function for adja-
cent pairs of output values is replaced with a scoring
function for each triple of consecutive output values.
In extended versions of this structure typically used
by LC-CRFs, scoring functions are also added that
combine output value triples with an input value.
These model structures are illustrated in Figure 1.
Similarly, third order and and fourth order models
can be used to further increase the span over which
scoring functions are defined.
Moving to higher order model structures increases
the distance over which the model can check con-
sistency. However, it also increases the number of
parameters the model must learn, making the model
more susceptible to sparse data problems. Thus, the
usefulness of a model structure for a given task will
depend on the types of constraints that are important
for the task itself, and on the size and diversity of the
training corpus.
3 Searching for Good Model Structures
We can therefore use simple search methods to look
for a suitable model structure for a given task and
training corpus. In particular, we have performed
several experiments using hill-climbing methods to
search for an appropriate model structure for a given
task. In order to apply hill-climbing methods, we
need to define:
1. The search space. I.e., concrete representations
for the set of model structures we will consider.
2. A set of operations for moving through that
search space.
802
3. An evaluation metric.
In Section 4, we will define the search space us-
ing transformations on output values. This will al-
low us to consider a wide variety of model struc-
tures without needing to make any direct modifica-
tions to the underlying sequence modelling systems.
Output value transformations will be concretely rep-
resented using Finite State Transducers (FSTs). In
Section 5, we will define the set of operations for
moving through the search space as modification op-
erations on FSTs. For the evaluation metric, we sim-
ply train and test the model, using a given model
structure, on held-out data.
4 Representing Model Structure with
Reversible Output Transformations
The common model structures described in Sec-
tion 2.1 differ from one another in that they exam-
ine varying sizes of ?windows? on the output struc-
ture. Rather than varying the size of the window, we
can achieve the same effect by fixing the window
size, but transforming the output values. For exam-
ple, consider the effects of transforming the output
values by replacing individual output tags with pairs
of adjacent output tags:
y1, y2, . . . , yt ?
?START, y1?, ?y1, y2?, ?y2, y3?, . . . , ?yt?1, yt?
E.g.:
I O O I I B I ?
OI IO OO OI II IB BI
Training a first order model based on these trans-
formed values is equivalent to training a second or-
der model based on the original values, since in each
case the local scoring functions will be based on
pairs of adjacent output tags. Similarly, transform-
ing the output values by replacing individual output
tags with triples of adjacent output tags is equivalent
to training a third order model based on the original
output values.
Of course, when we apply a model trained on this
type of transformed output to new inputs, it will gen-
erate transformed output values. Thus, the transfor-
mation must be reversible, so that we can map the
output of the model back to an un-transformed out-
put value.
This transformational approach has the advantage
that we can explore different model structures us-
ing off-the-shelf learners, without modifying them.
In particular, we can apply the transformation corre-
sponding to a given model structure to the training
corpus, and then train the off-the-shelf learner based
on that transformed corpus. To predict the value for
a new input, we simply apply the learned model to
generate a corresponding transformed output value,
and then use the inverse transformation to map that
value back to an un-transformed value.
Output encoding transformations can be used to
represent a large class of model structures, including
commonly used structures (first order, second order,
etc) as well as a number of ?hybrid? structures that
use different window sizes depending on the content
of the output tags.
Output encoding transformations can also be used
to represent a wide variety of other model struc-
tures. For example, there has been some debate
about the relative merits of different output encod-
ings for the chunking task (Tjong Kim Sang and
Veenstra, 1999; Tjong Kim Sang, 2000; Shen and
Sarkar, 2005). These encodings differ in whether
they define special tags for the beginning of chunks,
for the ends of chunks, and for boundaries between
chunks. The output transformation procedure de-
scribed here is capable of capturing all of the output
encodings used for chunking. Thus, this transforma-
tional method provides a unified framework for con-
sidering both the type of information that should be
encoded by individual tags (i.e., the encoding) and
the distance over which that information should be
evaluated (i.e., the order of the model). Under this
framework, we can use simple search procedures to
find an appropriate transformation for a given task.
4.1 Representing Transformations as FSTs
Finite State Transducers (FSTs) provide a natural
formalism for representing output transformations.
FSTs are powerful enough to capture different or-
ders of model structure, including hybrid orders; and
to capture different output encodings, such as the
ones considered in (Shen and Sarkar, 2005). FSTs
are efficient, so they add very little overhead. Fi-
nally, there exist standard algorithms for inverting
803
O:O
I:?
I:I
B:E
O:IO
O:O
I:?
I:I
B:E
O:EO
IOE1
IOE2
I:I
B:B
IOB1
O:O
O:O
I:B
I:I
B:B
O:O
IOB2
IOBES
O:O
I
:
?
I
:
B
B
:
B
O
:
S
O
B:B
I:I
O
:
S
O
Figure 2: FSTs for Five Common Chunk Encod-
ings. Each transducer takes an IOB1-encoded string
for a given output value, and generates the corre-
sponding string for the same output value, using a
new encoding. Note that the IOB1 FST is an iden-
tity transducer; and note that the transducers that
make use of the E tag must use -output edges to
delay the decision of which tag should be used until
enough information is available.
and determinizing FSTs. 2
4.1.1 Necessary Properties for
Output-Transformation FSTs
In order for an FST to be used to transform output
values, it must have the following three properties:
1. The FST?s inverse should be deterministic.3
Otherwise, we will be unable to convert
the model?s (transformed) output into an un-
transformed output value.
2. The FST should recognize exactly the set of
valid output values. If it does not recognize
some valid output value, then it won?t be able
to transform that value. If it recognizes some
invalid output value, then there exists an trans-
formed output value that would map back to an
invalid output value.
3. The FST should not modify the length of the
output sequence. Otherwise, it will not be pos-
2Note that we are not attempting to learn a transducer
that generates the output values from input values, as is done
in e.g. (Oncina et al, 1993) and (Stolcke and Omohundro,
1993). Rather, we we are interested in finding a transducer from
one output encoding to another output encoding that will be
more amenable to learning by the underlying Sequential Viterbi
Model.
3Or at least determinizable.
sible to align the output values with input val-
ues when running the model.
In addition, it seems desirable for the FST to have
the following two properties:
4. The FST should be deterministic. Otherwise, a
single training example?s output could be en-
coded in multiple ways, which would make
training the individual base decision classifiers
difficult.
5. The FST should generate every output string.
Otherwise, there would be some possible sys-
tem output that we are unable to map back to
an un-transformed output.
Unfortunately, these two properties, when taken to-
gether with the first three, are problematic. To see
why, assume an FST with an output alphabet of
size k. Property (5) requires that all possible out-
put strings be generated, and property (1) requires
that no string is generated for two input strings,
so the number of strings generated for an input
of length n must be exactly kn. But the number
of possible chunkings for an input of length n is
3n ? 3n?1 ? 3n?2; and there is no integer k such
that kn = 3n ? 3n?1 ? 3n?2.4
We must therefore relax at least one of these two
properties. Relaxing the property 4 (deterministic
FSTs) will make training harder; and relaxing the
property 5 (complete FSTs) will make testing harder.
In the experiments presented here, we chose to relax
the second property.
4.1.2 Inverting the Transformation
Recall that the motivation behind property 5 is
that we need a way to map any output generated
by the machine learning system back to an un-
transformed output value.
As an alternative to requiring that the FST gener-
ate every output string, we can define an extended
inversion function, that includes the inverted FST,
but also generates output values for transformed val-
ues that are not generated by the FST. In particular,
4To see why the number of possible chunkings is 3n ?
3n?1 ? 3n?2, consider the IOB1 encoding: it generates all
chunkings, and is valid for any of the 3n strings except those
that start with B (of which there are 3n?1) and those that in-
clude the sequence OB (of which there are 3n?2).
804
in cases where the transformed value is not gener-
ated by the FST, we can assume that one or more
of the transformed tags was chosen incorrectly; and
make the minimal set of changes to those tags that
results in a string that is generated by the FST. Thus,
we can compute the optimal un-transformed output
value corresponding to each transformed output us-
ing the following procedure:
1. Invert the original FST. I.e., replace each arc
?S ? Q[? : ?]? with an arc ?S ? Q[? : ?]?.
2. Normalize the FST such that each arc has ex-
actly one input symbol.
3. Convert the FST to a weighted FST by as-
signing a weight of zero to all arcs. This
weighted FST uses non-negative real-valued
weights, and the weight of a path is the sum
of the weights of all edges in that path.
4. For each arc ?S ? Q[x : ?]?, and each y 6= x,
add a new arc ?S ? Q[y : ?]? with a weight
one.
5. Determinize the resulting FST, using a vari-
ant of the algorithm presented in (Mohri,
1997). This determinization algorithm will
prune paths that have non-optimal weights.
In cases where determinization algorithm has
not completed by the time it creates 10,000
states, the candidate FST is assumed to be non-
determinizable, and the original FST is rejected
as a candidate.
The resulting FST will accept all sequences of
transformed tags, and will generate for each trans-
formed tag the un-transformed output value that is
generated with the fewest number of ?repairs? made
to the transformed tags.
5 FST Modification Operations
In order to search the space of output-transforming
FSTs, we must define a set of modification oper-
ations, that generate a new FST from a previous
FST. In order to support a hill-climbing search
strategy, these modification operations should make
small incremental changes to the FSTs. The selec-
tion of appropriate modification operations is impor-
tant, since it will significantly impact the efficiency
of the search process. In this section, I describe the
set of FST modification operations that are used for
the experiments described in this paper. These oper-
ations were chosen based our intuitions about what
modifications would support efficient hill-climbing
search. In future experiments, we plan to examine
alternative modification operations.
5.1 New Output Tag
The new output tag operation replaces an arc ?S ?
Q[? : ?x?]? with an arc ?S ? Q[? : ?y?]?, where
y is a new output tag that is not used anywhere else
in the transducer. When a single output tag appears
on multiple arcs, this operation effectively splits that
tag in two. For example, when applied to the identity
transducer for the IOB1 encoding shown in Figure 2,
this operation can be used to distinguish O tags that
follow other O tags from O tags that follow I or B
tags ? effectively increasing the order of the model
structure for just O tags.
5.2 Specialize Output Tag5
The specialize output tag operation is similar to the
new output tag operation, but rather than replacing
the output tag with a new tag, we ?subdivide? the
tag. When the model is trained, features will be in-
cluded for both the subdivided tag and the original
(undivided) tag.
5.3 Loop Unrolling
The loop unrolling operation acts on a single self-
loop arc e at a state S, and makes the following
changes to the FST:
1. Create a new state S?.
2. For each outgoing arc e1 = ?S ? Q[? : ?]? 6=
e, add add an arc e2 = ?S? ? Q[? : ?]?. Note
that if e1 was a self-loop arc (i.e., S = Q), then
e2 will point from S? to S.
3. Change the destination of loop arc e from S to
S?.
By itself, the loop unrolling operation just mod-
ifies the structure of the FST, but does not change
5This operation requires the use of a model where features
are defined over (input,output) pairs, such as MEMMs or LC-
CRFs.
805
the actual transduction performed by the FST. It is
therefore always immediately followed by applying
the new output tag operation or the specialize output
tag operation to the loop arc e.
5.4 Copy Tag Forward
The copy tag forward operation splits an existing
state in two, directing all incoming edges that gen-
erate a designated output tag to one copy, and all
remaining incoming edges to the other copy. The
outgoing edges of these two states are then distin-
guished from one another, using either the specialize
output tag operation (if available) or the new output
tag operation.
This modification operation creates separate
edges for different output histories, effectively in-
creasing the ?window size? of tags that pass through
the state.
5.5 Copy State Forward
The copy state forward operation is similar to the
copy tag forward operation; but rather than redirect-
ing incoming edges based on what output tags they
generate, it redirects incoming edges based on what
state they originate from. This modification opera-
tion allows the FST to encode information about the
history of states in the transformational FST as part
of the model structure.
5.6 Copy Feature Forward
The copy feature forward operation is similar to the
copy tag forward operation; but rather than redirect-
ing incoming edges based on what output tags they
generate, it redirects incoming edges based on a fea-
ture of the current input value. This modification op-
eration allows the transformation to subdivide out-
put tags based on features of the input value.
6 Hill Climbing System
Having defined a search space, a set of transforma-
tions to explore that space, and an evaluation met-
ric, we can use a hill-climbing system to search for
a good model structure. This approach starts with
a simple initial FST, and makes incremental local
changes to that FST until a locally optimal FST is
found. In order to help avoid sub-optimal local max-
ima, we use a fixed-size beam search. To increase
the search speed, we used a 12-machine cluster to
evaluate candidate FSTs in parallel. The hill climb-
ing system iteratively performs the following proce-
dure:
1. Initialize candidates to be the singleton set
containing the identity transducer.
2. Repeat ...
(a) Generate a new FST, by applying a ran-
dom modification operation to a randomly
selected member of the candidates
set.
(b) Evaluate the new FSTs, and test its perfor-
mance on the held-out data set. (This is
done in parallel.)
(c) Once the FST has been evaluated, add it to
the candidates set.
(d) Sort the candidates set by their score
on the held-out data, and discard all but
the 10 highest-scoring candidates.
... until no improvement is made for twenty
consecutive iterations.
3. Return the candidate FST with the highest
score.
7 Noun Phrase Chunking Experiment
In order to test this approach to finding a good model
structure, we applied our hill-climbing system to the
task of noun phrase chunking. The base system
was a Linear Chain CRF, implemented using Mal-
let (McCallum, 2002). The set of features used are
listed in Figure 1. Training and testing were per-
formed using the noun phrase chunking corpus de-
scribed in Ramshaw & Marcus (1995) (Ramshaw
and Marcus, 1995). A randomly selected 10% of the
original training corpus was used as held-out data,
to provide feedback to the hill-climbing system.
7.1 NP Chunking Experiment: Results
Over 100 iterations, the hill-climbing system in-
creased chunking performance on the held-out data
from a F-score of 94.93 to an F-score of 95.32.
This increase was reflected in an improvement on
the test data from an F-score of 92.48 to an F-score
806
Feature Description
yi The current output tag.
yi, wi+n A tuple of the current output tag and
the i + nth word, ?2 ? n ? 2.
yi, wi, wi?1 A tuple of the current output tag, the
current word, and the previous word.
yi, wi, wi+1 A tuple of the current output tag, the
current word, and the next word.
yi, ti+n A tuple of the current output tag and
the part of speech tag of the i + nth
word, ?2 ? n ? 2.
yi, ti+n,
ti+n+1
A tuple of the current output tag and
the two consecutive part of speech
tags starting at word i + n, ?2 ?
n ? 1.
yi+n?1, ti+n,
ti+n+1
A tuple of the current output tag, and
three consecutive part of speech tags
centered on word i+n,?1 ? n ? 1.
Table 1: Feature Set for the CRF NP Chunker. yi
is the ith output tag; wi is the ith word; and ti is the
part-of-speech tag for the ith word.
System F1 (Held-out) F1 (Test)
Baseline (first order) 94.93 92.48
Second order 95.14 92.63
Learned structure 95.32 92.80
Table 2: Results for NP Chunking Experiment.
of 92.80.6 As a point of comparison, a simple sec-
ond order model achieves an intermediate F-score of
92.63 on the test data. Thus, the model learned by
the hill-climbing system outperforms both the sim-
ple first-order model and the simple second-order
model.
Figure 3 shows how the scores of FSTs on held-
out data changed as the hill-climbing system ran.
Figure 4 shows the search tree explored by the hill-
climbing system.
6The reason that held-out scores are significantly higher than
test scores is that held-out data was taken from the same sec-
tions of the original corpus as the training data; but test data was
taken from new sections. Thus, there was more lexical overlap
between the training data and the held-out data than between
the training data and the testing data.
...
Figure 3: Performance on Heldout Data for NP
Chunking Experiment. In this graph, each point
corresponds to a single transducer generated by the
hill-climbing system. The height of each trans-
ducer?s point indicates its score on held-out data.
The line indicates the highest score that has been
achieved on the held-out data by any transducer.
Figure 4: Hill Climbing Search Tree for NP
Chunking Experiment This tree shows the ?an-
cestry? of each transducer tried by the hill climb-
ing system. Lighter colors indicate higher scores
on the held-out data. After one hundred iterations,
the five highest scoring transducers were fst047,
fst058, fst083, fst102, and fst089.
807
S0
S
1
S
2
O:O
1
O
:
O
1
O:O
1
B:B
1
B
:
B
2
I
:
I
1
I
+
<
t
[
-
2
;
-
1
]
=
N
N
P
;
,
>
:
I
2
I
-
<
t
[
-
2
;
-
1
]
=
N
N
P
;
,
>
:
I
3
I
+<t[-2;-1]=NNP;,>
:I
2
I
-<t[-2;-1]=NNP;,>
:I
3
Figure 5: Final FST. The highest-scoring FST gen-
erated by the hill-climbing algorithm, after a run of
100 iterations. For a discussion of this transducer,
see Section 7.1.1.
7.1.1 NP Chunking Experiment: The Selected
Transformation
Figure 5 shows the FST for the best output trans-
formation found after 100 iterations of the hill-
climbing algorithm. Inspection of this FST reveals
that it transforms the original set of three tags (I , O,
and B) to six new tags: I1, I2, I3, O, B1, and B2.
The first three of these tags are used at the begin-
ning of a chunk: I1 is used if the preceding tag was
O; B1 is used if the preceding tag was B; and B2
is used if the preceding tag was I . This is similar to
a second order model, in that it records information
about both the current tag and the previous tag.
The next tag, O, is used for all words outside of
chunks. Thus, the hill-climbing system found that
increasing the window size used for O chunks does
not help to learn any useful constraints with neigh-
boring tags.
Finally, two tags are used for words that are inside
a chunk, but not at the beginning of the chunk: I2
and I3. The choice of which tag should be used de-
pends on the input feature that tests whether the cur-
rent word is a comma, and the previous word was a
proper noun (NNP). At first, this might seem like an
odd feature to distinguish. But note that in the Wall
Street Journal, it is quite common for proper nouns
to include internal commas; but for other nouns, it is
fairly uncommon. By dividing the I tag in two based
on this feature, the model can use separate distribu-
tions for these two cases. Thus, the model avoids
conflating two contexts that are significantly differ-
ent from one another for the task at hand.
8 Discussion
Sequential Viterbi Models are capable of learning to
model the probability of local patterns on the out-
put structure. But the distance that these patterns
can span is limited by the model?s structure. This
distance can be lengthened by moving to higher or-
der model structures, but only at the expense of an
increase in the number of model parameters, along
with the data sparsity issues that can arise from that
increase. Therefore, it makes sense to be more selec-
tive about how we extend the model structure. Using
reversible output transformations, it is possible to
define model structures that extend the reach of the
model only where necessary. And as we have shown
here, it is possible to find a suitable output transfor-
mation for a given task by using simple search pro-
cedures.
9 Acknowledgements
We gratefully acknowledge the support of the
National Science Foundation Grant NSF-0415923,
Word Sense Disambiguation, the DTO-AQUAINT
NBCHC040036 grant under the University of
Illinois subcontract to University of Penn-
sylvania 2003-07911-01, NSF-ITR-0325646:
Domain-Independent Semantic Interpretation, and
Defense Advanced Research Projects Agency
(DARPA/IPTO) under the GALE program,
DARPA/CMO Contract No. HR0011-06-C-0022.
References
Ethem Alpaydin, 2004. Introduction to Machine Learn-
ing, chapter 7. The MIT Press.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov models
for information extraction and segmentation. In Proc.
17th International Conf. on Machine Learning, pages
591?598. Morgan Kaufmann, San Francisco, CA.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
808
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2):269?311.
Jose? Oncina, Pedro Garc??a, and Enrique Vidal. 1993.
Learning subsequential transducers for pattern recog-
nition tasks. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 15:448?458, May.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarowsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
82?94, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings of
HLT-NAACL, pages 134?141.
Hong Shen and Anoop Sarkar. 2005. Voting between
multiple data representations for text chunking. In Ad-
vances in Artificial Intelligence: 18th Conference of
the Canadian Society for Computational Studies of In-
telligence, May.
Andreas Stolcke and Stephen Omohundro. 1993. Hidden
Markov Model induction by Bayesian model merging.
In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors,
Advances in Neural Information Processing Systems 5.
Morgan Kaufman, San Mateo, Ca.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press. To appear.
Erik Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In Proceedings of EACL?99,
Bergen. Association for Computational Linguistics.
Erik Tjong Kim Sang. 2000. Noun phrase recognition
by system combination. In Proceedings of BNAIC,
Tilburg, The Netherlands.
809
