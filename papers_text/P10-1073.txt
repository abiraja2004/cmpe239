Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 710?719,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Kernel Based Discourse Relation Recognition with Temporal  
Ordering Information 
 
 
WenTing Wang1                   Jian Su1                   Chew Lim Tan2 
1Institute for Infocomm Research 
1 Fusionopolis Way, #21-01 Connexis 
Singapore 138632 
{wwang,sujian}@i2r.a-star.edu.sg 
2Department of Computer Science 
University of Singapore 
Singapore 117417 
tacl@comp.nus.edu.sg 
 
  
 
Abstract 
Syntactic knowledge is important for dis-
course relation recognition. Yet only heu-
ristically selected flat paths and 2-level 
production rules have been used to incor-
porate such information so far. In this 
paper we propose using tree kernel based 
approach to automatically mine the syn-
tactic information from the parse trees for 
discourse analysis, applying kernel func-
tion to the tree structures directly. These 
structural syntactic features, together 
with other normal flat features are incor-
porated into our composite kernel to cap-
ture diverse knowledge for simultaneous 
discourse identification and classification 
for both explicit and implicit relations. 
The experiment shows tree kernel ap-
proach is able to give statistical signifi-
cant improvements over flat syntactic 
path feature. We also illustrate that tree 
kernel approach covers more structure in-
formation than the production rules, 
which allows tree kernel to further incor-
porate information from a higher dimen-
sion space for possible better discrimina-
tion. Besides, we further propose to leve-
rage on temporal ordering information to 
constrain the interpretation of discourse 
relation, which also demonstrate statistic-
al significant improvements for discourse 
relation recognition on PDTB 2.0 for 
both explicit and implicit as well. 
1 Introduction 
Discourse relations capture the internal structure 
and logical relationship of coherent text, includ-
ing Temporal, Causal and Contrastive relations 
etc. The ability of recognizing such relations be-
tween text units including identifying and classi-
fying provides important information to other 
natural language processing systems, such as 
language generation, document summarization, 
and question answering. For example, Causal 
relation can be used to answer more sophisti-
cated, non-factoid ?Why? questions. 
Lee et al (2006) demonstrates that modeling 
discourse structure requires prior linguistic anal-
ysis on syntax. This shows the importance of 
syntactic knowledge to discourse analysis. How-
ever, most of previous work only deploys lexical 
and semantic features (Marcu and Echihabi, 
2002; Pettibone and PonBarry, 2003; Saito et al, 
2006; Ben and James, 2007; Lin et al, 2009; Pit-
ler et al, 2009) with only two exceptions (Ben 
and James, 2007; Lin et al, 2009). Nevertheless, 
Ben and James (2007) only uses flat syntactic 
path connecting connective and arguments in the 
parse tree. The hierarchical structured informa-
tion in the trees is not well preserved in their flat 
syntactic path features. Besides, such a syntactic 
feature selected and defined according to linguis-
tic intuition has its limitation, as it remains un-
clear what kinds of syntactic heuristics are effec-
tive for discourse analysis. 
The more recent work from Lin et al (2009) 
uses 2-level production rules to represent parse 
tree information. Yet it doesn?t cover all the oth-
er sub-trees structural information which can be 
also useful for the recognition. 
In this paper we propose using tree kernel 
based method to automatically mine the syntactic 
710
information from the parse trees for discourse 
analysis, applying kernel function to the parse 
tree structures directly. These structural syntactic 
features, together with other flat features are then 
incorporated into our composite kernel to capture 
diverse knowledge for simultaneous discourse 
identification and classification. The experiment    
shows that tree kernel is able to effectively in-
corporate syntactic structural information and 
produce statistical significant improvements over 
flat syntactic path feature for the recognition of 
both explicit and implicit relation in Penn Dis-
course Treebank (PDTB; Prasad et al, 2008). 
We also illustrate that tree kernel approach cov-
ers more structure information than the produc-
tion rules, which allows tree kernel to further 
work on a higher dimensional space for possible 
better discrimination. 
Besides, inspired by the linguistic study on 
tense and discourse anaphor (Webber, 1988), we 
further propose to incorporate temporal ordering 
information to constrain the interpretation of dis-
course relation, which also demonstrates statis-
tical significant improvements for discourse rela-
tion recognition on PDTB v2.0 for both explicit 
and implicit relations. 
The organization of the rest of the paper is as 
follows. We briefly introduce PDTB in Section 
2. Section 3 gives the related work on tree kernel 
approach in NLP and its difference with produc-
tion rules, and also linguistic study on tense and 
discourse anaphor. Section 4 introduces the 
frame work for discourse recognition, as well as 
the baseline feature space and the SVM classifi-
er. We present our kernel-based method in Sec-
tion 5, and the usage of temporal ordering feature 
in Section 6. Section 7 shows the experiments 
and discussions.  We conclude our works in Sec-
tion 8. 
2 Penn Discourse Tree Bank 
The Penn Discourse Treebank (PDTB) is the 
largest available annotated corpora of discourse 
relations (Prasad et al, 2008) over 2,312 Wall 
Street Journal articles. The PDTB models dis-
course relation in the predicate-argument view, 
where a discourse connective (e.g., but) is treated 
as a predicate taking two text spans as its argu-
ments. The argument that the discourse connec-
tive syntactically bounds to is called Arg2, and 
the other argument is called Arg1. 
The PDTB provides annotations for both ex-
plicit and implicit discourse relations. An explicit 
relation is triggered by an explicit connective. 
Example (1) shows an explicit Contrast relation 
signaled by the discourse connective ?but?. 
 
     (1). Arg1. Yesterday, the retailing and finan-
cial services giant reported a 16% drop in 
third-quarter earnings to $257.5 million, 
or 75 cents a share, from a restated $305 
million, or 80 cents a share, a year earlier. 
             Arg2. But the news was even worse for 
Sears's core U.S. retailing operation, the 
largest in the nation. 
 
In the PDTB, local implicit relations are also 
annotated. The annotators insert a connective 
expression that best conveys the inferred implicit 
relation between adjacent sentences within the 
same paragraph. In Example (2), the annotators 
select ?because? as the most appropriate connec-
tive to express the inferred Causal relation be-
tween the sentences. There is one special label 
AltLex pre-defined for cases where the insertion 
of an Implicit connective to express an inferred 
relation led to a redundancy in the expression of 
the relation. In Example (3), the Causal relation 
derived between sentences is alternatively lexi-
calized by some non-connective expression 
shown in square brackets, so no implicit connec-
tive is inserted. In our experiments, we treat Alt-
Lex Relations the same way as normal Implicit 
relations. 
 
     (2). Arg1. Some have raised their cash posi-
tions to record levels. 
            Arg2. Implicit = Because High cash po-
sitions help buffer a fund when the market 
falls. 
 
     (3). Arg1. Ms. Bartlett?s previous work, 
which earned her an international reputa-
tion in the non-horticultural art world, of-
ten took gardens as its nominal subject. 
             Arg2. [Mayhap this metaphorical con-
nection made] the BPC Fine Arts Com-
mittee think she had a literal green thumb. 
 
The PDTB also captures two non-implicit cas-
es: (a) Entity relation where the relation between 
adjacent sentences is based on entity coherence 
(Knott et al, 2001) as in Example (4); and (b) No 
relation where no discourse or entity-based cohe-
rence relation can be inferred between adjacent 
sentences. 
 
711
    (4).   But for South Garden, the grid was to be 
a 3-D network of masonry or hedge walls 
with real plants inside them. 
              In a Letter to the BPCA, kelly/varnell 
called this ?arbitrary and amateurish.? 
 
Each Explicit, Implicit and AltLex relation is 
annotated with a sense. The senses in PDTB are 
arranged in a three-level hierarchy. The top level 
has four tags representing four major semantic 
classes: Temporal, Contingency, Comparison 
and Expansion. For each class, a second level of 
types is defined to further refine the semantic of 
the class levels. For example, Contingency has 
two types Cause and Condition. A third level of 
subtype specifies the semantic contribution of 
each argument. In our experiments, we use only 
the top level of the sense annotations. 
3 Related Work 
Tree Kernel based Approach in NLP.  While 
the feature based approach may not be able to 
fully utilize the syntactic information in a parse 
tree, an alternative to the feature-based methods, 
tree kernel methods (Haussler, 1999) have been 
proposed to implicitly explore features in a high 
dimensional space by employing a kernel func-
tion to calculate the similarity between two ob-
jects directly. In particular, the kernel methods 
could be very effective at reducing the burden of 
feature engineering for structured objects in NLP 
research (Culotta and Sorensen, 2004). This is 
because a kernel can measure the similarity be-
tween two discrete structured objects by directly 
using the original representation of the objects 
instead of explicitly enumerating their features. 
Indeed, using kernel methods to mine structur-
al knowledge has shown success in some NLP 
applications like parsing (Collins and Duffy, 
2001; Moschitti, 2004) and relation extraction 
(Zelenko et al, 2003; Zhang et al, 2006). How-
ever, to our knowledge, the application of such a 
technique to discourse relation recognition still 
remains unexplored. 
Lin et al (2009) has explored the 2-level pro-
duction rules for discourse analysis. However, 
Figure 1 shows that only 2-level sub-tree struc-
tures (e.g. ?? - ?? ) are covered in production 
rules. Other sub-trees beyond 2-level (e.g. ?? - ?? ) 
are only captured in the tree kernel, which allows 
tree kernel to further leverage on information 
from higher dimension space for possible better 
discrimination. Especially, when there are 
enough training data, this is similar to the study  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
on language modeling that N-gram beyond uni-
gram and bigram further improves the perfor-
mance in large corpus. 
Tense and Temporal Ordering Information.   
Linguistic studies (Webber, 1988) show that a 
tensed clause ??  provides two pieces of semantic 
information: (a) a description of an event (or sit-
uation) ?? ; and (b) a particular configuration of 
the point of event (??), the point of reference 
(??) and the point of speech (??). Both the cha-
racteristics of ??  and the configuration of ??, ?? 
and ?? are critical to interpret the relationship of 
event ??  with other events in the discourse mod-
el. Our observation on temporal ordering infor-
mation is in line with the above, which is also 
incorporated in our discourse analyzer. 
4 The Recognition Framework 
In the learning framework, a training or testing 
instance is formed by a non-overlapping 
clause(s)/sentence(s) pair. Specifically, since im-
plicit relations in PDTB are defined to be local, 
only clauses from adjacent sentences are paired 
for implicit cases. During training, for each dis-
course relation encountered, a positive instance 
is created by pairing the two arguments. Also a 
Figure 1. Different sub-tree sets for ?1 used by 
2-level production rules and convolution tree 
kernel approaches. ?? -??  and ?1  itself are cov-
ered by tree kernel, while only ?? -??  are covered 
by production rules. 
Decomposition 
C 
E 
G 
F 
H 
A 
B 
D 
(?1) A 
B C 
(??) D 
F E 
(??) 
C 
D 
(??) E 
G 
(??) 
F 
H 
(??) 
D 
E 
G 
F 
H 
(??) (??) A 
C 
D 
B 
D 
E 
G 
F 
H 
C 
(?? ) C (??) 
D 
F E 
(??) A 
C 
D 
B 
F E 
712
set of negative instances is formed by paring 
each argument with neighboring non-argument 
clauses or sentences. Based on the training in-
stances, a binary classifier is generated for each 
type using a particular learning algorithm. Dur-
ing resolution, (a) clauses within same sentence 
and sentences within three-sentence spans are 
paired to form an explicit testing instance; and 
(b) neighboring sentences within three-sentence 
spans are paired to form an implicit testing in-
stance. The instance is presented to each explicit 
or implicit relation classifier which then returns a 
class label with a confidence value indicating the 
likelihood that the candidate pair holds a particu-
lar discourse relation. The relation with the high-
est confidence value will be assigned to the pair. 
4.1 Base Features 
In our system, the base features adopted include 
lexical pair, distance and attribution etc. as listed 
in Table 1. All these base features have been 
proved effective for discourse analysis in pre-
vious work. 
 
 
 
4.2 Support Vector Machine 
In theory, any discriminative learning algorithm 
is applicable to learn the classifier for discourse 
analysis. In our study, we use Support Vector 
Machine (Vapnik, 1995) to allow the use of ker-
nels to incorporate the structure feature. 
Suppose the training set ? consists of labeled 
vectors { ?? ,?? }, where ??  is the feature vector 
of a training instance and ??  is its class label. The 
classifier learned by SVM is: 
? ? = ???   ????? ? ?? + ?
?=1
  
where ??  is the learned parameter for a feature 
vector ?? , and ? is another parameter which can 
be derived from ??  . A testing instance ? is clas-
sified as positive if ? ? > 01. 
One advantage of SVM is that we can use tree 
kernel approach to capture syntactic parse tree 
information in a particular high-dimension space. 
In the next section, we will discuss how to use 
kernel to incorporate the more complex structure 
feature. 
5 Incorporating Structural Syntactic 
Information 
A parse tree that covers both discourse argu-
ments could provide us much syntactic informa-
tion related to the pair. Both the syntactic flat 
path connecting connective and arguments and 
the 2-level production rules in the parse tree used 
in previous study can be directly described by the 
tree structure. Other syntactic knowledge that 
may be helpful for discourse resolution could 
also be implicitly represented in the tree. There-
fore, by comparing the common sub-structures 
between two trees we can find out to which level 
two trees contain similar syntactic information, 
which can be done using a convolution tree ker-
nel. 
The value returned from the tree kernel re-
flects the similarity between two instances in 
syntax. Such syntactic similarity can be further 
combined with other flat linguistic features to 
compute the overall similarity between two in-
stances through a composite kernel. And thus an 
SVM classifier can be learned and then used for 
recognition. 
5.1 Structural Syntactic Feature 
Parsing is a sentence level processing. However, 
in many cases two discourse arguments do not 
occur in the same sentence. To present their syn-
tactic properties and relations in a single tree 
structure, we construct a syntax tree for each pa-
ragraph by attaching the parsing trees of all its 
sentences to an upper paragraph node. In this 
paper, we only consider discourse relations with-
in 3 sentences, which only occur within each pa-
                                                 
1 In our task, the result of ? ?  is used as the confidence 
value of the candidate argument pair ? to hold a particular 
discourse relation. 
Feature 
Names 
 Description 
(F1)  cue phrase 
(F2) neighboring punctuation 
(F3)  position of connective if 
presents 
(F4) extents of arguments 
(F5)  relative order of  arguments 
(F6)  distance between  arguments 
(F7)  grammatical role of  arguments 
(F8)  lexical pairs 
(F9) attribution  
Table 1. Base Feature Set 
713
ragraph, thus paragraph parse trees are sufficient. 
Our 3-sentence spans cover 95% discourse rela-
tion cases in PDTB v2.0. 
Having obtained the parse tree of a paragraph, 
we shall consider how to select the appropriate 
portion of the tree as the structured feature for a 
given instance. As each instance is related to two 
arguments, the structured feature at least should 
be able to cover both of these two arguments. 
Generally, the more substructure of the tree is 
included, the more syntactic information would 
be provided, but at the same time the more noisy 
information would likely be introduced. In our 
study, we examine three structured features that 
contain different substructures of the paragraph 
parse tree: 
Min-Expansion This feature records the mi-
nimal structure covering both arguments 
and connective word in the parse tree. It 
only includes the nodes occurring in the 
shortest path connecting Arg1, Arg2 and 
connective, via the nearest commonly 
commanding node. For example, consi-
dering Example (5), Figure 2 illustrates 
the representation of the structured feature 
for this relation instance. Note that the 
two clauses underlined with dashed lines 
are attributions which are not part of the 
relation. 
 
     (5). Arg1. Suppression of the book, Judge 
Oakes observed, would operate as a prior 
restraint and thus involve the First 
Amendment. 
              Arg2. Moreover, and here Judge Oakes 
went to the heart of the question, ?Respon-
sible biographers and historians constantly 
use primary sources, letters, diaries and 
memoranda.? 
 
Simple-Expansion Min-Expansion could, to 
some degree, describe the syntactic rela-
tionships between the connective and ar-
guments. However, the syntactic proper-
ties of the argument pair might not be 
captured, because the tree structure sur-
rounding the argument is not taken into 
consideration. To incorporate such infor-
mation, Simple-Expansion not only con-
tains all the nodes in Min-Expansion, but 
also includes the first-level children of  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
       these nodes2. Figure 3 illustrates such a 
feature for Example (5). We can see that 
the nodes ?PRN? in both sentences are in-
cluded in the feature. 
Full-Expansion This feature focuses on the 
tree structure between two arguments. It 
not only includes all the nodes in Simple-
Expansion, but also the nodes (beneath 
the nearest commanding parent) that cov-
er the words between the two arguments. 
Such a feature keeps the most information 
related to the argument pair. Figure 4 
                                                 
2 We will not expand the nodes denoting the sentences other 
than where the arguments occur. 
Figure 2. Min-Expansion tree built from gol-
den standard parse tree for the explicit dis-
course relation in Example (5). Note that to 
distinguish from other words, we explicitly 
mark up in the structured feature the arguments 
and connective, by appending a string tag 
?Arg1?, ?Arg2? and ?Connective? respective-
ly. 
Figure 3. Simple-Expansion tree for the expli-
cit discourse relation in Example (5).  
714
shows the structure for feature Full-
Expansion of Example (5). As illustrated, 
different from in Simple-Expansion, each 
sub-tree of ?PRN? in each sentence is ful-
ly expanded and all its children nodes are 
included in Full-Expansion. 
 
 
 
 
 
 
 
 
 
 
 
 
 
5.2 Convolution Parse Tree Kernel 
Given the parse tree defined above, we use the 
same convolution tree kernel as described in 
(Collins and Duffy, 2002) and (Moschitti, 2004). 
In general, we can represent a parse tree ? by a 
vector of integer counts of each sub-tree type 
(regardless of its ancestors):  
? ? = (#?? ???????? ?? ???? 1,? , # ??  
     ???????? ?????? ?,? , # ?? ???????? ??   
     ???? ?). 
This results in a very high dimensionality 
since the number of different sub-trees is expo-
nential in its size. Thus, it is computational in-
feasible to directly use the feature vector ?(?). 
To solve the computational issue, a tree kernel 
function is introduced to calculate the dot prod-
uct between the above high dimensional vectors 
efficiently. 
Given two tree segments ?1  and ?2 , the tree 
kernel function is defined:  
   ? ?1 ,?2 = < ? ?1 ,? ?2 > 
                   =  ? ?1  ? ,? ?2 [?]?  
                   =    ?? ?1 ? ??(?2)??2??2?1??1  
where  ?1and ?2 are the sets of all nodes in trees 
?1and ?2, respectively; and ??(?) is the indicator 
function that is 1 iff a subtree of type ?  occurs 
with root at node ? or zero otherwise. (Collins 
and Duffy, 2002) shows that ?(?1 ,?2) is an in-
stance of convolution kernels over tree struc-
tures, and can be computed in ?( ?1 ,  ?2 ) by 
the following recursive definitions: 
            ? ?1 ,?2 =  ?? ?1 ? ??(?2)?                                                                                                   
(1) ? ?1 ,?2 = 0  if ?1  and ?2  do not have the 
same syntactic tag or their children are different; 
(2) else if both ?1 and  ?2 are pre-terminals (i.e. 
POS tags), ? ?1 ,?2 = 1 ? ?; 
(3)  else, ? ?1 ,?2 = 
              ? (1 + ?(??(
?? (?1)
?=1 ?1 , ?), ??(?2 , ?))),                                 
where ??(?1) is the number of the children of 
?1 , ??(?, ?)  is the ?
??  child of node ?  and ? 
(0 < ? < 1) is the decay factor in order to make 
the kernel value less variable with respect to the 
sub-tree sizes. In addition, the recursive rule (3) 
holds because given two nodes with the same 
children, one can construct common sub-trees 
using these children and common sub-trees of 
further offspring. 
    The parse tree kernel counts the number of 
common sub-trees as the syntactic similarity 
measure between two instances. The time com-
plexity for computing this kernel is ?( ?1 ?
 ?2 ). 
5.3 Composite Tree Kernel 
Besides the above convolution parse tree kernel 
? ????  ?1 , ?2 = ?(?1 ,?2) defined to capture the 
syntactic information between two instances ?1 
and ?2, we also use another kernel ? ????  to cap-
ture other flat features, such as base features (de-
scribed in Table 1) and temporal ordering infor-
mation (described in Section 6). In our study, the 
composite kernel is defined in the following 
way: 
? 1 ?1 , ?2 = ? ? ? ????  ?1 , ?2 + 
                                    1 ? ? ? ? ????  ?1 , ?2 . 
Here, ? (?,?) can be normalized by ?  ?, ? =
? ?, ?  ? ?, ? ? ? ?, ?   and ? is the coeffi-
cient. 
6 Using Temporal Ordering Informa-
tion 
In our discourse analyzer, we also add in tem-
poral information to be used as features to pre-
dict discourse relations. This is because both our 
observations and some linguistic studies (Web-
ber, 1988) show that temporal ordering informa-
tion including tense, aspectual and event orders 
between two arguments may constrain the dis-
course relation type. For example, the connective 
Figure 4. Full-Expansion tree for the explicit 
discourse relation in Example (5).  
715
word is the same in both Example (6) and (7), 
but the tense shift from progressive form in 
clause 6.a to simple past form in clause 6.b, indi-
cating that the twisting occurred during the state 
of running the marathon, usually signals a tem-
poral discourse relation; while in Example (7), 
both clauses are in past tense and it is marked as 
a Causal relation. 
 
     (6). a. Yesterday Holly was running a mara-
thon  
            b. when she twisted her ankle. 
 
      (7). a. Use of dispersants was approved 
            b. when a test on the third day showed  
some positive results. 
 
Inspired by the linguistic model from Webber 
(1988) as described in Section 3, we explore the 
temporal order of events in two adjacent sen-
tences for discourse relation interpretation. Here 
event is represented by the head of verb, and the 
temporal order refers to the logical occurrence 
(i.e. before/at/after) between events. For in-
stance, the event ordering in Example (8) can be 
interpreted as:  
     ????? ?????? ??????? ?????(????) . 
 
     8.  a.  John went to the hospital.  
          b. He had broken his ankle on a patch of 
ice. 
 
We notice that the feasible temporal order of 
events differs for different discourse relations. 
For example, in causal relations, cause event 
usually happens before effect event, i.e.           
     ????? ????? ??????? ?????(??????). 
So it is possible to infer a causal relation in 
Example (8) if and only if 8.b is taken to be the 
cause event and 8.a is taken to be the effect 
event. That is, 8.b is taken as happening prior to 
his going into hospital. 
In our experiments, we use the TARSQI3  sys-
tem to identify event, analyze tense and aspectual 
information, and label the temporal order of 
events. Then the tense and temporal ordering 
information is extracted as features for discourse 
relation recognition. 
 
                                                 
3 http://www.isi.edu/tarsqi/ 
7 Experiments and Results 
In this section we provide the results of a set of 
experiments focused on the task of simultaneous 
discourse identification and classification. 
7.1 Experimental Settings 
We experiment on PDTB v2.0 corpus. Besides 
four top-level discourse relations, we also con-
sider Entity and No relations described in Section 
2. We directly use the golden standard parse 
trees in Penn TreeBank. We employ an SVM 
coreference resolver trained and tested on ACE 
2005 with 79.5% Precision, 66.7% Recall and 
72.5% F1 to label coreference mentions of the 
same named entity in an article. For learning, we 
use the binary SVMLight developed by (Joa-
chims, 1998) and Tree Kernel Toolkits devel-
oped by (Moschitti, 2004). All classifiers are 
trained with default learning parameters. 
The performance is evaluated using Accuracy 
which is calculated as follow: 
???????? =
????????????+ ????????????
???
 
Sections 2-22 are used for training and Sec-
tions 23-24 for testing. In this paper, we only 
consider any non-overlapping clauses/sentences 
pair in 3-sentence spans. For training, there were 
14812, 12843 and 4410 instances for Explicit, 
Implicit and Entity+No relations respectively; 
while for testing, the number was 1489, 1167 and 
380. 
7.2 System with Structural Kernel 
Table 2 lists the performance of simultaneous 
identification and classification on level-1 dis-
course senses. In the first row, only base features 
described in Section 4 are used. In the second 
row, we test Ben and James (2007)?s algorithm 
which uses heuristically defined syntactic paths 
and acts as a good baseline to compare with our 
learned-based approach using the structured in-
formation. The last three rows of Table 2 reports 
the results combining base features with three 
syntactic structured features (i.e. Min-Expansion, 
Simple-Expansion and Full-Expansion) de-
scribed in Section 5. 
We can see that all our tree kernels outperform 
the manually constructed flat path feature in all 
three groups including Explicit only, Implicit 
only and All relations, with the accuracy increas-
ing by 1.8%, 6.7% and 3.1% respectively. Espe-
cially, it shows that structural syntactic informa-
tion is more helpful for Implicit cases which is 
generally much harder than Explicit cases. We  
716
 
 
 
 
conduct chi square statistical significance test on 
All relations between flat path approach and 
Simple-Expansion approach, which shows the 
performance improvements are statistical signifi-
cant (? < 0.05) through incorporating tree ker-
nel. This proves that structural syntactic informa-
tion has good predication power for discourse 
analysis in both explicit and implicit relations. 
We also observe that among the three syntactic 
structured features, Min-Expansion and Simple-
Expansion achieve similar performances which 
are better than the result for Full-Expansion. This 
may be due to that most significant information 
is with the arguments and the shortest path con-
necting connectives and arguments. However, 
Full-Expansion that includes more information 
in other branches may introduce too many details 
which are rather tangential to discourse recogni-
tion. Our subsequent reports will focus on Sim-
ple-Expansion, unless otherwise specified. 
As described in Section 5, to compute the 
structural information, parse trees for different 
sentences are connected to form a large tree for a 
paragraph. It would be interesting to find how 
the structured information works for discourse 
relations whose arguments reside in different 
sentences. For this purpose, we test the accuracy 
for discourse relations with the two arguments 
occurring in the same sentence, one-sentence 
apart, and two-sentence apart. Table 3 compares 
the learning systems with/without the structured 
feature present. From the table, for all three cas-
es, the accuracies drop with the increase of the 
distances between the two arguments. However, 
adding the structured information would bring 
consistent improvement against the baselines 
regardless of the number of sentence distance. 
This observation suggests that the structured syn-
tactic information is more helpful for inter-
sentential discourse analysis.  
We also concern about how the structured in-
formation works for identification and classifica-
tion respectively. Table 4 lists the results for the 
two sub-tasks. As shown, with the structured in-
formation incorporated, the system (Base + Tree 
Kernel) can boost the performance of the two 
baselines (Base Features in the first row andBase 
+ Manually selected paths in the second row), for 
both identification and classification respective-
ly. We also observe that the structural syntactic 
information is more helpful for classification task 
which is generally harder than identification. 
This is in line with the intuition that classifica-
tion is generally a much harder task. We find that 
due to the weak modeling of Entity relations, 
many Entity relations which are non-discourse 
relation instances are mis-identified as implicit 
Expansion relations. Nevertheless, it clearly di-
rects our future work. 
 
 
 
 
 
 
 
 
 
 
 
7.3 System with Temporal Ordering Infor-
mation 
To examine the effectiveness of our temporal 
ordering information, we perform experiments 
Features 
 
Accuracy 
Explicit Implicit All 
Base Features 67.1 29 48.6 
Base + Manually 
selected flat path 
features 
70.3 32 52.6 
Base + Tree kernel 
(Min-Expansion) 
71.9 38.6 55.6 
Base + Tree kernel 
(Simple-Expansion) 
72.1 38.7 55.7 
Base + Tree kernel 
(Full-Expansion) 
71.8 38.4 55.4 
Sentence Dis-
tance 
0 
(959) 
1 
(1746) 
2 
(331) 
Base Features 52 49.2 35.5 
Base + Manually 
selected flat path 
features 
56.7 52 43.8 
Base + Tree 
Kernel 
58.3 55.6 49.7 
Tasks Identifica-
tion 
Classifica-
tion 
Base Features 58.6 50.5 
Base + Manually 
selected flat path 
features 
59.7 52.6 
Base + Tree 
Kernel 
63.3 59.3 
Table 3. Results of the syntactic structured kernel 
for discourse relations recognition with argu-
ments in different sentences apart. 
Table 4. Results of the syntactic structured ker-
nel for simultaneous discourse identification and 
classification subtasks. 
Table 2. Results of the syntactic structured ker-
nels on level-1 discourse relation recognition. 
717
on simultaneous identification and classification 
of level-1 discourse relations to compare with 
using only base feature set as baseline. The re-
sults are shown in Table 5.  We observe that the 
use of temporal ordering information increases 
the accuracy by 3%, 3.6% and 3.2% for Explicit, 
Implicit and All groups respectively. We conduct 
chi square statistical significant test on All rela-
tions, which shows the performance improve-
ment is statistical significant (? < 0.05). It indi-
cates that temporal ordering information can 
constrain the discourse relation types inferred 
within a clause(s)/sentence(s) pair for both expli-
cit and implicit relations. 
 
 
 
 
We observe that although temporal ordering 
information is useful in both explicit and implicit 
relation recognition, the contributions of the spe-
cific information are quite different for the two 
cases. In our experiments, we use tense and as-
pectual information for explicit relations, while 
event ordering information is used for implicit 
relations. The reason is explicit connective itself 
provides a strong hint for explicit relation, so 
tense and aspectual analysis which yields a relia-
ble result can provide additional constraints, thus 
can help explicit relation recognition. However, 
event ordering which would inevitably involve 
more noises will adversely affect the explicit re-
lation recognition performance. On the other 
hand, for implicit relations with no explicit con-
nective words, tense and aspectual information 
alone is not enough for discourse analysis. Event 
ordering can provide more necessary information 
to further constrain the inferred relations. 
7.4 Overall Results 
We also evaluate our model which combines 
base features, tree kernel and tense/temporal or-
dering information together on Explicit, Implicit 
and All Relations respectively. The overall re-
sults are shown in Table 6. 
 
 
 
 
 
 
 
 
 
8 Conclusions and Future Works 
The purpose of this paper is to explore how to 
make use of the structural syntactic knowledge to 
do discourse relation recognition. In previous 
work, syntactic information from parse trees is 
represented as a set of heuristically selected flat 
paths or 2-level production rules. However, the 
features defined this way may not necessarily 
capture all useful syntactic information provided 
by the parse trees for discourse analysis. In the 
paper, we propose a kernel-based method to in-
corporate the structural information embedded in 
parse trees. Specifically, we directly utilize the 
syntactic parse tree as a structure feature, and 
then apply kernels to such a feature, together 
with other normal features. The experimental 
results on PDTB v2.0 show that our kernel-based 
approach is able to give statistical significant 
improvement over flat syntactic path method. In 
addition, we also propose to incorporate tempor-
al ordering information to constrain the interpre-
tation of discourse relations, which also demon-
strate statistical significant improvements for 
discourse relation recognition, both explicit and 
implicit. 
In future, we plan to model Entity relations 
which constitute 24% of Implicit+Entity+No re-
lation cases, thus to improve the accuracy of re-
lation detection. 
Reference 
Ben W. and James P. 2007. Automatically Identifying 
the Arguments of Discourse Connectives. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning, pages 
92-101.  
Culotta A. and Sorensen J. 2004. Dependency Tree 
Kernel for Relation Extraction. In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics (ACL 2004), pages 423-
429.  
Collins M. and Duffy N. 2001. New Ranking Algo-
rithms for Parsing and Tagging: Kernels over Dis-
Features 
 
Accuracy 
Explicit Implicit All 
Base Features 67.1 29 48.6 
Base + Tem-
poral Ordering 
Information 
70.1 32.6 51.8 
Relations Accuracy 
Explicit 74.2 
Implicit 40.0 
All 57.3 
Table 5. Results of tense and temporal order 
information on level-1 discourse relations. 
Table 6. Overall results for combined model 
(Base  + Tree Kernel + Tense/Temporal). 
718
crete Structures and the Voted Perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 2002), 
pages 263-270. 
Collins M. and Duffy N. 2002. Convolution Kernels 
for Natural Language. NIPS-2001. 
Haussler D. 1999. Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, 
University of California, Santa Cruz. 
Joachims T.  1999. Making Large-scale SVM Learn-
ing Practical. In Advances in Kernel Methods ? 
Support Vector Learning. MIT Press. 
Knott, A., Oberlander, J., O?Donnel, M., and Mellish, 
C. 2001. Beyond elaboration: the interaction of re-
lations and focus in coherent text. In T. Sanders, J. 
Schilperoord, and W. Spooren, editors, Text Re-
presentation: Linguistic and Psycholinguistics As-
pects, pages 181-196. Benjamins, Amsterdam. 
Lee A., Prasad R., Joshi A., Dinesh N. and Webber  
B. 2006. Complexity of dependencies in discourse: 
are dependencies in discourse more complex than 
in syntax? In Proceedings of the 5th International 
Workshop on Treebanks and Linguistic Theories. 
Prague, Czech Republic, December. 
Lin Z., Kan M. and Ng H. 2009. Recognizing Implicit 
Discourse Relations in the Penn Discourse Tree-
bank. In Proceedings of the 2009 Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP 2009), Singapore, August. 
Marcu D. and Echihabi A. 2002. An Unsupervised 
Approach to Recognizing Discourse Relations. In 
Proceedings of the 40th Annual Meeting of ACL, 
pages 368-375. 
Moschitti A. 2004. A Study on Convolution Kernels 
for Shallow Semantic Parsing. In Proceedings of 
the 42th Annual Meeting of the Association for 
Computational Linguistics (ACL 2004), pages 335-
342. 
Pettibone J. and Pon-Barry H. 2003. A Maximum En-
tropy Approach to Recognizing Discourse Rela-
tions in Spoken Language. Working Paper. The 
Stanford Natural Language Processing Group, June 
6. 
Pitler E., Louis A. and Nenkova A. 2009. Automatic 
Sense Predication for Implicit Discourse Relations 
in Text. In Proceedings of the Joint Conference of 
the 47th Annual Meeting of the Association for 
Computational Linguistics and the 4th International 
Joint Conference on Natural Language Processing 
of the Asian Federation of Natural Language 
Processing (ACL-IJCNLP 2009). 
Prasad R., Dinesh N., Lee A., Miltsakaki E., Robaldo 
L., Joshi A. and Webber B. 2008. The Penn Dis-
course TreeBank 2.0. In Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC 2008). 
Saito M., Yamamoto K. and Sekine S. 2006. Using 
phrasal patterns to identify discourse relations. In 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics (HLT-
NAACL 2006), pages 133?136, New York, USA. 
Vapnik V.  1995. The Nature of Statistical Learning 
Theory. Springer-Verlag, New York. 
Webber Bonnie. 1988. Tense as Discourse Anaphor. 
Computational Linguistics, 14:61?73. 
Zelenko D., Aone C. and Richardella A. 2003.  Ker-
nel Methods for Relation Extraction.  Journal of 
Machine Learning Research, 3(6):1083-1106. 
Zhang M., Zhang J. and Su J. Exploring Syntactic 
Features for Relation Extraction using a Convolu-
tion Tree Kernel. In Proceedings of the Human 
Language Technology conference - North Ameri-
can chapter of the Association for Computational 
Linguistics annual meeting (HLT-NAACL 2006), 
New York, USA. 
719
