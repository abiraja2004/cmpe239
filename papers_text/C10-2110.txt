Coling 2010: Poster Volume, pages 955?962,
Beijing, August 2010
The Role of Queries in Ranking Labeled Instances Extracted from Text
Marius Pas?ca
Google Inc.
mars@google.com
Abstract
A weakly supervised method uses
anonymized search queries to induce a
ranking among class labels extracted from
unstructured text for various instances.
The accuracy of the extracted class labels
exceeds that of previous methods, over
evaluation sets of instances associated
with Web search queries.
1 Introduction
Classes pertaining to unrestricted domains (e.g.,
west african countries, science fiction films, slr
cameras) and their instances (cape verde, avatar,
canon eos 7d) play a disproportionately important
role in Web search. They occur prominently in
Web documents and among search queries sub-
mitted most frequently by Web users (Jansen et
al., 2000). They also serve as building blocks in
formal representation of human knowledge, and
are useful in a variety of text processing tasks.
Recent work on offline acquisition of fine-
grained, labeled classes of instances applies
manually-created (Banko et al, 2007; Talukdar et
al., 2008) or automatically-learned (Snow et al,
2006) extraction patterns to large document col-
lections. Although various methods exploit addi-
tional textual resources to increase accuracy (Van
Durme and Pas?ca, 2008) and coverage (Talukdar
et al, 2008), some of the extracted class labels
are inevitably less useful (works) or spurious (car
makers) for an associated instance (avatar). In
Web search, the relative ranking of documents re-
turned for a query directly affects the outcome of
the search. Similarly, the relative ranking among
class labels extracted for a given instance influ-
ences any applications using the labels.
Our paper proposes the use of features other
than those computed over the underlying doc-
ument collection, such as the frequency of co-
occurrence or diversity of extraction patterns pro-
ducing a given pair (Etzioni et al, 2005), to deter-
mine the relative ranking of various class labels,
given a class instance. Concretely, the method
takes advantage of the co-occurrence of a class
label and an instance within search queries from
anonymized query logs. It re-ranks lists of class
labels produced for an instance by standard ex-
traction patterns, to promote class labels that co-
occur with the instance. This corresponds to a soft
ranking approach, focusing on the ranking of can-
didate extractions such as the less relevant ones
are ranked lower, as opposed to removed when
deemed unreliable based on various clues.
By using queries in ranking, the ranked lists
of class labels available for various instances are
instrumental in determining the classes to which
given sets of instances belong. The accuracy of
the class labels exceeds that of previous work,
over evaluation sets of instances associated with
Web search queries. The results confirm the use-
fulness of the extracted IsA repository, which re-
mains general-purpose and is not tailored to any
particular task.
2 Instance Class Ranking
2.1 Extraction of Instances and Classes
The initial extraction of labeled instances relies
on hand-written patterns from (Hearst, 1992),
widely used in work on extracting hierarchies
from text (Snow et al, 2006; Ponzetto and Strube,
955
2007):
?[..] C [such as|including] I [and|,|.]?,
where I is a potential instance (e.g., diderot) and
C is a potential class label (e.g., writers).
Following (Van Durme and Pas?ca, 2008), the
boundaries of potential class labels C are approx-
imated from the part-of-speech tags of the sen-
tence words, whereas the boundaries of instances
I are identified by checking that I occurs as an
entire query in query logs. Since users type many
queries in lower case, the collected data is con-
verted to lower case.
When applied to inherently-noisy Web docu-
ments, the extraction patterns may produce irrele-
vant extractions (Kozareva et al, 2008). Causes of
errors include incorrect detection of possible enu-
merations, as in companies such as Procter and
Gamble (Downey et al, 2007); incorrect estima-
tion of the boundaries of class labels, due to in-
correct attachment as in years from on a limited
number of vehicles over the past few years, includ-
ing the Chevrolet Corvette; subjective (famous ac-
tors) (Hovy et al, 2009), relational (competitors,
nearby landmarks) and otherwise less useful (oth-
ers, topics) class labels; or questionable source
sentences, as in Large mammals such as deer and
wild turkeys can be [..] (Van Durme and Pas?ca,
2008).
As a solution, recent work uses additional evi-
dence, as a means to filter the pairs extracted by
patterns, thus trading off coverage for higher pre-
cision. The repository extracted from a similarly-
sized Web document collection using the same
initial extraction patterns as here, after a weighted
intersection of pairs extracted with patterns and
clusters of distributionally similar phrases, con-
tains a total of 9,080 class labels associated with
263,000 instances in (Van Durme and Pas?ca,
2008). Subsequent extensions of the repository,
using data derived from tables within Web doc-
uments, increase instance coverage and induce a
ranking among class labels of each instance, but
do not increase the number of class labels (Taluk-
dar et al, 2008). Due to aggressive filtering, the
resulting number of class labels is higher than the
often-small sets of entity types studied previously,
but may still be insufficient given the diversity of
Web search queries.
2.2 Ranking of Classes per Instance
As an alternative, the soft ranking approach pro-
posed here attempts to rank better class labels
higher, without necessarily removing class labels
deemed incorrect according to various criteria.
For each instance I , the associated class labels are
ranked in the following stages:
1) Apply the scoring formula below, resulting
in a ranked list of class labels L1(I):
Score(I, C) = Size({Pattern(I,C)})2 ? Freq(I, C)
Thus, a class label C is deemed more relevant
for an instance I if C is extracted by multiple ex-
traction patterns and its original frequency-based
score is higher.
2) For each term within any class label from
L1(I), compute a score equal to the frequency
sum of the term within anonymized queries con-
taining the instance I as a prefix, and the term
anywhere else in the queries. Each class label is
assigned the geometric mean of the scores of its
terms, after ignoring stop words. The class labels
are ranked according to the means, resulting in a
ranked list L2(I). In case of ties, L2(I) preserves
the relative ranking from L1(I). Thus, a class la-
bel is deemed more relevant if its individual terms
occur in popular queries containing the instance.
3) Compute a merged ranked list of class labels
out of the ranked lists L1(I) and L2(I), by sorting
the class labels in decreasing order of the inverse
of the average rank, computed with the following
formula:
MergedScore(C) = 2Rank(C, L1) + Rank(C, L2)
where 2 is the number of input lists of class la-
bels, and Rank(C, Li) is the rank of C in the list
Li of class labels computed for the correspond-
ing input instance. The rank is set to 1000, if C
is not present in the list Li. By using only the
relative ranks of the class labels within the input
lists, and not on their scores, the outcome of the
merging is less sensitive to how class labels of a
given instance are scored within the IsA reposi-
tory. In case of ties, the scores of the class labels
from L1(I) serve as a secondary ranking criterion.
Note that the third stage is introduced because
relying on query logs to estimate the relevance of
956
class labels exposes the ranking method to signifi-
cant noise. On one hand, arguably useful class la-
bels (e.g., authors) may not occur in queries along
with the respective instances (diderot). On the
other hand, for each query containing an instance
and (part of) useful class labels, there are many
other queries containing, e.g., attributes (diderot
biography or diderot beliefs) or the name of a
book in the query diderot the nun. Therefore, the
ranked lists L2(I) may be too noisy to be used di-
rectly as rankings of the class labels for I .
3 Experimental Setting
3.1 Textual Data Sources
The acquisition of the IsA repository relies on un-
structured text available within Web documents
and search queries. The collection of queries is
a sample of 50 million unique, fully-anonymized
queries in English submitted by Web users in
2009. Each query is accompanied by its frequency
of occurrence in the logs. The document col-
lection consists of a sample of 100 million doc-
uments in English. The textual portion of the
documents is cleaned of HTML, tokenized, split
into sentences and part-of-speech tagged using the
TnT tagger (Brants, 2000).
3.2 Experimental Runs
The experimental runs correspond to different
methods for extracting and ranking pairs of an in-
stance and a class:
? as available in the repository from (Talukdar
et al, 2008), which is collected from a docu-
ment collection similar in size to the one used
here plus a collection of Web tables, in a run
denoted Rg;
? from the repository extracted here, with class
labels of an instance ranked based on the fre-
quency and the number of extraction patterns
(see Score(I, C) in Section 2), in run Rs;
? from the repository extracted here, with class
labels of an instance ranked based on the
MergedScore from Section 2, in run Ru.
3.3 Evaluation Procedure
The manual evaluation of open-domain informa-
tion extraction output is time consuming (Banko
et al, 2007). Fortunately, it is possible to im-
plement an automatic evaluation procedure for
ranked lists of class labels, based on existing re-
sources and systems. Assume that a gold stan-
dard is available, containing gold class labels that
are each associated with a gold set of their in-
stances. The creation of such gold standards is
discussed later. Based on the gold standard, the
ranked lists of class labels available within an IsA
repository can be automatically evaluated as fol-
lows. First, for each gold label, the ranked lists
of class labels of individual gold instances are re-
trieved from the IsA repository. Second, the in-
dividual retrieved lists are merged into a ranked
list of class labels, associated with the gold label.
The merged list is computed using an extension
of the MergedScore formula described earlier
in Section 2. Third, the merged list is compared
against the gold label, to estimate the accuracy of
the merged list. Intuitively, a ranked list of class
labels is a better approximation of a gold label, if
class labels situated at better ranks in the list are
closer in meaning to the gold label.
3.4 Evaluation Metric
Given a gold label and a list of class labels, if any,
derived from the IsA repository, the rank of the
highest class label that matches the gold label de-
termines the score assigned to the gold label, in
the form of the reciprocal rank, max(1/rankmatch).
Thus, if the gold label matches a class label at rank
1, 2, 3, 4 or 5 in the computed list, the gold label
receives a score of 1, 0.5, 0.33, 0.25 or 0.2 respec-
tively. The score is 0 if the gold label does not
match any of the top 20 class labels. The overall
score over the entire set of gold labels is the mean
reciprocal rank (MRR) score over all gold labels
from the set. Two types of MRR scores are auto-
matically computed:
? MRRf considers a gold label and a class la-
bel to match if they are identical;
? MRRp considers a gold label and a class la-
bel to match if one or more of their tokens
that are not stop words are identical.
957
During matching, all string comparisons are
case-insensitive, and all tokens are first converted
to their singular form (e.g., european countries
to european country) when available, by using
WordNet?s morphological routines. Thus, insur-
ance carriers and insurance companies are con-
sidered to not match in MRRf scores, but match
in MRRp scores, whereas insurance companies
and insurance company match in both MRRf and
MRRp scores. Note that both MRRf and MRRp
scores fail to give any credit to arguably valid
and useful class labels, such as insurers for the
gold label insurance carriers, or asian nations
for the gold label asia countries. On the other
hand, MRRp scores may give credit to less rele-
vant class labels, such as insurance policies for the
gold label insurance carriers. Therefore, MRRp
is an approximate, and MRRf is a conservative,
lower-bound estimate of the actual usefulness of
the computed ranked lists of class labels as ap-
proximations of the semantics of the gold labels.
4 Evaluation Results
4.1 Evaluation Sets of Queries
A random sample of anonymized, class-seeking
queries (e.g., video game characters or smart-
phone) submitted by Web users to Google
Squared 1 over a 30-day interval is filtered, to re-
move queries for which Google Squared returns
fewer than 10 instances at the time of the evalua-
tion. The resulting evaluation set of queries, de-
noted Qe, contains 807 queries, each associated
with a ranked list of between 10 and 100 instances
automatically extracted by Google Squared.
Since the instances available as input for each
query as part of Qe are automatically extracted,
they may (e.g., acorn a7000) or may not (e.g.,
konrad zuse) be true instances of the respective
queries (e.g., computers). A second evaluation
set Qm is assembled as a subset of 40 queries
from Qe, such that the instances available for each
query in Qm are correct. For this purpose, each
instance returned by Google Squared for the 40
1Google Squared (http://www.google.com/squared) is a
Web search tool taking as input class-seeking queries (e.g.,
insurance companies) and returning lists of instances (e.g.,
allstate, state farm insurance), along with attributes (e.g., in-
dustry, headquarters) and values for each instance.
Query Set: Sample of Queries
Qe (807 queries): 2009 movies, amino acids,
asian countries, bank, board games, buildings,
capitals, chemical functional groups, clothes,
computer language, dairy farms near modesto
ca, disease, egyptian pharaohs, eu countries,
french presidents, german islands, hawaiian is-
lands, illegal drugs, irc clients, lakes, mac-
intosh models, mobile operator india, nba
players, nobel prize winners, orchids, photo
editors, programming languages, renaissance
artists, roller costers, science fiction tv series,
slr cameras, soul singers, states of india, tal-
iban members, thomas edison inventions, u.s.
presidents, us president, water slides
Qm (40 queries): actors, airlines, birds, cars,
celebrities, computer languages, digital cam-
era, dog breeds, drugs, endangered animals,
european countries, fruits, greek gods, hor-
ror movies, ipods, names, netbooks, operat-
ing systems, park slope restaurants, presidents,
ps3 games, religions, renaissance artists, rock
bands, universities, university, vitamins
Table 1: Size and composition of evaluation sets
of queries associated with non-filtered (Qe) or
manually-filtered (Qm) instances
queries from Qm is reviewed by at least three hu-
man annotators. Instances deemed highly rele-
vant (out of 5 possible grades) with high inter-
annotator agreement are retained. As a result, the
40 queries from Qm are associated with between
8 and 33 human-validated instances.
Table 1 shows a sample of the queries from Qe
and queries from Qm. A small number of queries
are slight lexical variations of one another, such as
u.s. presidents and us presidents in Qe, or univer-
sities and university in Qm. In general, however,
the sets cover a wide range of domains of inter-
est, including entertainment for 2009 movies and
rock bands; biology for endangered animals and
amino acids; geography for asian countries and
hawaiian islands; food for fruits; history for egyp-
tian pharaohs and greek gods; health for drugs
and vitamins; and technology for photo editors
and ipods. Some of the queries from Table 1
are specific enough that computing them exactly,
958
Accuracy
IQ 3 5 10 15
CI 5 10 20 5 10 20 5 10 20 5 10 20
MRRf computed over Qe:
Rg 0.106 0.112 0.112 0.121 0.122 0.123 0.131 0.135 0.127 0.134 0.132 0.127
Rs 0.186 0.195 0.198 0.198 0.207 0.210 0.204 0.214 0.218 0.206 0.216 0.221
Ru 0.202 0.211 0.216 0.232 0.238 0.244 0.245 0.255 0.257 0.245 0.252 0.254
MRRp computed over Qe:
Rg 0.390 0.399 0.394 0.420 0.420 0.413 0.443 0.443 0.435 0.439 0.431 0.425
Rs 0.489 0.495 0.495 0.517 0.528 0.529 0.541 0.553 0.557 0.551 0.557 0.557
Ru 0.520 0.531 0.533 0.564 0.573 0.578 0.590 0.601 0.602 0.598 0.603 0.601
MRRf computed over Qm:
Rg 0.284 0.289 0.295 0.305 0.327 0.322 0.320 0.335 0.335 0.334 0.328 0.337
Rs 0.406 0.436 0.442 0.431 0.447 0.466 0.467 0.470 0.501 0.484 0.501 0.554
Ru 0.423 0.426 0.429 0.436 0.483 0.508 0.500 0.526 0.530 0.520 0.540 0.524
MRRp computed over Qm:
Rg 0.507 0.517 0.531 0.495 0.509 0.518 0.555 0.553 0.550 0.563 0.561 0.572
Rs 0.667 0.662 0.660 0.675 0.677 0.699 0.702 0.695 0.716 0.756 0.765 0.787
Ru 0.711 0.703 0.680 0.734 0.731 0.748 0.733 0.797 0.782 0.799 0.834 0.819
Table 2: Accuracy of instance set labeling, as full-match (MRRf ) or partial-match (MRRp) scores over
the evaluation sets of queries associated with non-filtered instances (Qe) or manually-filtered instances
(Qm), for various experimental runs (IQ=number of instances available in the input evaluation sets that
are used for retrieving class labels; CI=number of class labels retrieved from IsA repository per input
instance)
even from a comprehensive, perfect list of ex-
tracted instance, would be very difficult whether
done automatically or manually. Examples of
such queries are dairy farms near modesto ca and
science fiction tv series, but also mobile opera-
tor india (phrase expressed as keywords) in Qe, or
park slope restaurants (specific location) in Qm.
Access to a system such as Google Squared is
useful, but not necessary to conduct the evalua-
tion. Given other sets of queries, it is straightfor-
ward, albeit time consuming, to create evaluation
sets similar to Qm, by manually compiling correct
instances, for each selected query or concept.
Following the general evaluation procedure,
each query from the sets Qe and Qm acts as a gold
class label associated with its set of instances.
Given a query and its instances I from the evalu-
ation sets Qe or Qm, we compute merged, ranked
lists of class labels, by merging the ranked lists of
class labels available in the underlying IsA reposi-
tory for each instance I . The evaluation compares
the merged lists of class labels, on one hand, and
the corresponding queries from Qe or Qm, on the
other hand.
4.2 Accuracy of Class Labels
Table 2 summarizes results from comparative ex-
periments, quantifying a) horizontally, the impact
of alternative parameter settings on the computed
lists of class labels; and b) vertically, the compar-
ative accuracy of the experimental runs over the
query sets. The experimental parameters are the
number of input instances from the evaluation sets
that are used for retrieving class labels, IQ, set to
3, 5, 10 and 15; and the number of class labels
retrieved per input instance, CI , set to 5, 10 and
20.
The scores over Qm are higher than those
over Qe, confirming the intuition that the higher-
quality input set of instances available in Qm rel-
ative to Qe should lead to higher-quality class la-
bels for the corresponding queries. When IQ is
fixed, increasing CI leads to small, if any, score
improvements. Conversely, when CI is fixed,
959
even small values of IQ, such as 3 or 5 (that is,
very small sets of instances provided as input) pro-
duce scores that are competitive with those ob-
tained with a higher value like. This suggests that
useful class labels can be generated even in ex-
treme scenarios, where the number of instances
available as input is as small as 3 or 5.
For most combinations of parameter settings
and on both query sets, run Ru produces the high-
est scores. In particular, when IQ is set to 10 and
CI to 20, run Ru identifies the original query as
an exact match among the top four class labels
returned; and as a partial match among the top
two class labels returned, as an average over the
Qe set. In this case, the original query is iden-
tified at ranks 1, 2, 3, 4 and 5 for 16.8%, 8.7%,
6.1%, 3.7% and 1.7% of the queries, as an ex-
act match; and for 48.8%, 14.2%, 6.1%, 3.6% and
1.9% respectively, as a partial match. The corre-
sponding MRRf score of 0.257 over the Qe set
obtained with run Ru is higher than with run Rs,
and much higher than with run Rg. In all experi-
ments, the higher scores of Ru can be attributed to
higher coverage of class labels, relative to Rg; and
higher-quality lists of class labels, relative to Rs
but also to Rg, despite the fact that Rg combines
high-precision seed data with using both unstruc-
tured and structured text as sources of class labels
(cf. (Talukdar et al, 2008)). Among combinations
of parameter settings described in Table 2, values
around 15 for IQ and 20 for CI give the highest
scores over both Qe and Qm.
5 Related Work
5.1 Extraction of IsA Repositories
Knowledge including instances and classes can be
manually compiled by experts (Fellbaum, 1998)
or collaboratively by non-experts (Singh et al,
2002). Alternatively, classes of instances acquired
automatically from text are potentially less ex-
pensive to acquire, maintain and grow, and their
coverage and scope are theoretically bound only
by the size of the underlying data source. Ex-
isting methods for extracting classes of instances
acquire sets of instances that are each either un-
labeled (Wang and Cohen, 2008; Pennacchiotti
and Pantel, 2009; Lin and Wu, 2009), or as-
sociated with a class label (Pantel and Pennac-
chiotti, 2006; Banko et al, 2007; Wang and Co-
hen, 2009). When associated with a class la-
bel, the sets of instances may be organized as
flat sets or hierarchically, relative to existing hi-
erarchies such as WordNet (Snow et al, 2006) or
the category network within Wikipedia (Wu and
Weld, 2008; Ponzetto and Navigli, 2009). Semi-
structured text was shown to be a complemen-
tary resource to unstructured text, for the purpose
of extracting relations from Web documents (Ca-
farella et al, 2008).
The role of anonymized query logs in Web-
based information extraction has been explored
in the tasks of class attribute extraction (Pas?ca
and Van Durme, 2007) and instance set ex-
pansion (Pennacchiotti and Pantel, 2009). Our
method illustrates the usefulness of queries con-
sidered in isolation from one another, in ranking
class labels in extracted IsA repositories.
5.2 Labeling of Instance Sets
Previous work on generating relevant labels, given
sets or clusters of items, focuses on scenarios
where the items within the clusters are descrip-
tions of, or full-length documents within docu-
ment collections. The documents are available as
a flat set (Cutting et al, 1993; Carmel et al, 2009)
or are hierarchically organized (Treeratpituk and
Callan, 2006). Relying on semi-structured con-
tent assembled manually as part of the struc-
ture of Wikipedia articles, such as article titles
or categories, the method introduced in (Carmel
et al, 2009) derives labels for clusters contain-
ing 100 full-length documents each. In contrast,
our method relies on IsA relations automatically
extracted from unstructured text within arbitrary
Web documents, and computes labels given tex-
tual input that is orders of magnitude smaller, i.e.,
around 10 phrases (instances). The experiments
described in (Carmel et al, 2009) assign labels to
one of 20 sets of newsgroup documents from a
standard benchmark. Each set of documents is as-
sociated with a higher-level, coarse-grained label
used as a gold label against which the generated
labels are compared. In comparison, our experi-
ments compute text-derived class labels for finer-
grained, often highly-specific gold labels.
960
Reducing the granularity of the items to be la-
beled from full documents to condensed docu-
ment descriptions, (Geraci et al, 2006) submits
arbitrary search queries to external Web search en-
gines. It organizes the top 200 returned Web doc-
uments into clusters, by analyzing the text snip-
pets associated with each document in the output
from the search engines. Any words and phrases
from the snippets may be selected as labels for the
clusters, which in general leads to labels that are
not intended to capture any classes that may be as-
sociated to the query. For example, labels of clus-
ters generated in (Geraci et al, 2006) include arm-
strong ceilings, italia, armstrong sul sito and louis
jazz for the query armstrong; and madonnaweb,
music, madonna online and madonna itself for the
query madonna. The amount of text available as
input for the purpose of labeling is at least two or-
ders of magnitude larger than in our method, and
the task of selecting any phrases as labels, as op-
posed to selecting only labels that correspond to
classes, is more relaxed and likely easier.
Another approach specifically addresses the
problem of generating labels for sets of instances,
where the labels are extracted from unstructured
text. In (Pantel and Ravichandran, 2004), given a
collection of news articles that is both cleaner and
smaller than Web document collections, a syn-
tactic parser is applied to document sentences in
order to identify and exploit syntactic dependen-
cies for the purpose of selecting candidate class
labels. Such methods are comparatively less ap-
plicable to Web document collections, due to scal-
ability issues associated with parsing a large set
of Web documents of variable quality. Moreover,
the class labels generated in (Pantel and Ravichan-
dran, 2004) tend to be rather coarse-grained. For
example, the top labels generated for a set of Chi-
nese universities (qinghua university, fudan uni-
versity, beijing university) are university, institu-
tion, stock-holder, college and school.
6 Conclusion
The method presented in this paper produces an
IsA repository whose class labels have higher
coverage and accuracy than with recent meth-
ods operating on document collections. This is
done by injecting useful ranking signals from
inherently-noisy queries, rather than making bi-
nary, coverage-reducing quality decisions on the
extracted data. Current work investigates the use-
fulness of the extracted class labels in the gener-
ation of flat or hierarchical query refinements for
class-seeking queries.
Acknowledgments
The author thanks Randolph Brown for assistance
in assembling the evaluation sets of class-seeking
queries.
References
Banko, M., Michael J Cafarella, S. Soderland,
M. Broadhead, and O. Etzioni. 2007. Open infor-
mation extraction from the Web. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI-07), pages 2670?2676, Hyder-
abad, India.
Brants, T. 2000. TnT - a statistical part of speech
tagger. In Proceedings of the 6th Conference on
Applied Natural Language Processing (ANLP-00),
pages 224?231, Seattle, Washington.
Cafarella, M., A. Halevy, D. Wang, E. Wu, and
Y. Zhang. 2008. WebTables: Exploring the power
of tables on the Web. In Proceedings of the 34th
Conference on Very Large Data Bases (VLDB-08),
pages 538?549, Auckland, New Zealand.
Carmel, D., H. Roitman, and N. Zwerding. 2009. En-
hancing cluster labeling using Wikipedia. In Pro-
ceedings of the 32nd ACM Conference on Research
and Development in Information Retrieval (SIGIR-
09), pages 139?146, Boston, Massachusetts.
Cutting, D., D. Karger, and J. Pedersen. 1993.
Constant interaction-time scatter/gather browsing of
very large document collections. In Proceedings of
the 16th ACM Conference on Research and Devel-
opment in Information Retrieval (SIGIR-93), pages
126?134, Pittsburgh, Pennsylvania.
Downey, D., M. Broadhead, and O. Etzioni. 2007. Lo-
cating complex named entities in Web text. In Pro-
ceedings of the 20th International Joint Conference
on Artificial Intelligence (IJCAI-07), pages 2733?
2739, Hyderabad, India.
Etzioni, O., M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from
the Web: an experimental study. Artificial Intelli-
gence, 165(1):91?134.
Fellbaum, C., editor. 1998. WordNet: An Elec-
tronic Lexical Database and Some of its Applica-
tions. MIT Press.
961
Geraci, F., M. Pellegrini, M. Maggini, and F. Sebas-
tiani. 2006. Cluster generation and cluster la-
belling for Web snippets: A fast and accurate hi-
erarchical solution. In Proceedings of the 13th Con-
ference on String Processing and Information Re-
trieval (SPIRE-06), pages 25?36, Glasgow, Scot-
land.
Hearst, M. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the
14th International Conference on Computational
Linguistics (COLING-92), pages 539?545, Nantes,
France.
Hovy, E., Z. Kozareva, and E. Riloff. 2009. Toward
completeness in concept extraction and classifica-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-09), pages 948?957, Singapore.
Jansen, B., A. Spink, and T. Saracevic. 2000. Real
life, real users, and real needs: a study and analysis
of user queries on the Web. Information Processing
and Management, 36(2):207?227.
Kozareva, Z., E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pat-
tern linkage graphs. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL-08), pages 1048?1056, Columbus,
Ohio.
Lin, D. and X. Wu. 2009. Phrase clustering for dis-
criminative learning. In Proceedings of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-IJCNLP-09), pages 1030?
1038, Singapore.
Pas?ca, M. and B. Van Durme. 2007. What you seek
is what you get: Extraction of class attributes from
query logs. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI-
07), pages 2832?2837, Hyderabad, India.
Pantel, P. and M. Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING-ACL-
06), pages 113?120, Sydney, Australia.
Pantel, P. and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
the 2004 Human Language Technology Conference
(HLT-NAACL-04), pages 321?328, Boston, Mas-
sachusetts.
Pennacchiotti, M. and P. Pantel. 2009. Entity extrac-
tion via ensemble semantics. In Proceedings of the
2009 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-09), pages 238?
247, Singapore.
Ponzetto, S. and R. Navigli. 2009. Large-scale tax-
onomy mapping for restructuring and integrating
Wikipedia. In Proceedings of the 21st International
Joint Conference on Artificial Intelligence (IJCAI-
09), pages 2083?2088, Pasadena, California.
Ponzetto, S. and M. Strube. 2007. Deriving a large
scale taxonomy from Wikipedia. In Proceedings
of the 22nd National Conference on Artificial In-
telligence (AAAI-07), pages 1440?1447, Vancouver,
British Columbia.
Singh, P., T. Lin, E. Mueller, G. Lim, T. Perkins,
and W. Zhu. 2002. Open Mind Common Sense:
Knowledge acquisition from the general public. In
Proceedings of the ODBASE Conference (ODBASE-
02), pages 1223?1237.
Snow, R., D. Jurafsky, and A. Ng. 2006. Semantic tax-
onomy induction from heterogenous evidence. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL-06), pages 801?808, Sydney, Aus-
tralia.
Talukdar, P., J. Reisinger, M. Pas?ca, D. Ravichan-
dran, R. Bhagat, and F. Pereira. 2008. Weakly-
supervised acquisition of labeled class instances us-
ing graph random walks. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP-08), pages 582?
590, Honolulu, Hawaii.
Treeratpituk, P. and J. Callan. 2006. Automatically la-
beling hierarchical clusters. In Proceedings of the
7th Annual Conference on Digital Government Re-
search (DGO-06), pages 167?176, San Diego, Cali-
fornia.
Van Durme, B. and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of
labeled instances for open-domain information ex-
traction. In Proceedings of the 23rd National Con-
ference on Artificial Intelligence (AAAI-08), pages
1243?1248, Chicago, Illinois.
Wang, R. and W. Cohen. 2008. Iterative set expan-
sion of named entities using the web. In Proceed-
ings of the International Conference on Data Min-
ing (ICDM-08), pages 1091?1096, Pisa, Italy.
Wang, R. and W. Cohen. 2009. Automatic set instance
extraction using the Web. In Proceedings of the
47th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-IJCNLP-09), pages 441?
449, Singapore.
Wu, F. and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of the
17th World Wide Web Conference (WWW-08), pages
635?644, Beijing, China.
962
