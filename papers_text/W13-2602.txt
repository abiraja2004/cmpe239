Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 11?20,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
A model of generalization in distributional learning of phonetic categories
Bozena Pajak
Brain & Cognitive Sciences
University of Rochester
Rochester, NY 14627-0268
bpajak@bcs.rochester.edu
Klinton Bicknell
Psychology
UC San Diego
La Jolla, CA 92093-0109
kbicknell@ucsd.edu
Roger Levy
Linguistics
UC San Diego
La Jolla, CA 92093-0108
rlevy@ucsd.edu
Abstract
Computational work in the past decade
has produced several models accounting
for phonetic category learning from distri-
butional and lexical cues. However, there
have been no computational proposals for
how people might use another powerful
learning mechanism: generalization from
learned to analogous distinctions (e.g.,
from /b/?/p/ to /g/?/k/). Here, we present
a new simple model of generalization in
phonetic category learning, formalized in
a hierarchical Bayesian framework. The
model captures our proposal that linguis-
tic knowledge includes the possibility that
category types in a language (such as
voiced and voiceless) can be shared across
sound classes (such as labial and velar),
thus naturally leading to generalization.
We present two sets of simulations that
reproduce key features of human perfor-
mance in behavioral experiments, and we
discuss the model?s implications and di-
rections for future research.
1 Introduction
One of the central problems in language acqui-
sition is how phonetic categories are learned, an
unsupervised learning problem involving mapping
phonetic tokens that vary along continuous di-
mensions onto discrete categories. This task may
be facilitated by languages? extensive re-use of a
set of phonetic dimensions (Clements 2003), be-
cause learning one distinction (e.g., /b/?/p/ vary-
ing along the voice onset time (VOT) dimension)
might help learn analogous distinctions (e.g., /d/?
/t/, /g/?k/). Existing experimental evidence sup-
ports this view: both infants and adults general-
ize newly learned phonetic category distinctions to
untrained sounds along the same dimension (Mc-
Claskey et al 1983, Maye et al 2008, Perfors
& Dunbar 2010, Pajak & Levy 2011a). However,
while many models have been proposed to account
for learning of phonetic categories (de Boer &
Kuhl 2003, Vallabha et al 2007, McMurray et al
2009, Feldman et al 2009, Toscano & McMur-
ray 2010, Dillon et al 2013), there have been no
computational proposals for how generalization
to analogous distinctions may be accomplished.
Here, we present a new simple model of gener-
alization in phonetic category learning, formal-
ized in a hierarchical Bayesian framework. The
model captures our proposal that linguistic knowl-
edge includes the possibility that category types
in a language (such as voiced and voiceless) can
be shared across sound classes (defined as previ-
ously learned category groupings, such as vowels,
consonants, nasals, fricatives, etc.), thus naturally
leading to generalization.
One difficulty for the view that learning one dis-
tinction might help learn analogous distinctions is
that there is variability in how the same distinc-
tion type is implemented phonetically for differ-
ent sound classes. For example, VOT values are
consistently lower for labials (/b/?/p/) than for ve-
lars (/g/?/k/) (Lisker & Abramson 1970), and the
durations of singleton and geminate consonants
are shorter for nasals (such as /n/?/nn/) than for
voiceless fricatives (such as /s/?/ss/) (Giovanardi
& Di Benedetto 1998, Mattei & Di Benedetto
2000). Improving on our basic model, we imple-
ment a modification that deals with this difficulty
by explicitly building in the possibility for analo-
gous categories along the same dimension to have
different absolute phonetic values along that di-
mension (e.g., shorter overall durations for nasals
than for fricatives).
In Section 2 we discuss the relevant background
on phonetic category learning, including previ-
ous modeling work. Section 3 describes our ba-
sic computational model, and Section 4 presents
simulations demonstrating that the model can re-
11
produce the qualitative patterns shown by adult
learners in cases when there is no phonetic vari-
ability between sound classes. In Section 5 we
describe the extended model that accommodates
phonetic variability across sound classes, and in
Section 6 we show that the improved model qual-
itatively matches adult learner performance both
when the sound classes implement analogous dis-
tinction types in identical ways, and when they dif-
fer in the exact phonetic implementation. Section 7
concludes with discussion of future research.
2 Background
One important source of information for unsuper-
vised learning of phonetic categories is the shape
of the distribution of acoustic-phonetic cues. For
example, under the assumption that each phonetic
category has a unimodal distribution on a particu-
lar cue, the number of modes in the distribution
of phonetic cues can provide information about
the number of categories: a unimodal distribution
along some continuous acoustic dimension, such
as VOT, may indicate a single category (e.g., /p/,
as in Hawaiian); a bimodal distribution may sug-
gest a two-category distinction (e.g., /b/ vs. /p/, as
in English); and a trimodal distribution implies a
three-category distinction (e.g., /b/, /p/, and /ph/,
as in Thai). Infants extract this distributional infor-
mation from the speech signal (Maye et al 2002,
2008) and form category representations focused
around the modal values of categories (Kuhl 1991,
Kuhl et al 1992, Lacerda 1995). Furthermore, in-
formation about some categories bootstraps learn-
ing of others: infants exposed to a novel bimodal
distribution along the VOT dimension for one
place of articulation (e.g., alveolar) not only learn
that novel distinction, but also generalize it to an
analogous contrast for another (e.g., velar) place
of articulation (Maye et al 2008). This ability is
preserved beyond infancy, and is potentially used
during second language learning, as adults are also
able to both learn from distributional cues and use
this information when making category judgments
about untrained sounds along the same dimensions
(Maye & Gerken 2000, 2001, Perfors & Dunbar
2010, Pajak & Levy 2011a,b).
The phonetic variability in how different sound
classes implement the same distinction type might
in principle hinder generalization across classes.
However, there is evidence of generalization even
in cases when sound classes differ in the exact
phonetic implementation of a shared distinction
type. For example, learning a singleton/geminate
length contrast for the class of voiceless fricatives
(e.g., /s/?/ss/, /f/?/ff/) generalizes to the class of
sonorants (e.g., /n/?/nn/, /j/?/jj/) even when the ab-
solute durations of sounds in the two classes are
different ? overall longer for fricatives than for
sonorants (Pajak & Levy 2011a) ? indicating that
learners are able to accomodate the variability of
phonetic cues across different sound classes.
Phonetic categorization from distributional cues
has been modeled using Gaussian mixture mod-
els, where each category is represented as a Gaus-
sian distribution with a mean and covariance ma-
trix, and category learning involves estimating
the parameters of each mixture component and
? for some models ? the number of components
(de Boer & Kuhl 2003, Vallabha et al 2007, Mc-
Murray et al 2009, Feldman et al 2009, Toscano
& McMurray 2010, Dillon et al 2013).1 These
models are successful at accounting for distribu-
tional learning, but do not model generalization.
We build on this previous work (specifically, the
model in Feldman et al 2009) and implement gen-
eralization of phonetic distinctions across different
sound classes.
3 Basic generalization model
The main question we are addressing here con-
cerns the mechanisms underlying generalization.
How do learners make use of information about
some phonetic categories when learning other
categories? Our proposal is that learners expect
category types (such as singleton and geminate,
or voiced and voiceless) to be shared among
sound classes (such as sonorants and fricatives).
We implement this proposal with a hierarchical
Dirichlet process (Teh et al 2006), which allows
for sharing categories across data groups (here,
sound classes). We build on previous computa-
tional work in this area that models phonetic cate-
gories as Gaussian distributions. Furthermore, we
follow Feldman et al (2009) in using Dirichlet
processes (Ferguson 1973), which allow the model
to learn the number of categories from the data,
and implementing the process of learning from
distributional cues via nonparametric Bayesian in-
ference.
1In Dillon et al (2013) each phoneme is modeled as a
mixture of Gaussians, where each component is an allophone.
12
HG0?
Gc?0
zic
dic i ? {1..nc}
c ? C
Figure 1: The graphical representation of the basic
model.
H : ? ? N (?0, ?
2
?0 )
?2 ? InvChiSq(?0,?20 )
G0 ? DP(?,H)
Gc ? DP(?0,G0)
zic ? Gc
dic ? N (?zic ,?2zic)
fc ? N (0,?2f )
dic ? N (?zic ,?2zic)+ fc
Figure 2: Mathematical description of the model.
The variables below the dotted line refer to the ex-
tended model in Figure 6.
3.1 Model details
As a first approach, we consider a simplified sce-
nario of a language with a set of sound classes,
each of which contains an unknown number of
phonetic categories, with perceptual token defined
as a value along a single phonetic dimension.
The model learns the set of phonetic categories
in each sound class, and the number of categories
inferred for one class can inform the inferences
about the other class. Here, we make the simpli-
fying assumption that learners acquire a context-
independent distribution over sounds, although the
model could be extended to use linguistic con-
text (such as coarticulatory or lexical information;
Feldman et al 2009).
Figure 1 provides the graphical representation
of the model, and Figure 2 gives its mathematical
Variable Explanation
H
base distribution over means and
variances of categories
G0
distribution over possible
categories
Gc
distribution over categories in
class c
?,?0 concentration parameters
zic category for datapoint dic
dic datapoint (perceptual token)
nc number of datapoints in class c
C set of classes
fc offset parameter
? f standard deviation of prior on fc
Table 1: Key for the variables in Figures 1, 2,
and 6. The variables below the dotted line refer
to the extended model in Figure 6.
description. Table 1 provides the key to the model
variables. In the model, speech sounds are pro-
duced by selecting a phonetic category zic, which
is defined as a mean ?zic and variance ?2zic along
a single phonetic dimension,2 and then sampling
a phonetic value from a Gaussian with that mean
and variance. We assume a weak prior over cat-
egories that does not reflect learners? prior lan-
guage knowledge (but we return to the possible
role of prior language knowledge in the discus-
sion). Learners? beliefs about the sound inventory
(distribution over categories and mean and vari-
ance of each category) are encoded through a hier-
archical Dirichlet process. Each category is sam-
pled from the distribution Gc, which is the distri-
bution over categories in a single sound class. In
order to allow sharing of categories across classes,
the Gc distribution for each class is sampled from a
Dirichlet process with base distribution G0, which
is shared across classes, and concentration param-
eter ?0 (which determines the sparsity of the dis-
tribution over categories). G0, then, stores the full
set of categories realized in any class, and it is
sampled from a Dirichlet process with concentra-
tion parameter ? and base distribution H, which
is a normal inverse chi-squared prior on category
2Although we are modeling phonetic categories as having
values along a single dimension, the model can be straight-
forwardly extended to multiple dimensions, in which case the
variance would be replaced by a covariance matrix ?zic .
13
means and variances.3 The parameters of the nor-
mal inverse chi-squared distribution are: ?0 and ?0,
which can be thought of as pseudo-observations,
as well as ?0 and ?20 , which determine the prior
distribution over means and variances, as in Fig-
ure 2.
3.2 Inference
The model takes as input the parameters of the
base distribution H, the concentration parameters
?0 and ? , and the data, which is composed of a
list of phonetic values. The model infers a poste-
rior distribution over category labels for each data-
point via Gibbs sampling. Each iteration of Gibbs
sampling resamples the assignments of each data-
point to a lower-level category (in Gc) and also re-
samples the assignments of lower-level categories
to higher-level categories (in G0). We marginalize
over the category means and variances.
4 Simulations: basic model
The first set of simulations has three goals: first,
to establish that our model can successfully per-
form distributional learning and second, to show
that it can use information about one type of class
to influence judgements about another, in the case
that there is no variability in category structure
between classes. Finally, these simulations reveal
a limitation of this basic model, showing that it
cannot generalize in the presence of substantial
between-class variability in category realizations.
We address this limitation in Section 5.
4.1 The data
The data we use to evaluate the model come
from the behavioral experiments in Pajak & Levy
(2011a). Adult native English speakers were ex-
posed to novel words, where the middle conso-
nant varied along the length dimension from short
(e.g., [ama]) to long (e.g., [amma]). The distri-
butional information suggested either one cate-
gory along the length dimension (unimodal distri-
bution) or two categories (bimodal distribution),
as illustrated in Figure 3. In Experiment 1, the
training included sounds in the sonorant class (4
continua: [n]-...-[nn], [m]-...-[mm], [j]-...-[jj], [l]-
...-[ll]) with the duration range of 100?205msec.
In Experiment 2 the training included sounds in
3In the case of categories defined along multiple di-
mensions, the base distribution would be a normal inverse-
Wishart.
l l
l l
0
4
8
12
16
0
4
8
12
16
Expt1:sonorants
Expt2:fricatives
100 120 140 160 180 200 220 240 260 280
Stimuli length continuum (in msec)
Fa
m
ilia
riz
ati
on
 fre
qu
en
cy
bimodal unimodal
Figure 3: Experiment 1 & 2 training (Pajak and
Levy 2011a). The y axis reflects the frequency
of tokens from each training continuum. The four
points indicate the values of the untrained data-
points.
the voiceless fricative class (4 continua: [s]-...-
[ss], [f]-...-[ff], [T]-...-[TT], [S]-...-[SS]) with the du-
ration range of 140?280msec. The difference in
duration ranges between the two classes reflected
the natural duration distributions of sounds in
these classes: generally shorter for sonorants and
longer for fricatives (Greenberg 1996, Giovanardi
& Di Benedetto 1998, Mattei & Di Benedetto
2000).
Subsequently, participants? expectations about
the number of categories in the trained class and
another untrained class were probed by asking for
judgments about tokens at the endpoints of the
continua: participants were presented with pairs
of words (e.g., sonorant [ama]?[amma] or frica-
tive [asa]?[assa]) and asked whether these were
two different words in this language or two rep-
etitions of the same word. As illustrated in Ta-
ble 2, in the test phase of Experiment 1 the du-
rations of both the trained and the untrained class
were identical (100msec for any short consonant
and 205msec for any long consonant), whereas
in the test phase of Experiment 2 the durations
were class-specific: longer for trained fricatives
(140msec for a short fricative and 280msec for a
long fricative) and shorter for untrained sonorants
(100msec for a short sonorant and 205msec for a
long sonorant).
The experiment results are illustrated in Fig-
ure 4. The data from the ?trained? condition shows
that learners were able to infer the number of cat-
egories from distributional cues: they were more
14
Expt1?trained Expt1?untrained
Pro
po
rtio
n o
f 'd
iffe
re
nt'
 re
sp
on
se
s
0.0
0.2
0.4
0.6
0.8
1.0
Bimodal
Unimodal
Expt2?trained Expt2?untrained
Pro
po
rtio
n o
f 'd
iffe
re
nt'
 re
sp
on
se
s
0.0
0.2
0.4
0.6
0.8
1.0
Bimodal
Unimodal
Figure 4: Experiment 1 & 2 results: proportion of ?different? responses on ?different? trials (Pajak and
Levy, 2011a).
Expt. 1 Expt. 2
trained
(sonorants) (fricatives)
100ms ? 205ms 140ms ? 280ms
untrained
(fricatives) (sonorants)
100ms ? 205ms 100ms ? 205ms
Table 2: Experiment 1 & 2 test (Pajak and Levy,
2011a).
likely to posit two categories (i.e., respond ?dif-
ferent? on ?different? trials) when the distribution
was bimodal than when the distribution was uni-
modal. In addition, as demonstrated by the ?un-
trained? condition, learners used the information
about the trained class to make inferences about
the untrained class: they were more likely to ac-
cept length-based category distinctions for frica-
tives after learning the distinction for sonorants
(Expt. 1), and vice versa (Expt. 2). This general-
ization occurred both (a) when each class imple-
mented the distinction in exactly the same way
(with the same absolute durations; Expt. 1), and
(b) when the classes differed in how the shared dis-
tinction type was implemented (the absolute dura-
tions of the untrained class were shifted relative to
the trained class; Expt. 2).
The model simulations described below at-
tempt to replicate the key features of human per-
formance: distributional learning and generaliza-
tion. We model both experiments of Pajak &
Levy (2011a): (a) ?same durations? across classes
(Expt. 1), and (b) ?different durations? across
classes (Expt. 2). Thus, the datasets we used
were closely modeled after their experimental de-
sign: (1) Expt. 1 bimodal, (2) Expt. 1 unimodal,
(3) Expt. 2 bimodal, and (4) Expt. 2 unimodal. In
each dataset, the data consisted of a list of pho-
netic values (duration in msec), where each data-
point was tagged as belonging to either the sono-
rant or the fricative class. The frequencies of the
?trained? class were as listed in Figure 3 (simu-
lating a single training continuum). In addition to
the ?trained? class, each dataset included two dat-
apoints from the ?untrained? class with the values
as listed in Table 2 in the ?untrained? condition.
These two datapoints were included in order to
evaluate the model?s categorization of sounds for
which no distributional evidence is available, thus
assessing the extent of generalization. We sim-
ulated weak perceptual noise by adding to each
datapoint normally-distributed error with standard
deviation of 0.3 times the distance between adja-
cent continuum steps.
4.2 Methodology
We ran the basic model on each of the four
datasets. For each, we performed 1,000,000 iter-
ations of Gibbs sampling, and analyzed the re-
sults for the second half. To assess convergence,
we ran four Markov chains for each dataset, us-
ing two overdispersed initializations: (1) assigning
one category label to all datapoints, and (2) as-
signing a different label to each datapoint. We
used a weak prior base distribution H (?0 = .001;
?0 = .001; ?20 = 1; ?0 was set to the overall mean
of the data), and set the concentration parameters
? = ?0 = 1.
4.3 Results and discussion
The simulation results are illustrated in Figure 5,4
plotting the proportion of samples on which the
model assigned the datapoints to two different cat-
egories, as opposed to a single category.5 Note that
4All variables we report in all simulations appear to have
converged to the posterior, as assessed by R? values of 1.1 or
less, calculated across the 4 chains (Gelman & Rubin 1992).
5No models we report assign the trained category data-
points to more than two categories more than 1% of the time.
15
trained untrained
0.00
0.25
0.50
0.75
1.00
bim
od
al
un
imo
da
l
bim
od
al
un
imo
da
l
Pro
por
tion
 of
 2?
cat
ego
ry 
infe
re
nc
es
Basic model:
Experiment 1
trained untrained
0.00
0.25
0.50
0.75
1.00
bim
od
al
un
imo
da
l
bim
od
al
un
imo
da
l
Pro
por
tion
 of
 2?
cat
ego
ry 
infe
re
nc
es
Basic model:
Experiment 2
Figure 5: Simulation results for the basic model.
Error bars give 95% binomial confidence intervals,
computed using the estimated number of effec-
tively independent samples in the Markov chains.
in the ?trained? condition, this means categoriza-
tion of all datapoints along the continuum. In the
?untrained? condition, on the other hand, it is cat-
egorization of two datapoints: one from each end-
point of the continuum.
The results in the ?trained? conditions demon-
strate that the model was able to learn from the
distributional cues, thus replicating the success of
previous phonetic category learning models.
Of most interest here are the results in the ?un-
trained? condition. The figure on the left shows
the results modeling the ?same-durations? exper-
iment (Expt. 1), demonstrating that the model cat-
egorizes the two datapoints in the untrained sound
class in exactly the same way as it did for the
trained sound class: two categories in the bimodal
condition, and one category in the unimodal con-
dition. Thus, these results suggest that we can suc-
cessfully model generalization of distinction types
across sound classes in phonetic category learning
by assuming that learners have an expectation that
category types (such as short and long, or voice-
less and voiced) may be shared across classes.
The figure on the right shows the results model-
ing the ?different-durations? experiment (Expt. 2),
revealing a limitation of the model: failure to gen-
eralize when the untrained class has the same cat-
egory structure but different absolute phonetic val-
ues (overall shorter in the untrained class than in
the trained class). Instead, the model categorizes
both untrained datapoints as belonging to a single
category. This result diverges from the experimen-
tal results, where learners generalize the learned
distinction type in both cases, whether the abso-
lute phonetic values of the analogous categories
are identical or not. We address this problem in
the next section by implementing a modification
to the model that allows more flexibility in how
each class implements the same category types.
5 Extended generalization model
The goal of the extended model is to explicitly al-
low for phonetic variability across sound classes.
As a general approach, we could imagine func-
tions that transform categories across classes so
that the same categories can be ?reused? by be-
ing translated around to different parts of the pho-
netic space. These functions would be specific op-
erations representing any intrinsic differences be-
tween sound classes. Here, we use a very simple
function that can account for one widely attested
type of transformation: different absolute phonetic
values for analogous categories in distinct sound
classes (Ladefoged & Maddieson 1996), such as
longer overall durations for voiceless fricatives
than for sonorants. This type of transformation
has been successfully used in prior modeling work
to account for learning allophones of a single
phoneme that systematically vary in phonetic val-
ues along certain dimensions (Dillon et al 2013).
5.1 Model details
We implement the possibility for between-class
variability by allowing for one specific type of
idiosyncratic implementation of categories across
classes: learnable class-specific ?offsets? by which
the data in a class are shifted along the phonetic
dimension, as illustrated in Figure 6 (the key for
the variables is in Table 1).
5.2 Inference
Each iteration of MCMC now includes a
Metropolis-Hastings step to resample the offset
parameters fc, which uses a zero-mean Gaussian
proposal, with standard deviation ?p = range of data5 .
6 Simulations: extended model
This second set of simulations has two goals: (1) to
establish that the extended model can successfully
replicate the performance of the basic model in
both distributional learning and generalization in
the no-variability case, and (2) to show that ex-
plicitly allowing for variability across classes lets
the model generalize when there is between-class
variability in category realizations.
16
HG0?
Gc?0
zic
dic
fc
? f
i ? {1..nc}
c ? C
Figure 6: The graphical representation of the ex-
tended model.
6.1 Methodology
We used the same prior as in the first set of sim-
ulations, and used a Gaussian prior on the offset
parameter with standard deviation ? f = 1000. Be-
cause only the relative values of offset parameters
are important for category sharing across classes,
we set the offset parameter for one of the classes
to zero. The four Markov chains now crossed cate-
gory initialization with two different initial values
of the offset parameter.
6.2 Results and discussion
The simulation results are illustrated in Fig-
ure 7. The figure on the left demonstrates that
the extended model performs similarly to the ba-
sic model in the case of no variability between
classes. The figure on the right, on the other
hand, shows that ? unlike the basic model ?
the extended model succeeds in generalizing the
learned distinction type to an untrained sound
class when there is phonetic variability between
classes. These results suggest that allowing for
variability in category implementations across
sound classes may be necessary to account for
human learning. Taken together, these results are
consistent with our proposal that language learn-
ers have an expectation that category types can be
shared across sound classes. Furthermore, learn-
ers appear to have implicit knowledge of the ways
that sound classes can vary in their exact phonetic
implementations of different category types. This
trained untrained
0.00
0.25
0.50
0.75
1.00
bim
od
al
un
imo
da
l
bim
od
al
un
imo
da
l
Pro
por
tion
 of
 2?
cat
ego
ry 
infe
re
nc
es
Extended model:
Experiment 1
trained untrained
0.00
0.25
0.50
0.75
1.00
bim
od
al
un
imo
da
l
bim
od
al
un
imo
da
l
Pro
por
tion
 of
 2?
cat
ego
ry 
infe
re
nc
es
Extended model:
Experiment 2
Figure 7: Simulation results for the extended
model. Error bars give 95% binomial confidence
intervals, computed using the estimated number
of effectively independent samples in the Markov
chains.
type of knowledge may include ? as in our ex-
tended generalization model ? the possibility that
phonetic values of categories in one class can be
systematically shifted relative to another.
7 General discussion
In this paper we presented the first model of gen-
eralization in phonetic category learning, in which
learning a distinction type for one set of sounds
(e.g., /m/?/mm/) immediately generalizes to an-
other set of sounds (e.g., /s/?/ss/), thus reproduc-
ing the key features of adult learner performance
in behavioral experiments. This extends previous
computational work in phonetic category learn-
ing, which focused on modeling the process of
learning from distributional cues, and did not ad-
dress the question of generalization. The basic
premise of the proposed model is that learners?
knowledge of phonetic categories is represented
hierarchically: individual sounds are grouped into
categories, and individual categories are grouped
into sound classes. Crucially, the category struc-
ture established for one sound class can be di-
rectly shared with another class, although differ-
ent classes can implement the categories in id-
iosyncratic ways, thus mimicking natural variabil-
ity in how analogous categories (e.g., short /m/ and
/s/, or long /mm/ and /ss/) are phonetically imple-
mented for different sound classes.
The simulation results we presented succeed
in reproducing the human pattern of generaliza-
tion performance, in which the proportion of two-
category inferences about the untrained class is
17
very similar to that for the trained class. Note,
however, that there are clear quantitative dif-
ferences between the two in learning perfor-
mance: the model learns almost perfectly from
the available distributional cues (?trained? condi-
tion), while adult learners are overall very conser-
vative in accepting two categories along the length
dimension, as indicated by the overall low num-
ber of ?different? responses. There are two main
reasons why the model might be showing more
extreme categorization preferences than humans
in this particular task. First, humans have cogni-
tive limitations that the current model does not,
such as those related to memory or attention. In
particular, imperfect memory makes it harder for
humans to integrate the distributional information
from all the trials in the exposure, and longer train-
ing would presumably improve performance. Sec-
ond, adults have strong native-language biases that
affect learning of a second language (Flege 1995).
The population tested by Pajak & Levy (2011a)
consisted of adult native speakers of American En-
glish, a language in which length is not used con-
trastively. Thus, the low number of ?different? re-
sponses in the experiments can be attributed to
participants? prior bias against category distinc-
tions based on length. The model, on the other
hand, has only a weak prior that was meant to be
easily overridden by data.
This last point is of direct relevance for the
area of second language (L2) acquisition, where
one of the main research foci is to investigate
the effects of native-language knowledge on L2
learning. The model we proposed here can poten-
tially be used to systematically investigate the role
of native-language biases when learning category
distinctions in a new language. In particular, an L2
learner, whose linguistic representations include
two languages, could be implemented by adding
a language-level node to the model?s hierarchical
structure (through an additional Dirichlet process).
This extension will allow for category structures to
be shared not just within a language for different
sound classes, but also across languages, thus ef-
fectively acting as a native-language bias.
As a final note, we briefly discuss alternative
ways of modeling generalization in phonetic cat-
egory learning. In the model we described in this
paper, whole categories are generalized from one
class to another. However, one might imagine an-
other approach to this problem where generaliza-
tion is a byproduct of learners? attending more
to the dimension that they find to be relevant for
distinguishing between some categories in a lan-
guage. That is, learners? knowledge would not in-
clude the expectation that whole categories may
be shared across classes, as we argued here, but
rather that a given phonetic dimension is likely
to be reused to distinguish between categories in
multiple sound classes.
This intuition could be implemented in differ-
ent ways. In a Dirichlet process model of category
learning, the concentration parameter ? might be
learned, and shared for all classes along a given
phonetic dimension, thus producing a bias to-
ward having a similar number of categories across
classes. Alternatively, the variance of categories
along a given dimension might be learned, and
also shared for all classes. Under this scenario,
learning category variance along a given dimen-
sion would help categorize novel sounds along that
dimension. That is, two novel datapoints would be
likely categorized into separate categories if the in-
ferred variance along the relevant dimension was
smaller than the distance between the datapoints,
but into a single category if the inferred variance
was comparable to that distance.
Finally, this model assumes that sound classes
are given in advance, and that only the categories
within each class are learned. While this assump-
tion may seem warranted for some types of per-
ceptually dissimilar sound classes (e.g., conso-
nants and vowels), and also may be appropriate
for L2 acquisition, it is not clear that it is true for
all sound classes that allow for generalization in
infancy. It remains for future work to determine
how learners may generalize while simultaneously
learning the sound classes.
We plan to pursue all these directions in fu-
ture work with the ultimate goal of improving our
understanding how human learners represent their
linguistic knowledge and how they use it when ac-
quiring a new language.
Acknowledgments
We thank Gabriel Doyle and three anonymous
CMCL reviewers for useful feedback. This re-
search was supported by NIH Training Grant T32-
DC000041 from the Center for Research in Lan-
guage at UC San Diego to B.P. and NIH Training
Grant T32-DC000035 from the Center for Lan-
guage Sciences at University of Rochester to B.P.
18
References
de Boer, Bart & Patricia K. Kuhl. 2003. Inves-
tigating the role of infant-directed speech with
a computer model. Acoustic Research Letters
Online 4(4). 129?134.
Clements, George N. 2003. Feature economy in
sound systems. Phonology 20. 287?333.
Dillon, Brian, Ewan Dunbar & William Idsardi.
2013. A single-stage approach to learning
phonological categories: Insights from Inukti-
tut. Cognitive Science 37. 344?377.
Feldman, Naomi H., Thomas L. Griffiths &
James L. Morgan. 2009. Learning phonetic cat-
egories by learning a lexicon. In Proceedings
of the 31st Annual Conference of the Cognitive
Science Society, 2208?2213. Austin, TX: Cog-
nitive Science Society.
Ferguson, Thomas S. 1973. A Bayesian analy-
sis of some nonparametric problems. Annals of
Statistics 1. 209?230.
Flege, James E. 1995. Second-language speech
learning: theory, findings and problems. In
Winifred Strange (ed.), Speech perception and
linguistic experience: issues in cross-language
research, 229?273. Timonium, MD: York Press.
Gelman, Andrew & Donald B. Rubin. 1992. In-
ference from iterative simulation using multiple
sequences. Statistical Science 7. 457?511.
Giovanardi, Maurizio & Maria-Gabriella
Di Benedetto. 1998. Acoustic analysis of
singleton and geminate fricatives in Italian.
The European Journal of Language and Speech
(EACL/ESCA/ELSNET) 1998. 1?13.
Greenberg, Steven. 1996. The Switchboard tran-
scription project. Report prepared for the
1996 CLSP/JHU Workshop on Innovative Tech-
niques in Continuous Large Vocabulary Speech
Recognition.
Kuhl, Patricia K. 1991. Human adults and human
infants show a ?perceptual magnet effect? for
the prototypes of speech categories, monkeys
do not. Perception and Psychophysics 50(2).
93?107.
Kuhl, Patricia K., Karen A. Williams, Francisco
Lacerda, Kenneth N. Stevens & Bjo?rn Lind-
blom. 1992. Linguistic experience alters pho-
netic perception in infants by 6 months of age.
Science 255. 606?608.
Lacerda, Francisco. 1995. The perceptual magnet-
effect: An emergent consequence of exemplar-
based phonetic memory. In K. Ellenius &
P. Branderud (eds.), Proceedings of the 13th
International Congress of Phonetic Sciences,
140?147. Stockholm: KTH and Stockholm Uni-
versity.
Ladefoged, Peter & Ian Maddieson. 1996. The
sounds of the world?s languages. Oxford, UK;
Cambridge, MA: Blackwell.
Lisker, Leigh & Arthur S. Abramson. 1970. The
voicing dimensions: Some experiments in com-
parative phonetics. In Proceedings of the Sixth
International Congress of Phonetic Sciences,
Prague: Academia.
Mattei, Marco & Maria-Gabriella Di Benedetto.
2000. Acoustic analysis of singleton and gemi-
nate nasals in Italian. The European Journal of
Language and Speech (EACL/ESCA/ELSNET)
2000. 1?11.
Maye, Jessica & LouAnn Gerken. 2000. Learning
phonemes without minimal pairs. In S. Cather-
ine Howell, Sarah A. Fish & Thea Keith-Lucas
(eds.), Proceedings of the 24th Annual Boston
University Conference on Language Develop-
ment, 522?533. Somerville, MA: Cascadilla
Press.
Maye, Jessica & LouAnn Gerken. 2001. Learn-
ing phonemes: how far can the input take us?
In A. H-J. Do, L. Dom??nguez & A. Johansen
(eds.), Proceedings of the 25th Annual Boston
University Conference on Language Develop-
ment, 480?490. Somerville, MA: Cascadilla
Press.
Maye, Jessica, Daniel J. Weiss & Richard N.
Aslin. 2008. Statistical phonetic learning in
infants: facilitation and feature generalization.
Developmental Science 11(1). 122?134.
Maye, Jessica, Janet F. Werker & LouAnn Gerken.
2002. Infant sensitivity to distributional infor-
mation can affect phonetic discrimination. Cog-
nition 82. B101?B111.
McClaskey, Cynthia L., David B. Pisoni &
Thomas D. Carrell. 1983. Transfer of training
of a new linguistic contrast in voicing. Percep-
tion and Psychophysics 34(4). 323?330.
McMurray, Bob, Richard N. Aslin & Joseph C.
Toscano. 2009. Statistical learning of phonetic
categories: insights from a computational ap-
proach. Developmental Science 12(3). 369?
378.
Pajak, Bozena & Roger Levy. 2011a. How ab-
stract are phonological representations? Evi-
dence from distributional perceptual learning.
19
In Proceedings of the 47th Annual Meeting of
the Chicago Linguistic Society, Chicago, IL:
University of Chicago.
Pajak, Bozena & Roger Levy. 2011b. Phono-
logical generalization from distributional evi-
dence. In L. Carlson, C. Ho?lscher & T. Ship-
ley (eds.), Proceedings of the 33rd Annual Con-
ference of the Cognitive Science Society, 2673?
2678. Austin, TX: Cognitive Science Society.
Perfors, Amy & David Dunbar. 2010. Phonetic
training makes word learning easier. In S. Ohls-
son & R. Catrambone (eds.), Proceedings of the
32nd Annual Conference of the Cognitive Sci-
ence Society, 1613?1618. Austin, TX: Cogni-
tive Science Society.
Teh, Yee Whye, Michael I. Jordan, Matthew J.
Beal & David M. Blei. 2006. Hierarchical
Dirichlet processes. Journal of the American
Statistical Association 101(476). 1566?1581.
Toscano, Joseph C. & Bob McMurray. 2010. Cue
integration with categories: Weighting acoustic
cues in speech using unsupervised learning and
distributional statistics. Cognitive Science 34.
434?464.
Vallabha, Gautam K., James L. McClelland, Fer-
ran Pons, Janet F. Werker & Shigeaki Amano.
2007. Unsupervised learning of vowel cate-
gories from infant-directed speech. Proceedings
of the National Academy of Sciences 104(33).
13273?13278.
20
