Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 119?123,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Recognizing English Learners? Native Language from Their Writings 
 
 
Baoli LI 
Department of Computer Science 
Henan University of Technology 
1 Lotus Street, High & New Technology Industrial Development Zone 
Zhengzhou, China, 450001 
csblli@gmail.com 
 
 
 
 
 
 
Abstract 
Native Language Identification (NLI), which 
tries to identify the native language (L1) of a 
second language learner based on their writ-
ings, is helpful for advancing second language 
learning and authorship profiling in forensic 
linguistics. With the availability of relevant 
data resources, much work has been done to 
explore the native language of a foreign lan-
guage learner. In this report, we present our 
system for the first shared task in Native Lan-
guage Identification (NLI). We use a linear 
SVM classifier and explore features of words, 
word and character n-grams, style, and 
metadata. Our official system achieves accu-
racy of 0.773, which ranks it 18th among the 
29 teams in the closed track. 
1 Introduction 
Native Language Identification (NLI) (Ahn, 2011; 
Kochmar, 2011), which tries to identify the native 
language (L1) of a second language learner based 
on their writings, is expected to be helpful for ad-
vancing second language learning and authorship 
profiling in forensic linguistics. With the availabil-
ity of relevant data resources, much work has been 
done to explore the effective way to identify the 
native language of a foreign language learner 
(Koppel et al, 2005; Wong et al, 2011; Brooke 
and Hirst, 2012a, 2012b; Bykh and Meurers, 2012; 
Crossley and McNamara, 2012; Jarvis et al, 2012; 
Jarvis and Paquot, 2012; Tofighi et al, 2012; Tor-
ney et al 2012). 
To evaluate different techniques and approaches 
to Native Language Identification with the same 
setting, the first shared task in Native Language 
Identification (NLI) was organized by researchers 
from Nuance Communications and Educational 
Testing Service (Tetreault et al, 2013). A larger 
and more reliable data set, TOEFL11 (Blanchard et 
al., 2013), was used in this open evaluation. 
This paper reports our NLI2013 shared task sys-
tem that we built at the Department of Computer 
Science, Henan University of Technology, China. 
To be involved in this evaluation, we would like to 
obtain a more thorough knowledge of the research 
on native language identification and its state-of-
the-art, as we may focus on authorship attribution 
(Koppel et al, 2008) problems in the near future. 
The NLI2013 shared task is framed as a super-
vised text classification problem where the set of 
native languages (L1s), i.e. categories, is known, 
which includes Arabic, Chinese, French, German, 
Hindi, Italian, Japanese, Korean, Spanish, Telugu, 
and Turkish. A system is given a large part of the 
TOEFL11 dataset for training a detection model, 
and then makes predictions on the test writing 
samples. 
Inspired by our experience of dealing with dif-
ferent text classification problems, we decide to 
employ a linear support vector machine (SVM) in 
our NLI2013 system. We plan to take this system 
as a starting point, and may explore other complex 
classifiers in the future. Although in-depth syntac-
119
tic features may be helpful for this kind of tasks 
(Bergsma et al, 2012; Wong and Dras, 2011; 
Swanson and Charniak, 2012; Wong et al, 2012), 
we decide to explore the effectiveness of the tradi-
tional word and character features, as well as style 
features, in our system. We would like to verify on 
the first open available large dataset whether these 
traditional features work and how good they are. 
 
 
 
Figure 1. System Architecture. 
 
We submitted four runs with different feature 
sets. The run with all the features achieved the best 
accuracy of 0.773, which ranks our system 18th 
among the 29 systems in the closed track. 
In the rest of this paper we describe the detail of 
our system and analyze the results. Section 2 gives 
the overview of our system, while Section 3 dis-
cusses the various features in-depth. We present 
our experiments and discussions in Section 4, and 
conclude in Section 5. 
2 System Description  
Figure 1 gives the architecture of our NLI2013 
system, which takes machine learning framework. 
At the training stage, annotated data is first pro-
cessed through preprocessing and feature extrac-
tion, then fed to the classifier learning module, and 
we can finally obtain a NLI model. At the testing 
stage, each test sample goes through the same pre-
processing and feature extraction modules, and is 
assigned a category with the learned NLI model. 
Data Preprocessing: this module aims at trans-
forming the original data into a suitable format for 
the system, e.g. inserting the category information 
into the individual writing sample and attaching 
metadata to essays. 
Feature Extraction: this module tries to obtain 
all the useful features from the original data. We 
considered features like: word, word n-gram, char-
acter n-gram, style, and available metadata. 
Linear SVM training and testing: these two 
modules are the key components. The training 
module takes the transformed digitalized vectors as 
input, and train an effective NLI model, where the 
testing module just applies the learned model on 
the testing data. As linear support vector machines 
(SVM) achieves quite good performance on a lot 
of text classification problems, we use this general 
machine learning algorithm in our NLI2013 system. 
The excellent SVM implementation, Libsvm 
(Chang and Lin, 2011), was incorporated in our 
system and TFIDF is used to derive the feature 
values in vectors. Then, we turn to focus on what 
features are effective for native language identifi-
cation. We explore words, word n-grams, character 
n-grams, style, and metadata features in the system. 
3 Features 
In this section, we explain what kind of features we 
used in our NLI2013 system. 
3.1 Word and Word n-gram 
The initial feature set is words or tokens in the da-
taset. As the dataset is tokenized and sen-
tence/paragraph split, we simply use space to 
delimit the text and get individual tokens. We re-
move rare features that appear only once in the 
training dataset. Words or tokens are transformed 
to lowercase. 
Word n-grams are combined by consecutive 
words or tokens. They are expecting to capture 
some syntactic characteristics of writing samples. 
Two special tokens, ?BOS? and ?EOS?, which in-
dicate ?Beginning? and ?Ending?, are attached at 
the two ends of a sentence. We considered word 2-
grams and word 3-grams in our system. 
3.2 Character n-gram 
120
We assume sub-word features like prefix and 
suffix are useful for detecting the learners? native 
languages. To simplify the process rather than 
employing a complex morphological analyzer, we 
consider character n-grams as another important 
feature set. The n-grams are extracted from each 
sentence by regarding the whole sentence as a 
large word / string and replacing the delimited 
symbol (i.e. white space) with a special uppercase 
character ?S?. As what we did in getting word n-
grams, we attached two special character ?B? and 
?E? at the two ends of a sentence. Character 2-
grams, 3-grams, 4-grams, and 5-grams are used in 
our system. 
3.3 Style 
We would like to explore whether the traditional 
style features are helpful for this task as those fea-
tures are widely used in authorship attribution. We 
include the following style features: 
? __PARA__: a paragraph in an essay; 
? __SENT__: a sentence in an essay; 
? PARASENTLEN=NN: a paragraph of NN 
sentences long; 
? SENTWDLEN=NN: a sentence of 4*NN 
words long; 
? WDCL=NN: a word of NN characters long; 
3.4 Other 
As the TOEFL11 dataset includes two metadata for 
each essay, English language proficiency level 
(high, medium, or low) and Prompt ID, we include 
them as additional features in our system. 
4 Experiments and Results 
4.1 Dataset 
The dataset of the NLI2013 shared task contains 
12,100 English essays from the Test of English as 
a Foreign Language (TOEFL). Educational Testing 
Service (ETS) published the dataset through the 
LDC with the motivation to create a larger and 
more reliable data set for researchers to conduct 
Native Language Identification experiments on. 
This dataset, henceforth TOEFL11, comprises 11 
native languages (L1s) with 1,000 essays per lan-
guage. The 11 covered native languages are: Ara-
bic, Chinese, French, German, Hindi, Italian, 
Japanese, Korean, Spanish, Telugu, and Turkish. 
In addition, each essay in the TOEFL11 is marked 
with an English language proficiency level (high, 
medium, or low) based on the judgments of human 
assessment specialists. The essays are usually 300 
to 400 words long. 9,900 essays of this set are cho-
sen as the training data, 1,100 are for development 
and the rest 1,100 as test data.  
 
Runs HAUTCS-1 HAUTCS-2 HAUTCS-3 HAUTCS-4 
Accuracy 0.773 0.758 0.76 0.756 
ARA 0.7311 0.703 0.703 0.71 
CHI 0.82 0.794 0.794 0.782 
FRE 0.806 0.788 0.786 0.783 
GER 0.897 0.899 0.899 0.867 
HIN 0.686 0.688 0.694 0.707 
ITA 0.83 0.84 0.844 0.844 
JPN 0.832 0.792 0.798 0.81 
KOR 0.763 0.764 0.768 0.727 
SPA 0.703 0.651 0.651 0.65 
TEL 0.702 0.702 0.702 0.751 
TUR 0.736 0.715 0.716 0.698 
 
Table 1. Official results of our system. 
 
 
Figure 2. Performance of our official runs. 
 
4.2 Official Results 
Accuracy, which measures the percentage of how 
many essays are correctly detected, is used as the 
main evaluation metric in the NLI2013 shared task.  
Table 1 gives the official results of our system 
on the evaluation data. We submitted four runs 
with different feature sets: 
HAUTCS-1: all the features, which include 
words, word 2-grams, word 3-grams, character 2-
grams, character 3-grams, character 4-grams, 
                                                          
1
 This number, as well as others in the cells from this row to 
the bottom, is value of F-1 measure for each language. 
121
character 5-grams, style, and other metadata fea-
tures; 
HAUTCS-2:  uses words, word 2-grams, word 
3-grams, style, and other metadata features; 
HAUTCS-3: uses words, word 2-grams, word 
3-grams, and other metadata features; 
HAUTCS-4: uses words or tokens and other 
metadata features. 
For the runs HAUTCS-2, HAUTCS-3, and 
HAUTCS-4, we combined the development and 
training data for learning the identification model, 
where for the HAUTCS-1, it?s a pity that we forgot 
to include the development data for training the 
model. 
Our best run (HAUTCS-1) achieved the overall 
accuracy (0.773). The system performs best on the 
German category, but poorest on the Hindi catego-
ry, as can be easily seen on figure 2. 
Analyzing the four runs? performance showing 
on figure 2, we observe: word features are quite 
effective for Telugu and Hindi categories, but not 
powerful enough for others; word n-grams are 
helpful for languages Chinese, French, German, 
Korean, and Turkish, but useless for others; Style 
features only boost a little for French; Character n-
grams work for Arabic, Chinese, French, Japanese, 
Spanish, and Turkish; Spanish category prefers 
character n-grams, where Telugu category likes 
word features. As different features have different 
effects on different languages, a better NLI system 
is expected to use different features for different 
languages. 
After the evaluation, we experimented with the 
same setting as the HAUTCS-1 run, but included 
both training and development data for learning the 
NLI model. We got accuracy 0.781 on the new 
released test data, which has the same format with 
paragraph split as the training and development 
data. 
As we include style features like how many par-
agraphs in an essay, the old test data, which re-
moved the paragraph delimiters (i.e. single blank 
lines), may be not good for our trained model. 
Therefore, we did experiments with the new test 
data. Unfortunately, the accuracy 0.772 is a little 
poorer than that we obtained with the old test data. 
It seems that the simple style features are not effec-
tive in this task. As shown in table 1, HAUTCS-2 
performs poorer than HAUTCS-3, which helps us 
derive the same conclusion. 
4.3 Additional Experiments 
We did 10-fold cross validation on the training and 
development data with the same setting as the 
HAUTCS-1 run. The data splitting is given by the 
organizers. Accuracies of the 10 runs are show in 
table 2. The overall accuracy 0.799 is better than 
that on the test data. 
 
Fold 1 2 3 4 5 
Accuracy 0.802 0.795 0.81 0.791 0.79 
Fold 6 7 8 9 10 
Accuracy 0.805 0.789 0.803 0.798 0.805 
Table 2. Results of 10-fold cross validation on the train-
ing and development data. 
 
To check how metadata features work, we did 
another run HAUTCS-5, which uses only words as 
features. This run got the same overall accuracy 
0.756 on the old test data as HAUTCS-4 did, 
which demonstrates that those metadata features 
may not provide much useful information for na-
tive language identification. 
5 Conclusion and Future Work 
In this paper, we report our system for the 
NLI2013 shared task, which automatically detect-
ing the native language of a foreign English learner 
from her/his writing sample. The system was built 
on a machine learning framework with traditional 
features including words, word n-grams, character 
n-grams, and writing styles. Character n-grams are 
simple but quite effective. 
We plan to explore syntactic features in the fu-
ture, and other machine learning algorithms, e.g. 
ECOC (Li and Vogel, 2010), also deserve further 
experiments. As we discussed in section 4, we are 
also interested in designing a framework to use 
different features for different categories. 
 
Acknowledgments 
This work was supported by the Henan Provincial 
Research Program on Fundamental and Cutting-
Edge Technologies (No. 112300410007), and the 
High-level Talent Foundation of Henan University 
of Technology (No. 2012BS027). Experiments 
were performed on the Amazon Elastic Compute 
Cloud. 
122
References  
Ahn, C. S. 2011. Automatically Detecting Authors' Na-
tive Language. Master's thesis, Naval Postgraduate 
School, Monterey, CA. 
Bergsma, S., Post, M., and Yarowsky, D. 2012. Stylo-
metric analysis of scientific articles. In Proceedings 
of the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics: 
Human Language Technologies, pages 327?337, 
Montr?al, Canada. Association for Computational 
Linguistics. 
Blanchard, D., Tetreault, J., Higgins, D., Cahill, A., and 
Chodorow, M. 2013. TOEFL11: A Corpus of Non-
Native English. Technical report, Educational Test-
ing Service. 
Brooke, J. and Hirst, G. 2012a. Measuring interlanguage: 
Native language identification with l1-influence met-
rics. In Calzolari, N., Choukri, K., Declerck, T., 
Dogan, M. U., Maegaard, B., Mariani, J., Odijk, J., 
and Piperidis, S., editors, Proceedings of the Eighth 
International Conference on Language Resources and 
Evaluation (LREC-2012), pages 779?784, Istanbul, 
Turkey 
Brooke, J. and Hirst, G. 2012b. Robust, Lexicalized 
Native Language Identification. In Proceedings of 
COLING 2012, pages 391-408, Mumbai, India.  
Bykh, S. and Meurers, D. 2012. Native Language Iden-
tification using Recurring n-grams - Investigating 
Abstraction and Domain Dependence. In Proceedings 
of COLING 2012, pages 425-440, Mumbai, India. 
Chang, C.-C. and Lin C.-J. 2011. LIBSVM : a library 
for support vector machines. ACM Transactions on 
Intelligent Systems and Technology, 2:3:27:1-27. 
Crossley, S. A. and McNamara, D. 2012. Detecting the 
First Language of Second Language Writers Using 
Automated Indices of Cohesion, Lexical Sophistica-
tion, Syntactic Complexity and Conceptual 
Knowledge. In Jarvis, S. and Crossley, S. A., editors, 
Approaching Language Transfer through Text Classi-
fication, pages 106-126. Multilingual Matters. 
Jarvis, S., Casta?eda-Jim?nez, G., and Nielsen, R. 2012. 
Detecting L2 Writers' L1s on the Basis of Their Lex-
ical Styles. In Jarvis, S. and Crossley, S. A., editors, 
Approaching Language Transfer through Text Classi-
fication, pages 34-70. Multilingual Matters. 
Jarvis, S. and Paquot, M. 2012. Exploring the Role of n-
Grams in L1 Identification. In Jarvis, S. and Crossley, 
S. A., editors, Approaching Language Transfer 
through Text Classification, pages 71-105. Multilin-
gual Matters. 
Kochmar, E. 2011. Identification of a writer?s native 
language by error analysis. Master?s thesis, Universi-
ty of Cambridge. 
Koppel, M., Schler, J., and Zigdon, K. 2005. Determin-
ing an author?s native language by mining a text for 
errors. In Proceedings of the eleventh ACM 
SIGKDD international conference on Knowledge 
discovery in data mining, pages 624?628, Chicago, 
IL. ACM. 
Koppel, M., Schler, J., and Argamon, S. 2008. Compu-
tational methods in authorship attribution. Journal of 
the American Society for information Science and 
Technology, 60(1):9?26. 
Li, B., and Vogel, C. 2010. Improving Multiclass Text 
Classification with Error-Correcting Output Coding 
and Sub-class Partitions. In Proceedings of the 23rd 
Canadian Conference on Artificial Intelligence, pag-
es 4-15, Ottawa, Canada. 
Swanson, B. and Charniak, E. 2012. Native language 
detection with tree substitution grammars. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short 
Papers), pages 193?197, Jeju Island, Korea. 
Tetreault, J., Blanchard, D., and Cahill, A. 2013.  A 
report on the first native language identification 
shared task.  In Proceedings of the Eighth Workshop 
on Innovative Use of NLP for Building Educational 
Applications.  Atlanta, GA, USA. 
Tofighi, P.; K?se, C.; and Rouka, L.  2012.  Author?s 
native language identification from web-based 
texts.  International Journal of Computer and Com-
munication Engineering. 1(1):47-50 
Torney, R.; Vamplew, P.; and Yearwood, 
J.  2012.  Using psycholinguistic features for profil-
ing first language of authors.  Journal of the Ameri-
can Society for Information Science and 
Technology.  63(6):1256-1269. 
Wong, S.-M. J. and Dras, M. 2011. Exploiting Parse 
Structures for Native Language Identification. In 
Proceedings of the 2011 Conference on Empirical 
Methods in Natural Language Processing, pages 
1600?1610, Edinburgh, Scotland, UK. 
Wong, S.-M. J., Dras, M., and Johnson, M. 2012. Ex-
ploring Adaptor Grammars for Native Language 
Identification. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, pages 699?709, Jeju Island, Korea. 
123
