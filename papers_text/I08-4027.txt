Word Boundary Token Model for the SIGHAN Bakeoff 2007 
Tsai Jia-Lin 
Department of Information Management 
Tungnan University 
Taipei 222, Taiwan 
tsaijl@mail.tnu.edu.tw 
 
 
Abstract 
This paper describes a Chinese word seg-
mentation system based on word boundary 
token model and triple template matching 
model for extracting unknown words; and 
word support model for resolving segmen-
tation ambiguity. 
1 Introduction 
In the SIGHAN bakeoff 2007, we participated in 
the CKIP and the CityU closed tasks. Our Chinese 
word segmentation system is based on three mod-
els: (a) word boundary token (WBT) model and (b) 
triple context matching model for unknown word 
extraction, and (c) word support model for seg-
mentation disambiguation. Since the word support 
model and triple context matching model have 
been proposed in our previous work (Tsai, 2005, 
2006a and 2006b) at the SIGHAN bakeoff 2005 
(Thomas, 2005) and 2006 (Levow, 2006), the ma-
jor descriptions of this paper is on the WBT model. 
The remainder of this paper is arranged as follows. 
In Section 2, we present the WBT model for ex-
tracting words from each Chinese sentence. Scored 
results and analyses of our CWS system are pre-
sented in Section 3. Finally, in Section 4, we pre-
sent our conclusion and discuss the direction of 
future research. 
2 Word Boundary Token Model 
To develop the WBT model, first, we define word 
boundary token. Second, we give definition and 
computation of the WBT probability and the WBT 
frequency for a given corpus. Finally, algorithm of 
our WBT model for word extraction is given. 
2.1 Types of Word Boundary Token 
We classify WBT into three types: left, right and 
bi-direction. The left and right word boundary 
(WB) tokens are the immediately preceding word 
and the following word of a word in a Chinese 
sentence, respectively. Suppose W1W2W3 is a 
Chinese sentence comprised of three Chinese 
words W1, W2 and W3. To this case, W1 and W3 
are the left and the right WB tokens of W2, 
respectively. On the other hand, those words that 
can simultaneously be left and right WB tokens of 
a word in corpus are defined as bi-direction WB 
tokens. Suppose W4W2W1 is a Chinese sentence 
comprised of three Chinese words W4, W2 and W1. 
Following the above cases, W1 can be a bi-
direction WB token for W2. Table 1 is the Top 5 
left, right and bi-direction WB tokens derived by 
the Academia Sinica (AS) corpus (CKIP, 1995 and 
1996). From Table 1, the Top 1 left , right and bi-
direction WB tokens is ??(of).? 
 
 Left      Right Bi-Direction 
Top1 ?(of)      ?(of) ?(of) 
Top2 ?(is)      ?(is) ?(is) 
Top3 ?(at)      ?(already) ?(at) 
Top4 ?(a)      ?(at) ?(already) 
Top5 ?(has)      ?(one) ?(and) 
Table 1. Top 5 left, right and bi-direction WB to-
kens derived from the AS corpus 
151
Sixth SIGHAN Workshop on Chinese Language Processing
2.2 WBT Frequency and WBT Probability 
We first give the computation of WBT frequency, 
then, the computation of WBT probability. 
 
 (1) WBT frequency: we use WBT_F(string, WBT, 
L/R) as the function of WBT frequency, where 
string is a n-char string containing n Chinese 
characters, WBT is a word boundary token, and 
L/R indicates to compute left or right WBT 
frequency. Now, take WBT_F(??? (we)?, 
??(of)?, L) as example. First, we submit the 
query ????? to system corpus. Second, set 
the number of sentences including this query is  
the WBT_F(???(we)?, ??(of), L). 
(2) WBT Probability: we use WBT_P(string1, 
string2, WBT, L/R) as the function of WBT 
probability, where string1 and string2 are two 
n-char strings, WBT is a word boundary token, 
and L/R indicates to compute left or right WBT 
probability. The equations of left and the right 
WBT probability are: 
 
WBT_P(string1, string2, WBT, L) =  
WBT_F(string1, WBT, L) /  
(WBT_F(string1, WBT, L)+WBT_F(string2, WBT, L) )       (1) 
 
WBT_P(string1, string2, WBT, R) =  
WBT_F(string1, WBT, R) / 
(WBT_F(string1, WBT, R)+WBT_F(string2, WBT, R) )       (2) 
 
2.3 Algorithm of WBT Model 
We use WBTM(n, WBT, threshold_p, threshold_f) 
as the function of the WBT model, where n is the 
window size, threshold_p is the threshold value of 
WBT probability and threshold_f is the threshold 
value of WBT frequency. The algorithm of our 
WBT model applied to extract words from a given 
Chinese sentence is as follows: 
Step 1. INPUT: 
    n, WBT, threshold_p and  threshold_f; 
Step 2. IF sentence length is less or equal to n 
THEN GOTO Step 4; 
Step 3. 
SET loopCount to one 
REPEAT 
COMBINE the characters of sentence between 
loopCountth and (loopCount + n ? 1)th to be a 
string_a 
COMBINE the characters of sentence between 
(loopCount+1)th and (loopCount + n)th to be 
a string_b 
IF WBT_P(string_a, string_b, WBT, L) ? 
threshold_p AND 
WBT_P(string_a, string_b, WBT, R) ? 
threshold_p AND 
WBT_F(string_a, WBT, L) ? threshold_f 
AND 
WBT_F(string_a, WBT, R) ? threshold_f 
THEN SET string_a is as word 
ENDIF 
IF WBT_P(string_b, string_a, WBT, L) ? 
threshold_p AND 
WBT_P(string_b, string_a, WBT, R) ? 
threshold_p AND 
WBT_F(string_b, WBT, L) ? threshold_f 
AND 
WBT_F(string_b, WBT, R) ? threshold_f  
THEN SET string_b to a word 
ENDIF 
INCREMENT loopCount 
UNTIL loopCount > sentence length ? n 
Step 4. END. 
 
loopCount is 1 
       string_a = ??; string_b = ?? 
WBT_F(string_a, ???, L) = 0 
WBT_F(string_a, ???, R) = 7 
WBT_F(string_b, ???, L) = 0 
WBT_F(string_b, ???, R) = 0 
WBT_P(string_a, string_b, ???, L) = 0 
WBT_P(string_a, string_b, ???, R) = 1 
WBT_P(string_b, string_a, ???, L) = 0 
WBT_P(string_b, string_a, ???, R) = 0 
SET?? to a word 
loopCount is 2 
string_a = ??; string_b = ?? 
WBT_F(string_a, ???, L) = 0 
WBT_F(string_a, ???, R) = 0 
WBT_F(string_b, ???, L) = 0 
WBT_F(string_b, ???, R) = 0 
WBT_P(string_a, string_b, ???, L) = 0 
WBT_P(string_a, string_b, ???, R) = 0 
WBT_P(string_b, string_a, ???, L) = 0 
WBT_P(string_b, string_a, ???, R) = 0 
Table 2. An example of applying WBTM(2, ???, 0.95, 
1) to extract word ???? from the Chinese sentence 
?????? 
152
Sixth SIGHAN Workshop on Chinese Language Processing
Table 2 is an example of applying  WBTM(2, ???, 
0.95, 1) to extract words from the Chinese sentence 
?????? by the AS corpus 
3 Evaluation 
In the SIGHAN Bakeoff 2007, there are five train-
ing corpus for word segmentation (WS) task: AS 
(Academia Sinica), CityU (City University of 
Hong Kong) are traditional Chinese corpus; CTB 
(University of Colorado, United States), NCC 
(State Language Commission of P.R.C., Beijing) 
and SXU (Shanxi University, Taiyuan) are simpli-
fied Chinese corpus. For each corpus, there are 
closed and open tasks. In this Bakeoff, we attend 
the AS (Academia Sinica) and CityU (City Univer-
sity of Hong Kong) closed WS tasks. Tables 3 and 
4 show the details of CKIP and CityU tasks. From 
Table 3, it indicates that the CKIP should be a 10-
folds design. From Table 4, it indicates that the 
CityU should be a 5-folds design. 
 
  Training Testing 
Sentence 95,303  10,834 
Wordlist 48,114  14,662 
Table 3. The details of CKIP WS task 
 
  Training Testing 
Sentence 36,227    8,093 
Wordlist 43,639  23,303 
Table 4. The details of CityU WS task 
3.1 Our CWS System 
The major steps of our CWS system with word 
boundary token model, triple context matching 
model and word support model are as below: 
Step 0. Combine training corpus and testing corpus 
as system corpus; 
Step 1. Generate the BMM segmentation for the 
given Chinese sentence by system dictionary; 
Step 2. Use WBT model with system corpus to 
extract 2-char, 3-char and 4-char words from 
the given Chinese sentence, where WBT is set 
to ??,? ??,? ??,? ??,? ??,? threshold_p 
is set to 0.95 and threshold_f is set to 1; 
Step 3. Use TCT (triple context template) matching 
model to extract 2-char, 3-char and 4-char 
words from the segmented Chinese sentence 
of Step 1. The details of TCT matching model 
can be found in (Tsai, 2005); 
Step 4. Add the found words of Steps 2 and 3 into 
system dictionary; 
Step 5. Generate the BMM segmentation for the 
given Chinese sentence by system dictionary; 
Step 6. Use word support model to resolve Over-
lap Ambiguity (OA) and Combination Am-
biguity (CA) problems for the BMM seg-
mentation of Step 5. 
3.2 Bakeoff Scored Results 
Table 5 is the comparison of scored results be-
tween our CWS and the SIGHAN Bakeoff 2007 
baseline system for the CKIP closed WS task by 
the SIGHAN Bakeoff 2007. Table 6 is the com-
parison between our CWS and the SIGHAN Bake-
off 2007 baseline system for the CityU closed WS 
task by the SIGHAN Bakeoff 2007. 
 
 Baseline Our CWS Increase 
R 0.8978  0.915  0.0172 
P 0.8232  0.9001  0.0769 
F 0.8589  0.9075  0.0486 
Table 5. The comparison of scored results between 
our CWS system and the SIGHAN Bakeoff 2007 
baseline system for the CKIP closed WS task 
 
 Baseline Our CWS Increase 
R 0.9006  0.9191  0.0185 
P 0.8225  0.9014  0.0789 
F 0.8598  0.9102  0.0504 
Table 6. The comparison of scored results between 
our CWS system and the SIGHAN Bakeoff 2007 
baseline system for the CityU closed WS task 
 
From Tables 5 and 6, it shows the major im-
provement of our CWS for the baseline system is 
on the precision of word segmentation. That is to 
say, the major target system for improving our 
CWS system is the unknown word extraction sys-
tem, i.e. the word boundary model and the triple 
context template matching model. 
3.3 Analysis 
Table 7 is the coverage of 2-char, 3-char, 4-char 
and great than 4-char error words extracting by our 
CWS for the CKIP and the CityU closed WS tasks. 
 
 
153
Sixth SIGHAN Workshop on Chinese Language Processing
               Coverage (%)   
 2-char 3-char 4-char > 4-char 
CKIP  68% 24% 4% 4% 
CityU  78% 19% 2% 1%  
Total  75% 21% 3% 1% 
Table 7. The coverage of 2-char, 3-char, 4-char 
and great than 4-char error words extracting by our 
CWS for the CKIP and the CityU closed WS tasks 
 
From Table 7, it shows the major n-char unknown 
word extraction for improving our CWS system is 
on 2-char unknown word extraction. It is because 
that the total coverage of 2-char word errors ex-
traction of our CWS system for the CKIP and the 
CityU WS tasks is 75%. 
4 Conclusions 
In this paper, we describes a Chinese word seg-
mentation system based on word boundary token 
model and triple context matching model (Tsai, 
2005) for extracting unknown words; and word 
support model (Tsai, 2006a and 2006b) for resolv-
ing segmentation ambiguity. To develop the word 
boundary model, we define WBT and classify 
WBT into three types of left, right and bi-direction. 
As per three types of WBT, we define WBT prob-
ability and WBT frequency. 
 In the SIGHAN Bakeoff 2007, we take part in 
the CKIP and the CityU closed word segmentation 
tasks. The scored results show that our CWS can 
increase the Bakeoff baseline system with 4.86% 
and 5.04% F-measures for the CKIP and the CityU 
word segmentation tasks, respectively. On the 
other hand, we show that the major room for im-
proving our CWS system is the 2-char unknown 
word extraction of the word boundary model and 
triple context matching model. The performance of 
word support model is great and supports our pre-
vious work (Tsai, 2006a and 2006b). 
We believe one major advantage of the WBT 
model is to use it with web as live corpus to mini-
mum the corpus sparseness effect. Therefore, in 
the future, we shall investigate the WBT model 
with the web corpus, such as the searching results 
of GOOGLE and Yahoo!, etc. 
References 
CKIP (Chinese Knowledge Information Processing 
Group). 1995. Technical Report no. 95-02, the 
content and illustration of Sinica corpus of Aca-
demia Sinica. Institute of Information Science, 
Academia Sinica. 
CKIP (Chinese Knowledge Information Processing 
Group). 1996. A study of Chinese Word Bounda-
ries and Segmentation Standard for Information 
processing (in Chinese). Technical Report, Taiwan, 
Taipei, Academia Sinica. 
Levow, Gina-Anne. 2006. The Third International Chi-
nese Language Processing Bakeoff: Word Seg-
mentation and Named Entity Recognition, In 
Proceedings of SIGHAN5 the 3rd International 
Chinese Language Processing Bakeoff at Col-
ing/ACL 2006, July, Sydney, Australia, 108-117. 
Thomas, Emerson. 2005. The Second International Chi-
nese Word Segmentation Bakeoff, In Proceed-
ings of The 2nd International Chinese Word 
Segmentation Bakeoff at SIGHAN-4, October, 
Jeju Island, Korea, 123-133. 
Tsai, Jia-Lin. 2005. Report to BMM-based Chinese 
Word Segmentor with Context-based Unknown 
Word Identifier for the Second International Chi-
nese Word Segmentation Bakeoff, In Proceed-
ings of The 2nd International Chinese Word 
Segmentation Bakeoff at SIGHAN-4, October, 
Jeju Island, Korea, 142-145. 
Tsai, Jia-Lin. 2006. Using Word Support Model to Im-
prove Chinese Input System, In Proceedings of 
Coling/ACL 2006, July, Sydney, Australia, 842-
849. 
Tsai, Jia-Lin. 2006. BMM-based Chinese Word Seg-
mentor with Word Support Model for the 
SIGHAN Bakeoff 2006, In Proceedings of 
SIGHAN5 the 3rd International Chinese Lan-
guage Processing Bakeoff at Coling/ACL 2006, 
July, Sydney, Australia, 130-133. 
154
Sixth SIGHAN Workshop on Chinese Language Processing
