Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 1?9,
COLING 2010, Beijing, August 2010.
Intersecting Hierarchical and Phrase-Based Models of Translation:
Formal Aspects and Algorithms
Marc Dymetman Nicola Cancedda
Xerox Research Centre Europe
{marc.dymetman,nicola.cancedda}@xrce.xerox.com
Abstract
We address the problem of construct-
ing hybrid translation systems by inter-
secting a Hiero-style hierarchical sys-
tem with a phrase-based system and
present formal techniques for doing so.
We model the phrase-based component
by introducing a variant of weighted
finite-state automata, called ?-automata,
provide a self-contained description
of a general algorithm for intersect-
ing weighted synchronous context-free
grammars with finite-state automata, and
extend these constructs to ?-automata.
We end by briefly discussing complexity
properties of the presented algorithms.
1 Introduction
Phrase-based (Och and Ney, 2004; Koehn et
al., 2007) and Hierarchical (Hiero-style) (Chi-
ang, 2007) models are two mainstream ap-
proaches for building Statistical Machine Trans-
lation systems, with different characteristics.
While phrase-based systems allow a direct cap-
ture of correspondences between surface-level
lexical patterns, but at the cost of a simplistic
handling of re-ordering, hierarchical systems are
better able to constrain re-ordering, especially
for distant language pairs, but tend to produce
sparser rules and often lag behind phrase-based
systems for less distant language pairs. It might
therefore make sense to capitalize on the com-
plementary advantages of the two approaches by
combining them in some way.
This paper attempts to lay out the formal
prerequisites for doing so, by developing tech-
niques for intersecting a hierarchical model and
a phrase-based model. In order to do so, one first
difficulty has to be overcome: while hierarchical
systems are based on the mathematically well-
understood formalism of weighted synchronous
CFG?s, phrase-based systems do not correspond
to any classical formal model, although they are
loosely connected to weighted finite state trans-
ducers, but crucially go beyond these by allow-
ing phrase re-orderings.
One might try to address this issue by limiting
a priori the amount of re-ordering, in the spirit
of (Kumar and Byrne, 2005), which would allow
to approximate a phrase-based model by a stan-
dard transducer, but this would introduce further
issues. First, limiting the amount of reorder-
ing in the phrase-based model runs contrary to
the underlying intuitions behind the intersection,
namely that the hierarchical model should be
mainly responsible for controlling re-ordering,
and the phrase-based model mainly responsible
for lexical choice. Second, the transducer result-
ing from the operation could be large. Third,
even if we could represent the phrase-based
model through a finite-state transducer, intersect-
ing this transducer with the synchronous CFG
would actually be intractable in the general case,
as we indicate later.
We then take another route. For a fixed source
sentence x, we show how to construct an au-
tomaton that represents all the (weighted) tar-
get sentences that can be produced by apply-
ing the phrase based model to x. However, this
??-automaton? is non-standard in the sense that
each transition is decorated with a set of source
sentence tokens and that the only valid paths are
1
those that do not traverse two sets containing the
same token (in other words, valid paths cannot
?consume? the same source token twice).
The reason we are interested in ?-automata
is the following. First, it is known that inter-
secting a synchronous grammar simultaneously
with the source sentence x and a (standard) target
automaton results in another synchronous gram-
mar; we provide a self-contained description of
an algorithm for performing this intersection, in
the general weighted case, and where x is gener-
alized to an arbitrary source automaton. Second,
we extend this algorithm to ?-automata. The
resulting weighted synchronous grammar repre-
sents, as in Hiero, the ?parse forest? (or ?hy-
pergraph?) of all weighted derivations (that is
of all translations) that can be built over x, but
where the weights incorporate knowledge of the
phrase-based component; it can therefore form
the basis of a variety of dynamic programming
or sampling algorithms (Chiang, 2007; Blunsom
and Osborne, 2008), as is the case with standard
Hiero-type representations. While in the worst
case the intersected grammar can contain an ex-
ponential number of nonterminals, we argue that
such combinatorial explosion will not happen in
practice, and we also briefly indicate formal con-
ditions under which it will not be allowed to hap-
pen.
2 Intersecting weighted synchronous
CFG?s with weighted automata
We assume that the notions of weighted finite-
state automaton [W-FSA] and weighted syn-
chronous grammar [W-SCFG] are known (for
short descriptions see (Mohri et al, 1996) and
(Chiang, 2006)), and we consider:
1. A W-SCFG G, with associated source
grammar Gs (resp. target grammar Gt); the
terminals of Gs (resp. Gt) vary over the
source vocabulary Vs (resp. target vocab-
ulary Vt).
2. A W-FSA As over the source vocabulary
Vs, with initial state s# and final state s$.
3. A W-FSA At over the target vocabulary Vt,
with initial state t# and final state t$.
The grammar G defines a weighted synchronous
language LG over (Vs, Vt), the automaton As a
weighted language Ls over Vs, and the automa-
ton At a weighted language Lt over Vt. We
then define the intersection language L? between
these three languages as the synchronous lan-
guage denoted L? = Ls e LG e Lt over (Vs, Vt)
such that, for any pair (x, y) of a source and a
target sentence, the weight L?(x, y) is defined
by L?(x, y) ? Ls(x) ? LG(x, y) ? Lt(y), where
Ls(x), LG(x, y), Lt(y) are the weights associ-
ated to each of the component languages.
It is natural to ask whether there exists a syn-
chronous grammar G? generating the language
L?, which we will now show to be the case.1
Our approach is inspired by the construction in
(Bar-Hillel et al, 1961) for the intersection of a
CFG and an FSA and the observation in (Lang,
1994) relating this construction to parse forests,
and also partially from (Satta, 2008), although,
by contrast to that work, our construction, (i)
is done simultaneously rather than as the se-
quence of intersecting As with G, then the re-
sulting grammar with At, (ii) handles weighted
formalisms rather than non-weighted ones.
We will describe the construction of G? based
on an example, from which the general construc-
tion follows easily. Consider a W-SCFG gram-
mar G for translating between French and En-
glish, with initial nonterminal S, and containing
among others the following rule:
N? A manque a` B / B misses A : ?, (1)
where the source and target right-hand sides are
separated by a slash symbol, and where ? is a
non-negative real weight (interpreted multiplica-
tively) associated with the rule.
Now let?s consider the following ?rule
scheme?:
t0
s0Nt3s4 ? t2s0At3s1 s1manques2 s2 a`s3 t0s3Bt1s4 /
t0
s3Bt1s4 t1missest2 t2s0At3s1 (2)
1We will actually only need the application of this result
to the case where As is a ?degenerate? automaton describ-
ing a single source sentence x, but the general construction
is not harder to do than this special case and the resulting
format for G? is well-suited to our needs below.
2
This scheme consists in an ?indexed? version of
the original rule, where the bottom indices si
correspond to states of As (?source states?), and
the top indices ti to states of At (?target states?).
The nonterminals are associated with two source
and two target indices, and for the same nonter-
minal, these four indices have to match across
the source and the target RHS?s of the rule. As
for the original terminals, they are replaced by
?indexed terminals?, where source (resp. tar-
get) terminals have two source (resp. target) in-
dices. The source indices appear sequentially
on the source RHS of the rule, in the pattern
s0, s1, s1, s2, s2 . . . sm?1, sm, with the nonter-
minal on the LHS receiving source indices s0
and sm, and similarly the target indices appear
sequentially on the target RHS of the rule, in the
pattern t0, t1, t1, t2, t2 . . . tn?1, tn, with the non-
terminal on the LHS receiving target indices t0
and tn. To clarify, the operation of associating
indices to terminals and nonterminals can be de-
composed into three steps:
s0Ns4 ? s0As1 s1manques2 s2 a` s3 s3Bs4 /
B misses A
t0Nt3 ? A manque a` B /
t0Bt1 t1missest2 t2At3
t0
s0Nt3s4 ? t2s0At3s1 s1manques2 s2 a` s3 t0s3Bt1s4 /
t0
s3Bt1s4 t1missest2 t2s0At3s1
where the first two steps corresponds to handling
the source and target indices separately, and the
third step then assembles the indices in order to
get the same four indices on the two copies of
each RHS nonterminal. The rule scheme (2) now
generates a family of rules, each of which corre-
sponds to an arbitrary instantiation of the source
and target indices to states of the source and tar-
get automata respectively. With every such rule
instantiation, we associate a weight ?? which is
defined as:
?? ? ? ?
?
si s-termsi+1
?As(si, s-term, si+1)
?
?
tj t-termtj+1
?At(tj , t-term, tj+1), (3)
where the first product is over the indexed source
terminals sis-termsi+1 , the second product
over the indexed target terminals tj t-termtj+1 ;
?As(si, s-term, si+1) is the weight of the transi-
tion (si, s-term, si+1) according to As, and sim-
ilarly for ?At(tj , t-term, tj+1). In these prod-
ucts, it may happen that ?As(si, s-term, si+1) is
null (and similarly for At), and in such a case,
the corresponding rule instantiation is consid-
ered not to be realized. Let us consider the multi-
set of all the weighted rule instantiations for (1)
computed in this way, and for each rule in the
collection, let us ?forget? the indices associated
to the terminals. In this way, we obtain a col-
lection of weighted synchronous rules over the
vocabularies Vs and Vt, but where each nonter-
minal is now indexed by four states.2
When we apply this procedure to all the rules
of the grammar G, we obtain a new weighted
synchronous CFG G?, with start symbol t#s#St$s$ ,
for which we have the following Fact, of which
we omit the proof for lack of space.
Fact 1. The synchronous language LG? associ-
ated with G? is equal to L? = Ls e LG e Lt.
The grammar G? that we have just constructed
does fulfill the goal of representing the bilat-
eral intersection that we were looking for, but
it has a serious defect: most of its nontermi-
nals are improductive, that is, can never pro-
duce a bi-sentence. If a rule refers to such an
improductive nonterminal, it can be eliminated
from the grammar. This is the analogue for a
SCFG of the classical operation of reduction for
CFG?s; while, conceptually, we could start from
G? and perform the reduction by deleting the
many rules containing improductive nontermi-
nals, it is equivalent but much more efficient to
do the reverse, namely to incrementally add the
productive nonterminals and rules of G? starting
from an initially empty set of rules, and by pro-
ceeding bottom-up starting from the terminals.
We do not detail this process, which is relatively
2It is possible that the multiset obtained by this simpli-
fying operation contains duplicates of certain rules (pos-
sibly with different weights), due to the non-determinism
of the automata: for instance, two sequences such
as?s1manques2 s2 a`s3 ? and ?s1manques?2 s?2 a`s3 ? become in-distinguishable after the operation. Rather than producing
multiple instances of rules in this way, one can ?conflate?
them together and add their weights.
3
straightforward.3
A note on intersecting SCFGs with transduc-
ers Another way to write Ls e LG e Lt is as
the intersection (Ls ? Lt) ? LG. (Ls ? Lt) can
be seen as a rational language (language gener-
ated by a finite state transducer) of an especially
simple form over Vs ? Vt . It is then natural
to ask whether our previous construction can be
generalized to the intersection of G with an arbi-
trary finite-state transducer. However, this is not
the case. Deciding the emptiness problem for
the intersection between two finite state trans-
ducers is already undecidable, by reduction to
Post?s Correspondence problem (Berstel, 1979,
p. 90) and we have extended the proof of this fact
to show that intersection between a synchronous
CFG and a finite state transducer also has an un-
decidable emptiness problem (the proof relies on
the fact that a finite state transducer can be sim-
ulated by a synchronous grammar). A fortiori,
this intersection cannot be represented through
an (effectively constructed) synchronous CFG.
3 Phrase-based models and ?-automata
3.1 ?-automata: definition
Let Vs be a source vocabulary, Vt a target vocab-
ulary. Let x = x1, . . . , xM be a fixed sequence
of words over a certain source vocabulary Vs.
Let us denote by z a token in the sequence x,
and by Z the set of the M tokens in x. A ?-
automaton over x has the general form of a stan-
dard weighted automaton over the target vocabu-
lary, but where the edges are also decorated with
elements ofP(Z), the powerset ofZ (see Fig. 1).
An edge in the ?-automaton between two states
q and q? then carries a label of the form (?, ?),
where ? ? P(Z) and ? ? Vt (note that here we
do not allow ? to be the empty string ). A path
from the initial state of the automaton to its fi-
nal state is defined to be valid iff each token of x
appears in exactly one label of the path, but not
necessarily in the same order as in x. As usual,
the output associated with the path is the ordered
3This bottom-up process is analogous to chart-parsing,
but here we have decomposed the construction into first
building a semantics-preserving grammar and then reduc-
ing it, which we think is formally neater.
sequence of target labels on that path, and the
weight of the path is the product of the weights
on its edges.
?-automata and phrase-based translation
A mainstream phrase-based translation system
such as Moses (Koehn et al, 2007) can be ac-
counted for in terms of ?-automata in the follow-
ing way. To simplify exposition, we assume that
the language model used is a bigram model, but
any n-gram model can be accommodated. Then,
given a source sentence x, decoding works by at-
tempting to construct a sequence of phrase-pairs
of the form (x?1, y?1), ..., (x?k, y?k) such that each
x?i corresponds to a contiguous subsequence of
tokens of x, the x?i?s do not overlap and com-
pletely cover x, but may appear in a different
order than that of x; the output associated with
the sequence is simply the concatenation of all
the y?i?s in that sequence.4 The weight associ-
ated with the sequence of phrase-pairs is then
the product (when we work with probabilities
rather than log-probabilities) of the weight of
each (x?i+1, y?i+1) in the context of the previous
(x?i, y?i), which consists in the product of several
elements: (i) the ?out-of-context? weight of the
phrase-pair (x?i+1, y?i+1) as determined by its fea-
tures in the phrase table, (ii) the language model
probability of finding y?i+1 following y?i,5 (iii)
the contextual weight of (x?i+1, y?i+1) relative to
(x?i, y?i) corresponding to the distorsion cost of
?jumping? from the token sequence x?i to the to-
ken sequence x?i+1 when these two sequences
may not be consecutive in x.6
Such a model can be represented by a ?-
automaton, where each phrase-pair (x?, y?) ? for
4We assume here that the phrase-pairs (x?i, y?i) are such
that y?i is not the empty string (this constraint could be re-
moved by an adaptation of the -removal operation (Mohri,
2002) to ?-automata).
5This is where the bigram assumption is relevant: for
a trigram model, we may need to encode in the automaton
not only the immediately preceding phrase-pair, but also
the previous one, and so on for higher-order models. An
alternative is to keep the n-gram language model outside
the ?-automaton and intersect it later with the grammar G?
obtained in section 4, possibly using approximation tech-
niques such as cube-pruning (Chiang, 2007).
6Any distorsion model ? in particular ?lexicalized re-
ordering? ? that only depends on comparing two consec-
utive phrase-pairs can be implemented in this way.
4
# h
b a
r
k
$
tcl f
tcl1
tcl2
{ces}
these
{avocats, marrons}
totally
?
lawyers?
corrupt {cuits}
finished
{sont}
are
{avocats}
avocadoes
{sont}
are
{cuits}
cooked
{$}
$
{marrons}
brown
{$}
$
# h
b a
r
k
$
tcl f
tcl1
tcl2
these
totally
lawyers
corrupt
finishedare
avocadoes
are cooked $brown
$
Figure 1: On the top: a ?-automaton with two valid paths shown. Each box denotes a state corresponding to a phrase
pair, while states internal to a phrase pair (such as tcl1 and tcl2) are not boxed. Above each transition we have indicated
the corresponding target word, and below it the corresponding set of source tokens. We use a terminal symbol $ to denote
the end of sentence both on the source and on the target. The solid path corresponds to the output these totally corrupt
lawyers are finished, the dotted path to the output these brown avocadoes are cooked. Note that the source tokens are not
necessarily consumed in the order given by the source, and that, for example, there exists a valid path generating these
are totally corrupt lawyers finished and moving according to h ? r ? tcl1 ? tcl2 ? tcl ? f ; Note, however, that
this does not mean that if a biphrase such as (marrons avocats, avocado chestnuts) existed in the phrase
table, it would be applicable to the source sentence here: because the source words in this biphrase would not match the
order of the source tokens in the sentence, the biphrase would not be included in the ?-automaton at all. On the bottom:
The target W-FSA automaton At associated with the ?-automaton, where we are ignoring the source tokens (but keeping
the same weights).
x? a sequence of tokens in x and (x?, y?) an entry
in the global phrase table ? is identified with a
state of the automaton and where the fact that the
phrase-pair (x??, y??) = ((x1, ..., xk), (y1, ..., yl))
follows (x?, y?) in the decoding sequence is mod-
eled by introducing l ?internal? transitions with
labels (?, y1), (?, y2), ..., (?, yl), where ? =
{x1, ..., xk}, and where the first transition con-
nects the state (x?, y?) to some unique ?internal
state? q1, the second transition the state q1 to
some unique internal state q2, and the last tran-
sition qk to the state (x??, y??).7 Thus, a state
(x??, y??) essentially encodes the previous phrase-
pair used during decoding, and it is easy to see
that it is possible to account for the different
weights associated with the phrase-based model
by weights associated to the transitions of the ?-
automaton.8
7For simplicity, we have chosen to collect the set of all
the source tokens {x1, ..., xk} on the first transition, but we
could distribute it on the l transitions arbitrarily (but keep-
ing the subsets disjoint) without changing the semantics of
what we do. This is because once we have entered one of
the l internal transitions, we will always have to traverse
the remaining internal transitions and collect the full set of
source tokens.
8By creating states such as ((x?, y?), (x??, y??)) that en-
Example Let us consider the following French
source sentence x: ces avocats marrons sont
cuits (idiomatic expression for these totally cor-
rupt lawyers are finished). Let?s assume that the
phrase table contains the following phrase pairs:
h: (ces, these)
a: (avocats, avocados)
b: (marrons, brown)
tcl: (avocats marrons,
totally corrupt lawyers)
r: (sont, are)
k: (cuits, cooked)
f: (cuits, finished).
An illustration of the corresponding ?-
automaton SA is shown at the top of Figure 1,
with only a few transitions made explicit, and
with no weights shown.9
code the two previous phrase-pairs used during decoding,
it is possible in principle to account for a trigram language
model, and similarly for higher-order LMs. This is simi-
lar to implementing n-gram language models by automata
whose states encode the n? 1 words previously generated.
9Only two (valid) paths are shown. If we had shown the
full ?-automaton, then the graph would have been ?com-
plete? in the sense that for any two box states B,B?, we
would have shown a connection B ? B?1... ? B?k?1 ?
B?, where the B?i are internal states, and k is the length of
the target side of the biphrase B?.
5
4 Intersecting a synchronous grammar
with a ?-automaton
Intersection of a W-SCFG with a ?-
automaton If SA is a ?-automaton over
input x, with each valid path in SA we asso-
ciate a weight in the same way as we do for
a weighted automaton. For any target word
sequence in V ?t we can then associate the sum
of the weights of all valid paths outputting that
sequence. The weighted language LSA,x over
Vt obtained in this way is called the language
associated with SA. Let G be a W-SCFG over
Vs, Vt, and let us denote by LG,x the weighted
language over Vs, Vt corresponding to the
intersection {x} e G e V ?t , where {x} denotes
the language giving weight 1 to x and weight 0
to other sequences in V ?s , and V ?t denotes the
language giving weight 1 to all sequences in V ?t .
Note that non-null bi-sentences in LG,x have
their source projection equal to x and therefore
LG,x can be identified with a weighted language
over Vt. The intersection of the languages LSA,x
and LG,x is denoted by LSA,x e LG,x.
Example Let us consider the following W-
SCFG (where again, weights are not explicitly
shown, and where we use a terminal symbol $
to denote the end of a sentence, a technicality
needed for making the grammar compatible with
the SA automaton of Figure 1):
S ? NP VP $ / NP VP $
NP ? ces N A / these A N
VP ? sont A / are A
A ? marrons / brown
A ? marrons / totally corrupt
A ? cuits / cooked
A ? cuits / finished
N ? avocats / avocadoes
N ? avocats / lawyers
It is easy to see that, for instance, the sen-
tences: these brown avocadoes are cooked $,
these brown avocadoes are finished $, and these
totally corrupt lawyers are finished $ all belong
to the intersection LSA,x e LG,x, while the sen-
tences these avocadoes brown are cooked $, to-
tally corrupt lawyers are finished these $ belong
only to LSA,x.
Building the intersection We now describe
how to build a W-SCFG that represents the inter-
section LSA,x eLG,x. We base our explanations
on the example just given.
A Relaxation of the Intersection At the
bottom of Figure 1, we show how we can as-
sociate an automaton At with the ?-automaton
SA: we simply ?forget? the source-sides of the
labels carried by the transitions, and retain all the
weights. As before, note that we are only show-
ing a subset of the transitions here.
All valid paths for SAmap into valid paths for
At (with the same weights), but the reverse is not
true because some validAt paths can correspond
to traversals of SA that either consume several
time the same source token or do not consume all
source tokens. For instance, the sentence these
brown avocadoes brown are $ belongs to the
language of At, but cannot be produced by SA.
Let?s however consider the intersection {x} e
G e At, where, with a slight abuse of notation,
we have notated {x} the ?degenerate? automaton
representing the sentence x, namely the automa-
ton (with weights on all transitions equal to 1):
?
ces marronsavocats sont cuits $
0 1 2 3 4 5 6
This is a relaxation of the true intersection, but
one that we can represent through a W-SCFG, as
we know from section 2.10
This being noted, we now move to the con-
struction of the full intersection.
The full intersection We discussed in sec-
tion 2 how to modify a synchronous grammar
rule in order to produce the indexed rule scheme
(2) in order to represent the bilateral intersection
of the grammar with two automata. Let us redo
that construction here, in the case of our example
10Note that, in the case of our very simple example, any
target string that belongs to this relaxed intersection (which
consists of the eight sentences these {brown | totally cor-
rupt} {avocadoes | lawyers} are {cooked | finished}) actu-
ally belongs to the full intersection, as none of these sen-
tences corresponds to a path in SA that violates the token-
consumption constraint. More generally, it may often be
the case in practice that the W-SCFG, by itself, provides
enough ?control? of the possible target sentences to pre-
vent generation of sentences that would violate the token-
consumption constraints, so that there may be little differ-
ence in practice between performing the relaxed intersec-
tion {x} e G e At and performing the full intersection
{x} eG e LSA,x.
6
W-SCFG, of the target automaton represented on
the bottom of Figure 1, and of the source automa-
ton {x}.
The construction is then done in three steps:
s0NPs3 ? s0cess1 s1Ns2 s2As3 /
these A N
t0NPt3 ? ces N A /
t0theset1 t1At2 t2Nt3
t0
s0NPt3s3 ? s0cess1 t2s1Nt3s2 t1s2At2s3 /
t0theset1 t1s2At2s3 t2s1Nt3s2
In order to adapt that construction to the case
where we want the intersection to be with a ?-
automaton, what we need to do is to further spe-
cialize the nonterminals. Rather than specializ-
ing a nonterminal X in the form tsXt?s? , we spe-
cialize it in the form: tsXt
?,?
s? , where ? representsa set of source tokens that correspond to ?collect-
ing? the source tokens in the ?-automaton along
a path connecting the states t and t?.11
We then proceed to define a new rule scheme
associated to our rule, which is obtained as be-
fore in three steps, as follows.
s0NPs3 ? s0cess1 s1Ns2 s2As3 /
these A N
t0NPt3,?03 ? ces N A /
t0theset1,?01 t1At2,?12 t2Nt3,?23
t0
s0NPt3,?03s3 ? s0cess1 t2s1Nt3,?23s2 t1s2At2,?12s3 /
t0theset1,?01 t1s2At2,?12s3 t2s1Nt3,?23s2
The only difference with our previous tech-
nique is in the addition of the ??s to the top in-
dices. Let us focus on the second step of the an-
notation process:
t0NPt3,?03 ? ces N A /
t0theset1,?01 t1At2,?12 t2Nt3,?23
11To avoid a possible confusion, it is important to note
right away that ? is not necessarily related to the tokens
appearing between the positions s and s? in the source sen-
tence (that is, between these states in the associated source
automaton), but is defined solely in terms of the source to-
kens along the t, t? path. See example with ?persons? and
?people? below.
Conceptually, when instanciating this scheme,
the ti?s may range over all possible states of
the ?-automaton, and the ?ij over all subsets of
the source tokens, but under the following con-
straints: the RHS ??s (here ?01, ?12, ?23) must
be disjoint and their union must be equal to the ?
on the LHS (here ?03). Additionally, a ? associ-
ated with a target terminal (as ?01 here) must be
equal to the token set associated to the transition
that this terminal realizes between ?-automaton
states (here, this means that ?01 must be equal
to the token set {ces} associated with the transi-
tion between t0, t1 labelled with ?these?). If we
perform all these instantiations, compute their
weights according to equation (3), and finally re-
move the indices associated with terminals in the
rules (by adding the weights of the rules only dif-
fering by the indices of terminals, as done previ-
ously), we obtain a very large ?raw? grammar,
but one for which one can prove direct coun-
terpart of Fact 1. Let us call, as before G? the
raw W-SCFG obtained, its start symbol being
t#
s#S
t$,?alls$ , with ?all the set of all source tokens
in x.
Fact 2. The synchronous language LG? associ-
ated with G? is equal to ({x}, LSA,x e LG,x).
The grammar that is obtained this way, despite
correctly representing the intersection, contains
a lot of useless rules, this being due to the fact
that many nonterminals can not produce any out-
put. The situation is wholly similar to the case
of section 2, and the same bottom-up techniques
can be used for activating nonterminals and rules
bottom-up.
The algorithm is illustrated in Figure 2, where
we have shown the result of the process of acti-
vating in turn the nonterminals (abbreviated by)
N1, A1, A2, NP1, VP1, S1. As a consequence
of these activations, the original grammar rule
NP ? ces N A /these A N (for instance)
becomes instantiated as the rule:
#
0 NPtcl,{ces,avocats,marrons}3 ?
0ces1 tcl21 Ntcl,?2 h2Atcl2,{avocats,marrons}3 /
#theseh,{ces} h2Atcl2,{avocats,marrons}3 tcl21 Ntcl,?2
7
# h r $
tcl f
tcl1
tcl2
{ces}
these
{avocats, marrons}
totally
?
lawyers?
corrupt {cuits}
finished
{sont}
are
{$}
$
ces marronsavocats sont cuits $
S 1
S 1
N 1 A1 A2
VP1NP1
NP1 VP1
A2
A1
N 1
0 1 2 3 4 5 6
2 ,
1 2
2,{ , }
2 3
# ,{ , , }
0 3
,{ }
4 5
,{ , }
3 5
# ,{ , , , , ,$ $}
0 6
1:
1:
1:
2 :
1:
1:
tcl tcl
h tcl avocats marrons
tcl ces avocats marrons
r f cuits
tcl f sont cuits
ces avocats marrons sont cuits
N N
A A
NP NP
A A
VP VP
S S
?
Figure 2: Building the intersection. The bottom of the figure shows some active non-terminals associated with the source
sequence, at the top these same non-terminals associated with a sequence of transitions in the ?-automaton, corresponding
to the target sequence these totally corrupt lawyers are finished $. To avoid cluttering the drawing, we have used the
abbreviations shown on the right. Note that while A1 only spans marrons in the bottom chart, it is actually decorated with
the source token set {avocats,marrons}: such a ?disconnect? between the views that the W-SCFG and the ?-automaton
have of the source tokens is not ruled out.
that is, after removal of the indices on terminals:
#
0 NPtcl,{ces,avocats,marrons}3 ?
ces tcl21 Ntcl,?2 h2Atcl2,{avocats,marrons}3 /
these h2Atcl2,{avocats,marrons}3 tcl21 Ntcl,?2
Note that while the nonterminal tcl21 Ntcl,?2 by
itself consumes no source token (it is associated
with the empty token set), any actual use of this
nonterminal (in this specific rule or possibly in
some other rule using it) does require travers-
ing the internal node tcl2 and therefore all the
internal nodes ?belonging? to the biphrase tcl
(because otherwise the path from # to $ would
be disconnected); in particular this involves con-
suming all the tokens on the source side of tcl,
including ?avocats?.12
Complexity considerations The bilateral in-
tersection that we defined between a W-SCFG
12In particular there is no risk that a derivation relative
to the intersected grammar generates a target containing
two instances of ?lawyers?, one associated to the expansion
of tcl21 Ntcl,?2 and consuming no source token, and anotherone associated with a different nonterminal and consuming
the source token ?avocats?: this second instance would in-
volve not traversing tcl1, which is impossible as soon as
tcl2
1 Ntcl,?2 is used.
and two W-FSA?s in section 2 can be shown to
be of polynomial complexity in the sense that it
takes polynomial time and space relative to the
sum of the sizes of the two automata and of the
grammar to construct the (reduced) intersected
grammar G?, under the condition that the gram-
mar right-hand sides have length bounded by a
constant.13
The situation here is different, because the
construction of the intersection can in princi-
ple introduce nonterminals indexed not only by
states of the automata, but also by arbitrary sub-
sets of source tokens, and this may lead in ex-
treme cases to an exponential number of rules.
Such problems however can only happen in sit-
uations where, in a nonterminal tsXt
?,?
s? , the set
? is allowed to contain tokens that are ?unre-
lated? to the token set {personnes} appearing
between s and s? in the source automaton. An il-
lustration of such a situation is given by the fol-
lowing example. Suppose that the source sen-
13If this condition is removed, and for the simpler case
where the source (resp. target) automaton encodes a single
sentence x (resp. y), (Satta and Peserico, 2005) have shown
that the problem of deciding whether (x, y) is recognized
by G is NP-hard relative to the sum of the sizes. A conse-
quence is then that the grammar G? cannot be constructed
in polynomial time unless P = NP .
8
tence contains the two tokens personnes and
gens between positions i, i + 1 and j, j + 1 re-
spectively, with i and j far from each other, that
the phrase table contains the two phrase pairs
(personnes, persons) and (gens, people), but
that the synchronous grammar only contains the
two rules X ? personnes/people and Y ?
gens/persons, with these phrases and rules ex-
hausting the possibilities for translating gens
and personnes; Then the intersected grammar
will contain such nonterminals as tiXt
?,{gens}
i+1 and
r
jY
r?,{personnes}
j+1 , where in the first case the token
set {gens} in the first nonterminal is unrelated to
the tokens appearing between i, i + 1, and simi-
larly in the second case.
Without experimentation on real cases, it
is impossible to say whether such phenomena
would empirically lead to combinatorial explo-
sion or whether the synchronous grammar would
sufficiently constrain the phrase-base component
(whose re-ordering capabilities are responsible
in fine for the potential NP-hardness of the trans-
lation process) to avoid it. Another possible ap-
proach is to prevent a priori a possible combi-
natorial explosion by adding formal constraints
to the intersection mechanism. One such con-
straint is the following: disallow introduction of
t
iX
t?,?
j when the symmetric difference between
? and the set of tokens between positions i and
j in the source sentence has cardinality larger
than a small constant. Such a constraint could
be interpreted as keeping the SCFG and phrase-
base components ?in sync?, and would be better
adapted to the spirit of our approach than limit-
ing the amount of re-ordering permitted to the
phrase-based component, which would contra-
dict the reason for using a hierarchical compo-
nent in the first place.
5 Conclusion
Intersecting hierarchical and phrase-based mod-
els of translation could allow to capitalize on
complementarities between the two approaches.
Thus, one might train the hierarchical compo-
nent on corpora represented at the part-of-speech
level (or at a level where lexical units are ab-
stracted into some kind of classes) while the
phrase-based component would focus on transla-
tion of lexical material. The present paper does
not have the ambition to demonstrate that such
an approach would improve translation perfor-
mance, but only to provide some formal means
for advancing towards that goal.
References
Bar-Hillel, Y., M. Perles, and E. Shamir. 1961. On for-
mal properties of simple phrase structure grammars.
Zeitschrift fu?r Phonetik, Sprachwissenschaft und Kom-
municationsforschung, 14:143?172.
Berstel, Jean. 1979. Transductions and Context-Free Lan-
guages. Teubner, Stuttgart.
Blunsom, P. and M. Osborne. 2008. Probabilistic inference
for machine translation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 215?223. Association for Computational
Linguistics. Slides downloaded.
Chiang, David. 2006. An introduction to synchronous
grammars. www.isi.edu/?chiang/papers/
synchtut.pdf, June.
Chiang, David. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL. The Association
for Computer Linguistics.
Kumar, Shankar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation. In
Proc. HLT/EMNLP.
Lang, Bernard. 1994. Recognition can be harder than pars-
ing. Computational Intelligence, 10:486?494.
Mohri, Mehryar, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech processing.
In ECAI-96 Workshop on Extended Finite State Models
of Language.
Mohri, Mehryar. 2002. Generic epsilon-removal and input
epsilon-normalization algorithms for weighted trans-
ducers. International Journal of Foundations of Com-
puter Science, 13:129?143.
Och, Franz Josef and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Comput. Linguist., 30(4):417?449.
Satta, Giorgio and Enoch Peserico. 2005. Some compu-
tational complexity results for synchronous context-free
grammars. In HLT ?05: Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 803?810,
Morristown, NJ, USA. Association for Computational
Linguistics.
Satta, Giorgio. 2008. Translation algorithms by means of
language intersection. Submitted. www.dei.unipd.
it/?satta/publ/paper/inters.pdf.
9
