Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 117?122,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Uses of Monolingual In-Domain Corpora for Cross-Domain  
Adaptation with Hybrid MT Approaches 
 
 
An-Chang Hsieh, Hen-Hsen Huang and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan 
{achsieh,hhhuang}@nlg.csie.ntu.edu.tw;hhchen@ntu.edu.tw 
 
  
 
Abstract 
Resource limitation is challenging for cross-
domain adaption. This paper employs patterns 
identified from a monolingual in-domain cor-
pus and patterns learned from the post-edited 
translation results, and translation model as 
well as language model learned from pseudo 
bilingual corpora produced by a baseline MT 
system. The adaptation from a government 
document domain to a medical record 
domain shows the rules mined from the 
monolingual in-domain corpus are useful, 
and the effect of using the selected pseudo 
bilingual corpus is significant.   
1 Introduction 
Bilingual dictionary and corpus are important 
resources for MT applications. They are used for 
lexical choice and model construction. However, 
not all resources are available in bilingual forms 
in each domain. For example, medical records 
are in English only in some countries. In such a 
case, only bilingual dictionary and monolingual 
corpus is available. Lack of bilingual corpus 
makes domain adaptation more challenging.  
A number of adaptation approaches (Civera 
and Juan, 2007; Foster and Kuhn 2007; Foster et al, 
2010, Matsoukas et al, 2009; Zhao et al, 2004) 
have been proposed. They address the reliability 
of a model in a new domain and count the do-
main similarities between a model and the in-
domain development data. The domain relevance 
in different granularities including words, 
phrases, sentences, documents and corpora are 
considered. Ueffing et al (2007) propose semi-
supervised methods which use monolingual data 
in source language to improve translation per-
formance. Schwenk (2008) present lightly-
supervised training to generate additional train-
ing data from the translation results of monolin-
gual data. To deal with the resource-poor issue, 
Bertoldi and Federico (2009) generate a pseudo 
bilingual corpus from the monolingual in-domain 
corpus, and then train a translation model from 
the pseudo bilingual corpus.   
Besides counting similarities and generating 
pseudo bilingual in-domain corpus, text simplifi-
cation (Zhu et al, 2010; Woodsend and Lapata, 
2011; Wubben et al, 2012) is another direction. 
Simplifying a source language text makes the 
translation easier in a background MT system. 
Chen et al (2012a) propose a method to simplify 
a sentence before MT and to restore the transla-
tion of the simplified part after MT. They focus 
on the treatments of input text only, but do not 
consider how to adapt the background MT to the 
specific domain. The translation performance 
depends on the coverage of the simplification 
rules and the quality of the background system. 
This paper adopts the simplification-
translation-restoration methodology (Chen et al, 
2012a), but emphasizes on how to update bilin-
gual translation rules, translation model and lan-
guage model, which are two kernels of rule-
based and statistics-based MT systems, respec-
tively. This paper is organized as follows. Sec-
tion 2 specifies the proposed hybrid MT ap-
proaches to resource-limited domains. The char-
acteristics of available resources including their 
types, their linguality, their belonging domains, 
and their belonging languages are analyzed and 
their uses in translation rule mining and model 
construction are presented. Section 3 discusses 
how to adapt an MT system from a government 
document domain to a medical record domain. 
The experimental setups reflect various settings. 
Section 4 concludes the remarks. 
 
117
 Figure 1: Hybrid MT Approaches 
 
2 Hybrid MT Approaches 
Figure 1 sketches the overall picture of our pro-
posed hybrid MT approaches. A resource is rep-
resented in terms of its linguality, domain, lan-
guage, and type, where MO/BI denotes mono-
lingual/bilingual, ID/OD denotes in-domain/out-
domain, and SL/TL denotes source lan-
guage/target language. For example, an MO-ID-
SL corpus and an MO-ID-TL corpus mean mon-
olingual in-domain corpora in source and in tar-
get languages, respectively. Similarly, a BI-OD 
corpus and a BI-ID dictionary denote a bilingual 
out-domain corpus, and a bilingual in-domain 
dictionary, respectively.   
Resources may be provided by some organi-
zations such as LDC, or collected from hetero-
geneous resources. The MO-ID-SL/TL corpus, 
the BI-OD corpus, and the BI-ID dictionary be-
long to this type. Besides, some outputs generat-
ed by the baseline MT systems are regarded as 
other kinds of resources for enhancing the pro-
posed methods incrementally. Initial translation 
results, selected translation results, and post-
edited translation results, which form pseudo 
bilingual in-domain corpora, belong to this type.   
The following subsections first describe the 
baseline systems with the original resources and 
then specify the advanced systems with the gen-
erated resources. 
2.1 A baseline translation system 
In an extreme case, only a bilingual out-domain 
corpus, a monolingual in-domain corpus in 
source/target language, a bilingual in-domain 
dictionary and a monolingual in-domain thesau-
rus in source language are available. The bilin-
gual out-domain corpus is used to train transla-
tion and language models by Moses. They form 
a background out-domain translation system. 
A pattern miner is used to capture the written 
styles in the monolingual in-domain corpus in 
source language. A monolingual in-domain the-
saurus in source language is looked up to extract 
the class (sense) information of words. Mono-
lingual patterns are mined by counting frequent 
word/class n-grams. Then, the bilingual in-
domain dictionary is introduced to formulate 
translation rules based on the mined monolin-
gual patterns. Here in-domain experts may be 
involved in reviewing the bilingual rules. The 
human cost will affect the number of translation 
rules formulated and thus its coverage. 
The baseline translation system is composed 
of four major steps shown as follows. (1) and (2) 
are pre-processing steps before kernel MT, and 
(4) is a post-processing step after kernel MT. 
(1) Identifying and translating in-domain 
segments from an input sentence by using 
translation rules. 
118
(2) Simplifying the input sentence by replac-
ing the in-domain segments as follows. 
(a) If an in-domain segment is a term in 
the bilingual in-domain dictionary, 
we find a related term (i.e., hypernym 
or synonym) in the in-domain thesau-
rus which has relatively more occur-
rences in the background SMT sys-
tem to replace the term. 
(b) If an in-domain segment is a noun 
phrase, we keep its head only, and 
find a related term of the head as (a). 
(c) If an in-domain segment is a verb 
phrase composed of a verb and a 
noun phrase, we keep the verb and 
simplify the noun phrase as (b). 
(d) If an in-domain segment is a verb 
phrase composed of a verb and a 
prepositional phrase, we keep the 
verb and remove the prepositional 
phrase if it is optional. If the preposi-
tional phrase is mandatory, it is kept 
and simplified as (e). 
(e) If an in-domain segment is a preposi-
tional phrase, we keep the preposition 
and simplify the noun phrase as (b). 
(f) If an in-domain segment is a clause, 
we simplify its children recursively as 
(a)-(e). 
(3) Translating the simplified source sentence 
by using the out-domain background MT 
system. 
(4) Restoring the results of the bilingual in-
domain segments translated in (1) back to 
the translation results generated in (3).  
The restoration is based on the internal 
alignment between the source and the tar-
get sentences. 
2.2 Incremental learning 
There are several alternatives to update the base-
line translation system incrementally. The first 
consideration is the in-domain translation rules.  
They are formed semi-automatically by domain 
experts.  The cost of domain experts results that 
only small portion of n-gram patterns along with 
the corresponding translation are generated. The 
post-editing results suggests more translation 
rules and they are fed back to revise the baseline 
translation system. 
The second consideration is translation model 
and language model in the Moses. In an ideal 
case, the complete monolingual in-domain cor-
pus in source language is translated by the base-
line translation system, then the results are post-
edited by domain experts, and finally the com-
plete post-edited bilingual corpus is fed back to 
revise both translation model and language 
model. However, the post-editing cost by do-
main experts is high. Only some samples of the 
initial translation are edited by domain experts.  
On the one hand, the sampled post-edited in-
domain corpus in target language is used to re-
vise the language model.  On the other hand, the 
in-domain bilingual translation result before 
post-editing is used to revise the translation 
model and the language model. Size and transla-
tion quality are two factors to be considered. We 
will explore the effect of different size of imper-
fect in-domain translation results on refining the 
baseline MT system.  Moreover, a selection 
strategy, e.g., only those translation results com-
pletely in target language are considered, is in-
troduced to sample ?relatively more accurate? 
bilingual translation results. 
In the above incremental learning, translation 
rules, translation model and language model are 
revised individually.  The third consideration is 
to merge some refinements together and exam-
ine their effects on the translation performance. 
3 Cross-Domain Adaptation  
To evaluate the feasibility of the proposed hy-
brid MT approaches, we adapt an English-
Chinese machine translation system from a gov-
ernment document domain to a medical record 
domain. The linguistic resources are described 
first and then the experimental results. 
3.1 Resource description 
Hong Kong parallel text (LDC2004T08), which 
contains official records, law codes, and press 
releases of the Legislative Council, the Depart-
ment of Justice, and the Information Services 
Department of the HKSAR, respectively, and 
UN Chinese-English Parallel Text collection 
(LDC2004E12) is used to train the translation 
model. These two corpora contain total 6.8M 
sentences. The Chinese counterpart of the above 
parallel corpus and the Central News Agency 
part of the Tagged Chinese Gigaword 
(LDC2007T03) are used to train trigram lan-
guage model. These two corpora contain total 
18.8M sentences. The trained models are used in 
Step (3) of the baseline translation system. 
Besides the out-domain corpora for the devel-
opment of translation model and language model, 
we select 60,448 English medical records (1.8M 
sentences) from National Taiwan University 
119
Hospital (NTUH) to learn the n-gram patterns. 
Metathesaurus of the Unified Medical Language 
System (UMLS) provides medical classes of in-
domain words. A bilingual medical domain dic-
tionary composed of 71,687 pairs is collected. 
Total 7.2M word/class 2-grams~5-grams are 
identified. After parsing, there remain 57.2K 
linguistic patterns. A higher order pattern may 
be composed of two lower order patterns. Keep-
ing the covering patterns and ruling out the cov-
ered ones further reduce the size of the extracted 
patterns. The remaining 40.1K patterns are 
translated by dictionary look-up.  Because of the 
high cost of medical record domain experts (i.e., 
physicians), only a small portion is verified. Fi-
nally, 981 translation rules are formulated.  They 
are used in Step (1) of the baseline MT system. 
The detail rule mining and human correction 
process please refer to Chen et al (2012b). 
We further sample 2.1M and 1.1M sentences 
from NTUH medical record datasets, translate 
them by the baseline MT system, and get 2.1M- 
and 1.1M-pseudo bilingual in-domain corpora. 
We will experiment the effects of the corpus size. 
On the other hand, we apply the selection strate-
gy to select 0.95M ?good? translation from 
2.1M-pseudo bilingual in-domain corpus.  Fur-
thermore, some other 1,004 sentences are post-
edited by the domain experts. They are used to 
learn the advanced MT systems. 
To evaluate the baseline and the advanced 
MT systems, we sample 1,000 sentences differ-
ent from the above corpora as the test data, and 
translate them manually as the ground truth.  
3.2 Results and discussion 
Table 1 lists the methods along with the re-
sources they used. B is the baseline MT system. 
Most patterns appearing in the 57.2K learned n-
grams mentioned in Section 3.1 are not reviewed 
by physicians due to their cost. Part of these un-
reviewed patterns may occur in the post-edited 
data. They will be further introduced into M1. In 
the experiments, patterns appearing at least two 
times in the post-edited result are integrated into 
M1. Total 422 new patterns are identified. 
Translation model and language model in M1 is 
the same as those in baseline system.   
In M2-M6, the translation rules are the same 
as those in baseline MT system, only translation 
model and/or language model are re-trained. In 
 
 Translation Rules Translation Model Language Model Tuning Data 
B 981 bilingual translation rules 6.8M government domain bilingual 
sentences 
18.8M government/news domain 
Chinese sentences 
1000 government domain 
bilingual sentences 
M1 981 bilingual translation rules + 
422 mined  rules from post-
editing 
6.8M government domain bilingual 
sentences 
18.8M government/news domain 
Chinese sentences 
200 post-edited medical 
domain sentences 
M2 981 bilingual translation rules 6.8M government domain bilingual 
sentences 
804 post-edited Chinese sentences 200 post-edited medical 
domain sentences 
M3 981 bilingual translation rules 6.8M government domain bilingual 
sentences 
30,000 Chinese sentences selected 
from medical literature 
200 post-edited medical 
domain sentences 
M4 981 bilingual translation rules 1.1M pseudo medical domain bilingual 
sentences generated by M1 
1.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M5 981 bilingual translation rules 2.1M pseudo medical domain bilingual 
sentences generated by M1 
2.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M6 981 bilingual translation rules 0.95M selected pseudo medical do-
main bilingual sentences generated by 
M1 
0.95M selected pseudo medical do-
main Chinese sentences generated by 
M1 
200 post-edited medical 
domain sentences 
M12 981 bilingual translation rules + 
422 mined  rules from post-
editing 
6.8M government domain bilingual 
sentences 
804 post-edited Chinese sentences 200 post-edited medical 
domain sentences 
M13 981 bilingual translation rules + 
422 mined  rules from post-
editing 
6.8M government domain bilingual 
sentences 
30,000 medical domain Chinese sen-
tences 
200 post-edited medical 
domain sentences 
M14 981 bilingual translation rules + 
422 mined  rules from post-
editing 
1.1M pseudo medical domain bilingual 
sentences generated by M1 
1.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M15 981 bilingual translation rules + 
422 mined  rules from post-
editing 
2.1M pseudo medical domain bilingual 
sentences generated by M1 
2.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M16 981 bilingual translation rules + 
422 mined  rules from post-
editing 
0.95M selected pseudo medical do-
main bilingual sentences generated by 
M1 
0.95M selected pseudo medical do-
main Chinese sentences generated by 
M1 
200 post-edited medical 
domain sentences 
Table 1: Resources used in each hybrid MT method 
 
Method Bleu Method Bleu Method Bleu Method Bleu Method Bleu Method Bleu 
B 28.04 M2 39.45 M3 32.03 M4 34.86 M5 35.09 M6 40.48 
M1 39.72 M12 39.72 M13 32.85 M14 35.11 M15 35.52 M16 40.71 
Table 2: BLEU of each hybrid MT method 
120
M2, 804 post-edited sentences are used to train a 
new language model, without changing the 
translation model. In M3, paper abstracts in 
medical domain are used to derive a new lan-
guage model. M4, M5 and M6 are similar except 
that different sizes of corpora are used.  M4 and 
M5 use 1.1M and 2.1M sentences, respectively, 
while M6 uses 0.95M sentences chosen by using 
the selection strategy. M12-M16 are combina-
tions of M1 and M2-M6, respectively. Transla-
tion rules, translation model and language model 
are refined by using different resources. Total 
200 of the 1,004 post-edited sentences are se-
lected to tune the parameters of Moses in the 
advanced methods. 
Table 2 shows the BLEU of various MT 
methods. The BLEU of the MT system without 
employing simplification-translation-restoration 
methodology (Chen et al, 2012a) is 15.24. Ap-
parently, the method B, which employs the 
methodology, achieves the BLEU 28.04 and is 
much better than the original system. All the 
enhanced systems are significantly better than 
the baseline system B by t-test (p<0.05). Com-
paring M1 and M12-M16 with the correspond-
ing systems, we can find that introducing the 
mined patterns has positive effects. M1 is even 
much better than B. Although the number of the 
post-edited sentences is small, M2 and M12 
show such a resource has the strongest effects. 
The results of M3 and M13 depict that 30,000 
sentences selected from medical literature are 
not quite useful for medical record translation. 
Comparing M4 and M5, we can find larger 
pseudo corpus is useful.  M6 shows using the 
selected pseudo subset performs much better. 
Comparing the top 4 methods, the best method, 
M16, is significantly better than M12 and M1 
(p<0.05), but is not different from M6 signifi-
cantly (p=0.1662). 
We further analyze the translation results of 
the best methods M6 and M16 from two per-
spectives.  On the one hand, we show how the 
mined rules improve the translation. The follow-
ing list some examples for reference.  The un-
derlined parts are translated correctly by new 
mined patterns in M16. 
(1) Example: Stenting was done from distal 
IVC through left common iliac vein to ex-
ternal iliac vein. 
M6: ????? ? ? ?? ???? 
?? ? ????? ? ???? ? 
M16: ?? ????? ? ?? ???
? ?? ? ????? ? ???? ? 
(2) Example: We shifted the antibiotic to 
cefazolin. 
M6: ?? ? ??? ???? ? 
M16: ?? ? ??? ?? ? ???
? ? 
(3) Example: Enhancement of right side pleu-
ral, and mild pericardial effusion was not-
ed . 
M6: ?? ?? ? ?? ?? ? ? ?? 
? ??   ? ???? ? 
M16: ?? ? ? ?? ? ?? ???? 
? ??? ? 
On the other hand, we touch on which factors 
affect the translation performance of M16. Three 
factors including word ordering errors, word 
sense disambiguation errors and OOV (out-of-
vocabulary) errors are addressed as follows.  
The erroneous parts are underlined. 
(1) Ordering errors 
Example: Antibiotics were discontinued 
after 8 days of treatment. 
M16: ??? ?? ? 8? ? ?? ? 
Analysis: The correct translation result is 
?8 ? ? ?? ? ??? ????The 
current patterns are 2-5 grams, so that the 
longer patterns cannot be captured. 
(2) Word sense disambiguation errors 
Example: After tracheostomy, he was 
transferred to our ward for post operation 
care. 
M16: ????? ? ? ? ? ??? ?
? ?? ? ?? ?? ?? ? 
Analysis: The correct translation of ?post 
operation care? should be ??????.  
However, the 1,004 post-edited sentences 
are still not large enough to cover the pos-
sible patterns. Incremental update will in-
troduce more patterns and may decrease 
the number of translation errors. 
(3) OOV errors 
Example: Transcatheter intravenous uro-
kinase therapy was started on 1/11 for 24 
hours infusion. 
M16: transcatheter ?? ??? ? 1/11 
?? ?? ?? 24 ?? ?? ? 
Analysis: The word ?transcatheter? is an 
OOV. Its translation should be???".   
4 Conclusion 
This paper considers different types of resources 
in cross-domain MT adaptation. Several meth-
ods are proposed to integrate the mined transla-
121
tion rules, translation model and language model. 
The adaptation experiments show that the rules 
mined from the monolingual in-domain corpus 
are useful, and the effect of using the selected 
pseudo bilingual corpus is significant. 
Several issues such as word ordering errors, 
word sense disambiguation errors, and OOV 
errors still remain for further investigation in the 
future. 
 
Acknowledgments 
This work was partially supported by National 
Science Council (Taiwan) and Excellent Re-
search Projects of National Taiwan University 
under contracts NSC101-2221-E-002-195-MY3 
and 102R890858. We are very thankful to Na-
tional Taiwan University Hospital for providing 
NTUH the medical record dataset. 
References  
N. Bertoldi and M. Federico. 2009. Domain adapta-
tion for statistical machine translation with mono-
lingual resources. In Proceedings of the Fourth 
Workshop on Statistical Machine Translation, 
pages 182?189. 
H.B. Chen, H.H. Huang, H.H. Chen and C.T. Tan. 
2012a. A simplification-translation-restoration 
framework for cross-domain SMT applications. In 
Proceedings of COLING 2012, pages 545?560. 
H.B. Chen, H.H. Huang, J. Tjiu, C.Ti. Tan and H.H. 
Chen. 2012b. A statistical medical summary trans-
lation system. In Proceedings of 2012 ACM 
SIGHIT International Health Informatics Sympo-
sium, pages. 101-110. 
J. Civera and A. Juan. 2007. Domain adaptation in 
statistical machine translation with mixture model-
ing. In Proceedings of the Second Workshop on 
Statistical Machine Translation, pages 177?180. 
G. Foster and R. Kuhn. 2007. Mixture-model adapta-
tion for SMT. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 
128?135. 
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimina-
tive instance weighting for domain adaptation in 
statistical machine translation. In Proceedings of 
EMNLP 2010, pages 451?459. 
S. Matsoukas, A.I. Rosti, and B. Zhang. 2009. Dis-
criminative corpus weight estimation for machine 
translation. In Proceedings of EMNLP 2009, pages 
708?717. 
H. Schwenk. 2008. Investigations on large-scale 
lightly-supervised training. In Proceedings of 
IWSLT 2008, pages 182?189. 
N. Ueffing, G. Haffari and A. Sarkar. 2007. Trans-
ductive learning for statistical machine translation. 
In Proceedings of the 45th Annual Meeting of the 
Association of Computational Linguistics, pages 
25?32, 
K. Woodsend and M. Lapata. 2011. Learning to sim-
plify sentences with quasi-synchronous grammar 
and integer programming. In Proceedings of 
EMNLP 2011, pages 409?420. 
S. Wubben and  A. van den Bosch, and  E. Krahmer. 
2012. Sentence simplification by monolingual ma-
chine translation. In Proceedings of ACL 2012, 
pages 1015?1024. 
B. Zhao, M. Eck, M. and S. Vogel. 2004. Language 
model adaptation for statistical machine translation 
via structured query models. In Proceedings of 
COLING 2004, pages 411?417. 
Z. Zhu, D. Bernhard, and I. Gurevych. 2010. A mon-
olingual tree-based translation model for sentence 
simplification. In Proceedings of COLING 2010, 
pages 1353?1361. 
122
