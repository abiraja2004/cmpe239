Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 645?648,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Simple Approach for HPSG Supertagging Using Dependency Information
Yao-zhong Zhang ? Takuya Matsuzaki ?
? Department of Computer Science, University of Tokyo
? School of Computer Science, University of Manchester
?National Centre for Text Mining, UK
{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii???
Abstract
In a supertagging task, sequence labeling
models are commonly used. But their lim-
ited ability to model long-distance informa-
tion presents a bottleneck to make further im-
provements. In this paper, we modeled this
long-distance information in dependency for-
malism and integrated it into the process of
HPSG supertagging. The experiments showed
that the dependency information is very in-
formative for supertag disambiguation. We
also evaluated the improved supertagger in the
HPSG parser.
1 Introduction
Supertagging is a widely used speed-up technique
for lexicalized grammar parsing. It was first
proposed for lexicalized tree adjoining grammar
(LTAG) (Bangalore and Joshi, 1999), then extended
to combinatory categorial grammar (CCG) (Clark,
2002) and head-driven phrase structure grammar
(HPSG) (Ninomiya et al, 2006). For deep parsing,
supertagging is an important preprocessor: an ac-
curate supertagger greatly reduces search space of
a parser. Not limited to parsing, supertags can be
used for NP chunking (Shen and Joshi, 2003), se-
mantic role labeling (Chen and Rambow, 2003) and
machine translation (Birch et al, 2007; Hassan et
al., 2007) to explore rich syntactic information con-
tained in them.
Generally speaking, supertags are lexical tem-
plates extracted from a grammar. These templates
encode possible syntactic behavior of a word. Al-
though the number of supertags is far larger than the
45 POS tags defined in Penn Treebank, sequence la-
beling techniques are still effective for supertagging.
Previous research (Clark, 2002) showed that a POS
sequence is very informative for supertagging, and
some extent of local syntactic information can be
captured by the context of surrounding words and
POS tags. However, since the context window
length is limited for the computational cost reasons,
there are still long-range dependencies which are not
easily captured in sequential models (Zhang et al,
2009). In practice, the multi-tagging technique pro-
posed by Clark (2002) assigned more than one su-
pertag to each word and let the ambiguous supertags
be selected by the parser. As for other NLP applica-
tions which use supertags, resolving more supertag
ambiguities in supertagging stage is preferred. With
this consideration, we focus on supertagging and
aim to make it as accurate as possible.
In this paper, we incorporated long-distance in-
formation into supertagging. First, we used depen-
dency parser formalism to model long-distance re-
lationships between the input words, which is hard
to model in sequence labeling models. Then, we
combined the dependency information with local
context in a simple point-wise model. The experi-
ments showed that dependency information is very
informative for supertagging and we got a compet-
itive 93.70% on supertagging accuracy (fed golden
POS). In addition, we also evaluated the improved
supertagger in the HPSG parser.
2 HPSG Supertagging and Dependency
2.1 HPSG Supertags
HPSG (Pollard and Sag, 1994) is a lexicalist gram-
mar framework. In HPSG, a large number of
lexical entries is used to describe word-specific
syntactic characteristics, while only a small num-
ber of schemas is used to explain general con-
struction rules. These lexical entries are called
?HPSG supertags?. For example, one possi-
ble supertag for the word ?like? is written like
?[NP.nom<V.bse>NP.acc] lxm?, which indicates
645
the head syntactic category of ?like? is verb in base
form. It has a NP subject and a NP complement.
With such fine-grained grammatical type distinc-
tions, the number of supertags is much larger than
the number of tags used in other sequence labeling
tasks. The HPSG grammar used in our experiment
includes 2,308 supertags. This increases computa-
tional cost of sequence labeling models.
2.2 Why Use Dependency in Supertagging
By analyzing the internal structure of the supertags,
we found that subject and complements are two im-
portant syntactic properties for each supertag. If
we could predict subject and complements of the
word well, supertagging would be an easier job to
do. However, current widely used sequence labeling
models have the limited ability to catch these long-
distance syntactic relations. In supertagging stage,
tree structures are still not constructed. Dependency
formalism is an alternative way to describe these two
syntactic properties. Based on this observation, we
think dependency information could assist supertag
prediction.
Figure 1: Model structure of incorporating dependency
information into the supertagging stage. Dotted arrows
describe the augmented long distance dependency infor-
mation provided for supertag prediction.
3 Our Method
3.1 Modeling Dependency for Supertags
First of all, we need to characterize the dependency
between words for supertagging. Since exact de-
pendency locations are not encoded in supertags, to
make use of state-of-the-art dependency parser, we
recover HPSG supertag dependencies with the aid
of HPSG treebanks. The dependencies are extracted
from each branch in the HPSG trees by regarding
the non-head daughter as the modifier of the head-
daughter. HPSG schemas are expressed in depen-
dency arcs.
To model the dependency, we follow mainstream
dependency parsing formalism. Two representa-
tive methods for dependency parsing are transition-
based model like MaltParser (Nivre, 2003) and
graph-based model like MSTParser1 (McDonald et
al., 2005). Previous research (Nivre and McDon-
ald, 2008) showed that MSTParser is more accurate
than MaltParser for long dependencies. Since our
motivation is to capture long-distance dependency
as a complement for local supertagging models, we
use the projective MSTParser formalism to model
dependencies.
{(pi ? pj)&sj |(j, i) ? E}
MOD-IN {(pi ? wj)&sj|(j, i) ? E}
{(wi ? pj)&sj|(j, i) ? E}
{(wi ? wj)&sj |(j, i) ? E}
{(pi ? pj)&si|(i, j) ? E}
MOD-OUT {(pi ? wj)&si|(i, j) ? E}
{(wi ? pj)&si|(i, j) ? E}
{(wi ? wj)&si|(i, j) ? E}
Table 1: Non-local feature templates used for super-
tagging. Here, p, w and s represent POS, word
and schema respectively. Direction (Left/Right) from
MODIN/MODOUTword to the current word is also con-
sidered in the feature templates.
3.2 Integrating Dependency into Supertagging
There are several ways to combine long-distance
dependency into supertagging. Integrating depen-
dency information into training process would be
more intuitive. Here, we use feature-based integra-
tion. The base model is a point-wise averaged per-
ceptron (PW-AP) which has been shown very ef-
fective (Zhang et al, 2009). The improved model
structure is described in Figure 1. The long-distance
information is formalized as first-order dependency.
For the word being predicted, we extract its modi-
fiers (MODIN) and its head (MODOUT) (Table 1)
based on first-order dependency arcs. Then MODIN
and MODOUT relations are combined as features
with local context for supertag prediction. To com-
pare with previous work, the basic local context fea-
tures are the same as in Matsuzaki et al (2007).
1http://sourceforge.net/projects/mstparser/
646
4 Experiments
We evaluated dependency-informed supertagger
(PW-DEP) both by supertag accuracy 2 and by a
HPSG parser. The experiments were conducted on
WSJ-HPSG treebank (Miyao, 2006). Sections 02-
21 were used to train the dependency parser, the
dependency-informed supertagger and the HPSG
parser. Section 23 was used as the testing set. The
evaluation metric for HPSG parser is the accuracy
of predicate-argument relations in the parser?s out-
put, as in previous work (Sagae et al, 2007).
Model Dep Acc%? Acc%
PW-AP / 91.14
PW-DEP 90.98 92.18
PW-AP (gold POS) / 92.48
PW-DEP (gold POS) 92.05 93.70
100 97.43
Table 2: Supertagging accuracy on section 23. (?)
Dependencies are given by MSTParser evaluated with
labeled accuracy. PW-AP is the baseline point-wise
averaged perceptron model. PW-DEP is point-wise
dependency-informed model. The automatically tagged
POS tags were given by a maximum entropy tagger with
97.39% accuracy.
4.1 Results on Supertagging
We first evaluated the upper-bound of dependency-
informed supertagging model, given gold standard
first-order dependencies. As shown in Table 2,
with such long-distance information supertagging
accuracy can reach 97.43%. Comparing to point-
wise model (PW-AP) which only used local con-
text (92.48%), this absolute 4.95% gain indicated
that dependency information is really informative
for supertagging. When automatically predicted de-
pendency relations were given, there still were ab-
solute 1.04% (auto POS) and 1.22% (gold POS) im-
provements from baseline PW-AP model.
We also compared supertagging results with pre-
vious works (reported on section 22). Here we
mainly compared the dependency-informed point-
wise models with perceptron-based Bayes point ma-
chine (BPM) plus CFG-filter (Zhang et al, 2009).
To the best of our knowledge, these are the state-of-
the-art results on the same dataset with gold POS
2?UNK? supertags are ignored in evaluation as previous.
Figure 2: HPSG Parser F-score on section 23, given au-
tomatically tagged POS.
tags. CFG-filtering can be considered as an al-
ternative way of incorporating long-distance con-
straints on supertagging results. Although our base-
line system was slightly behind (PW-AP: 92.16%
vs. BPM:92.53%), the final accuracies of grammati-
cally constrained models were very close (PW-DEP:
93.53% vs. BPM-CFG: 93.60%); They were not sta-
tistically significantly different (P-value is 0.26). As
the result of oracle PW-DEP indicated, supertagging
accuracy can be further improved with better depen-
dency modeling (e.g., with a semi-supervised de-
pendency parser), which makes it more extensible
and attractive than using CFG-filter after the super-
tagging process.
4.2 HPSG parsing results
We also evaluated the dependency-informed su-
pertagger in a HPSG parser. Considering the effi-
ciency, we use the HPSG parser3 described by Ma-
tsuzaki et al (2007).
In practice, several supertag candidates are re-
served for each word to avoid parsing failure. To
evaluate the quality of the two supertaggers, we re-
stricted the number of each word?s supertag candi-
dates fed to the HPSG parser. As shown in Figure 2,
for the case when only one supertag was predicted
for each word, F-score of the HPSG parser using
dependency-informed supertagger is 5.06% higher
than the parser using the baseline supertagger mod-
ule. As the candidate number increased, the gap nar-
rowed: when all candidates were given, the gains
gradually came down to 0.2%. This indicated that
3Enju v2.3.1, http://www-tsujii.is.s.u-tokyo.ac.jp/enju.
647
improved supertagger can optimize the search space
of the deep parser, which may contribute to more ac-
curate and fast deep parsing. From another aspect,
supertagging can be viewed as an interface to com-
bine different types of parsers.
As for the overall parsing time, we didn?t opti-
mize for speed in current setting. The parsing time4
saved by using the improved supertagger (around
6.0 ms/sen, 21.5% time reduction) can not compen-
sate for the extra cost of MSTParser (around 73.8
ms/sen) now. But there is much room to improve the
final speed (e.g., optimizing the dependency parser
for speed or reusing acquired dependencies for ef-
fective pruning). In addition, small beam-size can be
?safely? used with improved supertagger for speed.
Using shallow dependencies in deep HPSG pars-
ing has been previously explored by Sagae et al
(2007), who used dependency constraints in schema
application stage to guide HPSG tree construction
(F-score was improved from 87.2% to 87.9% with
a single shift-reduce dependency parser). Since the
baseline parser is different, we didn?t make a direct
comparison here. However, it would be interesting
to compare these two different ways of incorporat-
ing the dependency parser into HPSG parsing. We
left it as further work.
5 Conclusions
In this paper, focusing on improving the accu-
racy of supertagging, we proposed a simple but
effective way to incorporate long-distance depen-
dency relations into supertagging. The experiments
mainly showed that these long-distance dependen-
cies, which are not easy to model in traditional se-
quence labeling models, are very informative for su-
pertag predictions. Although these were preliminary
results, the method shows its potential strength for
related applications. Not limited to HPSG, it can be
extended to other lexicalized grammar supertaggers.
Acknowledgments
Thanks to the anonymous reviewers for valuable
comments. We also thank Goran Topic for his self-
less help. The first author was supported by The
University of Tokyo Fellowship (UT-Fellowship).
4Tested on section 23 (2291 sentences) using an AMD
Opteron 2.4GHz server, given all supertag candidates.
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan).
References
Srinivas Bangalore and Aravind K. Joshi. 1999. Super-
tagging: An approach to almost parsing. Computa-
tional Linguistics, 25:237?265.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003.
Stephen Clark. 2002. Supertagging for combinatory cat-
egorial grammar. In Proceedings of the 6th Interna-
tional Workshop on Tree Adjoining Grammars and Re-
lated Frameworks (TAG+ 6), pages 19?24.
Hany Hassan, Mary Hearne, and Andy Way. 2007. Su-
pertagged phrase-based statistical machine translation.
In Proceedings of ACL 2007, pages 288?295.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007. Efficient hpsg parsing with supertagging and
cfg-filtering. In Proceedings of IJCAI-07.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL-05.
Yusuke Miyao. 2006. From Linguistic Theory to Syn-
tactic Analysis: Corpus-Oriented Grammar Develop-
ment and Feature Forest Model. Ph.D. Dissertation,
The University of Tokyo.
Takashi Ninomiya, Yoshimasa Tsuruoka, Takuya Matsu-
zaki, and Yusuke Miyao. 2006. Extremely lexicalized
models for accurate and fast hpsg parsing. In Proceed-
ings of EMNLP-2006, pages 155?163.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT.
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of IWPT-03, pages
149?160. Citeseer.
Carl Pollard and Ivan A. Sag. 1994. Head-driven Phrase
Structure Grammar. University of Chicago / CSLI.
Kenji Sagae, Yusuke Miyao, and Jun?ichi Tsujii. 2007.
Hpsg parsing with shallow dependency constraints. In
Proceedings of ACL-07.
Libin Shen and Aravind K. Joshi. 2003. A snow based
supertagger with application to np chunking. In Pro-
ceedings of ACL 2003, pages 505?512.
Yao-zhong Zhang, Takuya Matsuzaki, and Jun?ichi Tsu-
jii. 2009. Hpsg supertagging: A sequence labeling
view. In Proceedings of IWPT-09, Paris, France.
648
