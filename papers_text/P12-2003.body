Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 11?16,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparison of Chinese Parsers for Stanford Dependencies
Wanxiang Che?
car@ir.hit.edu.cn
Valentin I. Spitkovsky?
vals@stanford.edu
Ting Liu?
tliu@ir.hit.edu.cn
?School of Computer Science and Technology
Harbin Institute of Technology
Harbin, China, 150001
?Computer Science Department
Stanford University
Stanford, CA, 94305
Abstract
Stanford dependencies are widely used in nat-
ural language processing as a semantically-
oriented representation, commonly generated
either by (i) converting the output of a con-
stituent parser, or (ii) predicting dependencies
directly. Previous comparisons of the two ap-
proaches for English suggest that starting from
constituents yields higher accuracies. In this
paper, we re-evaluate both methods for Chi-
nese, using more accurate dependency parsers
than in previous work. Our comparison of per-
formance and efficiency across seven popular
open source parsers (four constituent and three
dependency) shows, by contrast, that recent
higher-order graph-based techniques can be
more accurate, though somewhat slower, than
constituent parsers. We demonstrate also that
n-way jackknifing is a useful technique for
producing automatic (rather than gold) part-
of-speech tags to train Chinese dependency
parsers. Finally, we analyze the relations pro-
duced by both kinds of parsing and suggest
which specific parsers to use in practice.
1 Introduction
Stanford dependencies (de Marneffe and Man-
ning, 2008) provide a simple description of rela-
tions between pairs of words in a sentence. This
semantically-oriented representation is intuitive and
easy to apply, requiring little linguistic expertise.
Consequently, Stanford dependencies are widely
used: in biomedical text mining (Kim et al., 2009),
as well as in textual entailment (Androutsopou-
los and Malakasiotis, 2010), information extrac-
tion (Wu and Weld, 2010; Banko et al., 2007) and
sentiment analysis (Meena and Prabhakar, 2007).
In addition to English, there is a Chinese ver-
sion of Stanford dependencies (Chang et al., 2009),
(a) A constituent parse tree.
(b) Stanford dependencies.
Figure 1: A sample Chinese constituent parse tree and its
corresponding Stanford dependencies for the sentence
China (??) encourages (??) private (??)
entrepreneurs (???) to invest (??) in
national (??) infrastructure (??) construction (??).
which is also useful for many applications, such as
Chinese sentiment analysis (Wu et al., 2011; Wu et
al., 2009; Zhuang et al., 2006) and relation extrac-
tion (Huang et al., 2008). Figure 1 shows a sample
constituent parse tree and the corresponding Stan-
ford dependencies for a sentence in Chinese. Al-
though there are several variants of Stanford depen-
dencies for English,1 so far only a basic version (i.e,
dependency tree structures) is available for Chinese.
Stanford dependencies were originally obtained
from constituent trees, using rules (de Marneffe et
al., 2006). But as dependency parsing technolo-
gies mature (Ku?bler et al., 2009), they offer increas-
ingly attractive alternatives that eliminate the need
for an intermediate representation. Cer et al. (2010)
reported that Stanford?s implementation (Klein and
Manning, 2003) underperforms other constituent
1nlp.stanford.edu/software/dependencies_manual.pdf
11
Type Parser Version Algorithm URL
Constituent Berkeley 1.1 PCFG code.google.com/p/berkeleyparser
Bikel 1.2 PCFG www.cis.upenn.edu/?dbikel/download.html
Charniak Nov. 2009 PCFG www.cog.brown.edu/?mj/Software.htm
Stanford 2.0 Factored nlp.stanford.edu/software/lex-parser.shtml
Dependency MaltParser 1.6.1 Arc-Eager maltparser.org
Mate 2.0 2nd-order MST code.google.com/p/mate-tools
MSTParser 0.5 MST sourceforge.net/projects/mstparser
Table 1: Basic information for the seven parsers included in our experiments.
parsers, for English, on both accuracy and speed.
Their thorough investigation also showed that con-
stituent parsers systematically outperform parsing
directly to Stanford dependencies. Nevertheless, rel-
ative standings could have changed in recent years:
dependency parsers are now significantly more ac-
curate, thanks to advances like the high-order maxi-
mum spanning tree (MST) model (Koo and Collins,
2010) for graph-based dependency parsing (McDon-
ald and Pereira, 2006). Therefore, we deemed it im-
portant to re-evaluate the performance of constituent
and dependency parsers. But the main purpose of
our work is to apply the more sophisticated depen-
dency parsing algorithms specifically to Chinese.
Number of \in Train Dev Test Total
files 2,083 160 205 2,448
sentences 46,572 2,079 2,796 51,447
tokens 1,039,942 59,955 81,578 1,181,475
Table 2: Statistics for Chinese TreeBank (CTB) 7.0 data.
2 Methodology
We compared seven popular open source constituent
and dependency parsers, focusing on both accuracy
and parsing speed. We hope that our analysis will
help end-users select a suitable method for parsing
to Stanford dependencies in their own applications.
2.1 Parsers
We considered four constituent parsers. They are:
Berkeley (Petrov et al., 2006), Bikel (2004), Char-
niak (2000) and Stanford (Klein and Manning,
2003) chineseFactored, which is also the default
used by Stanford dependencies. The three depen-
dency parsers are: MaltParser (Nivre et al., 2006),
Mate (Bohnet, 2010)2 and MSTParser (McDonald
and Pereira, 2006). Table 1 has more information.
2A second-order MST parser (with the speed optimization).
2.2 Corpus
We used the latest Chinese TreeBank (CTB) 7.0 in
all experiments.3 CTB 7.0 is larger and has more
sources (e.g., web text), compared to previous ver-
sions. We split the data into train/development/test
sets (see Table 2), with gold word segmentation, fol-
lowing the guidelines suggested in documentation.
2.3 Settings
Every parser was run with its own default options.
However, since the default classifier used by Malt-
Parser is libsvm (Chang and Lin, 2011) with a poly-
nomial kernel, it may be too slow for training models
on all of CTB 7.0 training data in acceptable time.
Therefore, we also tested this particular parser with
the faster liblinear (Fan et al., 2008) classifier. All
experiments were performed on a machine with In-
tel?s Xeon E5620 2.40GHz CPU and 24GB RAM.
2.4 Features
Unlike constituent parsers, dependency models re-
quire exogenous part-of-speech (POS) tags, both in
training and in inference. We used the Stanford tag-
ger (Toutanova et al., 2003) v3.1, with the MEMM
model,4 in combination with 10-way jackknifing.5
Word lemmas ? which are generalizations of
words ? are another feature known to be useful
for dependency parsing. Here we lemmatized each
Chinese word down to its last character, since ? in
contrast to English ? a Chinese word?s suffix often
carries that word?s core sense (Tseng et al., 2005).
For example, bicycle (???), car (??) and
train (??) are all various kinds of vehicle (?).
3www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2010T07
4nlp.stanford.edu/software/tagger.shtml
5Training sentences in each fold were tagged using a model
based on the other nine folds; development and test sentences
were tagged using a model based on all ten of the training folds.
12
Dev Test
Type Parser UAS LAS UAS LAS Parsing Time
Constituent Berkeley 82.0 77.0 82.9 77.8 45:56
Bikel 79.4 74.1 80.0 74.3 6,861:31
Charniak 77.8 71.7 78.3 72.3 128:04
Stanford 76.9 71.2 77.3 71.4 330:50
Dependency MaltParser (liblinear) 76.0 71.2 76.3 71.2 0:11
MaltParser (libsvm) 77.3 72.7 78.0 73.1 556:51
Mate (2nd-order) 82.8 78.2 83.1 78.1 87:19
MSTParser (1st-order) 78.8 73.4 78.9 73.1 12:17
Table 3: Performance and efficiency for all parsers on CTB data: unlabeled and labeled attachment scores (UAS/LAS)
are for both development and test data sets; parsing times (minutes:seconds) are for the test data only and exclude gen-
eration of basic Stanford dependencies (for constituent parsers) and part-of-speech tagging (for dependency parsers).
3 Results
Table 3 tabulates efficiency and performance for all
parsers; UAS and LAS are unlabeled and labeled at-
tachment scores, respectively ? the standard crite-
ria for evaluating dependencies. They can be com-
puted via a CoNLL-X shared task dependency pars-
ing evaluation tool (without scoring punctuation).6
3.1 Chinese
Mate scored highest, and Berkeley was the most ac-
curate of constituent parsers, slightly behind Mate,
using half of the time. MaltParser (liblinear) was by
far the most efficient but also the least performant; it
scored higher with libsvm but took much more time.
The 1st-order MSTParser was more accurate than
MaltParser (libsvm) ? a result that differs from that
of Cer et al. (2010) for English (see ?3.2). The Stan-
ford parser (the default for Stanford dependencies)
was only slightly more accurate than MaltParser (li-
blinear). Bikel?s parser was too slow to be used in
practice; and Charniak?s parser ? which performs
best for English ? did not work well for Chinese.
3.2 English
Our replication of Cer et al.?s (2010, Table 1) evalua-
tion revealed a bug: MSTParser normalized all num-
bers to a <num> symbol, which decreased its scores
in the evaluation tool used with Stanford dependen-
cies. After fixing this glitch, MSTParser?s perfor-
mance improved from 78.8 (reported) to 82.5%, thus
making it more accurate than MaltParser (81.1%)
and hence the better dependency parser for English,
consistent with our results for Chinese (see Table 3).
6ilk.uvt.nl/conll/software/eval.pl
Our finding does not contradict the main qualita-
tive result of Cer et al. (2010), however, since the
constituent parser of Charniak and Johnson (2005)
still scores substantially higher (89.1%), for English,
compared to all dependency parsers.7 In a separate
experiment (parsing web data),8 we found Mate to
be less accurate than Charniak-Johnson ? and im-
provement from jackknifing smaller ? on English.
4 Analysis
To further compare the constituent and dependency
approaches to generating Stanford dependencies, we
focused on Mate and Berkeley parsers ? the best
of each type. Overall, the difference between their
accuracies is not statistically significant (p > 0.05).9
Table 4 highlights performance (F1 scores) for the
most frequent relation labels. Mate does better on
most relations, noun compound modifiers (nn) and
adjectival modifiers (amod) in particular; and the
Berkeley parser is better at root and dep.10 Mate
seems to excel at short-distance dependencies, pos-
sibly because it uses more local features (even with
a second-order model) than the Berkeley parser,
whose PCFG can capture longer-distance rules.
Since POS-tags are especially informative of Chi-
nese dependencies (Li et al., 2011), we harmonized
training and test data, using 10-way jackknifing (see
?2.4). This method is more robust than training a
7One (small) factor contributing to the difference between
the two languages is that in the Chinese setup we stop with basic
Stanford dependencies ? there is no penalty for further conver-
sion; another is not using discriminative reranking for Chinese.
8sites.google.com/site/sancl2012/home/shared-task
9For LAS, p ? 0.11; and for UAS, p ? 0.25, according to
www.cis.upenn.edu/?dbikel/download/compare.pl
10An unmatched (default) relation (Chang et al., 2009, ?3.1).
13
Relation Count Mate Berkeley
nn 7,783 91.3 89.3
dep 4,651 69.4 70.3
nsubj 4,531 87.1 85.5
advmod 4,028 94.3 93.8
dobj 3,990 86.0 85.0
conj 2,159 76.0 75.8
prep 2,091 94.3 94.1
root 2,079 81.2 82.3
nummod 1,614 97.4 96.7
assmod 1,593 86.3 84.1
assm 1,590 88.9 87.2
pobj 1,532 84.2 82.9
amod 1,440 85.6 81.1
rcmod 1,433 74.0 70.6
cpm 1,371 84.4 83.2
Table 4: Performance (F1 scores) for the fifteen most-
frequent dependency relations in the CTB 7.0 develop-
ment data set attained by both Mate and Berkeley parsers.
parser with gold tags because it improves consis-
tency, particularly for Chinese, where tagging accu-
racies are lower than in English. On development
data, Mate scored worse given gold tags (75.4 versus
78.2%).11 Lemmatization offered additional useful
cues for overcoming data sparseness (77.8 without,
versus 78.2% with lemma features). Unsupervised
word clusters could thus also help (Koo et al., 2008).
5 Discussion
Our results suggest that if accuracy is of primary
concern, then Mate should be preferred;12 however,
Berkeley parser offers a trade-off between accuracy
and speed. If neither parser satisfies the demands
of a practical application (e.g., real-time processing
or bulk-parsing the web), then MaltParser (liblinear)
may be the only viable option. Fortunately, it comes
with much headroom for improving accuracy, in-
cluding a tunable margin parameter C for the classi-
fier, richer feature sets (Zhang and Nivre, 2011) and
ensemble models (Surdeanu and Manning, 2010).
Stanford dependencies are not the only popular
dependency representation. We also considered the
11Berkeley?s performance suffered with jackknifed tags (76.5
versus 77.0%), possibly because it parses and tags better jointly.
12Although Mate?s performance was not significantly better
than Berkeley?s in our setting, it has the potential to tap richer
features and other advantages of dependency parsers (Nivre and
McDonald, 2008) to further boost accuracy, which may be diffi-
cult in the generative framework of a typical constituent parser.
conversion scheme of the Penn2Malt tool,13 used
in a series of CoNLL shared tasks (Buchholz and
Marsi, 2006; Nivre et al., 2007; Surdeanu et al.,
2008; Hajic? et al., 2009). However, this tool relies
on function tag information from the CTB in deter-
mining dependency relations. Since these tags usu-
ally cannot be produced by constituent parsers, we
could not, in turn, obtain CoNLL-style dependency
trees from their output. This points to another advan-
tage of dependency parsers: they need only the de-
pendency tree corpus to train and can conveniently
make use of native (unconverted) corpora, such as
the Chinese Dependency Treebank (Liu et al., 2006).
Lastly, we must note that although the Berkeley
parser is on par with Charniak?s (2000) system for
English (Cer et al., 2010, Table 1), its scores for Chi-
nese are substantially higher. There may be subtle
biases in Charniak?s approach (e.g., the conditioning
hierarchy used in smoothing) that could turn out to
be language-specific. The Berkeley parser appears
more general ? without quite as many parameters
or idiosyncratic design decisions ? as evidenced by
a recent application to French (Candito et al., 2010).
6 Conclusion
We compared seven popular open source parsers ?
four constituent and three dependency ? for gen-
erating Stanford dependencies in Chinese. Mate, a
high-order MST dependency parser, with lemmati-
zation and jackknifed POS-tags, appears most accu-
rate; but Berkeley?s faster constituent parser, with
jointly-inferred tags, is statistically no worse. This
outcome is different from English, where constituent
parsers systematically outperform direct methods.
Though Mate scored higher overall, Berkeley?s
parser was better at recovering longer-distance re-
lations, suggesting that a combined approach could
perhaps work better still (Rush et al., 2010, ?4.2).
Acknowledgments
We thank Daniel Cer, for helping us replicate the English ex-
perimental setup and for suggesting that we explore jackknifing
methods, and the anonymous reviewers, for valuable comments.
Supported in part by the National Natural Science Founda-
tion of China (NSFC) via grant 61133012, the National ?863?
Major Project grant 2011AA01A207, and the National ?863?
Leading Technology Research Project grant 2012AA011102.
13w3.msi.vxu.se/?nivre/research/Penn2Malt.html
14
Second author gratefully acknowledges the continued help
and support of his advisor, Dan Jurafsky, and of the Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program, under the Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opinions, findings,
and conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reflect the views
of DARPA, AFRL, or the US government.
References
