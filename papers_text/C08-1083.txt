Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 657?664
Manchester, August 2008
A Discriminative Alignment Model for Abbreviation Recognition
Naoaki Okazaki
?
okazaki@is.s.u-tokyo.ac.jp
Sophia Ananiadou
?
sophia.ananiadou@manchester.ac.uk
Jun?ichi Tsujii
??
tsujii@is.s.u-tokyo.ac.jp
?
Graduate School of Information
Science and Technology
University of Tokyo
7-3-1 Hongo, Bunkyo-ku
Tokyo 113-8656, Japan
?
School of Computer Science,
University of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre
131 Princess Street, Manchester M1 7DN, UK
Abstract
This paper presents a discriminative align-
ment model for extracting abbreviations
and their full forms appearing in actual
text. The task of abbreviation recognition
is formalized as a sequential alignment
problem, which finds the optimal align-
ment (origins of abbreviation letters) be-
tween two strings (abbreviation and full
form). We design a large amount of fine-
grained features that directly express the
events where letters produce or do not pro-
duce abbreviations. We obtain the optimal
combination of features on an aligned ab-
breviation corpus by using the maximum
entropy framework. The experimental re-
sults show the usefulness of the alignment
model and corpus for improving abbrevia-
tion recognition.
1 Introduction
Abbreviations present two major challenges in nat-
ural language processing: term variation and am-
biguity. Abbreviations substitute for expanded
terms (e.g., dynamic programming) through the
use of shortened term-forms (e.g., DP). At the
same time, the abbreviation DP appearing alone in
text is ambiguous, in that it may refer to different
concepts, e.g., data processing, dirichlet process,
differential probability. Associating abbreviations
and their full forms is useful for various applica-
tions including named entity recognition, informa-
tion retrieval, and question answering.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The task of abbreviation recognition, in which
abbreviations and their expanded forms appearing
in actual text are extracted, addresses the term vari-
ation problem caused by the increase in the num-
ber of abbreviations (Chang and Sch?utze, 2006).
Furthermore, abbreviation recognition is also cru-
cial for disambiguating abbreviations (Pakhomov,
2002; Gaudan et al, 2005; Yu et al, 2006), pro-
viding sense inventories (lists of abbreviation def-
initions), training corpora (context information of
full forms), and local definitions of abbreviations.
Hence, abbreviation recognition plays a key role in
abbreviation management.
Numerous researchers have proposed a variety
of heuristics for recognizing abbreviation defini-
tions, e.g., the use of initials, capitalizations, syl-
lable boundaries, stop words, lengths of abbrevia-
tions, and co-occurrence statistics (Park and Byrd,
2001; Wren and Garner, 2002; Liu and Fried-
man, 2003; Okazaki and Ananiadou, 2006; Zhou
et al, 2006; Jain et al, 2007). Schwartz and
Hearst (2003) implemented a simple algorithm that
finds the shortest expression containing all alpha-
numerical letters of an abbreviation. Adar (2004)
presented four scoring rules to choose the most
likely expanded form in multiple candidates. Ao
and Takagi (2005) designed more detailed condi-
tions for accepting or discarding candidates of ab-
breviation definitions.
However, these studies have limitations in dis-
covering an optimal combination of heuristic rules
from manual observations of a corpus. For exam-
ple, when expressions transcrip
:
tion factor 1 and
thyroid transcription factor 1 are full-form can-
didates for the abbreviation TTF-1
1
, an algorithm
should choose the latter expression over the shorter
1
In this paper, we use straight and
::::
wavy underlines to rep-
resent correct and incorrect origins of abbreviation letters.
657
expression (former). Previous studies hardly han-
dle abbreviation definitions where full forms (e.g.,
water activity) shuffle their abbreviation letters
(e.g., AW). It is also difficult to reject ?negative?
definitions in a text; for example, an algorithm
should not extract an abbreviation definition from
the text, ?the replicon encodes a large
:::
replic
:
ation
protein (RepA),? since RepA provides a descrip-
tion of the protein rather than an abbreviation.
In order to acquire the optimal rules from
the corpora, several researchers applied machine
learning methods. Chang and Sch?utze (2006) ap-
plied logistic regression to combine nine features.
Nadeau and Turney (2005) also designed seven-
teen features to classify candidates of abbrevia-
tion definitions into positive or negative instances
by using the Support Vector Machine (SVM).
Notwithstanding, contrary to our expectations, the
machine-learning approach could not report better
results than those with hand-crafted rules.
We identify the major problem in the previ-
ous machine-learning approach: these studies did
not model associations between abbreviation let-
ters and their origins, but focused only on indirect
features such as the number of abbreviation letters
that appear at the head of a full form. This was
probably because the training corpus did not in-
clude annotations on the exact origins of abbrevia-
tion letters but only pairs of abbreviations and full
forms. It was thus difficult to design effective fea-
tures for abbreviation recognition and to reuse the
knowledge obtained from the training processes.
In this paper, we formalize the task of abbrevi-
ation recognition as a sequential alignment prob-
lem, which finds the optimal alignment (origins of
abbreviation letters) between two strings (abbrevi-
ation and full form). We design a large amount
of features that directly express the events where
letters produce or do not produce abbreviations.
Preparing an aligned abbreviation corpus, we ob-
tain the optimal combination of the features by us-
ing the maximum entropy framework (Berger et
al., 1996). We report the remarkable improve-
ments and conclude this paper.
2 Proposed method
2.1 Abbreviation alignment model
We express a sentence x as a sequence of letters
(x
1
, ..., x
L
), and an abbreviation candidate y in the
sentence as a sequence of letters (y
1
, ..., y
M
). We
define a letter mapping a = (i, j) to indicate that
the abbreviation letter y
j
is produced by the letter
in the full form x
i
. A null mapping a = (i, 0) indi-
cates that the letter in the sentence x
i
is unused to
form the abbreviation. Similarly, a null mapping
a = (0, j) indicates that the abbreviation letter y
j
does not originate from any letter in x. We de-
fine a
(x)
and a
(y)
in order to represent the first and
second elements of the letter mapping a. In other
words, a
(x)
and a
(y)
are equal to i and j respec-
tively, when a = (i, j). Finally, an abbreviation
alignment a is defined as a sequence of letter map-
pings, a = (a
1
, ..., a
T
), where T represents the
number of mappings in the alignment.
Let us consider the following example sentence:
We investigate the effect of thyroid tran-
scription factor 1 (TTF-1).
This sentence contains an abbreviation candidate
TTF-1 in parentheses
2
. Figure 1 illustrates the
correct alignment a (bottom line) and its two-
dimensional representation for the example sen-
tence
3
; the abbreviation letters ?t,? ?t,? ?f,? ?-,? and
?1? originate from x
30
, x
38
, x
52
, nowhere (null
mapping), and x
59
respectively.
We directly model the conditional probability of
the alignment a, given x and y, using the maxi-
mum entropy framework (Berger et al, 1996),
P (a|x,y) =
exp {? ? F (a,x,y)}
?
a?C(x,y)
exp {? ? F (a,x,y)}
.
(1)
In Formula 1, F = {f
1
, ..., f
K
} is a global feature
vector whose elements present K feature func-
tions, ? = {?
1
, ..., ?
K
} denotes a weight vector
for the feature functions, and C(x,y) yields a set
of possible alignments for the given x and y. We
obtain the following decision rule to choose the
most probable alignment a? for given x and y,
a? = argmax
a?C(x,y)
P (a|x,y). (2)
Note that a set of possible alignments C(x,y)
always includes a negative alignment whose ele-
ments are filled with null-mappings (refer to Sec-
tion 2.3 for further detail). This allows the formula
to withdraw the abbreviation candidate y when any
expression in x is unlikely to be a definition.
2
Refer to Section 3.1 for text makers for abbreviations.
3
We ignore non-alphabetical letters in abbreviations.
658
    We investigate the effect of thyroid transcription factor 1 (TTF-1) ...x:
a
            ~   ~  ~    ~~  ~  ~ ~       ~        ~    ~  ~   ~ ~ 
8 words 4 lettersmin(|y|+5, 2|y|)
<NIL> T  T  F  -   1  <SF>
y:
0 0 0 0 0 0 0 0 0
1
2
3
5
91 13 16 2122 25 28 30 38 47 52 55 59 61
1
0
1
2
3
4
5
6
t =
i
 j
a =
a 
2 3 4 5 6 7 8 9 10 11 12 13 14
Null outsideOther positions Abbreviation Null inside Associate inside
a = ((9,0), (13,0), (16,0), (21,0), (22, 0), (25,0), (28,0), (30,1), (38,2), (47,0), (52,3), (55,0), (59,5), (61,6))
Figure 1: The correct alignment for the example sentence and its two dimensional representation.
2.2 Features
The main advantage of the discriminative align-
ment model is its ability to incorporate a wide
range of non-independent features. Inspired
by feature engineering for Conditional Random
Fields (CRFs) (Lafferty et al, 2001), we design
two kinds of features: unigram (state) features de-
fined on each letter mapping, and bigram (tran-
sition) features defined on each pair of adjacent
letter mappings. Given a triplet, a, x, and y, a
global feature function f
k
(a,x,y) ? F sums up
the boolean values (0 or 1) of the corresponding
local feature g
k
(a,x,y, t) at t ? {1, ..., T},
f
k
(a,x,y) =
T
?
t=1
g
k
(a,x,y, t). (3)
In other words, f
k
(a,x,y) counts the number of
times the local feature is fired in the alignment a.
A unigram feature corresponds to the observa-
tion at x
i
and y
j
associated by a mapping a
t
=
(i, j). A unigram feature encodes the condition
where the letter in the full form x
i
is chosen or
unchosen for producing the abbreviation letter y
j
.
For example, we may infer from the letter mapping
at a
8
= (30, 1) in Figure 1, that x
30
is mapped to
y
1
because: x
30
is at the head of the word, y
1
is a
capital letter, and both x
30
and y
1
are at the head
of the word and abbreviation.
Bigram features, combining two observations at
a
s
and a
t
(1 ? s < t ? T ), are useful in capturing
the common characteristics shared by an abbrevi-
ation definition. For instance, we may presume in
Figure 1 that the head letters in the full form might
be selectively used for producing the abbreviation,
based on the observations at a
8
= (30, 1) and
a
9
= (38, 2). In order to focus on the conditions
for consecutive non-null mappings, we choose the
previous position s for the given t.
s =
?
?
?
t? 1
(
a
t(y)
= 0 ? ?u : a
u(y)
= 0
)
max
1?u<t
{
u | a
u(y)
6= 0
}
(otherwise)
(4)
Formula 4 prefers the non-null mapping that is the
most adjacent to t over the previous mapping (t ?
1). In Figure 1, transitions a
9
?a
11
and a
11
?a
13
exist for this reason.
In this study, we express unigram and bi-
gram features with atomic functions (Table 1)
that encode observation events of x
a
t(x)
, y
a
t(y)
,
a
t
, x
a
s(x)
?x
a
t(x)
, and y
a
s(y)
?y
a
t(y)
. Atomic
functions x ctype, y ctype, x position, and
y position present common heuristics used by
previous studies. The function x word examines
the existence of stop words (e.g., the, of, in) to
prevent them from producing abbreviation letters.
We also include x pos (part-of-speech of the word)
since a number of full forms are noun phrases.
Functions x diff , x diff wd, and y diff are de-
signed specifically for bigram features, receiving
two positions s and t in their arguments. The
function x diff mainly deals with abbreviation def-
initions that include consecutive letters of their
full forms, e.g., amplifier (AMP). The function
659
Function Return value
x ctype
?
(a,x, t) x
a
t(x)
+?
is {U (uppercase), L (lowercase), D (digit), W (whitespace), S (symbol) } letter
x position
?
(a,x, t) x
a
t(x)
+?
is at the {H (head), T (tail), S (syllable head), I (inner), W (whitespace) } of the word
x char
?
(a,x, t) The lower-cased letter of x
a
t(x)
+?
x word
?
(a,x, t) The lower-cased word (offset position ?) containing the letter x
a
t(x)
x pos
?
(a,x, t) The part-of-speech code of the word (offset position ?) containing the letter x
a
t(x)
y ctype(a,y, t) y
a
t(y)
is {N (NIL) U (uppercase), L (lowercase), D (digit), S (symbol) } letter
y position(a,y, t) y
a
t(y)
is at the {N (NIL) H (head), T (tail), I (inner)} of the word
y char(a,y, t) The lower-cased letter of y
a
t(y)
a state(a,y, t) {SKIP (a
t(y)
= 0),MATCH (1 ? a
t(y)
? |y|),ABBR (a
t(y)
= |y|+ 1)}
x diff(a,x, s, t) (a
t(x)
? a
s(x)
) if letters x
a
t(x)
and x
a
s(x)
are in the same word, NONE otherwise
x diff wd(a,x, s, t) The number of words between x
a
t(x)
and x
a
s(x)
y diff(a,y, s, t) (a
t(y)
? a
s(y)
)
Table 1: Atomic functions to encode observation events in x and y
Combination Rules
unigram(t) xy unigram(t)? {a state(t)}
xy unigram(t) x unigram(t)? y unigram(t)?
(
x unigram(t)? y unigram(t)
)
x unigram(t) x state
0
(t)? x state
?1
(t)? x state
1
(t)
?
(
x state
?1
(t)? x state
0
(t)
)
?
(
x state
0
(t)? x state
1
(t)
)
y unigram(t)
{
y ctype(t), y position(t), y ctype(t)y position(t)
}
x state
?
(t)
{
x ctype
?
(t), x position
?
(t), x char
?
(t), x word
?
(t), x pos
?
(t), x ctype
?
(t)x position
?
(t),
x position
?
(t)x pos
?
(t), x pos
?
(t)x ctype
?
(t), x ctype
?
(t)x position
?
(t)x pos
?
(t)
}
bigram(s, t) xy bigram(s, t)? {a state(s)a state(t)}
xy bigram(s, t)
(
x state
0
(s)? x state
0
(t)? trans(s, t)
)
?
(
y unigram(s)? y unigram(t)? trans(s, t)
)
?
(
x state
0
(s)? y unigram(s)? x state
0
(t)? y unigram(t)? trans(s, t)
)
trans(s, t)
{
x diff(s, t), x diff wd(s, t), y diff(s, t)
}
Table 2: Generation rules for unigram and bigram features.
x diff wd measures the distance of two words.
The function y diff models the ordering of abbre-
viation letters; this function always returns non-
negative values if the abbreviation contains letters
in the same order as in its full form.
We express unigram and bigram features with
the atomic functions. For example, Formula 5 de-
fines a unigram feature for the event where the cap-
ital letter in a full-form word x
a
t(x)
produces the
identical abbreviation letter y
a
t(y)
.
g
k
(a,x,y, t) =
?
?
?
?
?
?
?
?
?
?
?
1 x ctype
0
(a,x, t) = U
? y ctype(a,y, t) = U
? a state(a,y, t) = MATCH
0 (otherwise)
(5)
For notation simplicity, we rewrite this boolean
function as (arguments a, x, and y are omitted),
1
{x ctype
0
(t)y ctype(t)a state(t)=U;U;MATCH}
. (6)
In this formula, 1
{v=v?}
is an indicator function that
equals 1 when v = v? and 0 otherwise. The term
v presents a generation rule for a feature, i.e., a
combination rule of atomic functions.
Table 2 displays the complete list of gener-
ation rules for unigram and bigram features
4
,
unigram(t) and bigram(s, t). For each genera-
tion rule in unigram(t) and bigram(s, t), we de-
fine boolean functions that test the possible values
yielded by the corresponding atomic function(s).
2.3 Alignment candidates
Formula 1 requires a sum over the possible align-
ments, which amounts to 2
LM
for a sentence (L
letters) with an abbreviation (M letters). It is
unrealistic to compute the partition factor of the
formula directly; therefore, the factor has been
computed by dynamic programing (McCallum et
al., 2005; Blunsom and Cohn, 2006; Shimbo and
Hara, 2007) or approximated by the n-best list of
highly probable alignments (Och and Ney, 2002;
Liu et al, 2005). Fortunately, we can prune align-
ments that are unlikely to present full forms, by in-
troducing the natural assumptions for abbreviation
definitions:
4
In Table 2, a set of curly brackets {} denotes a list (array)
rather than a mathematical set. Operators ? and ? present
concatenation and Cartesian product of lists. For instance,
when A = {a, b} and B = {c, d}, A?B = {a, b, c, d} and
A?B = {ac, ad, bc, bd}.
660
  investigate the effect of thyroid transcription factor 1
0   0  0    00  0  0 0       0        0    0  0   00   0  0    00  0  0 0       1        2    3  0   50   0  0    00  0  0 1       2        0    3  0   50   0  0    00  0  0 1       0        2    3  0   50   0  0    00  0  0 2       1        0    3  0   50   0  0    00  0  0 2       0        1    3  0   50   0  0    00  0  3 0       0        1    0  2   50   0  0    00  0  3 0       1        2    0  0   50   0  0    00  0  3 0       1        0    0  2   5
.   .  .    ..  .  . .       .        .    .  .   ..   .  .    ..  .  . .       .        .    .  .   ..   .  .    ..  .  . .       .        .    .  .   .
x: ~   ~  ~    ~~  ~  ~ ~       ~        ~    ~  ~   ~
min(|y|+5, 2|y|) = 8 words, (|y| = 4; y = "TTF-1")
94 13 16 2122 25 28 30 38 47 52 55 59ia =
ShffleShffleShffleShffleShffle
#0
#1
#2
#3
#4
#5
#6
#7
#8
Figure 2: A part of the possible alignments for the
abbreviation TTF-1 in the example sentence.
1. A full form may appear min(m + 5, 2m)
words before its abbreviation in the same sen-
tence, where m is the number of alphanu-
meric letters in the abbreviation (Park and
Byrd, 2001).
2. Every alphanumeric letter in an abbreviation
must be associated with the identical (case-
insensitive) letter in its full form.
3. An abbreviation letter must not originate from
multiple letters in its full form; a full-form let-
ter must not produce multiple letters.
4. Words in a full form may be shuffled at most
d times, so that all alphanumeric letters in the
corresponding abbreviation appear in the re-
arranged full form in the same order. We de-
fine a shuffle operation as removing a series
of word(s) from a full form, and inserting the
removed word(s) to another position.
5. A full form does not necessarily exist in the
text span defined by assumption 1.
Due to the space limitation, we do not describe
the algorithm for obtaining possible alignments
that are compatible with these assumptions. Al-
ternatively, Figure 2 illustrates a part of possible
alignments C(x,y) for the example sentence. The
alignment #2 represents the correct definition for
the abbreviation TTF-1. We always include the
negative alignment (e.g., #0) where no abbrevia-
tion letters are associated with any letters in x.
The alignments #4?8 interpret the generation
process of the abbreviation by shuffling the words
in x. For example, the alignment #6 moves the
word ?of? to the position between ?factor? and
?1?. Shuffled alignments cover abbreviation defini-
tions such as receptor of estrogen (ER) and water
activity (AW). We call the parameter d, distortion
parameter, which controls the acceptable level of
reordering (distortion) for the abbreviation letters.
2.4 Parameter estimation
Parameter estimation for the abbreviation
alignment model is essentially the same as
for general maximum entropy models. Given
a training set that consists of N instances,
(
(a
(1)
,x
(1)
,y
(1)
), ..., (a
(N)
,x
(N)
,y
(N)
)
)
, we
maximize the log-likelihood of the conditional
probability distribution by using the maximum
a posterior (MAP) estimation. In order to avoid
overfitting, we regularize the log-likelihood with
either the L
1
or L
2
norm of the weight vector ?,
L
1
=
N
?
n=1
logP (a
(n)
|x
(n)
,y
(n)
)?
||?||
1
?
1
, (7)
L
2
=
N
?
n=1
logP (a
(n)
|x
(n)
,y
(n)
)?
||?||
2
2
2?
2
2
. (8)
In these formulas, ?
1
and ?
2
are regularization pa-
rameters for the L
1
and L
2
norms. Formulas 7
and 8 are maximized by the Orthant-Wise Limited-
memory Quasi-Newton (OW-LQN) method (An-
drew and Gao, 2007) and the Limited-memory
BFGS (L-BFGS) method (Nocedal, 1980)
5
.
3 Experiments
3.1 Aligned abbreviation corpus
The Medstract Gold Standard Corpus (Pustejovsky
et al, 2002) was widely used for evaluating abbre-
viation recognition methods (Schwartz and Hearst,
2003; Adar, 2004). However, we cannot use
this corpus for training the abbreviation alignment
model, since it lacks annotations on the origins of
abbreviation letters. In addition, the size of the
corpus is insufficient for a supervised machine-
learning method.
Therefore, we built our training corpus with
1,000 scientific abstracts that were randomly cho-
sen from the MEDLINE database. Although the
alignment model is independent of linguistic pat-
terns for abbreviation definitions, in the corpus we
found only three abbreviation definitions that were
described without parentheses. Hence, we em-
ployed parenthetical expressions, full-form ?(? ab-
breviation ?)?, to locate possible abbreviation def-
initions (Wren and Garner, 2002). In order to ex-
clude parentheses inserting clauses into passages,
5
We used Classias for parameter estimation:
http://www.chokkan.org/software/classias/
661
we consider the inner expression of parentheses as
an abbreviation candidate, only if the expression
consists of two words at most, the length of the ex-
pression is between two to ten characters, the ex-
pression contains at least an alphabetic letter, and
the first character is alphanumeric.
We asked a human annotator to assign refer-
ence abbreviation alignments for 1,420 parentheti-
cal expressions (instances) in the corpus. If a par-
enthetical expression did not introduce an abbre-
viation, e.g., ?... received treatment at 24 months
(RRMS),? the corresponding instance would have
a negative alignment (as #0 in Figure 2). Eventu-
ally, our aligned corpus consisted of 864 (60.8%)
abbreviation definitions (with positive alignments)
and 556 (39.2%) other usages of parentheses (with
negative alignments). Note that the log-likelihood
in Formula 7 or 8 increases only if the probabilistic
model predicts the reference alignments, regard-
less of whether they are positive or negative.
3.2 Baseline systems
We prepared five state-of-the-art systems of ab-
breviation recognition as baselines: Schwartz
and Hearst?s method (SH) (Schwartz and Hearst,
2003), SaRAD (Adar, 2004), ALICE (Ao and
Takagi, 2005), Chang and Sch?utze?s method
(CS) (Chang and Sch?utze, 2006), and Nadeau and
Turney?s method (NT) (Nadeau and Turney, 2005).
We utilized the implementations available on the
Web for SH
6
, CS
78
, and ALICE
9
, and we repro-
duced SaRAD and NT, based on their papers.
Our implementation of NT consists of a classi-
fier that discriminates between positive (true) and
negative (false) full forms, using all of the feature
functions presented in the original paper. Although
the original paper presented heuristics for gener-
ating full-form candidates, we replaced the candi-
date generator with the function C(x,y), so that
the classifier and our alignment model can receive
the same set of full-form candidates. The classi-
fier of the NT system was modeled by the LIB-
SVM implementation
10
with Radial Basis Func-
6
Abbreviation Definition Recognition Software:
http://biotext.berkeley.edu/software.html
7
Biomedical Abbreviation Server:
http://abbreviation.stanford.edu/
8
We applied a score cutoff of 0.14.
9
Abbreviation LIfter using Corpus-based Extraction:
http://uvdb3.hgc.jp/ALICE/ALICE index.html
10
LIBSVM ? A Library for Support Vector Machines:
http://www.csie.ntu.edu.tw/
?
cjlin/libsvm/
System P R F1
Schwartz & Hearst (SH) .978 .940 .959
SaRAD .891 .919 .905
ALICE .961 .920 .940
Chang & Sch?utze (CS) .942 .900 .921
Nadeau & Turney (NT) .954 .871 .910
Proposed (d = 0; L
1
) .973 .969 .971
Proposed (d = 0; L
2
) .964 .968 .966
Proposed (d = 1; L
1
) .960 .981 .971
Proposed (d = 1; L
2
) .957 .976 .967
Table 3: Performance on our corpus.
tion (RBF) kernel
11
. If multiple full-form can-
didates for an abbreviation are classified as posi-
tives, we choose the candidate that yields the high-
est probability estimate.
3.3 Results
We trained and evaluated the proposed method on
our corpus by performing 10-fold cross valida-
tion
12
. Our corpus includes 13 out of 864 (1.5%)
abbreviation definitions in which the abbreviation
letters are shuffled. Thus, we have examined two
different distortion parameters, d = 0, 1 in this
experiment. The average numbers of candidates
produced by the candidate generator C(x,y) per
instance were 8.46 (d = 0) and 69.1 (d = 1), re-
spectively. The alignment model was trained in a
reasonable execution time
13
, ca. 5 minutes (d = 0)
and 1.5 hours (d = 1).
Table 3 reports the precision (P), recall (R), and
F1 score (F1) on the basis of the number of cor-
rect abbreviation definitions recognized by each
system. The proposed method achieved the best
F1 score (0.971) of all systems. The inclusion of
distorted abbreviations (d = 1) gained the high-
est recall (0.981 with L
1
regularization). Base-
line systems with refined heuristics (SaRAD and
ALICE) could not outperform the simplest sys-
tem (SH). The previous approaches with machine
learning (CS and NT) were roughly comparable to
rule-based methods.
We also evaluated the alignment model on the
Medstract Gold Standard development corpus to
examine the adaptability of the alignment model
trained with our corpus (Table 4). Since the origi-
11
We tuned kernel parameters C = 128 and ? = 2.0 by
using the grid-search tool in the LIBSVM distribution.
12
We determined the regularization parameters as ?
1
= 3
and ?
2
= 3 after testing {0.1, 0.2, 0.3, 0.5, 1, 2, 3, 5, 10} for
the regularization parameters. The difference between the
highest and lowest F1 scores was 1.8%.
13
On Intel Dual-Core Xeon 5160/3GHz CPU, excluding
time for feature generation and data input/output.
662
System P R F1
Schwartz & Hearst (SH) .942 .891 .916
SaRAD .909 .859 .884
ALICE .960 .945 .953
Chang & Sch?utze (CS) .858 .852 .855
Nadeau & Turney (NT) .889 .875 .882
Proposed (d = 1; L
1
) .976 .945 .960
Table 4: Performance on Medstract corpus.
# Atomic function(s) F1
(1) x position + x ctype .905
(2) (1) + x char + y char .885
(3) (1) + x word + x pos .941
(4) (1) + x diff + x diff wd + y diff .959
(5) (1) + y position + y ctype .964
(6) All atomic functions .966
Table 5: Effect of atomic functions (d = 0; L
2
).
nal version of the Medstract corpus includes anno-
tation errors, we used the version revised by Ao
and Takagi (2005). For this reason, the perfor-
mance of ALICE might be over-estimated in this
evaluation; ALICE delivered much better results
than Schwartz & Hearst?s method on this corpus.
The abbreviation alignment model trained with
our corpus (d = 1; L
1
) outperformed the baseline
systems for all evaluation metrics. It is notable that
the model could recognize abbreviation definitions
with shuffled letters, e.g., transfer of single embryo
(SET) and inorganic phosphate (PI), without any
manual tuning for this corpus. In some false cases,
the alignment model yielded incorrect probability
estimates. For example, the probabilities of the
alignments prepubertal bipolarity, bi
:
polarity, and
non-definition (negative) for the abbreviation BP
were computed as 3.4%, 89.6%, and 6.7%, respec-
tively; but the first expression prepubertal bipolar-
ity is the correct definition for the abbreviation.
Table 5 shows F1 scores of the proposed method
trained with different sets of atomic functions. The
baseline setting (1), which built features only with
x position and x ctype functions, gained a 0.905
F1 score; further, adding more atomic functions
generally improves the score. However, the x char
and y char functions decreased the performance
since the alignment model was prone to overfit to
the training data, relying on the existence of spe-
cific letters in the training instances. Interestingly,
the model was flexible enough to achieve a high
performance with four atomic functions (5).
Table 6 demonstrates the ability for our ap-
proach to obtain effective features; the table shows
the top 10 (out of 850,009) features with high
# Feature ?
1 U: x position
0
=H;y ctype
0
=U;y position
0
=H/M 1.7370
2 B: y position
0
=I/y position
0
=I/x diff=1/M-M 1.3470
3 U: x ctype
?1
=L;x ctype
0
=L/S 0.96342
4 B: x ctype
0
=L/x ctype
0
=L/x diff wd=0/M-M 0.94009
5 U: x position
0
=I;x char
1
=?t?/S 0.91645
6 U: x position
0
=H;x pos
0
=NN;y ctype
0
=U/M 0.86786
7 U: x ctype
?1
=S;x
c
type
0
=L;M 0.86474
8 B: x char
0
=?o?/x ctype
0
=L/y diff=0/M-S 0.71262
9 U: x char
?1
=?o?;x ctype
0
=L/M 0.69764
10 B: x position
0
=H/x ctype
0
=U/y diff=1/M-M 0.66418
Table 6: Top ten features with high weights.
weights assigned by the MAP estimation with L
1
regularization. A unigram and bigram features
have prefixes ?U:? and ?B:? respectively; a feature
expresses conditions at s (bigram features only),
conditions at t, and mapping status (match or skip)
separated by ?/? symbols. For example, the #1 fea-
ture associates a letter at the head of a full-form
word with the uppercase letter at the head of its
abbreviation. The #4 feature is difficult to obtain
from manual observations, i.e., the bigram feature
suggests the production of two abbreviation letters
from two lowercase letters in the same word.
4 Conclusion
We have presented a novel approach for recogniz-
ing abbreviation definitions. The task of abbrevi-
ation recognition was successfully formalized as
a sequential alignment problem. We developed
an aligned abbreviation corpus, and obtained fine-
grained features that express the events wherein
a full forum produces an abbreviation letter. The
experimental results showed remarkable improve-
ments and usefulness of the alignment approach
for abbreviation recognition. We expect the use-
fullness of the discriminative model for building
an comprehensible abbreviation dictionary.
Future work would be to model cases in which
a full form yields non-identical letters (e.g., ?one?
? ?1? and ?deficient? ? ?-?), and to demonstrate
this approach with more generic linguistic patterns
(e.g., aka, abbreviated as, etc.). We also plan to
explore a method for training a model with an un-
aligned abbreviation corpus, estimating the align-
ments simultaneously from the corpus.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Scientific Research on Priority Areas (MEXT,
Japan), and Solution-Oriented Research for Sci-
ence and Technology (JST, Japan).
663
References
Adar, Eytan. 2004. SaRAD: A simple and robust
abbreviation dictionary. Bioinformatics, 20(4):527?
533.
Andrew, Galen and Jianfeng Gao. 2007. Scalable train-
ing of L
1
-regularized log-linear models. In Proceed-
ings of the 24th International Conference on Ma-
chine Learning (ICML 2007), pages 33?40.
Ao, Hiroko and Toshihisa Takagi. 2005. ALICE: An
algorithm to extract abbreviations from MEDLINE.
Journal of the American Medical Informatics Asso-
ciation, 12(5):576?586.
Berger, Adam L., Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Blunsom, Phil and Trevor Cohn. 2006. Discrimina-
tive word alignment with conditional random fields.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics (Coling-ACL 2006), pages 65?72.
Chang, Jeffrey T. and Hinrich Sch?utze. 2006. Abbre-
viations in biomedical text. In Ananiadou, Sophia
and John McNaught, editors, Text Mining for Biol-
ogy and Biomedicine, pages 99?119. Artech House,
Inc.
Gaudan, Sylvain, Harald Kirsch, and Dietrich Rebholz-
Schuhmann. 2005. Resolving abbreviations to their
senses in Medline. Bioinformatics, 21(18):3658?
3664.
Jain, Alpa, Silviu Cucerzan, and Saliha Azzam. 2007.
Acronym-expansion recognition and ranking on the
web. In Proceedings of the IEEE International Con-
ference on Information Reuse and Integration (IRI
2007), pages 209?214.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the 18th International Con-
ference on Machine Learning (ICML 2001), pages
282?289.
Liu, Hongfang and Carol Friedman. 2003. Mining ter-
minological knowledge in large biomedical corpora.
In the 8th Pacific Symposium on Biocomputing (PSB
2003), pages 415?426.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL 2005), pages 459?466.
McCallum, Andrew, Kedar Bellare, and Fernando
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In Proceedings of the 21st Conference on Un-
certainty in Artificial Intelligence (UAI 2005), pages
388?395.
Nadeau, David and Peter D. Turney. 2005. A super-
vised learning approach to acronym identification.
In the 8th Canadian Conference on Artificial Intel-
ligence (AI?2005) (LNAI 3501), page 10 pages.
Nocedal, Jorge. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35(151):773?782.
Och, Franz Josef and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics (ACL 2002), pages 295?302.
Okazaki, Naoaki and Sophia Ananiadou. 2006. Build-
ing an abbreviation dictionary using a term recogni-
tion approach. Bioinformatics, 22(24):3089?3095.
Pakhomov, Serguei. 2002. Semi-supervised maximum
entropy based approach to acronym and abbreviation
normalization in medical texts. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics (ACL 2002), pages 160?167.
Park, Youngja and Roy J. Byrd. 2001. Hybrid text min-
ing for finding abbreviations and their definitions. In
Proceedings of the 2001 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2001), pages 126?133.
Pustejovsky, James, Jos?e Casta?no, Roser Saur??, Anna
Rumshinsky, Jason Zhang, and Wei Luo. 2002.
Medstract: creating large-scale information servers
for biomedical libraries. In Proceedings of the ACL-
02 workshop on Natural language processing in the
biomedical domain, pages 85?92.
Schwartz, Ariel S. and Marti A. Hearst. 2003. A simple
algorithm for identifying abbreviation definitions in
biomedical text. In the 8th Pacific Symposium on
Biocomputing (PSB 2003), pages 451?462.
Shimbo, Masashi and Kazuo Hara. 2007. A dis-
criminative learning model for coordinate conjunc-
tions. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 610?619.
Wren, Jonathan D. and Harold R. Garner. 2002.
Heuristics for identification of acronym-definition
patterns within text: towards an automated construc-
tion of comprehensive acronym-definition dictionar-
ies. Methods of Information in Medicine, 41(5):426?
434.
Yu, Hong, Won Kim, Vasileios Hatzivassiloglou, and
John Wilbur. 2006. A large scale, corpus-based
approach for automatically disambiguating biomedi-
cal abbreviations. ACM Transactions on Information
Systems (TOIS), 24(3):380?404.
Zhou, Wei, Vetle I. Torvik, and Neil R. Smalheiser.
2006. ADAM: another database of abbreviations in
MEDLINE. Bioinformatics, 22(22):2813?2818.
664
