Proceedings of the SIGDIAL 2013 Conference, pages 270?279,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Gesture Semantics Reconstruction Based on Motion Capturing and
Complex Event Processing: a Circular Shape Example
Thies Pfeiffer, Florian Hofmann
Artificial Intelligence Group
Faculty of Technology
Bielefeld University, Germany
(tpfeiffe|fhofmann)
@techfak.uni-bielefeld.de
Florian Hahn, Hannes Rieser, Insa Ro?pke
Collaborative Research Center
?Alignment in Communication? (CRC 673)
Bielefeld University, Germany
(fhahn2|hannes.rieser|iroepke)
@uni-bielefeld.de
Abstract
A fundamental problem in manual based
gesture semantics reconstruction is the
specification of preferred semantic con-
cepts for gesture trajectories. This issue
is complicated by problems human raters
have annotating fast-paced three dimen-
sional trajectories. Based on a detailed
example of a gesticulated circular trajec-
tory, we present a data-driven approach
that covers parts of the semantic recon-
struction by making use of motion captur-
ing (mocap) technology. In our FA3ME
framework we use a complex event pro-
cessing approach to analyse and annotate
multi-modal events. This framework pro-
vides grounds for a detailed description of
how to get at the semantic concept of cir-
cularity observed in the data.
1 Introduction
Focussing on iconic gestures, we discuss the ben-
efit of motion capturing (mocap) technology for
the reconstruction of gesture meaning and speech
meaning: A fundamental problem is the specifica-
tion of semantic concepts for gesture trajectories,
e.g., for describing circular movements or shapes.
We start with demonstrating the limitations of our
manual based annotation. Then we discuss two
strategies of how to deal with these, pragmatic in-
ference vs. low level annotation based on mocap
technology yielding a more precise semantics. We
then argue that the second strategy is to be pre-
ferred to the inferential one.
The annotation of mocap data can be re-
alised semi-automatically by our FA3ME frame-
work for the analysis and annotation of multi-
modal events, which we use to record multi-modal
corpora. For mocap we use the tracking sys-
tem ART DTrack2 (advanced realtime tracking
GmbH, 2013), but the framework is not restricted
to this technical set-up. In cooperation with others
(e.g., (Kousidis et al, 2012)), we also have used
products from Vicon Motion Systems (2013) and
the Microsoft Kinect (Microsoft, 2013). Pfeiffer
(2013) presents an overview on mocap technology
for documenting multi-modal studies.
We thus provide details about the way gestures
are analysed with FA3ME and about the procedure
to reconstruct the gesture meaning for the circular
movement in our chosen example. We conclude
with a discussion of how these low-level recon-
structions can be integrated into the reconstruction
of speech and gesture meaning.
2 From Linguistic Annotation to MoCap
In this section we describe our methodology for
the reconstruction of gesture meaning, speech
meaning and its interfacing, illustrated by an ex-
ample. We then show a shortcoming of our
corpus-based annotation and discuss two possible
solutions to amend it, pragmatic inference vs. se-
mantics based on mocap technology. The technol-
ogy described in Section 3 will in the end enable
us to get the preferred reconstruction of gesture se-
mantics.
The reconstruction of the gesture meaning and
its fusion with speech meaning to get a multi-
modal proposition works as follows: On the
speech side we start with a manual transcription,
upon which we craft a context free syntax analy-
sis followed by a formal semantics. On the ges-
ture side we build an AVM-based representation
of the gesture resting on manual annotation.1 Tak-
ing the gesture as a sign with independent mean-
ing (Rieser, 2010), this representation provides the
basis for the formal gesture semantics. In the next
1We do not use an explicit gesture model, which would
go against our descriptive intentions. The range of admissible
gestures is fixed by annotation manuals and investigations in
gesture typology.
270
Figure 1: Our example: a circular gesture (left:
video still) to describe the path around the pond
(right).
step, the gesture meaning and the speech meaning
are fused into an interface (Ro?pke et al, 2013).
Every step in these procedures is infested by un-
derspecification which, however, we do not deal
with here. These are, for instance, the selection
of annotation predicates, the attribution of logical
form to gestures and the speech analysis.
In our example, we focus on two gesture pa-
rameters, the movement feature of the gesture
and the representation technique used. It orig-
inates from the systematically annotated corpus,
called SaGA, the Bielefeld Speech-and-Gesture
Alignment-corpus (Lu?cking et al, 2012). It con-
sists of 25 dialogues of dyads conversing about a
?bus ride? through a Virtual Reality town. One
participant of each dyad, the so-called Route-
Giver (RG), has done this ride and describes the
route and the sights passed to the other participant,
the so-called Follower (FO). The taped conversa-
tions are annotated in a fine-grained way.
In the example, the RG describes a route section
around a pond to the FO. While uttering ?Du gehst
drei Viertel rum/You walk three quarters around?,
she produces the gesture depicted in Figure 1. In-
tuitively, the gesture conveys a circularity infor-
mation not expressed in the verbal meaning. In or-
der to explicate the relation of speech and gesture
meaning, we use our methodology as described
above. To anticipate, we get a clear contribution
of the speech meaning which is restricted by the
gesture meaning conveying the roundness infor-
mation. The first step is to provide a syntactical
analysis which you can see in Figure 2.2
2The gesture stroke extends over the whole utterance.
Verb phrases can feature so-called ?sentence brackets?. Here,
due to a sentence bracket, the finite verb stem ?gehst? is sepa-
rated from its prefix (?rum?). Together they embrace the Ger-
man Mittelfeld, here ?drei Viertel?. Observe the N-ellipsis
??? in the NP. The prefix and the finite verb stem cannot be
fully interpreted on their own and are therefore marked with
S
VP
VPref*
rum
around
VFin**
NP
N
?
Quant
N
Viertel
quarters
NUM
drei
three
VFin*
gehst
walk
NP
PN
Du
You
gesture stroke
Figure 2: Syntax analysis
The speech representation is inspired by a
Montague-Parsons-Reichenbach event ontology,
and uses type-logic notions. Ignoring the embed-
ding in an indirect speech act3, the speech se-
mantics represents an AGENT (the FO) who is
engaged in a WALK-AROUND event e related to
some path F, and a THEME relating the WALK-
AROUND event e with the path F.
?eyF 3/4x(WALK-AROUND(e) ?
AGENT(e, FO) ? THEME(e, x)
? F(x, y)) (1)
The gesture semantics is obtained using the an-
notated gesture features. The relevant features are
the movement of the wrist (Path of Wrist) and the
Representation Technique used.
?
??
Path of Wrist ARC<ARC<
ARC<ARC
Representation Technique Drawing
?
??
4
Interpreting the values ARC<ARC<
ARC<ARC and Drawing, respectively, the
calculated gesture semantics represents a bent
trajectory consisting of four segments:
an asterisk.
3We have treated the function of speech-gesture ensem-
bles in dialogue acts and dialogues elsewhere (Rieser and
Poesio (2009), Rieser (2011), Hahn and Rieser (2011),
Lu?ecking et al (2012)).
4This is a shortened version of the full gesture-AVM. Fea-
tures like hand shape etc. are ignored. See Rieser (2010) for
other annotation predicates.
271
?xy1y2y3y4(TRAJECTORY0(x) ? BENT(y1) ?
BENT(y2)? BENT(y3)? BENT(y4)?y1 < y2 < y3
< y4 ? SEGMENT(y1, x) ? SEGMENT(y2, x) ?
SEGMENT(y3, x) ? SEGMENT(y4, x)). (2)
The paraphrase is: There exists a TRAJECTORY0
x which consists of four BENT SEGMENTS y1, y2,
y3, y4. We abbreviate this formula to:
?x1(TRAJECTORY1(x1)) (3)
In more mundane verbiage: There is a particu-
lar TRAJECTORY1 x1. In a speech-gesture inter-
face5 (Rieser, 2010) both formulae are extended
by adding a parameter in order to compositionally
combine them:
?Y.?eyF 3/4x (WALK-AROUND(e) ?
AGENT(e, FO) ? THEME(e, x)
? F(x, y) ? Y(y)) (4)
We read this as: There is a WALK-AROUND event
e the AGENT of which is FO related to a three
quarters (path) F. This maps x onto y which is in
turn equipped with property Y.
?z.?x1(TRAJECTORY1(x1) ? x1 = z) (5)
This means ?There is a TRAJECTORY1 x1 identical
with an arbitrary z?. The extensions (4) and (5) are
based on the intuition that the preferred reading is
a modification of the (path) F by the gesture.
Taking the gesture representation as an argu-
ment for the speech representation, we finally get
a simplified multi-modal interface formula. The
resulting proposition represents an AGENT (FO)
who is engaged in a WALK-AROUND event e and a
THEME that now is specified as being related to
a bent trajectory of four arcs due to formula (2):
?ey 3/4x ?F(WALK-AROUND(e)?
AGENT(e, FO) ? THEME(e, x) ? F(x, y)
? TRAJECTORY1(y)) (6)
We take this to mean ?There is an AGENT FO?s
WALK-AROUND event e related to a three quarters
(path) F having a TRAJECTORY1 y?.
As a result, the set of models in which the
orginal speech proposition is true is restricted to
5How our model deals with interfacing speech meaning
and gesture meaning has been elaborated in a series of papers
(see footnote 3). We are well aware of the work on gesture-
speech integration by Lascarides and colleagues which we
deal with in a paper on interfaces (Rieser (2013)).
the set of models that contain a bent trajectory
standing in relation to the (path) F. But this restric-
tion is too weak. Intuitively, the gesture conveys
the meaning of a horizontal circular trajectory and
not just four bent arcs. To see the shortcoming,
note that the set of models also includes models
which include a path having four bends that do not
form a circular trajectory.
We envisage two methods to get the appropri-
ate circularity intuition: pragmatic enrichment and
an improvement of our gesture datum to capture
the additional information conveyed in the ges-
ture: By pragmatic enrichment, on the one hand,
horizontal orientation and circularity of the ges-
ture trajectory are inferred using abduction or de-
faults. However, the drawback of working with
defaults or abduction rules is that we would have
to set up too many of them depending on the vari-
ous shapes and functions of bent trajectories.
On the other hand, the datum can be improved
to yield a circularly shaped trajectory instead of
the weaker one consisting of four bent arcs. Our
motion capture data supports the second method:
The motion capture data allows us to compute the
complete trajectory drawn in the gesture space.
This will be the basis for producing a mapping
from gesture parameters to qualitative relations
which we need in the truth conditions. In the end,
we achieve a circular trajectory that is defined as
one approximating a circle, see Section 4.3.
In this mapping procedure resides an under-
specification, which is treated by fixing a thresh-
old for the application of qualitative predicates
through raters? decisions. This threshold value
will be used in giving truth conditions for, e.g.,
(11), especially for determining APPROXIMATE.
We prefer the second method since it captures
our hypothesis that the gesture as a sign conveys
the meaning circular trajectory. The gain of the
automated annotation via mocap which we will
see subsequently is an improvement of our orig-
inal gesture datum to a more empirically founded
one. As a consequence, the set of models that sat-
isfy our multi-modal proposition can be specified.
This is also the reason for explicitly focussing on
gesture semantics in this paper.
3 FA3ME - Automatic Annotation as
Complex Event Processing
The creation of FA3ME, our Framework for the
Automatic Annotation and Augmentation of Multi-
272
modal Events, is inter alia motivated by our key
insight from previous studies that human raters
have extreme difficulties when annotating 3D ges-
ture poses and trajectories. This is especially
true when they only have a restricted view on the
recorded gestures. A typical example is the restric-
tion to a fixed number of different camera angles
from which the gestures have been recorded. In
previous work (Pfeiffer, 2011), we proposed a so-
lution for the restricted camera perspectives based
on mocap data: Our Interactive Augmented Data
Explorer (IADE) allowed human raters to immerse
into the recorded data via virtual reality technol-
ogy. Using a 3D projection in a CAVE (Cruz-
Neira et al, 1992), the raters were enabled to
move freely around and through the recorded mo-
cap data, including a 3D reconstruction of the ex-
perimental setting. This interactive 3D visuali-
sation supported an advanced annotation process
and improved the quality of the annotations but
at high costs. Since then, we only know of Kipp
(2010) who makes mocap data visible for anno-
tators by presenting feature graphs in his annota-
tion tool Anvil in a desktop-based setting. In later
work, Nguyen and Kipp (2010) also support a 3D
model of the speaker, but this needed to be hand-
crafted by human annotators. A more holistic ap-
proach for gesture visualizations are the Gesture
Space Volumes Pfeiffer (2011), which summarize
gesture trajectories over longer timespans or mul-
tiple speakers.
The IADE system also allowed us to add visual
augmentations during the playback of the recorded
data. These augmentations were based on the
events from the mocap data, but aggregated sev-
eral events to higher-level representations. In a
study on pointing gestures (Lu?cking et al, 2013),
we could test different hypotheses about the con-
struction of the direction of pointing by adding
visual pointing rays shooting in a 3D reconstruc-
tion of the original real world setting. This al-
lowed us to asses the accuracy of pointing at a
very high level in a data-driven manner and derive
a new model for the direction of pointing (Pfeiffer,
2011).
3.1 Principles in FA3ME
In the FA3ME project, we iteratively refine our
methods for analysing multi-modal events. As a
central concept, FA3ME considers any recorded
datum as a first-level multi-modal event (see Fig-
ure 3, left). This can be a time-stamped frame
from a video camera, an audio sample, 6-degree-
of-freedom matrices from a mocap system or gaze
information from an eye-tracking system (e.g., see
Kousidis et al (2012)).
A distinctive factor of FA3ME is that we con-
sider annotations as second-level multi-modal
events. That is, recorded and annotated data
share the same representation. Annotations can be
added by both, human raters and classification al-
gorithms (the event rules in Figure 3, middle). An-
notations can themselves be target of annotations.
This allows us, for example, to create automatic
classifiers that rely on recorded data and manual
annotations (e.g., the first yellow event in Figure 3
depends on first-level events above and the blue
second-level event to the right). This is helpful
when classifiers for complex events are not (yet)
available. If, for instance, no automatic classifiers
for the stroke of a gesture exists, these annotations
can be made by human raters. Once this is done,
the automatic classifiers can describe the move-
ments during the meaningful phases by analysing
the trajectories of the mocap data.
Third-level multi-modal events are augmenta-
tions or extrapolations of the data. They might
represent hypotheses, such as in the example of
different pointing rays given above.
3.2 Complex Event Processing
In FA3ME, we consider the analysis of multi-
modal events as a complex event processing (CEP)
problem. CEP is an area of computer science ded-
icated to the timely detection, analysis, aggrega-
tion and processing of events (Luckham, 2002). In
the past years, CEP has gained an increased atten-
tion especially in the analysis of business relevant
processes where large amount of data, e.g., share
prices, with high update rates are analysed. This
has fostered many interesting tools and frame-
works for the analysis of structured events (Arasu
et al, 2004a; EsperTech, 2013; Gedik et al, 2008;
StreamBase, 2013). Hirte et al (2012) apply
CEP to a motion tracking stream from a Microsoft
Kinect for real-time interaction, but we know of
no uses of CEP for the processing of multi-modal
event streams for linguistic analysis.
Dedicated query languages have been devel-
oped by several CEP frameworks which allow us
to specify our event aggregations descriptively at
a high level of abstraction (Arasu et al, 2004b;
273
Figure 3: In FA3ME, incoming multi-modal events are handled by a complex event processing frame-
work that matches and aggregates events based on time windows to compose 2nd level multi-modal
events. All multi-modal events can then be mapped to tiers in an annotation tool.
Gedik et al, 2008). The framework we use for
FA3ME is Esper (EsperTech, 2013), which pro-
vides a SQL-like query language. As a central ex-
tension of SQL, CEP query languages introduce
the concept of event streams and time windows as
a basis for aggregation (see Figure 3).
The CEP approach of FA3ME allows us to cre-
ate second- and third-level multi-modal events on-
the-fly. We can thus provide near real-time anno-
tations of sensor events. However, we have to con-
sider the latencies introduced by sensors or com-
putations and back-date events accordingly.
As a practical result, once we have specified our
annotation descriptions formally in the language
of CEP, these descriptions can be used to create
classifiers that operate both on pre-recorded multi-
modal corpora and on real-time data. This makes
CEP interesting for projects where research in Lin-
guistics and Human-Computer Interaction meet.
4 From MoCap to Linguistic Models
In this section, we will now address the problem
of annotating the circular trajectory. In order to
get the preferred semantics we yet cannot rely ex-
clusively on the automatic annotation. We need
the qualitative predicate ?phase? to identify the
meaningful part of the gesture (the stroke). Addi-
tionally, the qualitative predicate ?representation
technique? is required to select the relevant mo-
cap trackers. For instance, the representation tech-
nique ?drawing? selects the marker of the tip of
the index finger. We thus require a hybrid model
of manual and automatic annotations. In the fol-
lowing, we will focus on the automatic annotation.
First of all, when using mocap to record data,
a frame of reference has to be specified as a ba-
Figure 4: The coordinate system of the speaker
(left). The orientations of the palms are classified
into eight main directions (right).
sis for all coordinate systems. We chose a person-
centered frame of reference anchored in the solar
plexus (see Figure 4). The coronal plane is de-
fined by the solar plexus and the two shoulders.
The transverse plane is also defined by the solar
plexus, perpendicular to the coronal plane with a
normal-vector from solar plexus to the point ST
(see Figure 4) between the two shoulders.
4.1 Basic Automatic Gesture Annotations
The analysis of mocap data allows us to create ba-
sic annotations that we use in our corpora on-the-
fly. This speeds up the annotation process and lets
human raters focus on more complex aspects. One
basic annotation that can be achieved automati-
cally is the classification of the position of gestur-
ing hands according to the gesture space model of
McNeill (1992). As his annotation schema (see
Figure 5, right) is tailored for the annotation of
274
Figure 5: Our extended gesture space categorisa-
tion (upper left) is based on the work of McNeill
(lower right).
video frames, we extended this model to support
mocap as well (see Figure 5, left). The important
point is that the areas of our schema are derived
from certain markers attached to the observed par-
ticipant. The upper right corner of the area C-UR
(Center-Upper Right), for example, is linked to the
marker for the right shoulder. Our schema thus
scales directly with the size of the participant. Be-
sides this, the sub-millimeter resolution of the mo-
cap system also allows us to have a more detailed
structure of the center area. The schema is also
oriented according to the current coronal plane of
the participant and not, e.g., according to the per-
spective of the camera.
A second example is the classification of the ori-
entation of the hand, which is classified according
to the scheme depicted in Figure 4, right. This
classification is made relative to the transversal
plane of the speaker?s body.
4.2 Example: The Circular Trajectory
For the detection and classification of gestures
drawing shapes two types of multi-modal events
are required. First, multi-modal events generated
by the mocap system for the hand. These events
contain matrices describing the position and ori-
entation of the back of the hand. Second, multi-
modal events that mark the gesture stroke (one
event for the start and one for the end) have to be
generated, either by hand or automatically. At the
moment, we rely on our manual annotations for
the existing SaGA corpus.
We realise the annotation of circular trajecto-
ries in two steps. First, we reduce the trajectory
provided by the mocap system to two dimensions.
Second, we determine how closely the 2D trajec-
tory approximates a circle.
Projection of the 3D Trajectory
The classifier for circles collects all events for the
hand that happened between the two events for the
stroke. As noted above, these events represent the
position and orientation of the hand in 3D-space.
There are several alternatives to reduce these three
dimensions to two for classifying a circle (a 3D
Object matching a 2D circle would be a sphere, a
circular trajectory through all 3 dimensions a spi-
ral). The principal approach is to reduce the di-
mensions by projecting the events on a 2D plane.
?xy (TRAJECTORY(x)? PROJECTION- OF(x, y)
? TRAJECTORY2D(y)) (7)
Which plane to chose depends on the choice
made for the annotation (e.g., global for the cor-
pus) and thus on the context. For the description
of gestures in dialogue there are several plausible
alternatives. First, the movements could be pro-
jected onto one of the three body planes (sagit-
tal plane, coronal plane, transversal plane). In our
context, the transversal plane is suitable, as we are
dealing with descriptions of routes, which in our
corpus are made either with respect to the body
of the speaker or with respect to the plane of an
imaginary map, both extend parallel to the floor.
Figure 6 (upper left) shows the circular movement
in the transversal plane. A different perspective
is presented in Figure 6 (right). There the perspec-
tive of a bystander is chosen. This kind of perspec-
tive can be useful for describing what the recipient
of a dialogue act perceives, e.g., to explain misun-
derstandings. For this purpose, the gesture could
also be annotated twice, once from the speaker?s
and once from the recipient?s perspective.
At this point we want to emphasise that posi-
tion and orientation of the planes do not have to be
static. They can be linked to the reference points
provided by the mocap system. Thus when the
speaker turns her body, the sagittal, coronal and
275
Figure 6: The circle-like gesture from our exam-
ple can be visualised based on the mocap data. The
right side shows the visualisation from the per-
spective of an interlocutor, the visualisation in the
upper left corner is a projection of the movement
on the transversal plane of the speaker.
transversal planes will move accordingly and the
gestures are always interpreted according to the
current orientation.
The plane used for projection can also be de-
rived from the gesture itself. Using principal com-
ponent analysis, the two main axes used by the
gesture can be identified. These axes can then have
arbitrary orientations. This could be a useful ap-
proach whenever 3D objects are described and the
correct position and orientation of the ideal circle
has to be derived from the gesture.
Circle Detection
Once the gesture trajectory has been projected
onto a 2D plane, the resulting coordinates are clas-
sified. For this, several sketch-recognition algo-
rithms have been proposed (e.g., (Alvarado and
Davis, 2004; Rubine, 1991)). These algorithms
have been designed for sketch-based interfaces
(such as tablets or digitisers), either for recognis-
ing commands or for prettifying hand-drawn dia-
grams. However, once the 3D trajectory has been
mapped to 2D, they can also be applied to natural
gestures. The individual sketch-recognition algo-
rithms differ in the way they are approaching the
classification problem. Many algorithms follow a
feature-based approach in which the primitives to
be recognised are described by a set of features
(such as aspect ratio or ratio of covered area) (Ru-
bine, 1991). This approach is especially suited,
when new primitives are to be learned by example.
An alternative approach is the model-based ap-
proach in which the primitives to be recognised are
described based on geometric models (Alvarado
and Davis, 2004; Hammond and Davis, 2006).
Some hybrid approaches also exist (Paulson et al,
2008). The model-based approaches are in line
with our declarative approach to modelling, and
are thus our preferred way for classifying shapes.
In our case, the projected 2D trajectory
of the gesture is thus classified by a model-
based sketch-recognition algorithm, which clas-
sifies the input into one of several shape classes
(circle, rectangle, ...) with a correspond-
ing member function ISSHAPE(y, CIRCLE) ?
[0 . . . 1]. By this, we can satisfy a subformula
APPROXIMATES(y, z) ? CIRCLE(z) by pre-setting
a certain threshold. The threshold has to be cho-
sen by the annotators, e.g., by rating positive and
negative examples, as it may vary between partic-
ipants and express the sloppiness of their gestures.
4.3 From MoCap to a Revision of Semantics
The result of the FA3ME reconstruction of our
gesture example can be expressed as follows:
?xyz (TRAJECTORY(x)
? PROJECTION- OF(x, y) ? TRAJECTORY2D(y)
? APPROXIMATES(y, z) ? CIRCLE(z)) (8)
So we have: There is a projection of TRAJEC-
TORY x, TRAJECTORY2D y, which is approximat-
ing a circle. We can now provide a description of
the domain which can satisfy formula (8). Conse-
quently, formula (8) is enhanced by definition (9).
CIRCULAR TRAJECTORY(x) =DEF
?yz(TRAJECTORY2(x)? PROJECTION- OF(x, y)?
APPROXIMATES(y, z) ? circle(z)) (9)
This definition reads as ?a CIRCULAR TRAJEC-
TORY x is a TRAJECTORY2 which has a PROJEC-
TION y that approximates some circle z?.
The formula (9) substitutes the TRAJECTORY1
notion. The improved multi-modal meaning is
(10):
?ey 3/4x ?F(WALK-AROUND(e)?
AGENT(e, FO) ? THEME(e, x) ? F(x, y)
? CIRCULAR TRAJECTORY(y)) (10)
Interfacing the new gesture representation with
the speech representation captures our intuition
that the gesture reduces the original set of mod-
els to a set including a circular-shaped trajectory.
276
Speech
Semantics
Gesture
Semantics
Linguistic Model
Classification
of Real World
Events
FA
3
ME
Preferred Models
Figure 7: Specification of gesture semantics due
to results of classification in FA3ME. Simulation
data feed into the gesture semantics which inter-
faces with the speech semantics.
The division of labour between linguistic seman-
tics and FA3ME technology regarding the seman-
tic reconstruction is represented in Figure 7.
By way of explanation: We have the multi-
modal semantics integrating speech semantics and
gesture semantics accomplished via ?-calculus
techniques as shown in Section 2. As also ex-
plained there, it alone would be too weak to de-
limit the models preferred with respect to the
gesture indicating roundness. Therefore FA3ME
technology leading to a definition of CIRCU-
LAR TRAJECTORY is used which reduces the set
of models to the preferred ones assuming a thresh-
old n for the gestures closeness of fit to a circle.
Thus, the relation between some gesture parame-
ters and qualitative relations like circular can be
considered as a mapping, producing values in the
range [0 . . . 1]. Still, it could happen that formula
(8) cannot be satisfied in the preferred models. As
a consequence, the multi-modal meaning would
then fall short of satisfaction.
5 Conclusion
During our work on the interface between speech
and gesture meaning our previous annotations
turned out to be insufficient to support the seman-
tics of concepts such as CIRCULAR TRAJECTORY.
This concept is a representative of many others
that for human annotators are difficult to rate with
the rigidity required for the symbolic level of se-
mantics. Scientific visualisations, such as depicted
in Figure 6, can be created to support the human
raters. However, there is still the problem of per-
spective distortions three dimensional gestures are
subject to when viewed from different angles and
in particular when viewed on a 2D screen. It is
also difficult to follow the complete trajectory of
such gestures over time. Thus, one and the same
gesture can be rated differently depending on the
rater, while an algorithm with a defined threshold
is not subject to these problems.
The presented hybrid approach based on quali-
tative human annotations, mocap and our FA3ME
framework is able to classify the particular 2D tra-
jectories we are interested in following a three-
step process: After the human annotator identi-
fied the phase and selected relevant trackers, the
dimensions are reduced to two and a rigid model-
based sketch-recognition algorithm is used to clas-
sify the trajectories. This classification is re-
peatable, consistent and independent of perspec-
tive. A first comparison of the manually anno-
tated data and the automatic annotations revealed
a high match. All differences between the annota-
tions can be explained by restrictions of the video
data which yielded a lower precision in the hu-
man annotations specifying the slant of the hand.
Thus, the main issues we had with the results
of human raters have been addressed, however a
more formal evaluation on a large corpus remains
to be done. What also remains is a specification
of membership functions for each kind of ges-
ture trajectories of interest (e.g., circular, rectan-
gular, etc.). For this, a formal specification of what
we commonly mean by, for instance, CIRCULAR,
RECTANGULAR etc. is required.
The automated annotation via mocap im-
proves our original gesture datum to capture the
circularity-information conveyed in the gesture.
We have a better understanding of the gesture
meaning adopted vis-a`-vis the datum considered.
As it turns out, resorting to pragmatic inference
cannot be avoided entirely, but we will exclude
a lot of unwarranted readings which the manual-
based logical formulae would still allow by us-
ing the approximation provided by body tracking
methods. Not presented here is the way third-level
multi-modal events are generated by re-simulating
the data in a 3D world model to generate context
events, e.g., to support pragmatics.
Acknowledgments
This work has been funded by the Deutsche
Forschungsgemeinschaft (DFG) in the Collabora-
tive Research Center 673, Alignment in Communi-
cation. We are grateful to three reviewers whose
arguments we took up in this version.
277
References
[advanced realtime tracking GmbH2013] A.R.T.
advanced realtime tracking GmbH. 2013.
Homepage. Retrieved May 2013 from
http://www.ar-tracking.de.
[Alvarado and Davis2004] Christine Alvarado and
Randall Davis. 2004. SketchREAD: a multi-
domain sketch recognition engine. In Proceedings
of the 17th annual ACM symposium on User
interface software and technology, UIST ?04, pages
23?32, New York, NY, USA. ACM.
[Arasu et al2004a] Arvind Arasu, Brian Babcock,
Shivnath Babu, John Cieslewicz, Mayur Datar,
Keith Ito, Rajeev Motwani, Utkarsh Srivastava, and
Jennifer Widom. 2004a. Stream: The stanford data
stream management system. Technical report, Stan-
ford InfoLab.
[Arasu et al2004b] Arvind Arasu, Shivnath Babu, and
Jennifer Widom. 2004b. CQL: A language for
continuous queries over streams and relations. In
Database Programming Languages, pages 1?19.
Springer.
[Cruz-Neira et al1992] Carolina Cruz-Neira, Daniel J.
Sandin, Thomas A. DeFanti, Robert V. Kenyon, and
John C. Hart. 1992. The cave: audio visual expe-
rience automatic virtual environment. Communica-
tions fo the ACM 35 (2), 35(6):64?72.
[EsperTech2013] EsperTech. 2013. Homepage of Es-
per. Retrieved May 2013 from http://esper.
codehaus.org/.
[Gedik et al2008] Bugra Gedik, Henrique Andrade,
Kun-Lung Wu, Philip S Yu, and Myungcheol Doo.
2008. SPADE: The System S Declarative Stream
Processing Engine. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1123?1134. ACM.
[Hahn and Rieser2011] Florian Hahn and Hannes
Rieser. 2011. Gestures supporting dialogue struc-
ture and interaction in the Bielefeld speech and
gesture alignment corpus (SaGA). In Proceedings
of SEMdial 2011, Los Angelogue, 15th Workshop on
the Semantics and Pragmatics of Dialogue, pages
182?183, Los Angeles, California.
[Hammond and Davis2006] Tracy Hammond and Ran-
dall Davis. 2006. LADDER: A language to describe
drawing, display, and editing in sketch recognition.
In ACM SIGGRAPH 2006 Courses, page 27. ACM.
[Hirte et al2012] Steffen Hirte, Andreas Seifert,
Stephan Baumann, Daniel Klan, and Kai-Uwe
Sattler. 2012. Data3 ? a kinect interface for OLAP
using complex event processing. Data Engineering,
International Conference on, 0:1297?1300.
[Kipp2010] Michael Kipp. 2010. Multimedia annota-
tion, querying and analysis in anvil. Multimedia in-
formation extraction, 19.
[Kousidis et al2012] Spyridon Kousidis, Thies Pfeiffer,
Zofia Malisz, Petra Wagner, and David Schlangen.
2012. Evaluating a minimally invasive labora-
tory architecture for recording multimodal conversa-
tional data. In Proceedings of the Interdisciplinary
Workshop on Feedback Behaviors in Dialog, IN-
TERSPEECH2012 Satellite Workshop, pages 39?42.
[Luckham2002] David Luckham. 2002. The Power
of Events: An Introduction to Complex Event Pro-
cessing in Distributed Enterprise Systems. Addison-
Wesley Professional.
[Lu?cking et al2012] Andy Lu?cking, Kirsten Bergman,
Florian Hahn, Stefan Kopp, and Hannes Rieser.
2012. Data-based analysis of speech and gesture:
the Bielefeld Speech and Gesture Alignment corpus
(SaGA) and its applications. Journal on Multimodal
User Interfaces, -:1?14.
[Lu?cking et al2013] Andy Lu?cking, Thies Pfeiffer, and
Hannes Rieser. 2013. Pointing and reference recon-
sidered. International Journal of Corpus Linguis-
tics. to appear.
[McNeill1992] David McNeill. 1992. Hand and Mind:
What Gestures Reveal about Thought. University of
Chicago Press, Chicago.
[Microsoft2013] Microsoft. 2013. Homepage of
KINECT for Windows. Retrieved May 2013
from http://www.microsoft.com/en-us/
kinectforwindows/develop/.
[Nguyen and Kipp2010] Quan Nguyen and Michael
Kipp. 2010. Annotation of human gesture using
3d skeleton controls. In Proceedings of the 7th In-
ternational Conference on Language Resources and
Evaluation. ELDA.
[Paulson et al2008] Brandon Paulson, Pankaj Rajan,
Pedro Davalos, Ricardo Gutierrez-Osuna, and Tracy
Hammond. 2008. What!?! no Rubine features?: us-
ing geometric-based features to produce normalized
confidence values for sketch recognition. In HCC
Workshop: Sketch Tools for Diagramming, pages
57?63.
[Pfeiffer2011] Thies Pfeiffer. 2011. Understanding
Multimodal Deixis with Gaze and Gesture in Con-
versational Interfaces. Berichte aus der Informatik.
Shaker Verlag, Aachen, Germany, December.
[Pfeiffer2013] Thies Pfeiffer. 2013. Documentation
with motion capture. In Cornelia Mu?ller, Alan
Cienki, Ellen Fricke, Silva H. Ladewig, David
McNeill, and Sedinha Teendorf, editors, Body-
Language-Communication: An International Hand-
book on Multimodality in Human Interaction, Hand-
books of Linguistics and Communication Science.
Mouton de Gruyter, Berlin, New York. to appear.
[Rieser and Poesio2009] Hannes Rieser and M. Poe-
sio. 2009. Interactive Gesture in Dialogue: a
PTT Model. In P. Healey, R. Pieraccini, D. Byron,
S. Yound, and M. Purver, editors, Proceedings of the
SIGDIAL 2009 Conference, pages 87?96.
278
[Rieser2010] Hannes Rieser. 2010. On factoring
out a gesture typology from the Bielefeld Speech-
And-Gesture-Alignment corpus (SAGA). In Ste-
fan Kopp and Ipke Wachsmuth, editors, Proceed-
ings of GW 2009: Gesture in Embodied Communi-
cation and Human-Computer Interaction, pages 47?
60, Berlin/Heidelberg. Springer.
[Rieser2011] Hannes Rieser. 2011. Gestures indicat-
ing dialogue structure. In Proceedings of SEMdial
2011, Los Angelogue, 15th Workshop on the Seman-
tics and Pragmatics of Dialogue, pages 9?18, Los
Angeles, California.
[Rieser2013] Hannes Rieser. 2013. Speech-gesture
Interfaces. An Overview. In Heike Wiese and
Malte Zimmermann, editors, Proceedings of 35th
Annual Conference of the German Linguistic Society
(DGfS), March 12-15 2013 in Potsdam, pages 282?
283.
[Ro?pke et al2013] Insa Ro?pke, Florian Hahn, and
Hannes Rieser. 2013. Interface Constructions for
Gestures Accompanying Verb Phrases. In Heike
Wiese and Malte Zimmermann, editors, Proceed-
ings of 35th Annual Conference of the German Lin-
guistic Society (DGfS), March 12-15 2013 in Pots-
dam, pages 295?296.
[Rubine1991] Dean Rubine. 1991. Specifying gestures
by example. In Proceedings of the 18th annual con-
ference on Computer graphics and interactive tech-
niques, SIGGRAPH ?91, pages 329?337, New York,
NY, USA. ACM.
[StreamBase2013] StreamBase. 2013. Homepage of
StreamBase. Retrieved May 2013 from http://
www.streambase.com/.
[Vicon Motion Systems2013] Vicon Motion Systems.
2013. Homepage. Retrieved May 2013 from
http://www.vicon.com.
279
