Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 73?81,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Automatic Detection of Stable Grammatical Features in N-Grams
Mikhail Kopotev1 Lidia Pivovarova1,2 Natalia Kochetkova3 Roman Yangarber1
1 University of Helsinki, Finland
2 St.Petersburg State University, Russia
3 Moscow Institute of Electronics and Mathematics, NRU HSE, Russia
Abstract
This paper presents an algorithm that allows
the user to issue a query pattern, collects
multi-word expressions (MWEs) that match
the pattern, and then ranks them in a uniform
fashion. This is achieved by quantifying the
strength of all possible relations between the
tokens and their features in the MWEs. The al-
gorithm collects the frequency of morphologi-
cal categories of the given pattern on a unified
scale in order to choose the stable categories
and their values. For every part of speech, and
for all of its categories, we calculate a normal-
ized Kullback-Leibler divergence between the
category?s distribution in the pattern and its
distribution in the corpus overall. Categories
with the largest divergence are considered to
be the most significant. The particular values
of the categories are sorted according to a fre-
quency ratio. As a result, we obtain morpho-
syntactic profiles of a given pattern, which in-
cludes the most stable category of the pattern,
and their values.
1 Introduction
In n-grams, the relations among words and among
their grammatical categories cover a wide spectrum,
ranging from idioms to syntactic units, such as a
verb phrase. In most cases, the words are linked to-
gether by both grammatical and lexical relations. It
is difficult to decide, which relation is stronger in
each particular case. For example, in the idiomatic
phrase meet the eye, the relationship is lexical rather
than grammatical. A phrasal verb meet up is similar
to single-word verbs and has its own meaning. It can
be interpreted as one lexeme, spelled as two words.
On the other hand, phrases like meet the require-
ments, meet the specifications, meet the demands
are traditionally called ?collocations.? However, the
question arises about the role played by the noun fol-
lowing the verb: is it a lexically free direct object,
or a part of stable lexical unit, or to some extend
both? These words are bound by both grammatical
and lexical relations, and we assume that the major-
ity of word combinations in any language have such
a dual nature.
Lastly, the relationship between the words in the
English phrase meet her differs from those above in
that it may be described as purely grammatical?the
verb meet receives a direct object.
Distinguishing collocations, i.e. ?co-occurrences
of words? from colligations, i.e. ?co-occurrence of
word forms with grammatical phenomena? (Gries
and Divjak, 2009) is not always a simple task; there
is no clear boundary between various types of word
combinations inasmuch as they can be simultane-
ously a collocation and a colligation?this type of
MWE is called collostructions in (Stefanowitsch and
Gries, 2003). It was proposed that language as such
is a ?constructicon? (Goldberg, 2006), which means
that fusion is its core nature. For this reason, devis-
ing formal methods to measure the strength of mor-
phological or lexical relations between words be-
comes a challenge.
Our approach aims to treat multi-word expres-
sions (MWEs) of various nature?idioms, multi-
word lexemes, collocations and colligations?on an
equal basis, and to compare the strength of vari-
ous possible relations between the tokens in a MWE
quantitatively. We search for ?the underlying cause?
73
for the frequent co-occurrence of certain words:
whether it is due to their morphological categories,
or lexical compatibility, or a combination of both. In
this paper, however, we focus on colligations, ignor-
ing collocations and collostructions.
For languages with rich morphology the situation
is more complicated, because each word may have
several morphological categories that are not inde-
pendent and interact with each other. This paper fo-
cuses on Russian, which not only has free word or-
der and rich morphology,1 but is also a language that
is well-investigated. A good number of corpora and
reference grammars are available to be used for eval-
uation. The data we use in this work is the n-gram
corpus, extracted from a deeply annotated and care-
fully disambiguated (partly manually) sub-corpus of
the Russian National Corpus (RNC). The size of dis-
ambiguated corpus used in this paper is 5 944 188
words of running text.
2 Related Work
Much effort has been invested in automatic extrac-
tion of MWEs from text. A great variety of method
are used, depending on the data, the particular tasks
and the types of MWEs to be extracted. Pecina
(2005) surveys 87 statistical measures and meth-
ods, and even that is not a complete list. The
most frequently used metrics, inter alia, are Mu-
tual Information (MI), (Church and Hanks, 1990), t-
score (Church et al, 1991), and log-likelihood (Dun-
ning, 1993). The common disadvantage of these is
their dependency on the number of words included
in the MWE. Although there is a large number of
papers that use MI for bigram extraction, only a few
use the MI measure for three or more collocates,
e.g., (Tadic? and S?ojat, 2003; Wermter and Hahn,
2006; Kilgarriff et al, 2012),
Frantzi et al (2000) introduced the c-value and
nc-value measures to extract terms of different
lengths. Daudaravicius (2010) has developed a
promising method that recognizes collocations in
text. Rather than extracting MWEs, this method cuts
the text into a sequence of MWEs of length from
1 to 7 words; the algorithm may produce different
1The Multitext-East specification, which aims to create an
unified cross-language annotation scheme, defines 156 morpho-
syntactic tags for Russian as compared to 80 tags for English
(http://nl.ijs.si/ME/V4/msd/html).
chunking for the same segment of text within dif-
ferent corpora. Nevertheless, extraction of variable-
length MWE is a challenging task; the majority of
papers in the field still use measures that take the
number of collocates as a core parameter.
Entropy and other probabilistic measures have
been used for MWE extraction since the earliest
work. For example, the main idea in (Shimohata et
al., 1997; Resnik, 1997), is that the MWE?s idiosyn-
crasy, (Sag et al, 2002), is reflected in the distribu-
tions of the collocates. Ramisch et al (2008) intro-
duced the Entropy of Permutation and Insertion:
EPI = ?
m?
a=0
p(ngrama) log[p(ngrama)] (1)
where ngram0 is the original MWE, and ngrama
are its syntactically acceptable permutations.
Kullback-Leibler divergence was proposed
by Resnik (1997) to measure selective prefer-
ence for the word sense disambiguation (WSD)
task. Fazly and Stevenson (2007) applied a set of
statistical measures to classify verb+noun MWEs
and used Kullback-Leibler divergence, among other
methods, to measure the syntactic cohesion of a
word combination. Van de Cruys and Moiro?n
(2007) used normalized Kullback-Leibler diver-
gence to find idiomatic expression with verbs in
Dutch.
Russian MWE-studies have emerged over the last
decade. Khokhlova and Zakharov (2009) applied
MI, t-score and log-likelihood to extract verb collo-
cations; Yagunova and Pivovarova (2010) studied
the difference between Russian lemma/token col-
locations and also between various genres; Do-
brov and Loukachevitch (2011) implemented term
extraction algorithms. However, there is a lack of
study of both colligations and collostructions in Rus-
sian. The only work known to us is by Sharoff
(2004), who applied the MI-score to extract prepo-
sitional phrases; however, the only category he used
was the POS.
As far as we aware, the algorithm we present in
this paper has not been applied to Russian or to other
languages.
3 Method
The input for our system is any n-gram of length 2?
4, where one position is a gap?the algorithm aims
74
Figure 1: Distributions of noun cases in the corpus and in
a sample?following the preposition ??? (in)
Figure 2: Distributions of nominal gender in the corpus
and in a sample?following the preposition ??? (in)
to find the most stable morphological categories of
words that can fill this gap. Moreover, the user can
specify the particular properties of words that can fill
the gap?for example, specify that the output should
include only plural nouns. Thus, the combination of
the surrounding words and morphological constrains
form an initial query pattern for the algorithm.
Our model tries to capture the difference between
distributions of linguistic features in the general cor-
pus as compared to distributions within the given
pattern. For example, Figure 1 shows the distribu-
tion of cases in the corpus overall vs. their distribu-
tion in words following the preposition ??? (in/into).
Figure 2 shows the corresponding distributions of
gender. Gender is distributed similarly in the cor-
pus and in the sample restricted by the pattern; by
contrast, the distribution of cases is clearly different.
This is due to the fact that the preposition governs
the case of the noun, but has no effect on gender. To
measure this difference between the distributions we
use the Kullback-Leibler divergence:
Div(C) =
N?
i=1
P patterni ? log(
P patterni
P corpusi
) (2)
where C is the morphological category in a
pattern?e.g., case or gender,?having the values
1..N , P patterni is the relative frequency of value i
restricted by the pattern, and P corpusi is the relative
frequency of the same value in the general corpus.
Since the number of possible values for a category is
variable?e.g., eleven for case, four for gender, and
hundreds of thousands for lemmas?the divergence
needs to be normalized. The normalization could
be done in various ways, e.g., against the entropy or
some maximal divergence in the data; in our experi-
ments, the best results were obtained using a variant
proposed in (Bigi, 2003), where the divergence be-
tween the corpus distribution and the uniform distri-
bution is used as the normalizing factor:
NormDiv(C) =
Div(C)
E(C) + log(n)
(3)
where E(C) is the entropy of category C and n is
the number of possible values of C; the term log(n)
is the entropy of the uniform distribution over n out-
comes (which is the maximal entropy). The category
with the highest value of normalized divergence is
seen as maximally preferred by the pattern.
However, divergence is unable to determine the
exact values of the category, and some of these val-
ues are clearly unreliable even if they seem to ap-
pear in the pattern. For example, Figure 1 shows
that preposition ??? (in) in the data is sometimes
followed by the nominative case, which is grammat-
ically impossible. This is due to a certain amount of
noise, which is unavoidable in a large corpus due to
mark-up errors or inherent morphological ambigu-
ity. In Russian, the nominative and accusative cases
often syncretize (assume identical forms), which can
cause inaccuracies in annotation. On the other hand,
some values of a category can be extremely rare;
thus, they will be rare within patterns as well. For
instance, the so-called ?second accusative? case (la-
beled ?acc2? in Figure 1) is rare in modern Russian,
75
which is why its appearance in combination with
preposition ??? (in) is significant, even though its
frequency is not much higher than the frequency of
the (erroneous) nominative case in the same pattern.
To find the significant values of a particular cate-
gory we use the ratio between the frequencies of the
value in a sample and in the corpus:
frequency ratio =
P patterni
P corpusi
(4)
If frequentcy ratio > 1, then the category?s value
is assumed to be selected by the pattern.
Finally, we note that the distribution of POS varies
considerably within every pattern as compared to its
distribution in the corpus. For example, prepositions
can be followed only by noun groups and can never
be followed by verbs or conjunctions. This means
the Kullback-Leibler divergence for any POS, nat-
urally assumes the highest value in any pattern; for
this reason, we exclude the POS category from con-
sideration in our calculation, aiming to find more
subtle and interesting regularities in the data.
To summarize, the algorithm works as follows:
for a given query pattern
1. search all words that appear in the query pattern
and group them according to their POS tags.
2. for every POS, calculate the normalized
Kullback-Leibler divergence for all of its cat-
egories; categories that show the maximum di-
vergence are considered to be the most signifi-
cant for the given pattern;
3. for every relevant category, sort its values ac-
cording to the frequency ratio; if frequency ra-
tio is less than 1, the value considered to be ir-
relevant for this pattern.
4 Experiments
In this paper, we conduct an in-depth evaluation fo-
cusing on a limited number of linguistic phenom-
ena, namely: bigrams beginning with single-token
prepositions, which impose strong morpho-syntactic
constraints in terms of case government. We in-
vestigate 25 prepositions, such as ????? (without),
??? (in/to), etc. We evaluate the corpus of bi-
grams systematically against these queries, although
we expect that the model we propose here pro-
duces relevant results for a much wider range of
constructions?to be confirmed in further work.
4.1 Prepositions and Morphological Category
A syntactic property of prepositions in Russian is
that they govern nominal phrases, i.e., that we expect
the largest normalized divergence in queries such as
{ Preposition + X }, where the POS of X is noun,
to occur exactly with the category of case. Figure 3
shows the normalized divergence for four lexical and
morphological categories. Among them, Case has
the maximal divergence for all prepositions, which
matches our expectation with 100% accuracy.
According to the figure, the morphological cat-
egory of Animacy2 is also interesting, in that it
has a high value for some prepositions, like ???-
???? (from under), ????? (under), ????? (above).
A good example is the preposition ???-???? (from
under). Its semantic properties cause inanimate
nouns to appear much more frequently than ani-
mate ones. Consequently, we observe a higher diver-
gence, due to inanimate nouns like ???-??? ??????
(from under ground), ???-??? ?????? (from under
the snow), etc. Another good example of hidden
semantic properties is a pair of prepositions ?????
(under) and ????? (above). One can expect that
their syntactic behaviour is more or less similar,
but the histogram shows that Animacy (surprisingly)
has a much higher divergence for ????? (under) to
be ignored. Indeed, a deeper corpus-based anal-
ysis reveals a stable, frequently used construction,
which gives many points to animate nouns, e.g.,
???????????????? ??? ???????? (disguised as a
bride). It is notable that this particular effect is not
mentioned in any grammar book, (to the best of our
knowledge).
To conclude, the Case category is the clear win-
ner in terms of having the greatest normalized di-
vergence, and the output fully matches the expecta-
tion on all 25 common prepositions that we tested.
Other results are also clearly interesting due to their
links to semantic properties, that is, to colloca-
tions. The next task is, therefore to discriminate
2Animacy is a morphological category of Russian nouns
based on whether the referent of the noun is considered sen-
tient or living. Most nouns denoting humans and animals are
animate, while the majority of other nouns are inanimate.
76
Figure 3: Normalized divergence of noun categories (grammemes) for pattern preposition+X.
between the runners-up, like Animacy for ?????
(under), which seem to be interesting to some ex-
tent, and clear losers like Gender, in the example
above. To do that we need to find an appropriate
threshold?preferably automatically?between rel-
evant and non-relevant results. The algorithm ranks
the categories according to their divergence; the cat-
egory that has the top rank is certainly meaning-
ful. The question is how to determine which among
the rest are significant as well; this is left for future
work.
4.2 Specific Values of the Category with
Maximum Divergence
The next question we explore is which particular
values of the maximally divergent category?here,
Case?are selected by a given preposition. As we
mentioned above, we use the frequency ratio for this
task. We collected a list of cases3 that appear af-
ter the given preposition, according to the algorithm
with frequency ratio > 1; which cases are pos-
sible according to grammatical descriptions,4 which
3The current annotation scheme of our data has eleven case
tags, namely: nom, voc, gen, gen2, dat, acc, acc2, ins, loc, loc2,
adnum.
4Note, that not all possible prep+case combinations are rep-
resented in the corpus; for example, the combination { ??????
(for the sake of) + gen2 } does not appear in our data, and only
eight times in the RNC overall. For evaluation we take into
cases were produced by the algorithm, and the num-
ber of correct cases in the system?s response. We
expect that by using the frequency ratio we can re-
duce the noise; for example, of the eight cases that
match the pattern { ?c? (with) + Noun } only four
are relevant.
The algorithm predicts the correct relevant set for
21 of 25 prepositions, giving a total precision of
95%, recall of 89%, and F-measure of 92%. The
prepositions highlighted in bold in Table 1 are those
that were incorrectly processed for various reasons;
the error analysis is presented below.
14: ??? (about) The algorithm unexpectedly flags
the voc (vocative) as a possible case after this prepo-
sition. This is incorrect; checking the data we dis-
covered that this mistake was due to erroneous an-
notation: the interjection ?o? (oh), as in ?O ????!?
(Oh God!), is incorrectly annotated as the preposi-
tion ?o? (about). The error occurs twice in the data.
However, as the vocative is extremely rare in the data
(its frequency in the corpus is less than 0,0004), two
erroneous tags are sufficient to give it a high rank.
Similar annotation errors for more frequent cases are
eliminated by the algorithm. For example, as we
mentioned in the previous section, the nominative
consideration only those prep+case combinations that appear at
least once in our dataset.
77
Preposition Meaning Expected cases Response
1 ??? without gen/gen2 gen/gen2
2 ? in/into acc/acc2/loc/loc2 acc/acc2/loc/loc2
3 ??? for gen/gen2 gen/gen2
4 ?? until gen/gen2 gen/gen2
5 ?? behind acc/ins acc/ins
6 ?? from gen/gen2 gen/gen2
7 ??-?? from behind gen/gen2 gen/gen2
8 ??-??? from under gen/gen2 gen/gen2
9 ? to dat dat
10 ????? beyond gen gen
11 ????? between ins ins
12 ?? on acc/loc/loc2 acc/loc/loc2
13 ??? above ins ins
14 ? about acc/loc loc/voc
15 ?? from gen/gen2 gen/gen2
16 ????? in front of ins ins
17 ???? in front of ins ins
18 ?? by/up to dat/loc/acc dat
19 ??? under acc/ins acc/ins
20 ??? at/by loc loc
21 ??? about acc acc
22 ???? for gen gen
23 ? with gen/gen2/acc/ins gen2/ins
24 ? near gen gen
25 ????? through acc acc/adnum
Expected 45
Response 42
Correct 40
Precision 0.95
Recall 0.89
F-measure 0.92
Table 1: Noun cases expected and returned by the algorithm for Russian prepositions.
case after preposition ??? (in) appears 88 times in
our data; however this case is not returned by the al-
gorithm, since it is below the frequency ratio thresh-
old.
25: ??????? (through/past) The adnumerative
(adnum) is a rare case in our data, so even a single
occurrence in a sample is considered important by
the algorithm. A single bigram is found in the data,
where the token ?????? (hours)?correctly anno-
tated with the adnum tag?predictably depends on
the Numeral, i.e., ????? (two), rather than on prepo-
sition ??????? (through/past), see Figure 4. The
numeral appears in post-position?a highly marked
word order that is admissible in this colloquial con-
struction in Russian: ?????? ???? ???? (lit.: after
hours two = idiom: after about two hours), where
Figure 4: Distributions of cases in the corpus and in a
sample. (Arrows indicate syntactic dependency.)
the preposition governs the Case of the numeral, and
the numeral governs a noun that precedes it.
Because our algorithm at the moment processes
linear sequences, these kinds of syntactic inversion
phenomena in Russian will pose a challenge. In gen-
eral this problem can be solved by using tree-banks
for MWE extraction, (Seretan, 2008; Martens and
Vandeghinste, 2010). However, an appropriate tree-
78
bank is not always available for a given language; in
fact, we do not have access to any Russian tree-bank
suitable for this task.
23: ??? (with) This is a genuine error. The algo-
rithm misses two of four correct cases, Genitive and
Accusative, because both are widely used across the
corpus, which reduces their frequency ratio in the
sub-sample. Our further work will focus on finding
flexible frequency ratio thresholds, which is now set
to one. Two of the correct cases (Instrumental and
Gen2) are well over the threshold, while Genitive,
with 0.6924, and Accusative, with 0.0440, fall short.
18: ???? (by/along) For this preposition the al-
gorithm predicts 1 case out of 3. This situation is
slightly different from the previous ones, since the
accusative and locative cases are much more rare
with preposition ???? (by/along) than the dative:
245 instances out of 15387 for accusative, and 222
for locative in our data. We hypothesize that this
means that such ?Prep+case? combinations are con-
strained lexically to a greater extent than grammat-
ically. To check this hypothesis we calculate the
frequency ratio for all lemmas that appear with the
respective patterns { ???? (by/along) + acc } and
{ ???? (by/along) + loc }. As a result, 15 distinct
lemmas were extracted by { ???? (by) + acc }; 13
out of them have frequency ratio > 1. The major-
ity of the lemmas belong to the semantic class ?part
of the body? and are used in a very specific Rus-
sian construction, which indicates ?an approximate
level?, e.g. ??? ??????? (up to (one?s) elbow), cf.
English ?up to one?s neck in work?. This construc-
tion has limited productivity, and we are satisfied
that the Accusative is omitted in the output for gram-
matical categories, since the algorithm outputs all
tokens that appear in the { ???? (by/along) + acc }
as relevant lemmas.
The case of { ???? (by) + loc } is more com-
plex: 44 of 76 combinations return a frequency
greater than 1. Analysis of annotation errors reveals
a compact collection of bureaucratic cliches, like
??? ????????? (upon arrival), ??? ??????????
(upon completion), etc., which all share the seman-
tics of ?immediately following X?, and are pragmat-
ically related. These are expressions belonging to
the same bureaucratic jargon and sharing the same
morphological pattern, however, they are below the
threshold. Again, we are faced with need to tune the
threshold to capture this kind of potentially interest-
ing lexical combinations. In general, semantic and
pragmatic factors influence the ability of words to
combine, and the algorithm shows it in some way,
though these aspects of the problem are beyond the
scope of our experiments in the current stage.
5 Discussion and Future Work
5.1 Development of the algorithm
We have presented a part an overall system under de-
velopment. In the preceding sections, we investigate
an area where collocations and colligations meet. To
summarize, the algorithm, based on the corpus of n-
grams, treats both morpho-syntactic and lexical co-
occurrences as a unified continuum, which has no
clear borders. The evaluation of the morphological
output raises some new questions for further devel-
opment:
? At present, the low precision for both low- and
high-frequency tags depends on the threshold,
which needs to be studied further.
? The values of divergences are currently not
normalized among the different query patterns.
This may be a difficult question, and we plan to
investigate this further. The algorithm provides
a way to compare the strength of very diverse
collocations, which have nothing in common,
in terms of their degree of idiomatization.
? We observe that the longer the n-gram, the
more we expect it to be a collocation; stable
bigrams appear more frequently to be colliga-
tions, while stable 4-grams are more often col-
locations. The problem is that those colloca-
tions with a highly frequent first collocate, e.g.,
??? (in), cannot be found using our algorithm
as it stands now.
? Token/lexeme stability is the next task we will
concentrate on. Wermter and Hahn (2006) and
Kilgarriff et al (2012) proposed that sorting
tokens/lexemes according to plain frequency
works well if there is no grammatical knowl-
edge at hand. We do have such knowledge. To
improve the accuracy of lexeme/token extrac-
tion we rely on the idea of grammatical pro-
79
files, introduced by Gries and Divjak (2009).
We plan to develop this approach with the
further assumption that the distribution of to-
kens/lexemes within a pattern is based on rel-
evant grammatical properties, which are ob-
tained in an earlier step of our algorithm. For
instance, for ??? ?? X? (not up to X) we have
found that the grammatical profile for X is
N.gen/gen2, and the token frequency ratio is
greater than 1 as well. Building the list of to-
kens that are the most stable for this pattern, we
compare their distributions within the pattern to
all N.gen/gen2 tokens in the corpus. This yields
the following tokens as the most relevant: ???
?? ?????? (lit.: not up to laughter.gen = id-
iom: no laughing matter);??? ?? ????? (lit.
not up to fat.gen2 = idiom: no time/place for
complacency), which reveals an interesting set
of idioms.
5.2 Extensions and Applications
The model has no restriction on the length of data
to be used, and is applicable to various languages.
Finnish (which is morphologically rich) and English
(morphologically poor) will be examined next. As
for Russian, so far the algorithm has been systemat-
ically evaluated against bigrams, although we have
3-, 4- and 5-grams at our disposal for future work.
A reliable method that is able to determine pat-
terns of frequently co-occurring lexical and gram-
matical features within a corpus can have far-
reaching practical implications. One particular ap-
plication that we are exploring is the fine-tuning
of semantic patterns that are commonly used in in-
formation extraction (IE), (Grishman, 2003). Our
work on IE focuses on different domains and differ-
ent languages, (Yangarber et al, 2007; Atkinson et
al., 2011). Analysis of MWEs that occur in extrac-
tion patterns would provide valuable insights into
how the patterns depend on the particular style or
genre of the corpus, (Huttunen et al, 2002). Subtle,
genre-specific differences in expression can indicate
whether a given piece of text is signaling the pres-
ence an event of interest.
5.3 Creating Teaching-Support Tools
Instructors teaching a foreign language are regu-
larly asked how words co-occur: What cases and
word forms appear after a given preposition? Which
ones should I learn by rote and which ones follow
rules? The persistence of such questions indicates
that this is an important challenge to be addressed?
we should aim to build a system that can automati-
cally generate an integrated answer. A tool that pro-
duces answers to these questions would be of great
help for teachers as well as students. The presented
algorithm can support an easy-to-use Web-based ap-
plication, or an application for a mobile device. We
plan to develop a service, which is able to process
queries described in the paper. This service would
be an additional interface to a corpus, aimed at find-
ing not only the linear context of words but also their
collocational and constructional preferences. We be-
lieve that such an interface would be useful for both
research and language-learning needs.
Acknowledgments
We are very grateful to the Russian National Cor-
pus developers, especially E. Rakhilina and O. Lya-
shevskaya, for providing us with the data.
References
Martin Atkinson, Jakub Piskorski, Erik van der Goot, and
Roman Yangarber. 2011. Multilingual real-time event
extraction for border security intelligence gathering.
In U. Kock Wiil, editor, Counterterrorism and Open
Source Intelligence, pages 355?390. Springer Lecture
Notes in Social Networks, Vol. 2, 1st edition.
Brigitte Bigi. 2003. Using Kullback-Leibler distance
for text categorization. In Fabrizio Sebastiani, edi-
tor, Advances in Information Retrieval, volume 2633
of Lecture Notes in Computer Science, pages 305?319.
Springer Berlin, Heidelberg.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational linguistics, 16(1):22?29.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Kindle. 1991. Using statistics in lexical analy-
sis. Lexical acquisition: exploiting on-line resources
to build a lexicon.
Vidas Daudaravicius. 2010. Automatic identification of
lexical units. Computational Linguistics and Intelli-
gent text processing CICling-2009.
Boris Dobrov and Natalia Loukachevitch. 2011. Mul-
tiple evidence for term extraction in broad domains.
In Proceedings of the 8th Recent Advances in Natu-
ral Language Processing Conference (RANLP 2011).
Hissar, Bulgaria, pages 710?715.
80
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational linguis-
tics, 19(1):61?74.
Afsaneh Fazly and Suzanne Stevenson. 2007. Dis-
tinguishing subtypes of multiword expressions using
linguistically-motivated statistical measures. In Pro-
ceedings of the Workshop on A Broader Perspective on
Multiword Expressions, pages 9?16. Association for
Computational Linguistics.
Katerina Frantzi, Sophia Ananiadou, and Hideki Mima.
2000. Automatic recognition of multi-word terms:
the c-value/nc-value method. International Journal on
Digital Libraries, 3(2):115?130.
Adele Goldberg. 2006. Constructions at work: The na-
ture of generalization in language. Oxford University
Press, USA.
Stefan Th. Gries and Dagmar Divjak. 2009. Behavioral
profiles: a corpus-based approach to cognitive seman-
tic analysis. New directions in cognitive linguistics,
pages 57?75.
Ralph Grishman. 2003. Information extraction. In
The Handbook of Computational Linguistics and Nat-
ural Language Processing, pages 515?530. Wiley-
Blackwell.
Silja Huttunen, Roman Yangarber, and Ralph Grishman.
2002. Diversity of scenarios in information extraction.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC 2002),
Las Palmas de Gran Canaria, Spain, May.
Maria Khokhlova and Viktor Zakharov. 2009. Statistical
collocability of Russian verbs. After Half a Century
of Slavonic Natural Language Processing, pages 125?
132.
Adam Kilgarriff, Pavel Rychly`, Vojtech Kova?r, and V?t
Baisa. 2012. Finding multiwords of more than two
words. In Proceedings of EURALEX2012.
Scott Martens and Vincent Vandeghinste. 2010. An effi-
cient, generic approach to extracting multi-word ex-
pressions from dependency trees. In CoLing Work-
shop: Multiword Expressions: From Theory to Appli-
cations (MWE 2010).
Pavel Pecina. 2005. An extensive empirical study of
collocation extraction methods. In Proceedings of the
ACL Student Research Workshop, pages 13?18. Asso-
ciation for Computational Linguistics.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for
the extraction of multiword expressions. In Proceed-
ings of the LREC Workshop-Towards a Shared Task for
Multiword Expressions (MWE 2008), pages 50?53.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How, pages 52?57. Washington, DC.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. Computational Lin-
guistics and Intelligent Text Processing, pages 189?
206.
Violeta Seretan. 2008. Collocation extraction based on
syntactic parsing. Ph.D. thesis, University of Geneva.
Serge Sharoff. 2004. What is at stake: a case study of
Russian expressions starting with a preposition. In
Proceedings of the Workshop on Multiword Expres-
sions: Integrating Processing, pages 17?23. Associ-
ation for Computational Linguistics.
Sayori Shimohata, Toshiyuki Sugio, and Junji Nagata.
1997. Retrieving collocations by co-occurrences and
word order constraints. In Proceedings of the eighth
conference on European chapter of the Association for
Computational Linguistics, pages 476?481. Associa-
tion for Computational Linguistics.
Anatol Stefanowitsch and Stefan Th Gries. 2003. Col-
lostructions: Investigating the interaction of words and
constructions. International journal of corpus linguis-
tics, 8(2):209?243.
Marko Tadic? and Kres?imir S?ojat. 2003. Finding multi-
word term candidates in Croatian. In Proceedings of
IESL2003 Workshop, pages 102?107.
Tim Van de Cruys and Begona Villada Moiro?n. 2007.
Lexico-semantic multiword expression extraction. In
Proceedings of the 17th Meeting of Computational
Linguistics in the Netherlands (CLIN), pages 175?190.
Joachim Wermter and Udo Hahn. 2006. You can?t beat
frequency (unless you use linguistic knowledge) ? a
qualitative evaluation of association measures for col-
location and term extraction. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 785?792.
Elena Yagunova and Lidia Pivovarova. 2010. The nature
of collocations in the Russian language. The experi-
ence of automatic extraction and classification of the
material of news texts. Automatic Documentation and
Mathematical Linguistics, 44(3):164?175.
Roman Yangarber, Clive Best, Peter von Etter, Flavio
Fuart, David Horby, and Ralf Steinberger. 2007.
Combining information about epidemic threats from
multiple sources. In Proceedings of the MMIES
Workshop, International Conference on Recent Ad-
vances in Natural Language Processing (RANLP
2007), Borovets, Bulgaria, September.
81
