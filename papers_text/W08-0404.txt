Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 28?36,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Generalizing local translation models
Michael Subotin
Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland
College Park, MD 20742
msubotin@umiacs.umd.edu
Abstract
We investigate translation modeling based on
exponential estimates which generalize essen-
tial components of standard translation mod-
els. In application to a hierarchical phrase-
based system the simplest generalization al-
lows its models of lexical selection and re-
ordering to be conditioned on arbitrary at-
tributes of the source sentence and its anno-
tation. Viewing these estimates as approxi-
mations of sentence-level probabilities moti-
vates further elaborations that seek to exploit
general syntactic and morphological patterns.
Dimensionality control with `1 regularizers
makes it possible to negotiate the tradeoff be-
tween translation quality and decoding speed.
Putting together and extending several recent
advances in phrase-based translation we ar-
rive at a flexible modeling framework that al-
lows efficient leveraging of monolingual re-
sources and tools. Experiments with features
derived from the output of Chinese and Arabic
parsers and an Arabic lemmatizer show signif-
icant improvements over a strong baseline.
1 Introduction
Effective handling of large and diverse inventories
of feature functions is one of the most pressing
open problems in machine translation. While min-
imum error training (Och, 2003) has by now be-
come a standard tool for interpolating a small num-
ber of aggregate scores, it is not well suited for
learning in high-dimensional feature spaces. At the
same time, although recent years have seen consid-
erable progress in development of general methods
for large-scale prediction of complex outputs (Bak?r
et al, 2007), their application to language transla-
tion has presented considerable challenges. Sev-
eral studies have shown that large-margin methods
can be adapted to the special complexities of the
task (Liang et al, 2006; Tillmann and Zhang, 2006;
Cowan et al, 2006) . However, the capacity of these
algorithms to improve over state-of-the-art baselines
is currently limited by their lack of robust dimen-
sionality reduction. Performance gains are closely
tied to the number and variety of candidate features
that enter into the model, and increasing the size of
the feature space not only slows down training in
terms of the number of iterations required for con-
vergence, but can also considerably reduce decod-
ing speed, leading to run-time costs that may be un-
acceptable in industrial settings. Vector space re-
gression has shown impressive performance in other
tasks involving string-to-string mappings (Cortes et
al., 2007), but its application to language transla-
tion presents a different set of open problems (Wang
et al, 2007). Other promising formalisms, which
have not yet produced end-to-end systems compet-
itive with standard baselines, include the approach
due to Turian et al(2006), the hidden-state syn-
chronous grammar-based exponential model studied
by Blunsom et al(2008), and a similar model in-
corporating target-side n-gram features proposed in
Subotin (2008).
Taken together the results of these studies point
to a striking overarching conclusion: the humble
relative frequency estimate of phrase-based mod-
els makes for a surprisingly strong baseline. The
present paper investigates a family of models that
28
capitalize on this practical insight to allow efficient
optimization of weights for a virtually unlimited
number of features. We take as a point of depar-
ture the observation that the essential translation
model scores comprising standard decoding deci-
sion rules can be recovered as special cases of a
more general family of models. As we discuss be-
low, they are equal to maximum likelihood solu-
tions for locally normalized ?piecewise? approxi-
mations to sentence-level probabilities, where word
alignment is used to determine the subset of fea-
tures observed in each training example. The cases
for which such solutions have a closed form corre-
spond to particular restrictions placed on the feature
space. Thus, relative frequency phrase models can
be obtained by limiting the feature space to indica-
tor functions for the phrase pairs consistent with an
alignment. By removing unnecessary restrictions we
restore the full flexibility of local exponential mod-
els, including their ability to use features depending
on arbitrary aspects of the source sentence and its
annotation. The availability of robust algorithms for
dimensionality reduction with `1 regularizers (Ng,
2004) means that we can start with a virtually un-
limited number of candidate features and negotiate
the tradeoff between translation quality and decod-
ing speed in a way appropriate for a given setting.
A further attractive property of locally normalized
models is the modest computational cost of their
training and ease of its parallelization. This is par-
ticularly so for the models we concentrate on in this
paper, defined so that parameter estimation decom-
poses into a large number of small optimization sub-
problems which can be solved independently.
Several variants of these models beyond rela-
tive frequencies have appeared in the literature be-
fore. Maximum entropy estimation for transla-
tion of individual words dates back to Berger et
al (1996), and the idea of using multi-class classi-
fiers to sharpen predictions normally made through
relative frequency estimates has been recently rein-
troduced under the rubric of word sense disambigua-
tion and generalized to substrings (Chan et al2007;
Carpuat and Wu 2007a; Carpuat and Wu 2007b).
Maximum entropy models for non-lexicalized re-
ordering rules for a phrase-based system with CKY
decoding has been described by Xiong et al(2006).
Some of our experiments, where exponential models
conditioned on the source sentence and its parse an-
notation are associated with all rewrite rules in a hi-
erarchical phrase-based system (Chiang, 2007) and
all word-level probabilities in standard lexical mod-
els, may be seen as a synthesis of these ideas.
The broader perspective of viewing the product of
such local probabilities as a particular approxima-
tion of sentence-level likelihood points the way be-
yond multi-class classification, and this type of gen-
eralization is the main original contribution of the
present work. Training a classifier to predict the tar-
get phrase for every source phrase is equivalent to
conjoining all contextual features of the model with
an indicator function for the surface form of some
rule in the grammar. We can also use features based
on less specific representation of a rule. Of par-
ticular importance for machine translations are rep-
resentations which generalize reordering informa-
tion beyond identity of individual words ? a type of
generalization that presents a challenge in hierarchi-
cal phrase-based translation. With generalized local
models this can be accomplished by adding features
tracking only ordering patterns of rules. We exper-
iment with a case of such models which allows us
to preserve decomposition of parameter estimation
into independent subproblems.
Besides varying the structure of the feature space,
we can also extend the range of normalization for
the exponential models beyond target phrases co-
occurring with a given source phrase in the phrase
table. This choice is especially natural for richly in-
flected languages, since it enables us to model mul-
tiple levels of morphological representation at once
and estimate probabilities for rules whose surface
forms have not been observed in training. We apply
a simple variant of this approach to Arabic-English
lexical models.
Experimental results across eight test sets in two
language pairs support the intuition that features
conjoined with indicator functions for surface forms
of rules yield higher gains for test sets with better
coverage in training data, while features based on
less specific representations become more useful for
test sets with lower baselines.
The types of features explored in this paper rep-
resent only a small portion of available options, and
much practical experimentation remains to be done,
particularly in order to find the most effective ex-
29
tensions of the feature space beyond multiclass clas-
sification. However, the results reported here show
considerable promise and we believe that the flexi-
bility of these models combined with their computa-
tional efficiency makes them potentially valuable as
an extension for a variety of systems using transla-
tion models with local conditional probabilities and
as a feature selection method for globally trained
models.
2 Hierarchical phrase-based translation
We take as our starting point David Chiang?s Hiero
system, which generalizes phrase-based translation
to substrings with gaps (Chiang, 2007). Consider
for instance the following set of context-free rules
with a single non-terminal symbol:
?A , A ? ? ?A1 A2 , A1 A2 ?
?A , A ? ? ? d? A1 ide?esA2 , A1 A2 ideas ?
?A , A ? ? ? incolores , colorless ?
?A , A ? ? ? vertes , green ?
?A , A ? ? ? dormentA , sleepA ?
?A , A ? ? ? furieusement , furiously ?
It is one of many rule sets that would suffice to
generate the English translation 1b for the French
sentence 1a.
1a. d? incolores ide?es vertes dorment furieusement
1b. colorless green ideas sleep furiously
As shown by Chiang (2007), a weighted gram-
mar of this form can be collected and scored by
simple extensions of standard methods for phrase-
based translation and efficiently combined with a
language model in a CKY decoder to achieve large
improvements over a state-of-the-art phrase-based
system. The translation is chosen to be the target-
side yield of the highest-scoring synchronous parse
consistent with the source sentence. Although a va-
riety of scores interpolated into the decision rule for
phrase-based systems have been investigated over
the years, only a handful have been discovered to be
consistently useful, as is in our experience also the
case for the hierarchical variant. Setting aside spe-
cialized components such as number translators, we
concentrate on the essential sub-models1 comprising
1To avoid confusion with features of the exponential models
described below we shall use the term ?model? for the terms
the translation model: the phrase models and lexical
models.
3 Local exponential translation models
3.1 Relative frequency solutions
Standard phrase models associate conditional proba-
bilities with subparts of translation hypotheses, usu-
ally computed as relative frequencies of counts of
extracted phrases.2 Let ry be the target side of a rule
and rx its source side. The weight of the rule in the
?reverse? phrase model would then be computed as
p(ry|rx) =
count(?rx, ry?)
?
ry? count(?r
x, ry??)
(1)
When used to score a translation hypothesis cor-
responding to some synchronous parse tree T , the
phrase model may be conceived as an approxima-
tion of the probability of a target sentence Y given a
source sentence X
p(Y |X) ?
?
r?T
p(ry|rx) (2)
Although there is nothing in current learning the-
ory that would prompt one to expect that expressions
of this form should be effective, their surprisingly
strong performance in machine translation in an em-
pirical observation borne out by many studies. In
order to build on this practical insight it is useful to
gain a clearer understanding of their formal proper-
ties.
We start by writing out an expression for the like-
lihood of training data which would give rise to max-
imum likelihood solutions like those in eq. 1. Con-
sider a feature vector whose components are indi-
cator functions for rules in the grammar, and let
us define an exponential model for a sentence pair
(Xm, Ym) of the form
p (Ym|Xm) ?
?
r?(Xm,Ym)
p(ry|rx) (3)
=
?
r?(Xm,Ym)
exp{w ? fr(Xm, Ym)}
?
r?:rx=r?x exp{w ? fr?(Xm, Ym)}
(4)
interpolated using MERT.
2Chiang (2007) uses a heuristic estimate of fractional counts
in these computations. For completeness we report both vari-
ants in the experiments.
30
where fr(Xm, Ym) is a restriction of the feature
vector such that all of its entries except for the one
corresponding to the rule r are zero and the summa-
tion is over all rules in the grammar with the same
source side. As can be verified by writing out the
likelihood for the training set and setting its gradi-
ent to zero, maximum likelihood estimation based
on eq. 4 yields estimates equal to relative frequency
solutions. In fact, because its normalization fac-
tors have non-zero parameters in common only for
rules which share the same source phrase, param-
eter estimation decomposes into independent opti-
mization subproblems, one for each source phrase
in the grammar. However, recovering relative fre-
quencies of the needed form requires further atten-
tion to the relationship between the definition of fea-
ture functions and phrase extraction. Computation
of phrase models in machine translation crucially re-
lies on a form of feature selection not widely known
in other contexts. A rule is considered to be ob-
served in a sentence pair only if it is consistent with
predictions of a word alignment model according to
heuristics for alignment combination and phrase ex-
traction. The standard recipes in translation model-
ing can thus be seen to include a feature selection
procedure that applies individually to each training
example.
3.2 Classifier solutions
We can now generalize these relative frequency
estimates by relaxing the restrictions they implic-
itly place on the form of permissible feature func-
tions. The simplest elaboration involves allow-
ing indicator functions for rules to be conjoined
with indicator functions for arbitrary attributes of
the source sentence or its annotation. This pre-
serves a decomposition of parameter estimation of
optimization subproblems associated with individ-
ual source phrase, but effectively replaces proba-
bilities p(ry|rx) in eqs. 2 and 3 with probabili-
ties conditioned on the source phrase together with
some of its source-side context. We may, for ex-
ample, conjoin an indicator function for the rule
?A , A ? ? ? d? A1 ide?esA2 , A1 A2 ideas ? with a
function telling us whether a part-of-speech tagger
has identified the word at the left edge of the source-
side gap A2 as an adjective, which would provide
additional evidence for the target side of this rule.
Combining a grammar-based formalism with con-
textual features raises a subtle question of whether
rules which have gaps at the edges and can match at
multiple positions of a training example should be
counted as having occurred together with their re-
spective contextual features once for each possible
match. To avoid favoring monotone rules, which
tend to match at many positions, over reordering
rules, which tend to match at a single span, we ran-
domly sample only one of such multiple matches for
training.
Unlike conventional phrase models, contextually-
conditioned probabilities cannot be stored in a pre-
computed phrase table. Instead, we store informa-
tion about features and their weights and compute
the normalization factors at run-time at the point
when they are first needed by the decoder.
At the expense of more complicated decoding
procedures we could also apply the same line of
reasoning to generalize the ?noisy channel? phrase
model p(rx|ry) to be conditioned on local target-
side context in a translation hypothesis, possibly
combining target-side annotation of the training set
with surface form of rules. We do not pursue this
elaboration in part because we are skeptical about its
potential for success. The current state of machine
translation rarely permits constructing well-formed
translations, so that most of the contextual features
on the target side would be rarely if at all observed
in the training data, resulting in sparse and noisy es-
timates. Furthermore, we have yet to find a case
where relative frequency estimates p(rx|ry) make a
useful contribution to the system when contextually-
conditioned ?reverse? probabilities are used, sug-
gesting that viewing translation modeling as approx-
imating sentence-level probabilities p(Y |X) may be
a more fruitful avenue in the long term.
For translation with phrases without gaps classi-
fier solutions of eq. 4 are equivalent to a maximum
entropy variant of the phrase sense disambigua-
tion approach studied by Carpuat & Wu (2007b).
These solutions are also closely related to the ap-
proximation known as piecewise training in graph-
ical model literature (Sutton and McCallum, 2005;
Sutton and Minka, 2006) and independently stated
in a more general form by Pe?rez-Cruz et al(2007).
Aside from formal differences between feature tem-
plates defined by graphical models and grammars,
31
which are beyond the scope of our discussion, there
are several further contrasts between these studies
and standard practice in machine translation in how
the learned parameters are used to make predic-
tions. Unlike inference in piecewise-trained graphi-
cal models, where all parameters for a given output
are added together without normalization, features
that enter into the score for a translation hypothe-
sis are restricted to be consistent with a single syn-
chronous parse and the local probabilities are nor-
malized in decoding as in training.
3.3 Lexical models
The use of conditional probabilities in standard lex-
ical models also gives us a straightforward way to
generalize them in the same way as phrase models.
Consider the lexical model pw(ry|rx), defined fol-
lowing Koehn et al(2003), with a denoting the most
frequent word alignment observed for the rule in the
training set.
pw(r
y|rx) =
n?
i=1
1
|j|(i, j) ? a|
?
(i,j)?a
p(wyi |w
x
j )
(5)
We replace p(wyi |w
x
j ) with context-conditioned
probabilities, computed similarly to eq. 4, but at the
level of individual words. Our experience suggests
that, unlike the analogous phrase model, the stan-
dard lexical model pw(rx|ry) is not made redundant
by this elaboration, and we use its baseline variant
in all our experiments. While this approach seeks to
make the most of practical insights underlying state-
of-the-art baselines, it is of course not the only way
to combine rule-based and word-based features. See
for example Sutton & Minka (2006) for a discussion
of alternatives that are closer in spirit to the idea of
approximating global probabilities.
3.4 Further generalizations
An immediate practical benefit of interpreting rela-
tive frequency and classifier estimates of translation
models as special cases is the possibility of gener-
alizing them further by introducing additional fea-
tures based on less specific representations of rules
and words.
Among the least specific and most potentially use-
ful representations of hierarchical phrases are those
limited to the patterns formed by gaps and words,
allowing the model to generalize reordering infor-
mation beyond individual tokens. We study two
types of ordering patterns. For rules with two gaps
we form features by conjoining contextual indicator
functions with functions indicating whether the gap
pattern is monotone or inverting. We also use an-
other type of ordering features, representing the pat-
tern formed by gaps and contiguous subsequences
of words. For example, the rule with the right-hand
side ? d? A1 ide?esA2 , A1 A2 ideas ?might be asso-
ciated with the pattern ? aA1 aA2 , A1 A2 a ?. Be-
cause some source-side patterns of this type apply
to many different rules it is no longer possible to de-
compose parameter estimation into small indepen-
dent optimization subproblems. For practical conve-
nience we enforce decomposition in the experiments
reported below in the following way. We define indi-
cator functions for sequences of closed-class words
and the most frequent part-of-speech tag for open-
class words on the source side. For the rule above
and a simple tag-set the pattern tracked by such an
indicator function would be d? A1 N A2 . We require
all reordering features to be conjoined with an indi-
cator function of this type, ensuring that each cor-
responds to a separate optimization subproblem. We
further split larger optimization subproblems, so that
parameters for identical reordering features are in
some cases estimated separately for different subsets
of rules.
Morphological inflection provides motivation for
another class of features not bound to surface repre-
sentations. In this paper we explore a particularly
simple example of this approach, adding features
conjoined with indicator functions for Arabic lem-
mas to the lexical models in Arabic-English trans-
lation. This preserves decomposition of parameter
estimation, with subproblems now associated with
individual lemmas rather than words. Lemma-based
features suggest another extension of the modeling
framework. Instead of computing the sums in nor-
malization factors over all English words aligned to
a given Arabic token in the training data, we let the
sum range over all English words aligned to Arabic
words sharing its lemma. This also defines probabil-
ities for Arabic words whose surface forms have not
been observed in training, although we do not take
advantage of estimates for out-of-vocabulary words
32
in the experiments below.
3.5 Regularization
We apply `1 regularization (Ng, 2004; Gao et al,
2007) to make learning more robust to noise and
control the effective dimensionality of the feature
space by subtracting a weighted sum of absolute val-
ues of parameter weights from the log-likelihood of
the training data
w? = argmax
w
LL(w) ?
?
i
Ci|wi| (6)
We optimize the objective using a variant of the
orthant-wise limited-memory quasi-Newton algo-
rithm proposed by Andrew & Gao (2007).3 All val-
ues Ci are set to 1 in most of the experiments below,
although we apply stronger regularization (Ci = 3)
to reordering features. Tuning regularization trade-
offs individually for different feature types is an at-
tractive option, but our experiments suggest that us-
ing cross-entropy on a held-out portion of training
data for that purpose does not help performance.
We leave investigation of the alternatives for future
work.
4 Experiments
4.1 Data and methods
We apply the models to Arabic-English and
Chinese-English translation, with training sets con-
sisting of 108,268 and 1,017,930 sentence pairs, re-
spectively.4 All conditions use word alignments
produced by sequential iterations of IBM model 1,
HMM, and IBM model 4 in GIZA++ , followed
3Our implementation of the algo-
rithm as a SciPy routine is available at
http://www.umiacs.umd.edu/?msubotin/owlqn.py
4The Arabic-English data came from Arabic News Transla-
tion Text Part 1 (LDC2004T17), Arabic English Parallel News
Text (LDC2004T18), and Arabic Treebank English Translation
(LDC2005E46). Chinese-English data came from Xinhua Chi-
nese English Parallel News Text Version 1 beta (LDC2002E18),
Chinese Treebank English Parallel Corpus (LDC2003E07),
Chinese English News Magazine Parallel Text (LDC2005T10),
FBIS Multilanguage Texts (LDC2003E14), Chinese News
Translation Text Part 1 (LDC2005T06), and the HKNews por-
tion of Hong Kong Parallel Text (LDC2004T08). Some sen-
tence pairs were not included in the training sets due to large
length discrepancies.
by ?diag-and? symmetrization (Koehn et al, 2003).
Thresholds for phrase extraction and decoder prun-
ing were set to values typical for the baseline sys-
tem (Chiang, 2007). Unaligned words at the outer
edges of rules or gaps were disallowed. A trigram
language model with modified interpolated Kneser-
Ney smoothing (Chen and Goodman, 1998) was
trained by the SRILM toolkit on the Xinhua por-
tion of the Gigaword corpus and the English side of
the parallel training set. Evaluation was based on
the BLEU score with 95% bootstrap confidence in-
tervals for the score and difference between scores,
calculated by scripts in version 11a of the NIST dis-
tribution. The 2002 NIST MT evaluation sets was
used for development. The 2003, 2004, 2005, and
2006 sets were used for testing.
The decision rule was based on the standard log-
linear interpolation of several models, with weights
tuned byMERT on the development set (Och, 2003).
The baseline consisted of the language model, two
phrase translation models, two lexical models, and
a brevity penalty. In the runs where generalized ex-
ponential models were used they replaced both of
the baseline phrase translation models. The feature
set used for exponential phrase models in the exper-
iments included all the rules in the grammar and all
aligned word pairs for lexical models. Elementary
contextual features were based on Viterbi parses ob-
tained from the Stanford parser. Word features in-
cluded identities of word unigrams and bigrams ad-
jacent to a given rule, possibly including rule words.
Part-of-speech features included similar ngrams up
to the length of 3 and the tags for rule tokens. These
features were collected for training by a straightfor-
ward extension of rule extraction algorithms imple-
mented in the baseline system for each possible lo-
cation of ngrams with respect to the rule: namely, at
the outer edges of the rule and at the edges of any
gaps that it has. Our models also include a subset
of contextual features formed by pairwise combina-
tions of these elementary features. A final type of
contextual features in these experiments was the se-
quence of the highest nodes in the parse tree that fill
the span of the rule and the sequences that fill its
gaps. We used an in-house Arabic tokenizer based
on a Java implementation of Buckwalter?s morpho-
logical analyzer and incorporating simple statistics
from the Penn Arabic treebank, also extending it to
33
perform lemmatization.
The total number of candidate features thus de-
fined is very large, and we use a number of sim-
ple heuristics to reduce it prior to training. They
are not essential to the estimates and were chosen
so that the models could be trained in a few hours
on a small cluster. With the exception of discarding
all except the 10 most frequent target phrases ob-
served with each source phrase,5 which benefits per-
formance, we expect that relaxing these restrictions
would improve the score. These limitations included
count-based thresholds on the frequency of contex-
tual features included into the model, the frequency
of rules and reordering patterns conjoined with other
features, and the size of optimization subproblems to
which contextual features are added. We don?t con-
join contextual features to rules whose source phrase
terminals are all punctuation symbols. For subprob-
lems of size exceeding a certain threshold, we train
on a subsample of available training instances. For
the Chinese-English task we do not add reorder-
ing features to problems with low-entropy distribu-
tions of inversion and reordering patterns and dis-
card rules with two non-terminals altogether if the
entropy of their reordering patterns falls under a
threshold. None of these restrictions were applied to
the baselines. Finally, we solve only those optimiza-
tion subproblems which include parameters needed
in the development and training sets. This leads to a
reduction of costs that is similar to phrase table fil-
tering and likewise does not affect the solution. At
decoding time all features for the translation models
and their weights are accessed from a disk-mapped
trie.
4.2 Results and discussion
The results are shown in tables 1 and 2. For both lan-
guage pairs we had a choice between using a base-
line that is computed in the same way as the other ex-
ponential models, with the exception of its use of rel-
ative frequency estimates and a baseline that incor-
porates averaged fractional counts for phrase mod-
els and lexical models, as used by Chiang (2007).
For the sake of completeness we report both (though
without performing statistical comparisons between
5This has prompted us to add an additional target-side token
to lexical models, which subsumes the discarded items under a
single category.
Condition MT03 MT04 MT05 MT06
Rel. freq. 48.24 43.92 47.53 37.94
Frac. 48.34 45.68 47.95 39.41
Context 49.47* 45.65 48.76 39.49
+lex 50.42* 46.07* 49.66* 39.32
+lex+lemma 49.86* 47.02* 49.29* 40.81*
Table 1: Arabic-English translation, BLEU scores on
testing. Conditions include two baselines: simple rela-
tive frequency (rel. freq.) and fractional estimates (frac.).
Experimental conditions: contextual features in phrase
models (context); same and contextual features in lexi-
cal models (+lex); same and lemma based features in lex-
ical models (+lex+lemma). Stars mark statistically sig-
nificant improvements over the fractional baseline which
produced a higher score on the dev-test MT02 set than
the other baseline (59.75 vs. 59.66).
Condition MT03 MT04 MT05 MT06
Rel. freq. 32.62 27.53 30.50 22.78
Frac. 32.56 27.98 30.42 23.16
Context 33.16* 28.35* 31.52* 23.67*
+lex 33.50* 28.14* 31.98* 23.05
+lex+reord 33.12* 28.27* 31.73* 23.45*
Table 2: Chinese-English translation, BLEU scores on
testing. Conditions include two baselines: simple rela-
tive frequency (rel. freq.) and fractional estimates (frac.).
Experimental conditions: contextual features in phrase
models (context); same and contextual features in lexi-
cal models (+lex); same and reordering features in phrase
models (+lex+reord). Stars mark statistically significant
improvements over the simple relative frequency baseline
which produced a higher score on the dev-test MT02 set
than the other baseline (33.62 vs. 33.53).
34
them). Statistical tests for experimental conditions
were performed in comparison to the baseline which
achieved higher score on the test-dev MT02 set: the
fractional count baseline for Arabic-English and the
simple relative count baseline for Chinese-English.
We test models with classifier solutions for phrase
models alone and for phrase models together with
lexical models in both language pairs. For Arabic-
English translation we also experiment with adding
features based on lemmas to lexical models, while
for Chinese-English we add ?reordering? features ?
features based on the ordering pattern of gaps for
rules with two gaps and features based on ordering
of gaps and words for rules with a single gap.
For both language pairs the results show con-
sistent distinctions in behavior of different mod-
els between the test sets giving rise to generally
higher scores (MT03 and MT05) and generally
lower scores (MT04 and MT06). The fractional
counts seem to be consistently more helpful for
test sets with poorer coverage, although the rea-
son for this is not immediately clear. For exponen-
tial models the two type of sets present two pos-
sible sources of difference. The lower-performing
sets have poorer coverage in the training data, and
they also may suffer from lower-quality annotation,
since the training sets for both the translation mod-
els and the annotation tools are dominated by text
in the same, newswire domain. Overall, the use of
features based on surface forms is more beneficial
for MT03 and MT05. Indeed, using lexical models
with contextual features in addition to phrase models
hurts performance on MT06 for Arabic-English and
on both MT04 and MT06 for Chinese-English. In
contrast, using features based on less specific repre-
sentations is more beneficial on test sets with poorer
coverage, while hurting performance on MT03 and
MT05. This agrees with our intuitions and also sug-
gests that the differences in coverage of training data
for the translation models may be playing a more
important role in these trends than coverage for an-
notation tools.
5 Conclusion
We have outlined a framework for translation
modeling that synthesizes several recent advances
in phrase-based machine translation and suggests
many other ways to leverage sub-token representa-
tions of words as well as syntactic and morpholog-
ical annotation tools, of which the experiments re-
ported here explore only a small fraction. Indeed,
the range and practicality of the available options is
perhaps its most attractive feature. The inital results
are promising and we are optimistic that continued
exploration of this class of models will uncover even
more effective uses.
Acknowledgments
I would like to thank David Chiang, Chris Dyer, Lise
Getoor, Kevin Gimpel, Adam Lopez, Nitin Mad-
nani, Smaranda Muresan, Noah Smith, Amy Wein-
berg and especially Philip Resnik for discussions re-
lating to this work. I am also grateful to David Chi-
ang for sharing source code of the Hiero translation
system and to the two anonymous reviewers for their
constructive comments.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Proc.
ICML 2007
Go?khan H. Bak?r, Thomas Hofmann, Bernhard
Scho?lkopf, Alexander J. Smola, Ben Taskar and
S. V. N. Vishwanathan, eds. 2007. Predicting
Structured Data. MIT Press.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing Computational Linguistics,
22(1).
Phil Blunsom, Trevor Cohn and Miles Osborne. 2008.
Discriminative Synchronous Transduction for Statisti-
cal Machine Translation In proc. ACL 2008.
Marine Carpuat and Dekai Wu. 2007a. Improving Sta-
tistical Machine Translation using Word Sense Disam-
biguation In Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL 2007)..
Marine Carpuat and Dekai Wu. 2007b. How Phrase
Sense Disambiguation outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation. In
11th Conference on Theoretical and Methodological
Issues in Machine Translation (TMI 2007).
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. ACL.
Stanley F. Chen and Joshua T. Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
35
Modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201-228.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2007. A General Regression Framework for Learning
String-to-String Mappings. In Predicting Structured
Data. MIT Press.
Brooke Cowan, Ivona Kucerova, and Michael Collins.
2006. A Discriminative Model for Tree-to-Tree Trans-
lation. In proceedings of EMNLP 2006.
J. Gao, G. Andrew, M. Johnson and K. Toutanova 2007.
A Comparative Study of Parameter Estimation Meth-
ods for Statistical Natural Language Processing. In
Proc. ACL 2007.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. Proceed-
ings of the Human Language Technology Conference
(HLT-NAACL 2003).
P. Liang, Alexandre Bouchard-Cote, D. Klein and B.
Taskar. 2006. An End-to-End Discriminative Ap-
proach to Machine Translation. In Association for
Computational Linguistics (ACL06).
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-
ization, and rotational invariance In Proceedings of
the Twenty-first International Conference on Machine
Learning
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In ACL 2003:
Proc. of the 41st Annual Meeting of the Association for
Computational Linguistics.
F. Pe?rez-Cruz, Z. Ghahramani and M. Pontil. 2007. Ker-
nel Conditional Graphical Models In Predicting Struc-
tured Data. MIT Press.
Michael Subotin. 2008. Exponential models for machine
translation. Generals paper, Department of Linguis-
tics, University of Maryland.
Charles Sutton and Andrew McCallum. 2005. Piecewise
training for undirected models. In Conference on Un-
certainty in Artificial Intelligence (UAI).
Charles Sutton and Tom Minka. 2006. Local Training
and Belief Propagation. Microsoft Research Technical
Report TR-2006-121..
Christoph Tillmann and Tong Zhang 2006. A Dis-
criminative Global Training Algorithm for Statistical
MT. In Association for Computational Linguistics
(ACL06).
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed 2006. Scalable Discriminative Learning for
Natural Language Parsing and Translation In Proceed-
ings of the 20th Annual Conference on Neural Infor-
mation Processing Systems (NIPS).
ZhuoranWang, John Shawe-Taylor, and Sandor Szedmak
2007. Kernel Regression Based Machine Translation.
In Proceedings of NAACL HLT.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy
based phrase reordering model for statistical machine
translation. In Proceedings of the 21st international
Conference on Computational Linguistics and the 44th
Annual Meeting of the ACL.
36
