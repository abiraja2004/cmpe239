Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 74?81,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Grammatical Error Correction as Multiclass Classification
with Single Model?
Zhongye Jia, Peilu Wang and Hai Zhao?
MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent Systems,
Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dongchuan Road, Shanghai 200240, China
{jia.zhongye,plwang1990}@gmail.com,zhaohai@cs.sjtu.edu.cn
Abstract
This paper describes our system in the shared
task of CoNLL-2013. We illustrate that gram-
matical error detection and correction can be
transformed into a multiclass classification
task and implemented as a single-model sys-
tem regardless of various error types with the
aid of maximum entropy modeling. Our sys-
tem achieves the F1 score of 17.13% on the
standard test set.
1 Introduction and Task Description
Grammatical error correction is the task of auto-
matically detecting and correcting erroneous word
usage and ill-formed grammatical constructions in
text (Dahlmeier et al, 2012). This task could be help-
ful for hundreds of millions of people around the world
that are learning English as a second language. Al-
though there have been much of work on grammatical
error correction, the current approaches mainly focus
on very limited error types and the result is far from
satisfactory.
The CoNLL-2013 shared task, compared with the
previous Help Our Own (HOO) tasks focusing on only
determiner and preposition errors, considers a more
comprehensive list of error types, including determiner,
preposition, noun number, verb form, and subject-
verb agreement errors. The evaluation metric used in
CoNLL-2013 is Max-Matching (M2) (Dahlmeier and
Ng, 2012) precision, recall and F1 between the system
edits and a manually created set of gold-standard ed-
its. The corpus used in CoNLL-2013 is NUS Corpus
of Learner English (NUCLE) of which the details are
described in (Dahlmeier et al, 2013).
In this paper, we describe the system submission
from the team 1 of Shanghai Jiao Tong Univer-
?This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), and the National
Basic Research Program of China (Grant No.2009CB320901
and Grant No.2013CB329401).
?Corresponding author
sity (SJT1). Grammatical error detection and correc-
tion problem is treated as multiclass classification task.
Unlike previous works (Dahlmeier et al, 2012; Ro-
zovskaya et al, 2012; Kochmar et al, 2012) that train
a model upon each error type, we use one single model
for all error types. Instead of the original error type, a
more detailed version of error types is used as class la-
bels. A rule based system generates labels from the
golden edits utilizing an extended version of Leven-
shtein edit distance. We use maximum entropy (ME)
model as classifier to obtain the error types and use
rules to do the correction. Corrections are made us-
ing rules. Finally, the corrections are filtered using lan-
guage model (LM).
2 System Architecture
Our system is a pipeline of grammatical error detection
and correction. We treats grammatical error detection
as a classification task. First all the tokens are relabeled
according to the golden annotation and a sequence of
modified version of error types is generated. This re-
labeling task is rule based using an extended version
of Levenshtein edit distance which will be discussed
in the following section. Then with the modified error
types as the class labels, a classifier using ME model
is trained. The grammatical error correction is also
rule based, which is basically the reverse of the rela-
beling phase. The modefied version of error types that
we used is much more detailed than the original five
types so that it enables us to use one rule to do the cor-
rection for each modified error type. After all, the cor-
rections are filtered by LM, to remove those corrections
that seem much worse than the original sentence.
As typical classification task, we have a training step
and a test step. The training step consists three phases:
? Error types relabeling.
? Training data refinement.
? ME training.
The test step includes three phases:
? ME classification.
74
? Error correction according to lebels.
? LM filtering.
2.1 Rebeling by Levenshtein Edit Distance
with Inflection
In CoNLL-2013 there are 5 error types but they cannot
be used directly as class labels, since they are too gen-
eral for error correction. For example, the verb form
error includes all verb form inflections such as con-
verting a verb to its infinitive form, gerund form, paste
tense, paste participle, passive voice and so on. Previ-
ous approaches (Dahlmeier et al, 2012; Rozovskaya et
al., 2012; Kochmar et al, 2012) manually decompose
each error types to more detailed ones. For example,
in (Dahlmeier et al, 2012), the determinater error is
decomposed into:
? replacement determiner (RD): { a? the }
? missing determiner (MD): { ?? a }
? unwanted determiner (UD): { a? ? }
For limited error types such as merely determinatives
error and preposition error in HOO 2012, manually de-
composition may be sufficient. But for CoNLL-2013
with 5 error types including complicated verb inflec-
tion, an automatic method to decompose error types is
needed. We present an extended version of Levenshtein
edit distance to decompose error types into more de-
tailed class labels and relabel the input with the labels
generated.
The original Levenshtein edit distance has 4 edit
types: unchange (U), addition (A), deletion (D) and
substitution (S). We extend the ?substitution? edit
into two types of edits: inflection (I) and the orig-
nal substitution (S). To judge whether two words can
be inflected from each other, the extended algorithm
needs lemmas as input. If the two words have the
same lemma, they can be inflected from each other.
The extended Levenshtein edit distance with inflec-
tion is shown in Algorithm 1. It takes the source to-
kens toksrc, destination tokens tokdst and their lemmas
lemsrc, lemdst as input and returns the edits E and the
parameters of edits P. For example, for the golden edit
{look? have been looking at}, the edits E returned by
DISTANCE will be {A,A,U ,A}, and the parameters
P of edits returned by DISTANCE will be {have, been,
looking, at}.
Then with the output of DISTANCE, the labels can
be generated by calculating the edits between original
text and golden edits. For those tokens without errors,
we directly assign a special label ??? to them. The
tricky part of the relabeling algorithm is the problem
of the edit ?addition?, A. A new token can only be
added before or after an existing token. Thus for la-
bels with addition, we must find some token that the
label can be assigned to. That sort of token is defined
as pivot. A pivot can be a token that is not changed in
Algorithm 1 Levenshtein edit distance with inflection
1: function DISTANCE(toksrc, tokdst, lemsrc,
lemdst)
2: (lsrc, ldst)? (len(toksrc), len(tokdst))
3: D[0 . . . lsrc][0 . . . ldst]? 0
4: B[0 . . . lsrc][0 . . . ldst]? (0, 0)
5: E[0 . . . lsrc][0 . . . ldst]? ?
6: for i? 1 . . . lsrc do
7: D[i][0]? i
8: B[i][0]? (i? 1, 0)
9: E[i][0]? D
10: end for
11: for j ? 1 . . . ldst do
12: D[0][j]? j
13: B[0][j]? (0, j ? 1)
14: E[0][j]? A
15: end for
16: for i? 1 . . . lsrc; j ? 1 . . . ldst do
17: if toksrc[i? 1] = tokdst[j ? 1] then
18: D[i][j]? D[i? 1][j ? 1]
19: B[i][j]? (i? 1, j ? 1)
20: E[i][j]? U
21: else
22: m ? min(D[i ? 1][j ? 1], D[i ?
1][j], D[i][j ? 1])
23: if m = D[i? 1][j ? 1] then
24: D[i][j]? D[i? 1][j ? 1] + 1
25: B[i][j]? (i? 1, j ? 1)
26: if lemsrc[i ? 1] = lemdst[j ? 1]
then
27: E[i][j]? S
28: else
29: E[i][j]? I
30: end if
31: else if m = D[i? 1][j] then
32: D[i][j]? D[i? 1][j] + 1
33: B[i][j]? (i? 1, j)
34: E[i][j]? D
35: else if m = D[i][j ? 1] then
36: D[i][j]? D[i][j ? 1] + 1
37: B[i][j]? (i, j ? 1)
38: E[i][j]? A
39: end if
40: end if
41: end for
42: (i, j)? (lsrc, ldst)
43: while i > 0 ? j > 0 do
44: insert E[i][j] into head of E
45: insert tokdst[j ? 1] into head of P
46: (i, j)? B[i][j]
47: end while
48: return (E,P)
49: end function
an edit, such as the ?apple? in edit {apple ? an ap-
ple}, or some other types of edit such as the inflection
of ?look? to ?looking? in edit {look? have been look-
75
ing at}. In the CoNLL-2013 task, the addition edits are
mostly adding articles or determinaters, so when gener-
ating the label, adding before the pivot is preferred and
only those trailing edits are added after the last pivot.
At last, the label is normalized to upper case.
The BNF syntax of labels is defined in Figure 1. The
the non-terminal ?inflection-rules? can be substituted
by terminals of inflection rules that are used for cor-
recting the error types of noun number, verb form, and
subject-verb agreement errors. All the inflection rules
are listed in Table 1. The ?stop-word? can be subsi-
tuted by terminals of stop words which contains all ar-
ticles, determinnaters and prepositions for error types
of determiner and preposition, and a small set of other
common stop words. All the stop words are listed in
Table 2.
?label? ::= ?simple-label? | ?compound-label?
?simple-label? ::= ?pivot? | ?add-before? | ?add-after?
?compound-label? ::= ?add-before? ?pivot?
| ?pivot? ?add-after?
| ?add-before? ?pivot? ?add-after?
?pivot? ::= ?inflection? | ?unchange? | ?substitution?
| ?deletion?
?add-before? ::= ?stop-word??
| ?stop-word???add-before?
?add-after? ::= ??stop-word?
| ??stop-word??add-after?
?substitution? ::= ?stop-word?
?inflection? ::= ?inflection-rules?
?unchange? ::= ?
?deletion? ::= ?
Figure 1: BNF syntax of label
Algorithm 2 is used to generate the label from the
extended Levenshtein edits according to the syntax de-
fined in Figure 1. It takes the original tokens, tokorig
and golden edit tokens, tokgold in an annotation and
their lemmas, lemorig, lemgold as input and returns the
generated label L. For our previous example of edit
{looked ? have been looking at}, the L returned by
RELABEL is {HAVE?BEEN?GERUND?AT}. Some
other examples of relabeling are demonstrated in Ta-
ble 3.
The correction step is done by rules according to the
labels. The labels are parsed with a simple LL(1) parser
and edits are made according to labels. A bit of extra
work has to be taken to handle the upper/lower case
problem. For additions and substitutions, the words
added or substituted are normalized to lowercase. For
inflections, original case of words are reserved. Then
a bunch of regular expressions are applied to correct
upper/lower case for sentence head.
Catalog Rules
Noun Number LEMMA, NPLURAL
Verb Number VSINGULAR, LEMMA
Verb Tense LEMMA, GERUND, PAST, PART
Table 1: Inflection rules
Catalog Words
Determinater a an the my your his her its our
their this that these those
Preposition about along among around as at
beside besides between by down
during except for from in inside
into of off on onto outside over
through to toward towards un-
der underneath until up upon with
within without
Modal Verb can could will would shall should
must may might
BE be am is are was were been
HAVE have has had
Other many much
Table 2: Stop words
Tokens Edit Label
to {to reveal?revealing} ?reveal GERUND
a {a woman?women} ?woman NPLURAL
developing {developing world THE?
wold ?the developing world} ?
a {a? ?} ?
in {in?on} ON
apple {apple?an apple} AN?
Table 3: Examples of relabeling
2.2 Training Corpus Refinement
The corpus used to train the grammatical error recog-
nition model is highly imbalanced. The original train-
ing corpus has 57,151 sentences and only 3,716 of
them contain at least one grammatical error, and only
5,049 of the total 1,161K words are needed to be cor-
rected. This characteristic makes it hard to get a well-
performed machine learning model, since the samples
to be recognized are so sparse and those large amount
of correct data will severely affect the machine learn-
ing process as it is an optimization on the global train-
ing data. While developing our system, we found
that only using sentences containing grammatical er-
rors will lead to a notable improvement of the result.
76
Algorithm 2 Relabeling using the extended Leven-
shtein edit distance
1: function RELABEL(tokorig , tokgold, lemorig ,
lemgold)
2: (E,P) ? DISTANCE(tokorig, tokgold,
lemorig , lemgold)
3: pivot? number of edits in E that are not A
4: L? ?
5: L? ??
6: while i < length of E do
7: if E[i] = A then
8: L? L+ label of edit E[i] with P[i]
9: i? i + 1
10: else
11: l? L+ label of edit E[i] with P[i]
12: pivot? pivot? 1
13: if pivot = 0 then
14: i? i + 1
15: while i < length of E do
16: l? l +?+ P[i]
17: i? i + 1
18: end while
19: end if
20: push l into L
21: L? ??
22: end if
23: end while
24: L? upper case of L
25: return L
26: end function
Inspired by this phenomenon, we propose a method to
refine the training corpus which will reduce the error
sparsity of the training data and notably improve the
recall rate.
The refined training corpus is composed of contexts
containing grammatical errors. To keep the informa-
tion which may have effects on the error identification
and classification, those contexts are selected based on
both syntax tree and n-gram, of which the process is
shown in Algorithm 3. For a word with error, its syntax
context of size c is those words in the minimal subtree
in the syntax tree with no less than c leaf nodes; and its
n-gram context of size n is n? 1 words before and af-
ter itself. In the CORPUSREFINE algorithm, the input c
gives the size of syntax context and n provides the size
of the n-gram context. These two parameters decide
the amount of information that may affect the recogni-
tion of errors. To obtain the context, given a sentence
containing a grammatical error, we first get a minimum
syntax tree whose number of terminal nodes exceed the
c threshold, then check whether the achieved context
containing the error word?s n-gram, if not, add the n-
gram to the context. At last the context is returned by
CORPUSREFINE.
An example illustrating this process is presented in
Figure 2. In this example, n and c are both set to 5,
Algorithm 3 Training Corpus Refine Algorithm
1: function CORPUSREFINE(sentence, c, n)
2: context? ?
3: if sentence contains no error then
4: return ?
5: end if
6: build the syntax tree T of sentence
7: enode? the node with error in T
8: e? enode
9: while True do
10: pnode? parent node of e in T
11: cnodes ? all the children nodes of pnode
in T
12: if len(cnodes) > c then
13: context? cnodes
14: break
15: end if
16: e? pnode
17: end while
18: i? the position of enode in context
19: l? size of context
20: if i < n then
21: add (n-i) words before context at the head
of context
22: end if
23: if l-i<n then
24: append (l-i) words after context in
context
25: end if
26: return context
27: end function
the minimal syntax tree and the context decided by it
are colored in green. Since the syntax context in the
green frame does not contain the error word?s 5-gram,
therefore, the 5-gram context in the blue frame is added
to the syntax context and the final achieved context of
this sentence is ?have to stop all works for the develop-
ment?.
2.3 LM Filter
All corrections are filtered using a large LM. Only
those corrections that are not too much worse than the
original sentences are accepted. Perplexity (PPL) of the
n-gram is used to judge whether a sentence is good:
PPL = 2?
?
w?n-gram p(w) log p(w),
We use rPPL, the ratio between the PPL before and
after correction, as a metric of information gain.
rPPL =
PPLcorrected
PPLoriginal
,
Only those corrections with an rPPL lower than a cer-
tain threshold are accepted.
77
N-gram Context
Then the innovators have to stop all works for the development .
S
RB
NP
VP .
DT NNS VBP
S
VP
TO
VP
VB
NP
PP
DT NNS IN
NP
DT NN
Syntax Context
Figure 2: Example of training corpus refinement
3 Features
The single model approach enables us only to optimize
one feature set for all error type in the task, which can
drastically reduce the computational cost in feature se-
lection.
As many previous works have proposed various of
features, we first collected features from different pre-
vious works including (Dahlmeier et al, 2012; Ro-
zovskaya et al, 2012; HAN et al, 2006; Rozovskaya
et al, 2011; Tetreault, Joel R and Chodorow, Martin,
2008). Then experiments with different features were
built to test these features? effectiveness and only those
have positive contribution to the final performance
were preserved. The features we used are presented in
Table 4, where word0 is the word that we are generat-
ing features for, and word and POS is the word itself
and it?s POS tag for various components. NPHead de-
notes the head of the minimum Noun Phrase (NP) in
syntax tree. wordNP?1 represents the word appearing
before NP in the sentence. NC stands for noun com-
pound and is composed of the last n words (n ? 2)
in NP which are tagged as a noun. Verb feature is
the word that is tagged as a verb whose direct object
is the NP containing current word. Adj feature repre-
sents the first word in the NP whose POS is adjective.
Prep feature denotes the preposition word if it imme-
diately precedes the NP. DPHead is the parent of the
current word in the dependency tree, and DPRel is the
dependency relation with parent.
4 Experiments
4.1 Data Sets
The CoNLL-2013 training data consist of 1,397 arti-
cles together with gold-standard annotation. The docu-
ments are a subset of the NUS Corpus of Learner En-
glish (NUCLE) (Dahlmeier et al, 2013). The official
test data consists of 50 new essays which are also from
NUCLE. The first 25 essays were written in response to
one prompt. The remaining 25 essays were written in
response to a second prompt. One of the prompts had
been used for the training data, while the other prompt
is new. More details of the data set are described in (Ng
et al, 2013).
We split the the entire training corpus ALL by article.
For our training step, we randomly pick 90% articles of
ALL and build a training corpus TRAIN of 1,258 arti-
cles. The rest 10% of ALL with 139 articles are for
developing corpus DEV .
For the final submission, we use the entire corpus
ALL for relabeling and training.
78
Feature Example
Lexical features
word0 (current word) the
wordi, (i = ?1,?2,?3) man, stared, at, old, oak,
tree
n words before word0,
(n=2, 3, 4)
stared+at,
man+stared+at. . .
n words after word0, (n=2,
3, 4)
old+oak, old+oak+tree . . .
wordi + POSi, (i =
?1,?2,?3)
stared+VBD, at+IN. . .
First word in NP the
ith word before/after NP,
(i = ?1,?2,?3)
man, stared, at, period . . .
wordNP?1 + NP at + ( the + old + oak +
tree )
Bag of words in NP old+oak+the+tree
NC oak tree
Adj + NC old oak tree
Adj POS + NC JJ+oak tree
POS features
POS0 (current word POS) NNS
POSi, (i = ?1,?2,?3) NN, VBD, IN . . .
POS?n + POS?(n?1) +
? ? ?+POS?1, (n = 2, 3, 4)
VBD + IN, NN + VBD +
IN . . .
POS1 + POS2 + ? ? ? +
POSn, (n = 2, 3, 4)
JJ + NN, JJ + NN + NNS
. . .
Bag of POS in NP DT+JJ+NN+NN
Head word features
NPHead of NP tree
NPHead POS NN
NPHead +
NPHead POS
tree+NN
Bag of POS in NP +
NPHead
DT JJ NN NN+tree
wordNP?1 + NPHead at+tree
Adj + NPHead old+tree
Adj POS + NPHead JJ+tree
wordNP?1 + Adj +
NPHead
at+old+tree
wordNP?1 +Adj POS +
NPHead
at+JJ+tree
Preposition features
Prep at
Prep + NPHead at+tree
Prep + Adj + NPHead at+old+tree
Prep + Adj POS +
NPHead
at+JJ+tree
Prep + NC at+(oak+tree)
Prep + NP at + ( the + old + oak +
tree )
Prep + NPHead POS at+NN
Prep + Adj +
NPHead POS
at+old+NN
Prep + Adj POS +
NPHead POS
at + JJ + NN
Prep + Bag of NP POS at + ( DT + JJ + NN )
Prep + NPHead POS +
NPHead
at + NN + tree
Lemma features
Lemma the
Dependency features
DPHead word tree
DPHead POS NN
DPRel det
Table 4: Features for grammatical error recognition.
The example sentence is:?That man stared at the old
oak tree.? and the current word is ?the? .
Feature Example
Verb features
Verb stared
Verb POS VBD
Verb + NPHead stared+tree
Verb + Adj + NPHead stared+old+tree
Verb + Adj POS +
NPHead
stared+JJ+tree
Verb + NP stared+(the+old+oak+tree)
Verb + Bag of NP POS stared+(DT+JJ+NN)
Table 5: Features Continued
Count Label
1146000 ?
3369 ?
3088 NPLURAL
2766 THE?
1470 LEMMA
706 A?
200?300 IN AN? THE ARE FOR TO OF
100
?200
ON A IS GERUND PAST VSINGULAR
50?100 WITH ?THE AT AN BY THIS INTO
5?50 FROM TO? WAS ABOUT WERE ?A
THESE TO?LEMMA OF? ?OF ARE?
?TO THROUGH BE?PAST AS AMONG
IN? BE? THEIR THE?LEMMA OVER
?ON HAVE? DURING FOR? ?WITH
PART ?IN HAVE WITHIN BE MANY
?AN THE?NPLURAL MUCH IS? WITH?
TOWARDS ?FOR HAVE?PART ?ABOUT
WILL ?UPON THEIR? HAVE?PAST
HAS?PART HAS? HAS BY? BEEN?
BE?? AN?LEMMA THAT? ITS HAD
FROM? BETWEEN A?LEMMA
4 WERE? UPON THOSE ON? MANY?
IS?? ?FROM CAN AS?
3 WILL?LEMMA WILL? TOWARD THIS?
THAT ITS? HAVE?? ?BE AT? ?AS
ABOUT?
2 WOULD WAS? TO?BE? THE??
ONTO IS?PAST IS?GERUND INSIDE
HAVE?BEEN? CAN?LEMMA ?BEEN
?AT
1 WOULD?LEMMA WITHOUT UN-
DER TO?THE TO?THAT?OF
TO?HAVE THIS?WILL? THIS?MAY
THIS?HAS? THESE? THE???OF
THEIR?LEMMA THE?GERUND
THE?A? THAT?HAS?BEEN?
THAT?HAS? SHOULD? ?OVER
OF?THE? OF?THE OF?GERUND OFF
OF?A? MAY? MAY IS?TO IS?LEMMA
INTO? ?INTO IN?THE? HIS HAVE?AN
HAS?PAST HAS?BEEN?GERUND
HAS?BEEN? HAD?? HAD? DURING?
COULD CAN?BE? CAN? BY?GERUND
?BY ?BETWEEN BESIDES BEEN?PART
BEEN AT?AN AT?A AS?THE AS?HAS
AROUND ARE?PAST ARE?A ARE??
A???OF AM?
Table 6: All labels after relabeling
79
4.2 Resources
We use the following NLP resources in our sys-
tem. For relabeling and correction, perl mod-
ule Lingua::EN::Inflect1 (Conway, 1998) is used
for determining noun and verb number and Lin-
gua::EN::VerbTense2 is used for determining verb
tense. A revised and extended version of maxi-
mum entropy model3 is used for ME modeling. For
lemmatization, the Stanford CoreNLP lemma annota-
tor (Toutanova et al, 2003; Toutanova and Manning,
2000) is used. The language model is built by the
SRILM toolkit (Stolcke and others, 2002). The corpus
for building LM is the EuroParl corpus (Koehn, 2005).
The English part of the German-English parallel cor-
pus is actually used. We use such a corpus to build
LM for the following reasons: 1. LM for grammatical
error correction should be trained from corpus that it-
self is grammatically correct, and the EuroParl corpus
has very good quality of writing; 2. the NUCLE cor-
pus mainly contains essays on subjects such as environ-
ment, economics, society, politics and so on, which are
in the same dormain as those of the EuroParl corpus.
4.3 Relabeling the Corpus
There are some complicated edits in the annotations
that can not be represented by our rules, for example
substitution of non-stopwords such as {human? peo-
ple} or {are not short of? do not lack}. The relabel-
ing phase will ignore those ones thus it may not cover
all the edits. After all, we get 174 labels after relabel-
ing on the entire corpus as shown in Table 6. These
labels are generated following the syntax defined in
Figure1 and terminals defined in Table 1 and Table 2.
Directly applying these labels for correction receives an
M2 score of Precission = 91.43%,Recall = 86.92%
and F1 = 89.12%. If the non-stopwords non-inflection
edits are included, of course the labels will cover all the
golden annotations and directly applying labels for cor-
rection can receive a score up to almost F1 = 100%.
But that will get nearly 1,000 labels which is too com-
putationally expensive for a classification task. Cut out
labels with very low frequency such as those exists only
once reduces the training time, but does not give signif-
icant performance improvement, since it hurts the cov-
erage of error detection. So we use all the labels for
training.
4.4 LM Filter Threshold
To choose the threshold for rPPL, we run a series of
tests on the DEV set with different thresholds. From
our empirical observation on those right corrections
and those wrong ones, we find the right ones seldomly
1http://search.cpan.org/?dconway/
Lingua-EN-Inflect-1.89/
2http://search.cpan.org/?jjnapiork/
Lingua-EN-VerbTense-3.003/
3http://www.nactem.ac.uk/tsuruoka/
maxent/
have rPPL > 2, so we test thresholds ranging from 1.5
to 3. The curves are shown in Figure 3.
 0.14
 0.145
 0.15
 0.155
 0.16
 0.165
 0.17
 1.4  1.6  1.8  2  2.2  2.4  2.6  2.8  3
Prec
ision
, Re
call 
and
 F1 c
urve
s
LM Filter Threshold
PrecisionRecallF1
Figure 3: Different thresholds of LM filters
With higher threshold, more correction with lower
information gain will be obtained. Thus the recall
grows higher but the precission grows lower. We can
observe a peak of F1 arround 1.8 to 2.0, and the thresh-
old chosen for final submission is 1.91.
4.5 Evaluation and Result
The evaluation is done by calculating the M2 precis-
sion, recall and F1 score between the system output
and golden annotation. All the error types are evalu-
ated jointly. Only one run of a team is permitted to be
submitted. Table 7 shows our result on our DEV data
set and the official test data set.
Data Set Precission Recall F1
DEV 16.03% 15.88% 15.95%
Official 40.04% 10.89% 17.13%
Table 7: Evaluation Results
5 Conclusion
In this paper, we presented the system from team 1 of
Shanghai Jiao Tong University that participated in the
HOO 2012 shared task. Our system achieves an F1
score of 17.13% on the official test set based on gold-
standard edits.
References
DM Conway. 1998. An algorithmic approach to en-
glish pluralization. In Proceedings of the Second An-
nual Perl Conference. C. Salzenberg. San Jose, CA,
O?Reilly.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
Evaluation for Grammatical Error Correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 568?572, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
80
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng
Ng. 2012. NUS at the HOO 2012 Shared Task. In
Proceedings of the Seventh Workshop on Building
Educational Applications Using NLP, pages 216?
224, Montre?al, Canada, June. Association for Com-
putational Linguistics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a Large Annotated Corpus of
Learner English: The NUS Corpus of Learner En-
glish. In Proceedings of the 8th Workshop on Inno-
vative Use of NLP for Building Educational Appli-
cations, Atlanta, Georgia, USA.
NA-RAE HAN, MARTIN CHODOROW, and CLAU-
DIA LEACOCK. 2006. Detecting Errors in En-
glish Article Usage by Non-Native Speakers. Nat-
ural Language Engineering, 12:115?129, 5.
Ekaterina Kochmar, ?istein Andersen, and Ted
Briscoe. 2012. HOO 2012 Error Recognition and
Correction Shared Task: Cambridge University Sub-
mission Report. In Proceedings of the Seventh Work-
shop on Building Educational Applications Using
NLP, pages 242?250, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
For Statistical Machine Translation. In MT summit,
volume 5.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The conll-
2013 shared task on grammatical error correction. In
Proceedings of the Seventeenth Conference on Com-
putational Natural Language Learning.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the 13th European Workshop on Natural Lan-
guage Generation, pages 263?266. Association for
Computational Linguistics.
Alla Rozovskaya, Mark Sammons, and Dan Roth.
2012. The UI System in the HOO 2012 Shared Task
on Error Correction. In Proceedings of the Seventh
Workshop on Building Educational Applications Us-
ing NLP, pages 272?280, Montre?al, Canada, June.
Association for Computational Linguistics.
Andreas Stolcke et al 2002. SRILM-An Extensible
Language Modeling Toolkit. In Proceedings of the
international conference on spoken language pro-
cessing, volume 2, pages 901?904.
Tetreault, Joel R and Chodorow, Martin. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics-
Volume 1, pages 865?872. Association for Compu-
tational Linguistics.
Kristina Toutanova and Christopher D Manning. 2000.
Enriching the Knowledge Sources Used in a Max-
imum Entropy Part-of-Speech Tagger. In Proceed-
ings of the 2000 Joint SIGDAT conference on Em-
pirical methods in natural language processing and
very large corpora: held in conjunction with the
38th Annual Meeting of the Association for Compu-
tational Linguistics-Volume 13, pages 63?70. Asso-
ciation for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich Part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173?180. Association for Compu-
tational Linguistics.
81
