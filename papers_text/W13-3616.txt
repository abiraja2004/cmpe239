Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 115?122,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
A Hybrid Model for Grammatical Error Correction 
 
 
Yang Xiang, Bo Yuan, Yaoyun Zhang*, Xiaolong Wang?, 
Wen Zheng, Chongqiang Wei 
Intelligent Computing Research Center, Key Laboratory of Network Oriented Intelligent 
Computation, Computer Science and technology Department, 
Harbin Institute of Technology Shenzhen Graduate School, 
Shenzhen, Guangdong, 518055, P.R. China 
{windseedxy, yuanbo.hitsz, xiaoni5122, zhengwen379, weichongqiang}@gmail.com  
wangxl@insun.hit.edu.cn? 
 
 
 
Abstract 
This paper presents a hybrid model for the 
CoNLL-2013 shared task which focuses on the 
problem of grammatical error correction. This 
year?s task includes determiner, preposition, 
noun number, verb form, and subject-verb 
agreement errors which is more comprehen-
sive than previous error correction tasks. We 
correct these five types of errors in different 
modules where either machine learning based 
or rule-based methods are applied. Pre-
processing and post-processing procedures are 
employed to keep idiomatic phrases from be-
ing corrected. We achieved precision of 
35.65%, recall of 16.56%, F1 of 22.61% in the 
official evaluation and precision of 41.75%, 
recall of 20.29%, F1 of 27.3% in the revised 
version. Some further comparisons employing 
different strategies are made in our experi-
ments.   
1 Introduction 
Automatic Grammatical Error Correction (GEC) 
for non-native English language learners has at-
tracted more and more attention with the devel-
opment of natural language processing, machine 
learning and big-data techniques. ?The CoNLL-
2013 shared task focuses on the problem of GEC 
in five different error types including determiner, 
preposition, noun number, verb form, and sub-
ject-verb agreement which is more complicated 
and challenging than previous correction tasks. 
Other than most previous works which concen-
trate most on determiner and preposition errors, 
more error types introduces the possibility of 
correcting multiple interacting errors such as de-
                                                 
? Corresponding author 
terminer vs. noun number and preposition vs. 
verb form. 
Generally, for GEC on annotated data such as 
the NUCLE corpus (Dahlmeier et al, 2013) in 
this year?s shared task which contains both origi-
nal errors and human annotations, there are two 
main types of approaches. One of them is the 
employment of external language materials. Alt-
hough there are minor differences on strategies, 
the main idea of this approach is to use frequen-
cies as a filter, such as n-gram counts, and take 
those phrases that have relatively high frequen-
cies as the correct ones. Typical works are shown 
in (Yi et al, 2008) and (Bergsma et al, 2009). 
Similar methods also exist in HOO shared tasks1 
such as the web 1TB n-gram features used by 
Dahlmeier and Ng (2012a) and the large-scale n-
gram model described by Heilman et al (2012). 
The other type is machine learning based ap-
proach which considers most on local context 
including syntactic and semantic features. Han et 
al. (2006) take maximum entropy as their classi-
fier and apply some simple parameter tuning 
methods. Felice and Pulman (2008) present their 
classifier-based models together with a few rep-
resentative features. Seo et al (2012) invite a 
meta-learning approach and show its effective-
ness. Dahlmeier and Ng (2011) introduce an al-
ternating structure optimization based approach. 
Most of the works mentioned above focus on 
determiner and preposition errors. Besides, Lee 
and Seneff (2008) propose a method to correct 
verb form errors through combining the features 
of parse trees and n-gram counts. To our 
knowledge, no one focused on noun form errors 
in specific researches. 
In this paper, we propose a hybrid model to 
solve the problem of GEC for five error types. 
                                                 
1 http://clt.mq.edu.au/research/projects/hoo/hoo2012 
115
Machine learning based methods are applied to 
solve determiner (ArtOrDet), preposition (Prep) 
and noun form (Nn) problems while rule-based 
methods are proposed for subject-verb agreement 
(SVA) and verb form (Vform) problems. We 
treat corrections of errors in each type as indi-
vidual sub problems the results of which are 
combined through a result combination module. 
Solutions on interacting error corrections were 
considered originally but dropped at last because 
of the bad effects brought about by them such as 
the accumulation of errors which lead to a very 
low performance. We perform feature selection 
and confidence tuning in machine learning based 
modules which contribute a lot to our perfor-
mance. Also, pre-processing and post-processing 
procedures are employed to keep idiomatic 
phrases from being corrected.  
Through experiments, we found that the result 
of the system was affected by many factors such 
as the selection of training samples and features, 
and the settings of confidence parameters in clas-
sifiers. Some of the factors make the whole sys-
tem too sensitive that it can easily be trapped into 
a local optimum. Some comparisons are shown 
in our experiments section. 
No other external language materials are in-
cluded in our model except for several NLP tools 
which will be introduced in ?5.2. We achieved 
precision of 35.65%, recall of 16.56% and F1 of 
22.61% in the official score of our submitted re-
sult. However, it was far from satisfactory main-
ly due to the ill settings of confidence parameters. 
Trying to find out a set of optimal confidence 
parameters, our model is able to reach an upper 
bound of precision of 34.23%, recall of 25.56% 
and F1 of 29.27% on the official test set. For the 
revised version, we achieved precision of 
41.75%, recall of 20.29%, and F1 of 27.3%. 
The remainder of this paper is arranged as fol-
lows. The next section introduces our system 
architecture. Section 3 describes machine learn-
ing based modules. Section 4 shows rule based 
modules. Experiments and analysis are arranged 
in Section 5. Finally, we give our discussion and 
conclusion in Section 6 and 7. 
2 System Architecture 
Initially, we treat errors of each type as individu-
al sub problems. Machine learning based meth-
ods are applied to solve ArtOrDet, Prep and Nn 
problems where similar problem solving steps 
are shared: sample generation, feature extraction, 
training, confidence tuning in development data, 
and testing. We apply some hand-crafted heuris-
tic rules in solving subject-verb agreement (SVA) 
and verb form (Vform) problems. Finally, results 
from different modules are combined together. 
The whole architecture of this GEC system is 
described in Figure 1. 
A pre-processing and a post-processing filter 
are utilized which include filters for some idio-
matic phrases extracted from the training dataset. 
The Frequent Pattern Growth Algorithm (FP-
Growth) is widely used for frequent pattern min-
ing in machine learning. In pre-processing, we 
firstly apply FP-Growth to gather the frequent 
items in the training set. Through some manual 
refinements, a few idiomatic phrases are re-
moved from the candidate set to be corrected. In 
post-processing, the idiomatic phrase list is used 
to check whether a certain collocation is still 
grammatical after several corrections are per-
formed. There are 996 idiomatic phrases in our 
list which is composed by mainly patterns from 
the training set and a series of hand-crafted ones. 
Typical phrases we extracted are in general, 
have/need to be done, on the other hand, a 
large/big number/amount of, at the same time, in 
public, etc.  
 
Figure 1. Architecture of our GEC system. 
3 Machine Learning Based Modules 
For the error types ArtOrDet, Prep and Nn, we 
choose machine learning based methods because 
we consider there is not enough evidence to di-
rectly determine which word or form to be used. 
Moreover, it is impossible to transfer all the cas-
es we encounter into rules. In this section, we 
describe our processing ideas for each error type 
respectively and then specifically introduce our 
feature selection and confidence tuning approach. 
3.1 Determiners 
Determiners in the error type ?ArtOrDet? contain 
articles a/an, the and other determiners such as 
Original texts 
Pre-processing 
Machine learning 
based modules 
Rule based mod-
ules 
Post-processing 
Corrected texts Result combination 
116
this, those, etc. This type of error accounts for a 
large proportion which is of great impact on the 
final result. We consider only articles since the 
other determiners are rarely used and the usages 
of them are sometimes ambiguous. Like ap-
proaches described in some previous works 
(Dahlmeier and Ng, 2012a; Felice and Pulman, 
2008), we assign three types a/an, the and empty 
for each article position and build a multi-class 
classifier.  
For training, developing and testing, all noun 
phrases (NPs) are chosen as candidate samples to 
be corrected. For NPs whose articles have been 
annotated in the corpus, the correct ones are their 
target categories, and for those haven?t been an-
notated, the target categories are their observed 
article types. Samples we make use of can be 
divided into two basic types in each category: 
with and without a wrong article. Two examples 
are shown below: 
with: a/empty big apples ~ empty category 
without: the United States ~ the category 
For each category in a, the, and empty, we use 
the whole with data and take samples of without 
ones from the set of correct NPs to make up 
training instances of one category. The reason 
why we make samples of the without ones is for 
the consideration that the classifier would always 
predicts the observed article and never proposes 
any corrections if given too many without sam-
ples, the case of which is mentioned in (Dahl-
meier and Ng, 2012a). However, we found that 
the ratio of with-without shows little effect in our 
model. The article a is regulated to a or an ac-
cording to pronunciation. 
Syntactic and semantic features are considered 
in feature extraction with the help of WordNet 
and the ?.conll? file provided. We adopt syntac-
tic features such as the surface word, phrase, 
part-of-speech, n-grams, constituent parse tree, 
dependency parse tree and headword of an NP; 
semantic features like noun category and hyper-
nym. Some expand operations are also done 
based on them (reference to Dahlmeier and Ng, 
2012a; Felice and Pulman, 2008). After feature 
extraction, we apply a genetic algorithm to do 
feature subset selection in order to reduce dimen-
sionality and filter out noisy features which is to 
be described in ?3.4. 
Maximum Entropy (ME) has been proven to 
behave well for heterogeneous features in natural 
language processing tasks and we adopt it to 
train our model. We have also tried several other 
classifiers including SVM, decision tree, Na?ve 
Bayes, and RankSVM but finally find ME per-
forms well and stably. It provides confidence 
scores for each category which we will make use 
of downstream.  
3.2 Prepositions 
Preposition error correction task is similar to the 
previous one except the different categories and 
corresponding features. Since there are 36 com-
mon prepositions listed by the shared task, origi-
nally, we assign 37 types including 36 preposi-
tions and empty for each preposition position and 
build a multi-class classifier. For training, devel-
oping and testing, each preposition as well as the 
empty position directly after a verb is considered 
as a candidate. Syntactic and semantic features 
extracted are similar to those in article error cor-
rection except for some specific cases for prepo-
sitions such as the verbs related to prepositions 
and the dependency relations. Similarly, we treat 
those preposition phrases with and without a cer-
tain preposition as the two types of samples in 
training (as described in ?3.1). Two examples are 
listed below: 
with: on/in the 1860s~ in category 
without: have to be done ~ to category 
Through statistics on the training data, we 
found that most prepositions have very few sam-
ples which may not contribute to the perfor-
mance at all and even bring about noise when 
assigned to wrong categories. After several 
rounds of experiments, we finally adopt a classi-
fier with seven prepositions which are frequently 
used in the whole corpus. They are on, of, in, at, 
to, with and for. As to the classifier, ME also 
outperforms the others. 
3.3 Noun Form 
Noun form may be interacting with determiners 
and verbs which may also have errors in the orig-
inal text. So errors may occur in the context fea-
tures extracted from the original text. However, 
if we use the context features that have been cor-
rected, more errors would be employed due to 
the low performance of the previous steps. 
Through statistics, we found that co-occurrence 
between two types of errors such as SVA and 
ArtOrDet only accounts for a small proportion. 
After a few experiments, we decided to give up 
interacting errors so as to avoid accumulated er-
rors.  
This is a binary classification problem. All 
head nouns in NPs are considered as candidates. 
Each category contains with and without samples 
similar to the cases in ?3.1 and ?3.2. Features are 
highly related to the deterministic factors for the 
117
head noun form such as the countability, Word-
Net type, name entity and whether there some 
specific dependency relations including det, 
amod etc.  
ME also outperforms other classifiers. 
3.4 Feature Selection Using Genetic Algo-
rithm 
Features we extracted are excessive and sparse 
after binarization. They bring noise in quality as 
well as complexity in computation and need to 
be selected a priori. In our work, it is a wrapper 
feature selection task. That is, we have to select a 
combination of features that perform well to-
gether rather than make sure each of them be-
haves well. This GEC task is interesting in fea-
ture selection because word surface features that 
are observed only once are also effective while 
we think that they overfit. Genetic algorithm 
(GA) has been proven to be useful in selecting 
wrapper features in classification (ElAlami, 2009; 
Anba-rasi et al 2010). We used GA to select fea-
tures as well as reduce feature dimensionality.  
We convert the features into a binary sequence 
in which each character represents one dimen-
sion.  Let ?1? indicates that we keep this dimen-
sion while ?0? means that we drop it, we use a 
binary sequence such as ?0111000?100? to de-
note a combination of feature dimensions. GA 
functions on the feature sequences and finally 
decides which features should be kept. The fit-
ness function we used is the evaluation measure 
F1 described in ?5.3. 
3.5 Confidence Tuning 
The Maximum Entropy classifier returns a confi-
dence score for each category given a testing 
sample. However, for different samples, the dis-
tribution of predicted scores varies a lot. For 
some samples, the classifier may have a very 
high predicted score for a certain category which 
means the classifier is confident enough to per-
form this prediction. But for some other samples, 
two or more categories may share close scores, 
the case of which means the classifier hesitates 
when telling them apart. 
We introduce a confidence tuning approach on 
the predicted results through a comparison be-
tween the observed category and the predicted 
category which is similar to the ?thresholding? 
approach described in Tetreault and Chodorow 
(2008). The main idea of the confidence tuning 
algorithm is: the choice between keep and drop is 
based on the difference between the confidence 
scores of the predicted category and the observed 
category. If this difference goes beyond a thresh-
old t, the prediction is kept while if it is under t, 
we won?t do any corrections. We believe this 
tuning strategy is especially appropriate in this 
task since to distinguish whether the observed 
category is correct or not affects a lot to the pre-
dicted result.  
The confidence threshold for each category is 
generated through a hill climbing algorithm in 
the development data aimed at maximizing F1-
meaure of the result.  
4 Rule-based Modules 
A few hand-crafted rules are applied to solve the 
verb related corrections including SVA and 
Vform. In these cases, the verb form is only re-
lated to some specific features as described by 
Lee and Seneff (2008). 
4.1 SVA 
SVA (Subject-verb-agreement) is particularly 
related to the noun subject that a verb determines. 
In the dependency tree, the number of the noun 
which has a relation nsubj with the verb deter-
mines the form of this verb. Through observation, 
we find that the verbs to be considered in SVA 
contain only bes (including am, is, are, was, 
were) and the verbs in simple present tense 
whose POSs are labeled with VBZ (singular) or 
VBP(plural).  
To pick out the noun subject is easy except for 
the verb that contained in a subordinate clause. 
We use semantic role labeling (SRL) to help 
solve this problem in which the coordinated can 
be extracted through a trace with the label ?R-
Argument?. The following Figure is an example 
generated by the SRL toolkit mate-tools (Bernd 
Bohnet, 2010)2. 
 
 
Figure 2. SRL for the demo sentence ?Jack, who 
will show me the way, is very tall.? The subject of 
the verb show can be traced through R-A0 -> A0. 
 
  However, the performance of this part is partly 
correlated with the noun form that may have er-
rors in the original text and the wrong SRL result 
brought about because of wrong sentence gram-
mars. 
                                                 
2 http://code.google.com/p/mate-tools/ 
118
4.2 Verb Form 
The cases are more complicated in the verb form 
error correction task. Modal, aspect and voice are 
all forms that should be considered for a verb. 
And sometimes, two or more forms are com-
bined together to perform its role in a sentence. 
For example, in the sentence: 
He has been working in this position for a 
long time. 
The bolded verb has been working is a com-
bination of the active voice work, the progressive 
aspect be+VBG and the perfect aspect has+VBN. 
It is a bit difficult for us to take all cases into 
consideration, so we just apply several simple 
rules and solve a subset of problems for this type. 
Some typical rules are listed below: 
1. The verb that has a dependency relation aux 
to preposition to is modified to its base form. 
2. The verb that has a dependency relation 
pcomp to preposition by is modified to its past 
form. 
3. The verb related to other prepositions (ex-
cept to and by) is modified to ~ing form. 
4. The verb depends on auxiliary do and mod-
al verb (including its inflections and negative 
form) is modified to its base form. 
We have also tried to use SRL and transitivity 
of a verb to determine the active and passive 
voice but it didn?t work well. 
5 Experiments and Analysis 
5.1 Data Description 
The NUCLE corpus introduced by NUS (Nation-
al University of Singapore) contains 1414 essays 
written by L2 students with relatively high profi-
ciency of English in which grammatical errors 
have been well annotated by native tutors. It has 
a small proportion of annotated errors which is 
much lower than other similar corpora (Dahl-
meier et al, 2013). In our experiments, we divide 
the whole corpus into 80%, 10% and 10% for 
training, developing and testing. And we use 90% 
and 10% for training and developing for the final 
test. 
5.2 External tools and corpora 
External tools we used include WordNet (Fell-
baum, 1998) for word base form and noun cate-
gory generation, Morphg (Minnen et al, 2000)3 
to generate inflections of nouns and verbs, mate-
tools (Bohnet, 2010) for SRL, Stanford-ner 
                                                 
3 http://www.informatics.sussex.ac.uk/research/groups/nlp 
/carroll/morph.html 
(Finkel et al, 2005)4 for name entity extraction 
and Longman online dictionary5  for generation 
of noun countability and verb transitivity.  
We didn?t employ any external corpora in our 
system. 
5.3 Experiments 
The performance of each machine learning mod-
ule is affected by the selection of training sam-
ples, features and confidence tuning for the max-
imum entropy classifier. All these factors con-
tribute more or less to the final performance and 
need to be carefully developed. In our experi-
ments, we focus on machine learning based 
modules and make comparisons on sample selec-
tion, confidence tuning and feature selection and 
list a series of results before and after applying 
our strategies.  
In our experiment, the performance is meas-
ured with precision, recall and F1-measure where 
1
2 precision recallF precision recall
? ?? ?
 
Precision is the amount of predicted correc-
tions that are also corrected by the manual anno-
tators divided by the whole amount of predicted 
corrections. Recall has the same numerator as 
precision while its denominator is the amount of 
manually corrected errors. They are in accord-
ance with those measurements generated by the 
official m2scorer (Dahlmeier and Ng, 2012c) to a 
great extent and easily to be integrated in our 
program. 
As we have mentioned in Section 3, we don?t 
employ all samples but make use of all with 
(with errors and annotations) instances and sam-
ple the without ones (without errors) for training. 
And the sampling for without type is totally ran-
dom without loss of generality. We apply the 
same strategy in all of these three error types 
(ArtOrDet, Prep and Nn) and try several ratios of 
with-without to find out whether this ratio has 
great impact on the final result and which ratio 
performs best. We use the 80%-10%-10% data 
(mentioned in ?5.1) for our experiments and 
make comparisons of different ratios on develop-
ing data. The experimental results are described 
in detail in Figure 3. 
Confidence tuning is applied in all these three 
error types which contributes most to the final 
performance in our model. We compare the re-
sults before and after tuning in all sample ratios 
                                                 
4 http://www-nlp.stanford.edu/software/CRF-NER.shtml 
5 http://www.ldoceonline.com/ 
119
that we designed and they are also depicted in 
Figure 3.  
Sample with:without
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.2
.4
.6
.8
precision before and after tuning
recall before and after tuning
F1 before and after tuning
 
Figure 3-1. Comparisons before and after tuning 
in ArtOrDet. 1:all means to use the whole without 
samples. 
Sample with:without
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.1
.2
.3
.4
.5
.6
presision before and after tuning
recall before and after tuning
F1 before and after tuning
 
Figure 3-2. Comparisons before and after tuning in 
Prep.  
1:1 1:2 1:3 1:6 1:8 1:10 1:all
P
R
F
0.0
.2
.4
.6
.8
1.0
precision before and after tuning
recall before and after tuning
F
1
 before and after tuning
Sample with:without
 
Figure 3-3. Comparisons before and after tuning 
in Nn. 
 
From the three groups of data in Figure 3, we 
notice that the ratio of samples has little impact 
on F1. This phenomenon shows that our conclu-
sion goes against the previous work by Dahl-
meier and Ng (2012a). We believe it is mainly 
due to our confidence tuning which makes the 
parameters vary much under different sample 
ratios, that is, if given the same parameters, the 
effect of sample ratio selection may become ob-
vious. Unfortunately, we didn?t do such a sys-
tematic comparison in our work. The improve-
ment under confidence tuning can be seen clearly 
in all ratios of with-without samples. The confi-
dence tuning algorithm employed in our work is 
better than the traditional tuning methods that 
assign a fixed threshold for each category or for 
all categories (about 1%~2% better measured by 
F1).  
However, although we are able to pick out the 
training data with a high F1 through confidence 
tuning for the developing data, it is difficult for 
us to choose a set of confidence parameters that 
also fits the test data well. Given several close 
F1s, the numerical values of denominators and 
numerators which determine the precision and 
recall can vary a lot. For example, one set that 
has a high precision and low recall may share the 
similar F1 with another set that has a low preci-
sion and high recall. Our work lacked of the de-
velopment on how to control the number of pro-
posed errors to make leverage on the perfor-
mance between developing set and testing set. It 
resulted in that the developing set and the testing 
set were not balanced at all, and our model was 
not able to keep the sample distribution as the 
training set. This is the main factor that leads to a 
low performance in our submitted result which 
can be clearly seen in Table 1. The upper bound 
performance of our system achieves precision of 
34.23%, recall of 25.56% and F1 of 29.27%, in 
which the F1 goes 7% beyond our submitted sys-
tem. We notice that results of all metrics of the 
three error types where machine learning algo-
rithms are applied improve with the simultaneous 
increase of numerators and denominators. This is 
especially noticeable in Prep. 
For the other two types SVA and Vform, we 
just apply several heuristic rules to solve a subset 
of problems and the case of Vform has not been 
solved well such as tense and voice. 
Genetic Algorithm (GA) is applied to process 
feature reduction and subset selection. This is 
done in ArtOrDet type in which we extract as 
many as 350,000 binary features. For error type 
Prep and Nn, the feature dimensionalities we 
constructed were not as high as that in ArtOrDet, 
and the improvements under GA were not obvi-
ous which we would not discuss in this work. 
Through experiments on a few sample ratios, we 
notice that feature selection using genetic algo-
rithm is able to reduce the feature dimensionality 
to about 170,000 which greatly lowers down the 
120
downstream computational complexity. However, 
the improvement contributed by GA after confi-
dence tuning is not obvious as that before confi-
dence tuning. We think it is partly because of the 
bad initialization of GA which is to be improved 
in our future work. The unfixed parameters may 
also lead to such a result which we didn?t discuss 
enough in our work. The comparison before and 
after GA is described in Figure 4. 
 
 Our submission% Upper bound% 
P(Det) 41.38(168/406) 36.44(254/697) 
R(Det) 24.35(168/690) 36.81(254/690) 
F1(Det) 30.66 36.63 
P(Prep) 13.79(4/29) 26.12(35/134) 
R(Prep) 1.29(4/311) 11.25(35/311) 
F1(Prep) 2.35 15.73 
P(Nn) 24.81(65/262) 27.27(102/374) 
R(Nn) 16.41(65/396) 25.76(102/396) 
F1(Nn) 19.76 26.49 
P(SVA) 
R(SVA) 
F1(SVA) 
24.42(21/86) 
16.94(21/124) 
20.00 
24.42(21/86) 
16.94(21/124) 
20.00 
P(Vform) 
R(Vform) 
F1(Vform) 
19.35(6/31) 
4.92(6/122) 
7.84 
19.35(6/31) 
4.92(6/122) 
7.84 
P(all) 35.65(272/763) 34.23(420/1227) 
R(all) 16.56(272/1643) 25.56(420/1643) 
F1(all) 22.61 29.27 
Table 1. Different performances according to dif-
ferent confidence parameters. Det stands for Ar-
tOrDet. 
 
Pre-processing and post-processing we pro-
pose also contribute to some extent which we 
could see from Table 2. Some idiomatic phrases 
are excluded from being corrected in pre-
processing which enhances precision while some 
are being modified in post-processing to improve 
recall. 
 
 Without pre-processing 
and post-processing% 
Final% 
P 
R 
F1 
33.72(265/768) 
16.13(265/1643) 
21.82 
35.65(272/763) 
16.56(272/1643) 
22.61 
Table 2. Comparison with and without pre-
processing and post-processing. 
 
We didn?t do much on the interacting errors 
problem since we didn?t work out perfect plans 
to solve it. So, in the result combination module, 
we just simply combine the result of each part 
together. 
Sample positive:negative
1:1 1:2 1:3 1:6
F
1
0.00
.05
.10
.15
.20
.25
.30
ME
ME+GA
ME+Tuning
ME+GA+Tuning
 
Figure 4. Comparisons before and after Genet-
ic Algorithm on ArtOrDet error type. ME, GA, 
and Tuning stand for Maximum Entropy, Ge-
netic Algorithm and confidence tuning. 
 
In the revised version, under further correc-
tions for the gold annotations, our model 
achieves precision of 41.75%, recall of 20.19% 
and F1 of 27.3%. 
6  Discussion 
Which factor contributes most to the final result 
in the problem of grammatical error correction? 
Since we didn?t include any external corpora, we 
discuss it here only according to the local classi-
fiers and context features.  
Based on our experiments, we find that, in our 
machine learning based modules, a tiny modifi-
cation of confidence parameter setting for each 
category, no matter which type of error, can have 
great impact on the final result. It results in that 
our model is much too sensitive to parameters 
which may easily lead to a poor behavior. Per-
haps a sufficient consideration of how to keep 
the distribution of samples, such as cross-
validation, may be helpful. In addition, the selec-
tion of classifiers, features and training samples 
all have effect on the result more or less, but not 
as obvious as that of the confidence threshold 
setting. 
7 Conclusion 
In this paper, we propose a hybrid model 
combining machine learning based modules and 
rule-based modules to solve the grammatical er-
ror correction task. We are able to solve a subset 
of the correction problems in which ArtOrDet 
and Nn perform better. However, our result in 
the testing data shows that our model is sensitive 
121
to parameters. How to keep the distribution of 
training samples needs to be further developed. 
Acknowledgement 
This work is supported in part by the National 
Natural Science Foundation of China (No. 612-
72383 and 61173075). 
References  
Bernd Bohnet. Top Accuracy and Fast Depend-
ency Parsing is not a Contradiction. In Pro-
ceedings of COLING, 2010. 
C. Fellbaum. WordNet: An Electronic Lexical 
Data-base. MIT Press. 1998. 
Daniel Dahlmeier, and Hwee Tou Ng. Grammat-
ical error correction with alternating structure 
optimization. In Proceedings of ACL. Associa-
tion for Computational Linguistics, 2011. 
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun 
Feng Ng. NUS at the HOO 2012 Shared Task. 
In Proceedings of the Seventh Workshop on 
Building Educational Applications Using NLP. 
Association for Computational Linguistics, 
2012a. 
Daniel Dahlmeier and Hwee Tou Ng. A beam-
search decoder for grammatical error correc-
tion. In Proceedings of the EMNLP. Associa-
tion for Computational Linguistics, 2012b. 
Daniel Dahlmeier and Hwee Tou Ng. Better 
Evaluation for Grammatical Error Correction. 
In Proceedings of NAACL, Association for 
Computational Linguistics, 2012c. 
Daniel Dahlmeier, Hwee Tou Ng and Siew Mei 
Wu. Building a Large Annotated Corpus of 
Learner English: The NUS Corpus of Learner 
English. In Proceedings of the 8th Workshop 
on Innovative Use of NLP for Building Educa-
tional Applications (BEA), 2013. 
De Felice, Rachele and Stephen G. Pulman. A 
classifier-based approach to preposition and 
determiner error correction in L2 English. In 
Proceedings of COLING. Association for 
Computational Linguistics, 2008. 
G. Minnen, J. Carroll and D. Pearce. Robust, 
applied morphological generation. In Proceed-
ings of the 1st International Natural Language 
Generation Conference, 2000. 
Jenny Rose Finkel, Trond Grenager, and Chris-
topher Manning. Incorporating Non-local In-
formation into Information Extraction Systems 
by Gibbs Sampling. In Proceedings of ACL , 
2005. 
Joel R. Tetreault and Martin Chodorow. The ups 
and downs of preposition error detection in 
ESL writing. In Proceedings of COLING, As-
sociation for Computational Linguistics, 2008. 
John Lee and Stephanie Seneff. Correcting mis-
use of verb forms. In Proceedings of ACL: 
HLT, 2008. 
M Anbarasi, E Anupriya, and NC Iyengar. En-
hanced prediction of heart disease with feature 
subset selection using genetic algorithm. In-
ternational Journal of Engineering Science 
and Technology,Vol.2(10),2010: 5370-5376. 
ME ElAlami. A filter model for feature subset 
selection based on genetic algorithm. 
Knowledge-Based Systems,Vol.22(5), 2009: 
356-362. 
Michael Heilman, Aoife Cahill, and Joel 
Tetreault. Precision isn't everything: a hybrid 
approach to grammatical error detection. In 
Proceedings of the Seventh Workshop on 
Building Educational Applications Using NLP. 
Association for Computational Linguistics, 
2012. 
Hongsuck Seo et al A meta learning approach to 
grammatical error correction. In Proceedings 
of ACL. Association for Computational Lin-
guistics, 2012. 
N.R. Han, M. Chodorow, and C. Leacock. 2006. 
Detecting errors in English article usage by 
non-native speakers. Natural Language Engi-
neering, Vol.12(02):115-129. 
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-
scale ngram models for lexical disambiguation. 
In Proceedings of IJCAI.2009. 
X. Yi, J. Gao, and W.B. Dolan. 2008. A web-
based English proofing system for English as a 
second language users. In Proceedings of 
IJCNLP.2008. 
122
