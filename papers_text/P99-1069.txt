Est imators  for Stochast ic "Unif ication-Based" Grammars* 
Mark  Johnson 
Cognitive and Linguistic Sciences 
Brown University 
S tuar t  Geman 
Applied Mathematics 
Brown University 
S tephen Canon 
Cognitive and Linguistic Sciences 
Brown University 
Zhiyi Chi 
Dept. of Statistics 
The University of Chicago 
Ste fan  R iez le r  
Inst i tut  fiir Maschinelle Sprachverarbeitung 
Universit~t Stuttgart  
Abstract 
Log-linear models provide a statistically sound 
framework for Stochastic "Unification-Based" 
Grammars (SUBGs) and stochastic versions of 
other kinds of grammars. We describe two 
computationally-tractable ways of estimating 
the parameters of such grammars from a train- 
ing corpus of syntactic analyses, and apply 
these to estimate a stochastic version of Lexical- 
Functional Grammar. 
1 Introduct ion 
Probabilistic methods have revolutionized com- 
putational inguistics. They can provide a 
systematic treatment of preferences in pars- 
ing. Given a suitable estimation procedure, 
stochastic models can be "tuned" to reflect the 
properties of a corpus. On the other hand, 
"Unification-Based" Grammars (UBGs) can ex- 
press a variety of linguistically-important syn- 
tactic and semantic onstraints. However, de- 
veloping Stochastic "Unification-based" Gram- 
mars (SUBGs) has not proved as straight- 
forward as might be hoped. 
The simple "relative frequency" estimator 
for PCFGs yields the maximum likelihood pa- 
rameter estimate, which is to say that it 
minimizes the Kulback-Liebler divergence be- 
tween the training and estimated istributions. 
On the other hand, as Abney (1997) points 
out, the context-sensitive dependencies that 
"unification-based" constraints introduce ren- 
der the relative frequency estimator suboptimal: 
in general it does not maximize the likelihood 
and it is inconsistent. 
* This research was supported by the National Science 
Foundation (SBR,-9720368), the US Army Research Of- 
fice (DAAH04-96-BAA5), and Office of Naval Research 
(N00014-97-1-0249). 
Abney (1997) proposes a Markov Random 
Field or log linear model for SUBGs, and the 
models described here are instances of Abney's 
general framework. However, the Monte-Carlo 
parameter estimation procedure that Abney 
proposes eems to be computationally imprac- 
tical for reasonable-sized grammars. Sections 3 
and 4 describe two new estimation procedures 
which are computationally tractable. Section 5 
describes an experiment with a small LFG cor- 
pus provided to us by Xerox PAaC. The log 
linear framework and the estimation procedures 
are extremely general, and they apply directly 
to stochastic versions of HPSG and other theo- 
ries of grammar. 
2 Features  in SUBGs 
We follow the statistical literature in using the 
term feature to refer to the properties that pa- 
rameters are associated with (we use the word 
"attribute" to refer to the attributes or features 
of a UBG's feature structure). Let ~ be the 
set of all possible grammatical or well-formed 
analyses. Each feature f maps a syntactic anal- 
ysis w E ~ to a real value f(w). The form of 
a syntactic analysis depends on the underlying 
linguistic theory. For example, for a PCFG w 
would be parse tree, for a LFG w would be a 
tuple consisting of (at least) a c-structure, an f- 
structure and a mapping from c-structure nodes 
to f-structure lements, and for a Chomskyian 
transformational grammar w would be a deriva- 
tion. 
Log-linear models are models in which the 
log probability is a linear combination of fea- 
ture values (plus a constant). PCFGs, Gibbs 
distributions, Maximum-Entropy distributions 
and Markov Random Fields are all examples of 
log-linear models. A log-linear model associates 
each feature fj with a real-valued parameter Oj. 
535 
A log-linear model with m features is one in 
which the likelihood P(w) of an analysis w is: 
PO(CO) -- 1 eEj= 1 ...... ojlj(~o) 
Zo 
Zo ----- Z eZJ=l ...... Ojfj(oJ) 
w'E~ 
While the estimators described below make 
no assumptions about the range of the .fi, in 
the models considered here the value of each 
feature fi(w) is the number of times a particu- 
lar structural arrangement or configuration oc- 
curs in the analysis w, so f i(w) ranges over the 
natural numbers. 
For example, the features of a PCFG are 
indexed by productions, i.e., the value f i(w) 
of feature fi is the number of times the 
ith production is used in the derivation w. 
This set of features induces a tree-structured 
dependency graph on the productions which 
is characteristic of Markov Branching Pro- 
cesses (Pearl, 1988; Frey, 1998). This tree 
structure has the important consequence that 
simple "relative-frequencies" yield maximum- 
likelihood estimates for the Oi. 
Extending a PCFG model by adding addi- 
tional features not associated with productions 
will in general add additional dependencies, de- 
stroy the tree structure, and substantially com- 
plicate maximum likelihood estimation. 
This is the situation for a SUBG, even if the 
features are production occurences. The uni- 
fication constraints create non-local dependen- 
cies among the productions and the dependency 
graph of a SUBG is usually not a tree. Conse- 
quently, maximum likelihood estimation is no 
longer a simple matter of computing relative 
frequencies. But the resulting estimation proce- 
dures (discussed in detail, shortly), albeit more 
complicated, have the virtue of applying to es- 
sentially arbitrary features--of the production 
or non-production type. That is, since estima- 
tors capable of finding maximum-likelihood pa- 
rameter estimates for production features in a 
SUBG will also find maximum-likelihood esti- 
mates for non-production features, there is no 
motivation for restricting features to be of the 
production type. 
Linguistically there is no particular reason 
for assuming that productions are the best fea- 
tures to use in a stochastic language model. 
For example, the adjunct attachment ambigu- 
ity in (1) results in alternative syntactic struc- 
tures which use the same productions the same 
number of times in each derivation, so a model 
with only production features would necessarily 
assign them the same likelihood. Thus models 
that use production features alone predict that 
there should not be a systematic preference for 
one of these analyses over the other, contrary to 
standard psycholinguistic results. 
1.a Bill thought Hillary \[vp\[vP left \] yesterday \] 
1.b Bill \[vP\[vP thought Hillary left \] yesterday \] 
There are many different ways of choosing 
features for a SUBG, and each of these choices 
makes an empirical claim about possible distri- 
butions of sentences. Specifying the features of 
a SUBG is as much an empirical matter as spec- 
ifying the grammar itself. For any given UBG 
there are a large (usually infinite) number of 
SUBGs that can be constructed from it, differ- 
ing only in the features that each SUBG uses. 
In addition to production features, the 
stochastic LFG models evaluated below used 
the following kinds of features, guided by the 
principles proposed by Hobbs and Bear (1995). 
Adjunct and argument features indicate adjunct 
and argument attachment respectively, and per- 
mit the model to capture a general argument 
attachment preference. In addition, there are 
specialized adjunct and argument features cor- 
responding to each grammatical function used 
in LFG (e.g., SUB J, OBJ, COMP, XCOMP, 
ADJUNCT, etc.). There are features indi- 
cating both high and low attachment (deter- 
mined by the complexity of the phrase being 
attached to). Another feature indicates non- 
right-branching nonterminal nodes. There is 
a feature for non-parallel coordinate structures 
(where parallelism is measured in constituent 
structure terms). Each f-structure attribute- 
atomic value pair which appears in any feature 
structure is also used as a feature. We also use 
a number of features identifying syntactic struc- 
tures that seem particularly important in these 
corpora, such as a feature identifying NPs that 
are dates (it seems that date interpretations of 
NPs are preferred). We would have liked to 
have included features concerning specific lex- 
ical items (to capture head-to-head dependen- 
cies), but we felt that our corpora were so small 
536 
that the associated parameters could not be ac- 
curately estimated. 
3 A pseudo- l i ke l ihood  es t imator  for 
log l inear  mode ls  
Suppose ~ = Wl,..-,Wn is a training cor- 
pus of n syntactic analyses. Letting f j(~) = 
~i=l,...,n fJ (wi), the log likelihood of the corpus 
and its derivatives are: 
logL0(~) = ~ Oj f j (~) -n logZo(2)  
j=l, . . . ,m 
0 log L0 (~) 
- - nEd/ j )  (3) 
oo j  
where Eo(fj) is the expected value of fj under 
the distribution determined by the parameters 
0. The maximum-likelihood estimates are the 0 
which maximize log Lo(~). The chief difficulty 
in finding the maximum-likelihood estimates i
calculating E0 (fj), which involves umming over 
the space of well-formed syntactic structures ft. 
There seems to be no analytic or efficient nu- 
merical way of doing this for a realistic SUBG. 
Abney (1997) proposes a gradient ascent, 
based upon a Monte Carlo procedure for esti- 
mating E0(fj). The idea is to generate random 
samples of feature structures from the distribu- 
tion P~i(w), where 0 is the current parameter 
estimate, and to use these to estimate E~(fj), 
and hence the gradient of the likelihood. Sam- 
ples are generated as follows: Given a SUBG, 
Abney constructs a covering PCFG based upon 
the SUBG and 0, the current estimate of 0. The 
derivation trees of the PCFG can be mapped 
onto a set containing all of the SUBG's syn- 
tactic analyses. Monte Carlo samples from the 
PCFG are comparatively easy to generate, and 
sample syntactic analyses that do not map to 
well-formed SUBG syntactic structures are then 
simply discarded. This generates a stream of 
syntactic structures, but not distributed accord- 
ing to P~(w) (distributed instead according to 
the restriction of the PCFG to the SUBG). Ab- 
ney proposes using a Metropolis acceptance- 
rejection method to adjust the distribution of 
this stream of feature structures to achieve de- 
tailed balance, which then produces a stream 
of feature structures distributed according to 
Po(w). 
While this scheme is theoretically sound, it 
would appear to be computationally impracti- 
cal for realistic SUBGs. Every step of the pro- 
posed procedure (corresponding to a single step 
of gradient ascent) requires a very large number 
of PCFG samples: samples must be found that 
correspond to well-formed SUBGs; many such 
samples are required to bring the Metropolis al- 
gorithm to (near) equilibrium; many samples 
are needed at equilibrium to properly estimate 
E0(Ij). 
The idea of a gradient ascent of the likelihood 
(2) is appealing--a simple calculation reveals 
that the likelihood is concave and therefore free 
of local maxima. But the gradient (in partic- 
ular, Ee(fj)) is intractable. This motivates an 
alternative strategy involving a data-based esti- 
mate of E0(fj): 
Ee(fj) = Ee(Ee(fj(w)ly(w))) (4) 
1 
= - ~ Ea(fj(w)ly(w) =yd(5)  
72 i=l,...,n 
where y(w) is the yield belonging to the syn- 
tactic analysis w, and Yi = y(wi) is the yield 
belonging to the i'th sample in the training cor- 
pus. 
The point is that Ee(fj(w)ly(w ) = Yi) is gen- 
erally computable. In fact, if f~(y) is the set of 
well-formed syntactic structures that have yield 
y (i.e., the set of possible parses of the string y), 
then 
Eo(fj( o)ly( ,) = = 
Ew'Ef~(yi) f J(w') e~-~k=x ...... Ok$1,(w') 
Hence the calculation of the conditional expec- 
tations only involves umming over the possible 
syntactic analyses or parses f~(Yi) of the strings 
in the training corpus. While it is possible to 
construct UBGs for which the number of pos- 
sible parses is unmanageably high, for many 
grammars it is quite manageable to enumerate 
the set of possible parses and thereby directly 
evaluate Eo(f j(w)ly(w ) = Yi). 
Therefore, we propose replacing the gradient, 
(3), by 
fj(w) - ~ Eo(fj(w)lY(W) = Yi) (6) 
i=l,...,n 
and performing a gradient ascent. Of course (6) 
is no longer the gradient of the likelihood func- 
537 
tion, but fortunately it is (exactly) the gradient 
of (the log of) another criterion: 
PLo(~) = I I  Po(w = wily(w) = yi) (7) 
i=l , . . . ,n  
Instead of maximizing the likelihood of the syn- 
tactic analyses over the training corpus, we 
maximize the conditional likelihood of these 
analyses given the observed yields. In our exper- 
iments, we have used a conjugate-gradient op- 
timization program adapted from the one pre- 
sented in Press et al (1992). 
Regardless of the pragmatic (computational) 
motivation, one could perhaps argue that the 
conditional probabilities Po(wly ) are as use- 
ful (if not more useful) as the full probabili- 
ties P0(w), at least in those cases for which 
the ultimate goal is syntactic analysis. Berger 
et al (1996) and Jelinek (1997) make this same 
point and arrive at the same estimator, albeit 
through a maximum entropy argument. 
The problem of estimating parameters for 
log-linear models is not new. It is especially dif- 
ficult in cases, such as ours, where a large sam- 
ple space makes the direct computation of ex- 
pectations infeasible. Many applications in spa- 
tial statistics, involving Markov random fields 
(MRF), are of this nature as well. In his 
seminal development of the MRF approach to 
spatial statistics, Besag introduced a "pseudo- 
likelihood" estimator to address these difficul- 
ties (Besag, 1974; Besag, 1975), and in fact our 
proposal here is an instance of his method. In 
general, the likelihood function is replaced by a 
more manageable product of conditional likeli- 
hoods (a pseudo-likelihood--hence the designa- 
tion PL0), which is then optimized over the pa- 
rameter vector, instead of the likelihood itself. 
In many cases, as in our case here, this sub- 
stitution side steps much of the computational 
burden without sacrificing consistency (more on 
this shortly). 
What are the asymptotics of optimizing a 
pseudo-likelihood function? Look first at the 
likelihood itself. For large n: 
1 logL0(~) 1 log I I  Po(wi) 
n n i=l,... ,n 
1 ~ logp0(w d
F& i=l,... ,n 
f Poo(w)logPo(w)dw (8) 
where 0o is the true (and unknown) parame- 
ter vector. Up to a constant, (8) is the nega- 
tive of the Kullback-Leibler divergence between 
the true and estimated istributions of syntac- 
tic analyses. As sample size grows, maximizing 
likelihood amofints to minimizing divergence. 
As for pseudo-likelihood: 
1 
- log PL0(~)  
n 
l l?g IX Po(w wi{y(w)=yi) 
n i=l,. . . ,n 
_-- _1 ~ logPo(w=wily(  w )=Yi )  
n i=l,... ,n 
EOo \ [ f  P0o (wly) log P0 (wly)dw\] 
So that maximizing pseudo-likelihood (at large 
samples) amounts to minimizing the average 
(over yields) divergence between the true and 
estimated conditional distributions of analyses 
given yields. 
Maximum likelihood estimation is consistent: 
under broad conditions the sequence of dis- 
tributions P0 , associated with the maximum 
r~ 
likelihood estimator for 0o given the samples 
Wl,...wn, converges to P0o. Pseudo-likelihood 
is also consistent, but in the present implemen- 
tation it is consistent for the conditional dis- 
tributions P0o (w\[y(w)) and not necessarily for 
the full distribution P0o (see Chi (1998)). It is 
not hard to see that pseudo-likelihood will not 
always correctly estimate P0o- Suppose there 
is a feature fi which depends only on yields: 
fi(w) = fi(y(w)). (Later we will refer to such 
features as pseudo-constant.) In this case, the 
derivative of PL0 (~) with respect o Oi is zero; 
PL0(~) contains no information about Oi. In 
fact, in this case any value of Oi gives the same 
conditional distribution Po(wly(w)); Oi is irrele- 
vant to the problem of choosing ood parses. 
Despite the assurance of consistency, pseudo- 
likelihood estimation is prone to over fitting 
when a large number of features is matched 
against a modest-sized training corpus. One 
particularly troublesome manifestation of over 
fitting results from the existence of features 
which, relative to the training set, we might 
term "pseudo-maximal": Let us say that a 
feature f is pseudo-maximal for a yield y iff 
538 
Vw' E ~)(y)f(w) ~ f ( J )  where w is any cor- 
rect parse of y, i.e., the feature's value on every 
correct parse w of y is greater than or equal 
to its value on any other parse of y. Pseudo- 
minimal features are defined similarly. It is easy 
to see that if f j is pseudo-maximal on each sen- 
tence of the training corpus then the param- 
eter assignment Oj = co maximizes the cor- 
pus pseudo-likelihood. (Similarly, the assign- 
ment Oj = -oo  maximizes pseudo-likelihood if 
f j  is pseudo-minimal over the training corpus). 
Such infinite parameter values indicate that the 
model treats pseudo-maximal features categori- 
cally; i.e., any parse with a non-maximal feature 
value is assigned a zero conditional probability. 
Of course, a feature which is pseudo-maximal 
over the training corpus is not necessarily 
pseudo-maximal for all yields. This is an in- 
stance of over fitting, and it can be addressed, 
as is customary, by adding a regularization term 
that promotes mall values of 0 to the objec- 
tive function. A common choice is to add a 
quadratic to the log-likelihood, which corre- 
sponds to multiplying the likelihood itself by 
a normal distribution. In our experiments, we 
multiplied the pseudo-likelihood bya zero-mean 
normal in 01,... Om, with diagonal covariance, 
and with standard eviation aj for 0j equal to 
7 times the maximum value of fj found in any 
parse in the training corpus. (We experimented 
with other values for aj, but the choice seems to 
have little effect). Thus instead of maximizing 
the log pseudo-likelihood, wechoose 0 to maxi- 
mize /3z 2 
log PL0(~) - ~ 2avJ2 (9) 
j=l,...,m J 
4 A max imum cor rect  es t imator  for 
log l inear  mode ls  
The pseudo-likelihood estimator described in 
the last section finds parameter values which 
maximize the conditional probabilities of the 
observed parses (syntactic analyses) given the 
observed sentences (yields) in the training cor- 
pus. One of the empirical evaluation measures 
we use in the next section measures the num- 
ber of correct parses selected from the set of 
all possible parses. This suggests another pos- 
sible objective function: choose ~ to maximize 
the number Co (~) of times the maximum likeli- 
hood parse (under 0) is in fact the correct parse, 
in the training corpus. 
Co(~) is a highly discontinuous function of 0, 
and most conventional optimization algorithms 
perform poorly on it. We had the most suc- 
cess with a slightly modified version of the sim- 
ulated annealing optimizer described in Press 
et al (1992). This procedure ismuch more com- 
putationally intensive than the gradient-based 
pseudo-likelihood procedure. Its computational 
difficulty grows (and the quality of solutions de- 
grade) rapidly with the number of features. 
5 Empirical evaluation 
Ron Kaplan and Hadar Shemtov at Xerox PArtC 
provided us with two LFG parsed corpora. The 
Verbmobil corpus contains appointment plan- 
ning dialogs, while the Homecentre corpus is 
drawn from Xerox printer documentation. Ta- 
ble 1 summarizes the basic properties of these 
corpora. These corpora contain packed c/f- 
structure representations (Maxwell III and Ka- 
plan, 1995) of the grammatical parses of each 
sentence with respect to Lexical-Functional 
grammars. The corpora also indicate which of 
these parses is in fact the correct parse (this 
information was manually entered). Because 
slightly different grammars were used for each 
corpus we chose not to combine the two corpora, 
although we used the set of features described in 
section 2 for both in the experiments described 
below. Table 2 describes the properties of the 
features used for each corpus. 
In addition to the two estimators described 
above we also present results from a baseline s- 
timator in which all parses are treated as equally 
likely (this corresponds tosetting all the param- 
eters Oj to zero). 
We evaluated our estimators using held-out 
test corpus ~test. We used two evaluation 
measures. In an actual parsing application a
SUBG might be used to identify the correct 
parse from the set of grammatical parses, so 
our first evaluation measure counts the number 
Co(~test) of sentences in the test corpus ~test 
whose maximum likelihood parse under the es- 
timated model 0 is actually the correct parse. 
If a sentence has 1 most likely parses (i.e., all 
1 parses have the same conditional probability) 
and one of these parses is the correct parse, then 
we score 1/l for this sentence. 
The second evaluation measure is the pseudo- 
539 
Number of sentences 
Number of ambiguous sentences 
Number of parses of ambiguous sentences 
Verbmobi l  corpus Homecent re  corpus 
540 980 
314 481 
3245 3169 
Table 1: Properties of the two corpora used to evaluate the estimators. 
Verbmobi l  corpus Homecent re  corpus 
Number of features 191 227 
Number of rule features 59 57 
Number of pseudo-constant features 19 41 
Number of pseudo-maximal features 12 4 
Number of pseudo-minimal features 8 5 
Table 2: Properties of the features used in the stochastic LFG models. The numbers of pseudo- 
maximal and pseudo-minimal features do not include pseudo-constant features. 
likelihood itself, PL~(wtest) .  The pseudo- 
likelihood of the test corpus is the likelihood of 
the correct parses given their yields, so pseudo- 
likelihood measures how much of the probabil- 
ity mass the model puts onto the correct anal- 
yses. This metric seems more relevant o ap- 
plications where the system needs to estimate 
how likely it is that the correct analysis lies in 
a certain set of possible parses; e.g., ambiguity- 
preserving translation and human-assisted dis- 
ambiguation. To make the numbers more man- 
ageable, we actually present he negative loga- 
rithm of the pseudo-likelihood rather than the 
pseudo-likelihood itself--so smaller is better. 
Because of the small size of our corpora we 
evaluated our estimators using a 10-way cross- 
validation paradigm. We randomly assigned 
sentences of each corpus into 10 approximately 
equal-sized subcorpora, each of which was used 
in turn as the test corpus. We evaluated on each 
subcorpus the parameters that were estimated 
from the 9 remaining subcorpora that served as 
the training corpus for this run. The evalua- 
tion scores from each subcorpus were summed 
in order to provide the scores presented here. 
Table 3 presents the results of the empiri- 
cal evaluation. The superior performance of 
both estimators on the Verbmobil corpus prob- 
ably reflects the fact that the non-rule fea- 
tures were designed to match both the gram- 
mar and content of that corpus. The pseudo- 
likelihood estimator performed better than the 
correct-parses timator on both corpora un- 
der both evaluation metrics. There seems to 
be substantial over learning in all these mod- 
els; we routinely improved performance by dis- 
carding features. With a small number of 
features the correct-parses timator typically 
scores better than the pseudo-likelihood estima- 
tor on the correct-parses valuation metric, but 
the pseudo-likelihood estimator always scores 
better on the pseudo-likelihood evaluation met- 
ric. 
6 Conc lus ion  
This paper described a log-linear model for 
SUBGs and evaluated two estimators for such 
models. Because stimators that can estimate 
rule features for SUBGs can also estimate other 
kinds of features, there is no particular reason to 
limit attention to rule features in a SUBG. In- 
deed, the number and choice of features trongly 
influences the performance of the model. The 
estimated models are able to identify the cor- 
rect parse from the set of all possible parses ap- 
proximately 50% of the time. 
We would have liked to introduce features 
corresponding to dependencies between lexical 
items. Log-linear models are well-suited for lex- 
ical dependencies, but because of the large num- 
ber of such dependencies substantially larger 
corpora will probably be needed to estimate 
such models. 1 
1Alternatively, it may be possible to use a simpler 
non-SUBG model of lexical dependencies timated from 
a much larger corpus as the reference distribution with 
540 
Baseline estimator 
Pseudo-likelihood estimator 
Correct-parses stimator 
Verbmobi l  corpus Homecent re  corpus 
C(~test) -logPL(~test) C(~test) - logPL(~test) 
9.7% 533 15.2% 655 
58.7% 396 58.8% 583 
53.7% 469 53.2% 604 
Table 3: An empirical evaluation of the estimators. C(~test) is the number of maximum likelihood 
parses of the test corpus that were the correct parses, and - log  PL(wtest) is the negative logarithm 
of the pseudo-likelihood f the test corpus. 
However, there may be applications which 
can benefit from a model that performs even at 
this level. For example, in a machine-assisted 
translation system a model like ours could 
be used to order possible translations o that 
more likely alternatives are presented before less 
likely ones. In the ambiguity-preserving trans- 
lation framework, a model ike this one could be 
used to choose between sets of analyses whose 
ambiguities cannot be preserved in translation. 
References 
Steven P. Abney. 1997. Stochastic Attribute- 
Value Grammars. Computational Linguis- 
tics, 23(4):597-617. 
Adam~L. Berger, Vincent J. Della Pietra, 
and Stephen A. Della Pietra. 1996. A 
maximum entropy approach to natural lan- 
guage processing. Computational Linguistics, 
22(1):39-71. 
J. Besag. 1974. Spatial interaction and the sta- 
tistical analysis of lattice systems (with dis- 
cussion). Journal of the Royal Statistical So- 
ciety, Series D, 36:192-236. 
J. Besag. 1975. Statistical analysis of non- 
lattice data. The Statistician, 24:179-195. 
Zhiyi Chi. 1998. Probability Models for Com- 
plex Systems. Ph.D. thesis, Brown University. 
Brendan J. Frey. 1998. Graphical Models for 
Machine Learning and Digital Communica- 
tion. The MIT Press, Cambridge, Mas- 
sachusetts. 
Jerry R. Hobbs and John Bear. 1995. Two 
principles of parse preference. In Antonio 
Zampolli, Nicoletta Calzolari, and Martha 
Palmer, editors, Linguistica Computazionale: 
Current Issues in Computational Linguistics: 
In Honour of Don Walker, pages 503-512. 
Kluwer. 
Frederick Jelinek. 1997. Statistical Methods for 
Speech Recognition. The MIT Press, Cam- 
bridge, Massachusetts. 
John T. Maxwell III and Ronald M. Kaplan. 
1995. A method for disjunctive constraint 
satisfaction. In Mary Dalrymple, Ronald M. 
Kaplan, John T. Maxwell III, and Annie 
Zaenen, editors, Formal Issues in Lexical- 
Functional Grammar, number 47 in CSLI 
Lecture Notes Series, chapter 14, pages 381- 
481. CSLI Publications. 
Judea Pearl. 1988. Probabalistic Reasoning in 
Intelligent Systems: Networks of Plausible 
Inference. Morgan Kaufmann, San Mateo, 
California. 
William H. Press, Saul A. Teukolsky, 
William T. Vetterling, and Brian P. Flannery. 
1992. Numerical Recipies in C: The Art of 
Scientific Computing. Cambridge University 
Press, Cambridge, England, 2nd edition. 
respect o which the SUBG model is defined, as described 
in Jelinek (1997). 
541 
