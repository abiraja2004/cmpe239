
This work investigates supervised word align-
ment methods that exploit inversion transduc-
tion grammar (ITG) constraints. We con-
sider maximum margin and conditional like-
lihood objectives, including the presentation
of a new normal form grammar for canoni-
calizing derivations. Even for non-ITG sen-
tence pairs, we show that it is possible learn
ITG alignment models by simple relaxations
of structured discriminative learning objec-
tives. For efficiency, we describe a set of prun-
ing techniques that together allow us to align
sentences two orders of magnitude faster than
naive bitext CKY parsing. Finally, we intro-
duce many-to-one block alignment features,
which significantly improve our ITG models.
Altogether, our method results in the best re-
ported AER numbers for Chinese-English and
a performance improvement of 1.1 BLEU over
GIZA++ alignments.
1 