
We introduce a large-scale semantic-network
annotation effort based on the MutliNet for-
malism. Annotation is achieved via a pro-
cess which incorporates several independent
tools including a MultiNet graph editing tool,
a semantic concept lexicon, a user-editable
knowledge-base for semantic concepts, and a
MultiNet parser. We present an evaluation
metric for these semantic networks, allowing
us to determine the quality of annotations in
terms of inter-annotator agreement. We use
this metric to report the agreement rates for a
pilot annotation effort involving three annota-
tors.
1 