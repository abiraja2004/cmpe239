
Can speaker gaze and speaker arm move-
ments be used as a practical informa-
tion source for naturalistic conversational
human?computer interfaces? To investi-
gate this question, we recorded (with eye
tracking and motion capture) a corpus of
interactions with a (wizarded) system. In
this paper, we describe the recording, anal-
ysis infrastructure that we built for such
studies, and analysis we performed on
these data. We find that with some initial
calibration, a ?minimally invasive?, sta-
tionary camera-based setting provides data
of sufficient quality to support interaction.
1 