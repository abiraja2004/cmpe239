
Probabilistic accounts of language process-
ing can be psychologically tested by com-
paring word-reading times (RT) to the con-
ditional word probabilities estimated by
language models. Using surprisal as a link-
ing function, a significant correlation be-
tween unlexicalized surprisal and RT has
been reported (e.g., Demberg and Keller,
2008), but success using lexicalized models
has been limited. In this study, phrase struc-
ture grammars and recurrent neural net-
works estimated both lexicalized and unlex-
icalized surprisal for words of independent
sentences from narrative sources. These
same sentences were used as stimuli in
a self-paced reading experiment to obtain
RTs. The results show that lexicalized sur-
prisal according to both models is a signif-
icant predictor of RT, outperforming its un-
lexicalized counterparts.
1 