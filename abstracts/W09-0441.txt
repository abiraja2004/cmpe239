
Automatic Machine Translation (MT)
evaluation metrics have traditionally been
evaluated by the correlation of the scores
they assign to MT output with human
judgments of translation performance.
Different types of human judgments, such
as Fluency, Adequacy, and HTER, mea-
sure varying aspects of MT performance
that can be captured by automatic MT
metrics. We explore these differences
through the use of a new tunable MT met-
ric: TER-Plus, which extends the Transla-
tion Edit Rate evaluation metric with tun-
able parameters and the incorporation of
morphology, synonymy and paraphrases.
TER-Plus was shown to be one of the
top metrics in NIST?s Metrics MATR
2008 Challenge, having the highest aver-
age rank in terms of Pearson and Spear-
man correlation. Optimizing TER-Plus
to different types of human judgments
yields significantly improved correlations
and meaningful changes in the weight of
different types of edits, demonstrating sig-
nificant differences between the types of
human judgments.
1 