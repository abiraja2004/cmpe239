
Recently, relaxation approaches have been
successfully used for MAP inference on NLP
problems. In this work we show how to extend
the relaxation approach to marginal inference
used in conditional likelihood training, pos-
terior decoding, confidence estimation, and
other tasks. We evaluate our approach for the
case of second-order dependency parsing and
observe a tenfold increase in parsing speed,
with no loss in accuracy, by performing in-
ference over a small subset of the full factor
graph. We also contribute a bound on the error
of the marginal probabilities by a sub-graph
with respect to the full graph. Finally, while
only evaluated with BP in this paper, our ap-
proach is general enough to be applied with
any marginal inference method in the inner
loop.
1 