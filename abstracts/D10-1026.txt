
We present three novel methods of compactly
storing very large n-gram language models.
These methods use substantially less space
than all known approaches and allow n-gram
probabilities or counts to be retrieved in con-
stant time, at speeds comparable to modern
language modeling toolkits. Our basic ap-
proach generates an explicit minimal perfect
hash function, that maps all n-grams in a
model to distinct integers to enable storage of
associated values. Extensions of this approach
exploit distributional characteristics of n-gram
data to reduce storage costs, including variable
length coding of values and the use of tiered
structures that partition the data for more effi-
cient storage. We apply our approach to stor-
ing the full Google Web1T n-gram set and all
1-to-5 grams of the Gigaword newswire cor-
pus. For the 1.5 billion n-grams of Gigaword,
for example, we can store full count informa-
tion at a cost of 1.66 bytes per n-gram (around
30% of the cost when using the current state-
of-the-art approach), or quantized counts for
1.41 bytes per n-gram. For applications that
are tolerant of a certain class of relatively in-
nocuous errors (where unseen n-grams may
be accepted as rare n-grams), we can reduce
the latter cost to below 1 byte per n-gram.
1 