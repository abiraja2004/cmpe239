
We propose a computational model of
visually-grounded spatial language under-
standing, based on a study of how people
verbally describe objects in visual scenes.
We describe our implementation of word
level visually-grounded semantics and their
embedding in a compositional parsing frame-
work. The implemented system selects the
correct referents in response to a broad range
of referring expressions for a large percentage
of test cases. In an analysis of the system?s
successes and failures we reveal how visual
context influences the semantics of utterances
and propose future extensions to the model
that take such context into account.
1 