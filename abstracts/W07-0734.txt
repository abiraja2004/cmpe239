
Meteor is an automatic metric for Ma-
chine Translation evaluation which has been
demonstrated to have high levels of corre-
lation with human judgments of translation
quality, significantly outperforming the more
commonly used Bleu metric. It is one of
several automatic metrics used in this year?s
shared task within the ACL WMT-07 work-
shop. This paper recaps the technical de-
tails underlying the metric and describes re-
cent improvements in the metric. The latest
release includes improved metric parameters
and extends the metric to support evalua-
tion of MT output in Spanish, French and
German, in addition to English.
1 