
This paper investigates new design options
for the feature space of a dependency parser.
We focus on one of the simplest and most
efficient architectures, based on a determin-
istic shift-reduce algorithm, trained with the
perceptron. By adopting second-order fea-
ture maps, the primal form of the perceptron
produces models with comparable accuracy
to more complex architectures, with no need
for approximations. Further gains in accu-
racy are obtained by designing features for
parsing extracted from semantic annotations
generated by a tagger. We provide experi-
mental evaluations on the Penn Treebank.
1 