
In this paper, we investigate the problem
of automatic generation of scientific sur-
veys starting from keywords provided by
a user. We present a system that can take
a topic query as input and generate a sur-
vey of the topic by first selecting a set
of relevant documents, and then selecting
relevant sentences from those documents.
We discuss the issues of robust evalua-
tion of such systems and describe an eval-
uation corpus we generated by manually
extracting factoids, or information units,
from 47 gold standard documents (surveys
and tutorials) on seven topics in Natural
Language Processing. We have manually
annotated 2,625 sentences with these fac-
toids (around 375 sentences per topic) to
build an evaluation corpus for this task.
We present evaluation results for the per-
formance of our system using this anno-
tated data.
1 