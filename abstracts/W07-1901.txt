
The non-verbal behaviour of an embodied
conversational agent is normally based on
recorded human behaviour. There are two
main ways that the mapping from human be-
haviour to agent behaviour has been imple-
mented. In some systems, human behaviour
is analysed, and then rules for the agent are
created based on the results of that analysis;
in others, the recorded behaviour is used di-
rectly as a resource for decision-making, us-
ing data-driven techniques. In this paper, we
implement both of these methods for select-
ing the conversational facial displays of an
animated talking head and compare them in
two user evaluations. In the first study, par-
ticipants were asked for subjective prefer-
ences: they tended to prefer the output of the
data-driven strategy, but this trend was not
statistically significant. In the second study,
the data-driven facial displays affected the
ability of users to perceive user-model tai-
loring in synthesised speech, while the rule-
based displays did not have any effect.
1 