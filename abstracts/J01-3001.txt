) 
LOCATION N (= Non-movable solid) 
DATE T (---- Abstract) 
TIME T (= Abstract) 
MONEY T (= Abstract) 
PERCENT T (---- Abstract) 
UNKNOWN Z (---- No  semantic restriction) 
The named entities identified as part of the preprocessing phase (Section 4.1) are 
used by this module, which requires first a mapping between the name types and 
LDOCE semantic odes, shown in Table 4. 
Any use of preferences for sense selection requires prior identification of the site 
in the sentence where such a relationship holds. Although prior identification was not 
done by syntactic methods in Wilks (1975), it is often easiest o think of the relation- 
ships as specified in grammatical terms, e.g., as subject-verb, verb-object, adjective- 
noun etc. We perform this step by means of a shallow syntactic analyzer (Stevenson 
1998) which finds the following grammatical relations: the subject, direct and indirect 
object of each verb (if any), and the noun modified by an adjective. Stevenson (1998) 
describes an evaluation of this system in which the relations identified were compared 
with those derived from Penn TreeBank parses (Marcus, Santorini, and Marcinkiewicz 
1993). It was found that the parser achieved 51% precision and 69% recall. 
The preference resolution algorithm begins by examining a verb and the nouns 
it dominates. Each sense of the verb applies a preference to those nouns such that 
some of their senses may be disallowed. Some verb senses will disallow all senses for 
a particular noun it dominates and these senses of the verb are immediately rejected. 
This process leaves us with a set of verb senses that do not conflict with the nouns 
that verb governs, and a set of noun senses licensed by at least one of those verb 
senses. For each noun, we then check whether it is modified by an adjective. If it is, 
we reject any senses of the adjectives which do not agree with any of the remaining 
noun senses. This approach is rather conservative in that it does not reject a sense 
unless it is impossible for it to fit into the preference pattern of the sentence. 
In order to explain this process more fully we provide a walk-through explanation 
of the procedure applied to a toy example shown in Table 5. It is assumed that the 
named-entity identifier has correctly identified John as a person and that the shallow 
parser has found the correct syntactic relations. In order to make this example as 
straightforward aspossible, we consider only the case in which the ambiguous words 
have few senses. The disambiguation process operates by considering the relations 
between the words in known grammatical relations, and before it begins we have 
essentially a set of possible senses for each word related via their syntax. This situation 
is represented by the topmost ree in Figure 4. 
Disambiguation is carried out by considering each verb sense in turn, beginning 
with run(l). As run is being used transitively, it places two restrictions on the sentence: 
first, the subject must satisfy the restriction human and the object abstract. In this 
334 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 5 
Sentence and lexicon for toy example of selectional preference resolution algorithm. 
Example sentence: 
John ran the hilly course. 
Sense Definition and Example Restriction 
John 
ran (1) 
ran (2) 
hilly (1) 
course (1) 
course (2) 
proper name 
to control an organisation run IBM 
to move quickly by foot run a marathon 
undulating terrain hilly road 
route race course 
programme of study physics course 
type:human 
subject:human object:abstract 
subject:human object:inanimate 
modifies:nonmovable sol id 
type:noumovable solid 
type:abstract 
run(l) 
restriction:human restriction:abstract 
John course(2) 
{ run(1 ),run(2) }
~ b j e c t - ~ b  I 
John { course(1),course(2) } 
f 
I adjective-noun~ 
I 
{hilly(l)} 
run(2) 
restriction:human restriction:inanimate 
John course(I) 
type:nonmovable solid 
hilly(l) 
Figure 4 
Restriction resolution in toy example. 
example, John has been identified as a named entity and marked as human, so the 
subject restriction is not broken. Note that, if the restriction were broken, then the 
verb sense run(l) would be marked as incorrect by this partial tagger and no further 
attempt would be made to resolve its restrictions. As this was not the case, we consider 
the direct-object slot, which places the restriction abst rac t  on the noun which fills it. 
course(2) fulfils this criterion, course is modif ied by hilly which expects a noun of type 
noumovable so l id .  However,  course(2) is marked abst rac t ,  which does not comply 
with this restriction. Therefore, assuming that run is being used in its second sense 
leads to a situation in which there is no set of senses which comply with all the 
restrictions placed on them; therefore run(l) is not the correct sense of run and the 
partial tagger marks this sense as wrong. This situation is represented by the tree at 
the bottom left of Figure 4. The sense course(2) is not rejected at this point since it may  
be found to be acceptable in the configuration of senses of another sense of run. 
The algorithm now assumes that run(2) is the correct sense. This implies that 
course(I) is the correct sense as it complies with the inanimate restriction that that verb 
sense places on the direct object. As well as complying with the restriction imposed 
by run(2), course(I) also complies with the one imposed by hilly(i), since nonmovable 
so l id  is subsumed by inanimate.  Therefore, assuming that the senses run(2) and 
335 
Computational Linguistics Volume 27, Number 3 
course(I) are being used does not lead to any restrictions being broken and the algo- 
rithm marks these as correct. 
Before leaving this example it is worth discussing a few additional points. The 
sense course(2) is marked as incorrect because there is no sense of run with which an 
interpretation of the sentence can be constructed using course(2). If there were further 
senses of run in our example, and course(2) was found to be suitable for those extra 
senses, then the algorithm would mark the second sense of course as correct. There is, 
however, no condition under which run(l) could be considered as correct hrough the 
consideration of further verb senses. Also, although John and hilly are not ambiguous in 
this example, they still participate in the disambiguation process. In fact they are vital 
to its success, as the correct senses could not have been identified without considering 
the restrictions placed by the adjective hilly. 
This partial tagger eturns, for all ambiguous noun, verb, and adjective occurrences 
in the text, the set of senses which satisfy the preferences imposed on those words. 
Adverbs do not have any selectional preferences in LDOCE and so are ignored by this 
partial tagger. 
4.5 Subject Codes 
Our final partial tagger is a re-implementation f the algorithm developed by Yarowsky 
(1992). This algorithm is dependent upon a categorization of words in the lexicon 
into subject areas--Yarowsky used the Roget large categories. In LDOCE, primary 
pragmatic odes indicate the general topic of a text in which a sense is likely to be 
used. For example, LN means "Linguistics and Grammar" and this code is assigned 
to some senses of words such as "ellipsis", "ablative", "bilingual" and "intransitive". 
Roget is a thesaurus, o each entry in the lexicon belongs to one of the large categories; 
but over half (56%) of the senses in LDOCE are not assigned a primary code. We 
therefore created a dummy category, denoted by --,  used to indicate a sense which 
is not associated with any specific subject area and this category is assigned to all 
senses without a primary pragmatic ode. These differences between the structures 
of LDOCE and Roget meant that we had to adapt the original algorithm reported in 
Yarowsky (1992). 
In Yarowsky's implementation, the correct subject category is estimated by apply- 
ing (6), which maximizes the sum of a Bayesian term (the fraction on the right) over 
all possible subject categories (SCat) for the ambiguous word over the words in its 
context (w). A context of 50 words on either side of the ambiguous word is used. 
ARGMAX Pr( w\[ S Cat) Pr( SCat) 
scat ~ log Pr(w) (6) 
w e context  
Yarowsky assumed the prior probability of each subject category to be constant, 
so the value Pr(SCat) has no effect on the maximization in (6), and (7) was in effect 
being maximized. 
ARCMAX Pr (w\]SCat) 
SCat ~ log Pr(w) (7) 
w e context  
By including a general pragmatic ode to deal with the lack of coverage, we created 
an extremely skewed distribution of codes across senses and Yarowsky's assumption 
that subject codes occur with equal probability is unlikely to be useful in this ap- 
plication. We gained a rough estimate of the probability of each subject category by 
determining the proportion of senses in LDOCE to which it was assigned and apply- 
ing the maximum likelihood estimate. It was found that results improved when the 
336 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
rough estimate of the likelihood of pragmatic odes was used. This procedure gener- 
ates estimates based on counts of types and it is possible that this estimate could be 
improved by counting tokens, although the problem of polysemy in the training data 
would have to be overcome in some way. 
The algorithm relies upon the calculation of probabilities gained from corpus tatis- 
tics: Yarowsky used the Grolier's Encyclopaedia, which comprised a 10 million word 
corpus. Our implementation used nearly 14 million words from the non-dialogue 
portion of the British National Corpus (Burnard 1995). Yarowsky used smoothing pro- 
cedures to compensate for data sparseness in the training corpus (detailed in Gale, 
Church, and Yarowsky \[1992b\]), which we did not implement. Instead, we attempted 
to avoid this problem by considering only words which appeared at least 10 times 
in the training contexts of a particular word. A context model is created for each 
pragmatic ode by examining 50 words on either side of any word in the corpus con- 
taining a sense marked with that code. Disambiguation is carried out by examining the 
same 100 word context window for an ambiguous word and comparing it against he 
models for each of its possible categories. Further details may be found in Yarowsky 
(1992). 
Yarowsky reports 92% correct disambiguation ver 12 test words, with an average 
of three possible Roget large categories. However, LDOCE has a higher level of aver- 
age ambiguity and does not contain as complete a thesaural hierarchy as Roget, so we 
would not expect such good results when the algorithm is adapted to LDOCE. Con- 
sequently, we implemented the approach as a partial tagger. The algorithm identifies 
the most likely pragmatic ode and returns the set of senses which are marked with 
that code. In LDOCE, several senses of a word may be marked with the same prag- 
matic code, so this partial tagger may return more than one sense for an ambiguous 
word. 
4.6 Collocation Extractor 
The final disambiguation module is the only feature-extractor in our system and is 
based on collocations. A set of 10 collocates are extracted for each ambiguous word 
in the text: first word to the left, first word to the right, second word to the left, 
second word to the right, first noun to the left, first noun to the right, first verb to 
the left, first verb to the right, first adjective to the left, and first adjective to the 
right. Some of these types of collocation were also used by Brown et al (1991) and 
Yarowsky (1993) (see Section 2.3). All collocates are searched for within the sentence 
which contains the ambiguous word. If some particular collocation does not exist for 
an ambiguous word, for example if it is the first or last word in a sentence, then a 
null value (NoColl) is stored instead. Rather than storing the surface form of the co- 
occurrence, morphological roots are stored instead, as this allows for a smaller set of 
collocations, helping to cope with data sparseness. The surface form of the ambiguous 
word is also extracted from the text and stored. The extracted collocations and surface 
form combine to represent the context of each ambiguous word. 
4.7 Combining Disambiguation Modules 
The results from the disambiguation modules (filter, partial taggers, and feature x- 
tractor) are then presented to a machine learning algorithm to combine their results. 
The algorithm we chose was the TIMBL memory-based learning algorithm (Daelemans 
et al 1999). Memory-based learning is another name for exemplar-based learning, as 
employed by Ng and Lee (Section 2.3). The TiMBL algorithm has already been used for 
various NLP tasks including part-of-speech tagging and PP-attachment (Daelemans et 
al. 1996; Zavrel, Daelemans, and Veenstra 1997). 
337 
Computational Linguistics Volume 27, Number 3 
Like PEBLS, which formed the core of Ng and Lee's LEXAS system, TiMBL classifies 
new examples by comparing them against previously seen cases. The class of the most 
similar example is assigned. At the heart of this approach is the distance metric A(X, Y) 
which computes the similarity between instances X and Y. This measure is calculated 
using the weighted overlap metric shown in (8), which calculates the total distance by 
computing the sum of the distance between each position in the feature vector. 
n 
A(X, Y) =- ~_, wi6(xi, yi) (8) 
i=1 
where: 
xl-yi if numeric, else ~ axi-  min~ ?5(xi, y i )  = i f  Xi = y i  (9) 
if xi # yi 
From (9) we can see that TiMBL treats numeric and symbolic features differently. 
For numeric features, the unweighted distance is computed as the difference between 
the values for that feature in each instance, divided by the maximum possible dis- 
tance computed over all pairs of instances in the database. 5 For symbolic features, the 
unweighted istance is 0 if they are identical, and 1 otherwise. For both numeric and 
symbolic features, this distance is multiplied by the weight for the particular feature, 
based on the Gain Ratio measure introduced by Quinlan (1993). This is a measure of 
the difference in uncertainty between the situations with and without knowledge of 
the value of that feature, as in (10). 
H(C) - ~-,v Pr(v) x H(CIv) (10) 
wi = H(v) 
Where C is the set of classifications, v ranges over all values of the feature i and 
H(C) is the entropy of the class labels. Probabilities are estimated from frequency 
of occurrence in the training data. The numerator of this formula determines the 
knowledge about the distribution of classes that is added by knowing the value of 
feature i. However, this measure can overestimate he value of features with large 
numbers of possible values. To compensate, it is divided by H(v), the entropy of the 
feature values. 
Word senses are presented to TiMBL in a feature-vector representation, with each 
sense which was not removed by the part of speech filter being represented by a 
separate vector. The vectors are formed from the following pieces of information in 
order: headword, homograph number, sense number, rank of sense (the order of the 
sense in the lexicon), part of speech from lexicon, output from the three partial tag- 
gers (simulated annealing, subject codes, and selectional restrictions), sur- 
face form of headword from the text, the ten collocates, and an indicator of whether 
the sense is appropriate or not in the context (correct or incorrect). 
Figure 5 shows the feature vectors generated for the word influence in the context 
shown. The final value in the feature vector shows whether the sense is correct or 
not in the particular context. We can see that, in this case, there is one correct sense, 
influence_l_la, the definition of which is "power to gain an effect on the mind of 
5 An earlier version of this system (Stevenson and Wilks 1999) used TiMBL version 1.0 (Daelemans et al 
1998), which supports only symbolic features. 
338 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Context 
Regard ing At lanta's new mil l ion dollar airport, the jury recommended "that when the new management  take 
charge Jan. 1 the airport be operated in a manner  that will  el iminate political influences". 
Feature Vectors 
Learning features Truth 
influence 1 la 1 n influences 1 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate correct 
influence 1 lb 2 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
influence 1 2 3 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
influence 1 3 4 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
influence 1 4 5 n influences 0 12.03 n NoColl manner NoColl eliminate NoCofl in NoColl political NoColl eliminate incorrect 
influence 1 5 6 n influences 0 12.03 n NoColl manner NoColl eliminate NoColl in NoCon political NoColl eliminate incorrect 
influence 1 6 7 n influences 0 12.03 n NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrect 
Figure 5 
Example feature-vector representation. 
or get results from, without asking or doing anything". Features 10-19 are produced 
by the collocation extractor, and these are identical since each vector is taken from 
the same content. Features 7-9 show the results of the partial taggers. The first is the 
output from simulated annealing, the second the subject  code, and the third the 
se lect iona l  res t r i c t ions .  All noun senses of influence share the same pragmatic 
code (--), and consequently this partial tagger eturns the same score for each sense. 
A final point worth noting is that in LDOCE, influence has a verb sense which the 
part-of-speech filter removed from consideration, and consequently this sense is not 
included in the feature-vector representation. 
The TiMBL algorithm is trained on tokens presented in this format. When disam- 
biguating unannotated text, the algorithm is applied to data presented in the same 
format without the classification. The unclassified vectors are then compared with all 
the training examples, and it is assigned the class of the closest one. 
5. Evaluation Strategy 
5.1 Evaluation Corpus 
The evaluation of WSD algorithms has recently become a much-studied area. Gale, 
Church, and Yarowsky (1992a), Resnik and Yarowsky (1997), and Melamed and Resnik 
(2000) each presented arguments for adopting various evaluation strategies, with 
Resnik and Yarowsky's proposal directly influencing the set-up of SENSEVAL (Kil- 
garriff 1998). At the heart of their proposals is the ability of human subjects to mark 
up text with the phenomenon i question (WSD in this case) and evaluate the results 
of computation. This linguistic phenomenon has proved to be far more elusive and 
complex than many others. We have discussed this at length elsewhere (Wilks 1997) 
and will assume here that humans can mark up text for senses to a sufficient degree. 
Kilgarriff (1993) questioned the possibility of creating sense-tagged texts, claiming the 
task to be impossible. However, it should be borne in mind that no alternative has 
yet been widely accepted and that Kilgarriff himself used the markup-and-test model 
for SENSEVAL. In the following discussion we compare the evaluation methodology 
adopted here with those proposed by others. 
339 
Computational Linguistics Volume 27, Number 3 
The standard evaluation procedure for WSD is to compare the output of the sys- 
tem against gold standard texts, but these are very labor-intensive to obtain; lexical 
semantic markup is generally considered to be a more difficult and time-consuming 
task than part-of-speech markup (Fellbaum et al 1998). Rather than expend a vast 
amount of effort on manual tagging we decided to combine two existing resources: 
SEMCOR (Landes, Leacock, and Tengi 1998), and SENSUS (Knight and Luk 1994). 
SEMCOR is a 200,000 word corpus with the content words manually tagged as part 
of the WordNet project. The semantic tagging was carried out by trained lexicogra- 
phers under disciplined conditions that attempted to keep tagging inconsistencies to
a minimum. SENSUS is a large-scale ontology designed for machine-translation a d 
was itself produced by merging the ontological hierarchies of WordNet, LDOCE (as 
derived by Bruce and Guthrie, see Section 4.4), and the Penman Upper Model (Bate- 
man et al, 1990) from ISI. To facilitate the merging of these three resources to produce 
SENSUS, Knight and Luk were required to derive a mapping between the senses in the 
two lexical resources. We used this mapping to translate the WordNet-tagged content 
words in SEMCOR to LDOCE tags. 
The mapping of senses is not one-to-one, and some WordNet synsets are mapped 
onto two or three LDOCE senses when WordNet does not distinguish between them. 
The mapping also contained significant gaps, chiefly words and senses not in the 
translation scheme. SEMCOR contains 91,808 words tagged with WordNet synsets, 
6,071 of which are proper names, which we ignored, leaving 85,737 words which 
could potentially be translated. The translation contains only 36,869 words tagged 
with LDOCE senses; however, this is a reasonable size for an evaluation corpus for the 
task, and it is several orders of magnitude larger than those used by other researchers 
working in large vocabulary WSD, for example Cowie, Guthrie, and Guthrie (1992), 
Harley and Glennon (1997), and Mahesh et al (1997). This corpus was also constructed 
without the excessive cost of additional hand-tagging and does not introduce any of 
the inconsistencies that can occur with a poorly controlled tagging strategy. 
Resnik and Yarowsky (1997) proposed to evaluate large vocabulary WSD systems 
by choosing a set of test words and providing annotated test and training examples 
for just these words, allowing supervised and unsupervised algorithms to be tested 
on the same vocabulary. This model was implemented in SENSEVAL (Kilgarriff 1998). 
However, for the evaluation of the system presented here, there would have been 
no benefit from using this strategy since it still involves the manual tagging of large 
amounts of data and this effort could be used to create a gold standard corpus in 
which all content words are disambiguated. It is possible that some computational 
techniques may evaluate well over a small vocabulary but may not work for a large 
set of words, and the evaluation strategy proposed by Resnik and Yarowsky will not 
discriminate between these cases. 
In our evaluation corpus, the most frequent ambiguous type is have, which appears 
604 times. A large number of words (2407) occur only once, and nearly 95% have 25 
occurrences or less. Table 6 shows the distribution of ambiguous types by number of 
corpus tokens. It is worth noting that, as would be expected, the observed istribution 
is highly Zipfian (Zipf 1935). 
Differences in evaluation corpora makes comparison difficult. However, some idea 
of the difficulty of WSD can be gained by calculating properties of the evaluation cor- 
pus. Gale, Church, and Yarowsky (1992a) suggest hat the lowest level of performance 
which can be reasonably expected from a WSD system is that achieved by assigning 
the most likely sense in all cases. Since the first sense in LDOCE is usually the most 
frequent, we calculate this baseline figure using a heuristic which assumes the first 
sense is always correct. This is the same baseline heuristic we used for the experiments 
340 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 6 
Occurrence of ambiguous words in the evaluation corpus. 
Occurrence Range Count 
1-25 5488 (94.6%) 
26-50 202 (3.5%) 
51-75 67 (1.2%) 
76-100 21 (0.04%) 
100-604 26 (0.4%) 
reported in Section 3, although those were for the homograph level. We applied the 
naive heuristic of always choosing the first sense in our corpus and found that 30.9% 
of senses were correctly disambiguated. 
Another measure that gives insight into an evaluation corpus is to count the av- 
erage polysemy, i.e., the number of possible senses we can expect for each ambiguous 
word in the corpus. The average polysemy is calculated by counting the sum of pos- 
sible senses for each ambiguous token and dividing by the number of tokens. This is 
represented by (11), where w ranges over all ambiguous tokens in the corpus, S(w) is 
the number of possible senses for word w, and N is the number of ambiguous tokens. 
The average polysemy for our evaluation corpus is 14.62. 
Average polysemy = ~w in text S( w) (11) 
N 
Our annotated corpus has the unusual property that more than one sense may 
be marked as correct for a particular token. This is an unavoidable side-effect of a 
mapping between lexicon senses which is not one-to-one. However, it does not imply 
that WSD is easier in this corpus than one in which only a single sense is marked 
for each token, as can be shown from an imaginary example. The worst case for a 
WSD algorithm is when each of the possible semantic tags for a given word occurs 
with equal frequency in a corpus, and so the prior probabilities exhibit a uniform, 
uninformative distribution. Then a corpus with an average polysemy of 5, and 2 senses 
marked correct on each ambiguous token, will have a baseline not less than 40%. 
However, one with an average polysemy of 2, and only a single sense on each, will 
have a baseline of at least 50%. Test corpora in which each ambiguous token has 
exactly two senses were used by Brown et al (1991), Yarowsky (1995) and others. 
Our system was tested using a technique known as 10-fold cross validation. This 
process is carried out by splitting the available data into ten roughly equal subsets. 
One of the subsets is chosen as the test data and the TiMBL algorithm is trained on the 
remainder. This is repeated ten times, so that each subset is used as test data exactly 
once, and results are averaged across all of the test runs. This technique provides two 
advantages: first, the best use can be made of the available data, and secondly, the 
computed results are more statistically reliable than those obtained by simply setting 
aside a single portion of the data for testing. 
5.2 Evaluation Metrics 
The choice of scoring metric is an important one in the evaluation of WSD algorithms. 
The most commonly used metric is the ratio of words for which the system has as- 
signed the correct sense compared to those which it attempted todisambiguate. Resnik 
and Yarowsky (1997) dubbed this the exact match metric, which is usually expressed 
341 
Computational Linguistics Volume 27, Number 3 
as a percentage calculated according to the formula in (12). 
Exact match = Number of correctly assigned senses x 100% (12) 
Number of senses assigned 
Resnik and Yarowsky criticize this metric because it assumes a WSD system com- 
mits to a particular sense. They propose an alternative metric based on cross-entropy 
that compares the probabilities for each sense as assigned by a WSD system against 
those in the gold standard text. The formula in (13) shows the method for computing 
this metric, where the WSD system has processed N words and Pr(csi) is the proba- 
bility assigned to the correct sense of word i. 
N 1 
N ~ l?g2 Pr(csi) (13) 
i=1 
This evaluation metric may be useful for disambiguation systems that assign probabil- 
ities to each sense, such as those developed by Resnik and Yarowsky, since it provides 
more information than the exact match metric. However, for systems which simply 
choose a single sense and do not measure confidence, it provides far less information. 
When a WSD assigns only one sense to a word and that sense is incorrect, hat word is 
scored as ~.  Consequently, the formula in (13) returns c~ if there is at least one word 
in the test set for which the tagger assigns a zero probability to the correct sense. For 
WSD systems which assign exactly one sense to each word, this metric returns 0 if 
all words are tagged correctly, and cx~ otherwise. This metric is potentially very useful 
for the evaluation of WSD systems that return non-zero probabilities for each possible 
sense; however, it is not useful for the metric presented in this paper and others that 
are not based on probabilistic models. 
Melamed and Resnik (2000) propose a metric for scoring WSD output when there 
may be more than one correct sense in the gold standard text, as with the evaluation 
corpus we use. They mention that when a WSD system returns more than one sense 
it is difficult to tell if they are intended to be disjunctive or conjunctive. The score 
for a token is computed by dividing the number of correct senses identified by the 
algorithm by the total it returns, making the metric equivalent to precision in infor- 
mation retrieval (van Rijsbergen 1979). 6For systems which return exactly one sense 
for each word, this equates to scoring a token as 1 if the sense returned is correct, and 
0 otherwise. For the evaluation of the system presented here, the metric proposed by 
Melamed and Resnik is then equivalent to the exact match metric. 
The exact match metric has the advantage of being widely used in the WSD lit- 
erature. In our experiments he exact match figure is computed at the LDOCE sense 
level, where the number of tokens correctly disambiguated to the sense level is di- 
vided by the number ambiguous at that level. At the homograph level, the number 
correctly disambiguated to the homograph is divided by the number which are poly- 
homographic. 
6. Performance 
Using the evaluation procedure described in the previous ection, it was found that the 
system correctly disambiguated 90% of the ambiguous instances to the fine-grained 
sense level, and in excess of 94% to the homograph level. 
6 The metric operates lightly differently for systems that assign probabilities to senses, 
342 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 7 
System results, baselines, and corpus characteristics. Sense level results are calculated over all 
polysemous words in the evaluation corpus while those reported for the homograph level are 
calculated only over polyhomographic ones. 
Entire Subcorpora 
Corpus Noun Verb Adjective Adverb 
Sense level Accuracy 90.37% 91.24% 88.38% 91.09% 70.61% 
Baseline 30.90% 34.56% 18.46% 25.76% 36.73% 
Tokens 36,774 26,091 6,465 3,310 908 
Types 5,804 4.041 1,021 1,006 125 
Average Polysemy 14.62 13.65 24.35 6.07 4.43 
Homograph level Accuracy 94.65% 94.63% 95.26% 96.89% 90.67% 
Baseline 71.24% 73.47% 60.72% 87.10% 86.87% 
Tokens 18,219 11,380 5,194 1,326 319 
Types 1,683 1,264 709 201 34 
Average Polysemy 2.52 2.32 2.81 2.95 3.13 
In order to analyze the effectiveness of our tagger in more detail, we split the 
main corpus into sub-corpora by grammatical category. In other words, we created 
four individual sub-corpora containing the ambiguous words which had been part- 
of-speech tagged as nouns, verbs, adjectives, and adverbs. The figures characterizing 
each of these corpora are shown in Table 7. The majority of the ambiguous words 
were nouns, with far fewer verbs and adjectives, and less than one thousand adverbs. 
The average polysemy for nouns, at both sense and homograph levels, is roughly 
the same as the overall corpus average although it is noticably higher for verbs at 
the sense level. At the sense level the average polysemy figures are much lower for 
adjectives and adverbs. This is because it is common for English words to act as either 
a noun or a verb and, since these are the most polysemous grammatical categories, 
the average polysemy count becomes large due to the cumulative ffect of polysemy 
across grammatical categories. However, words that can act as adjectives or adverbs 
are unlikely to be nouns or verbs. This, plus the fact that adjectives and adverbs are 
generally less polysemous in LDOCE, means that their average polysemy in text is far 
lower than it is for nouns or verbs. 
Table 7 shows the accuracy of our system over the four subcorpora. We can see 
that the tagger achieves higher results at the homograph level than the sense level 
on each of the four subcorpora, which is consistent with the result over the whole 
corpus. 
There is quite a difference in the tagger's results across the different subcorpora--  
91% for nouns and 70% for adverbs. Perhaps the learning algorithm does not perform 
as well on adverbs because that corpus is significantly smaller than the other three. 
This hypothesis was checked by testing our system on portions of each of the three 
subcorpora that were roughly equal in size to the adverb subcorpus. We found that the 
reduced data caused a slight loss of accuracy on each of the three subcorpora; how- 
ever, there was still a marked difference between the results for the adverb subcorpus 
and the other three. Further analysis showed that the differences in performance over 
different subcorpora seem linked to the behavior of different partial taggers when 
used in combination. In the following section we describe this behavior in more de- 
tail. 
343 
Computational Linguistics Volume 27, Number 3 
6.1 Interaction of Knowledge Sources 
In order to gauge the contribution of each knowledge source separately, we imple- 
mented a set of simple disambiguation algorithms, each of which uses the output 
from a single partial tagger. Each algorithm takes the result of its partial tagger and 
checks it against the disambiguated text to see if it is correct. If the partial tagger eturns 
more than one sense, as do the simulated annealing, subject code and se lect iona l  
preference taggers, the first sense is taken to break the tie. For the partial tagger based 
on Yarowsky's ubject-code algorithm, we choose the sense with the highest saliency 
value. If more than one sense has been assigned the maximum value, the tie is again 
broken by choosing the first sense. Therefore, each partial tagger eturns a single sense 
and the exact match metric is used to determine the proportion of tokens for which 
that tagger eturns the correct sense. The part-of-speech filter is run before the partial 
taggers make their decision and so they only consider the set of senses it did not re- 
move. The results of each tagger, computed at both sense and homograph levels over 
the evaluation corpus and four subcorpora, re shown in Table 7. 
We can see that the partial taggers that are most effective are those based on the 
simulated annealing algorithm and Yarowsky's ubject code approach. The success of 
these modules upports our decision to use existing disambiguation algorithms that 
have already been developed rather than creating new ones. 
The most successful of the partial taggers is the one based on Yarowsky's algorithm 
for modelling thesaural categories by wide contexts. This consistently achieves over 
70% correct disambiguation a d seems particularly successful when disambiguating 
adverbs (over 85% correct). It is quite surprising that this algorithm is so successful for 
adverbs, since it would seem quite reasonable to expect an algorithm based on subject 
codes to be more successful on nouns and less so on modifiers uch as adjectives and 
adverbs. 
Yarowsky (1992) reports that his algorithm achieves 92% correct disambiguation, 
which is nearly 13% higher than achieved in our implementation. However, Yarowsky 
tested his implementation  a restricted vocabulary of 12 words, the majority of which 
were nouns, and used Roget large categories as senses. The baseline performance for 
this corpus is 66.5%, considerably higher than the 30.9% computed for the corpus 
used in our experiments. Another possible reason for the difference in results is the 
fact that Yarowsky used smoothing algorithms to avoid problems with the probability 
estimates caused by data sparseness. We did not employ these procedures and used 
simple corpus frequency counts when calculating the probabilities ( ee Section 4.5). It 
is not possible to say for sure that the differences between implementations did not 
lead to the differences in results, but it seems likely that the difference in the semantic 
granularity of LDOCE subject codes and Roget categories was an important factor. 
The second partial tagger based on an existing approach is the one which uses 
simulated annealing to optimize the overlap of words shared by the dictionary defini- 
tions for a set of senses. In Section 4.3 we noted that Cowie et al (1992) reported 47% 
correct disambiguation to the sense level using this technique, while in our adaptation 
over 17% more words are correctly disambiguated. Our application filtered out senses 
with the incorrect part of speech in addition to using a different method to calculate 
overlap that takes account of short definitions. It seems likely that these changes are 
the source of the improved results. 
Our least successful partial tagger is the one based on selectional preferences. 
Although its overall result is slightly below the overall corpus baseline, it is very suc- 
cessful at disambiguating verbs. This is consistent with the work of Resnik (1997), who 
reported that many words do not have strong enough selectional restrictions to carry 
out WSD. We expected preferences to be successful for adjectives as well, although 
344 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
Table 8 
Performance of individual partial taggers (at sense level). 
All Nouns Verbs Adjectives Adverbs 
simulated annealing (I) 65.24% 66.50% 67.51% 49.02% 50.61% 
selectional preferences (2) 44.85% 40.73% 75.80% 27.56% 0% 
subject codes (3) 79.41% 79.18% 72.75% 73.73% 85.50% 
this is not the case in our evaluation. This is because the sense discrimination of ad- 
jectives is carried out after that for nouns in our algorithm (see Section 4.4), and the 
former is hindered by the low results of the latter. Adverbs cannot be disambiguated 
by preference methods against LDOCE because it does not contain the appropriate 
information. 
Our analysis of the behavior of the individual partial taggers provides ome clues 
to the behavior of the overall system, consisting of all taggers, on the different sub- 
corpora, as shown in Table 7. The system performs to roughly the same level over 
the noun, verb, and adjective sub-corpora with only a 3% difference between the best 
and worst performance. The system's worst performance is on the abverb sub-corpus, 
where it disambiguates only slightly more than 70% of tokens successfully. This may 
be due to the fact that only two partial taggers provide evidence for this grammatical 
category. However, the system still manages to disambiguate most of the adverbs to the 
homograph level successfully, and this is probably because the part-of-speech filter has 
ruled out the incorrect homographs, not because the partial taggers performed well. 
One can legitimately wonder whether in fact the different knowledge sources for 
WSD are all ways of encoding the same semantic information, in a similar way that 
one might suspect ransformation rules and statistics encode the same information 
about part-of-speech tag sequences in different formats. However, the fact that an op- 
timized combination ofour partial taggers yields a significantly higher figure than any 
one tagger operating independently, shows that they must be orthogonal information 
sources. 
6.2 The overall value of the part-of-speech filter 
We have already examined the usefulness of part-of-speech tags for semantic disam- 
biguation in Section 3. However, we now want to know the effect it has within a 
system consisting of several disambiguation modules. It was found that accuracy at 
the sense level reduced to 87.87% and to 93.36% at the homograph level when the 
filter was removed. Although the system's performance did not decrease by a large 
amount, the part-of-speech filter brings the additional benefit of reducing the search 
space for the three partial taggers. In addition, the fact that these results are not af- 
fected much by the removal of the part-of-speech filter, shows that the WSD modules 
alone do a reasonable job of resolving part-of-speech ambiguity as a side-effect of 
semantic disambiguation. 
7. Conclusion 
Previously reported WSD systems that enjoyed a high level of accuracy have often 
operated on restricted vocabularies and employed a single WSD methodology. These 
methods have often been pursued for sound reasons to do with evaluation, but have 
been limited in their applicability and also in their persuasiveness regarding the scal- 
345 
Computational Linguistics Volume 27, Number 3 
ability and interaction of the various WSD partial methods. This paper reported a 
system which disambiguated all content words in a text, as defined by a standard 
machine readable dictionary, with a high degree of accuracy. 
Our evaluation shows that disambiguation can be carried out with more accurate 
results when several knowledge sources are combined. It remains unclear exactly what 
it means to optimize the combination of modules within a learning system like T?MBL: 
we could, in further work, treat the part-of-speech tagger as a partial tagger and not 
a filter, and we could allow the system to learn some "optimal" weighting of all 
the partial taggers. It also remains an interesting question whether, because of the 
undoubted existence of novel senses in text, a sense tagger can ever reach the level 
that part-of-speech tagging has. However, we believe we have shown that interesting 
combinations of WSD methods on a substantial training corpus are possible, and that 
this can show, among other things, the relative independence of the types of semantic 
information expressed by the various forms of lexical input. 
Acknowledgments 
The work described here was supported by 
the European Union Language Engineering 
project ECRAN - Extraction of Content: 
Research at Near-market (LE-2110). One of 
the authors was also supported by the 
EPSRC grant MALT (GR/M73521) while 
writing this paper. We are grateful for the 
feedback from many colleagues in Sheffield, 
especially Mark Hepple, and for the 
detailed comments from the anonymous 
reviewers of an earlier version of this paper. 
Gillian Callaghan was extremely helpful in 
the preparation of the final version of this 
paper. Any errors are our own. 
References 
Bateman, John, Robert Kasper, Joharu~a 
Moore, and Richard Whimey. 1990. A 
general organization of knowledge for 
natural language processing: the 
PENMAN upper model, Technical report, 
USC/Information Sciences Institute, 
Marina del Rey, CA. 
Brill, Eric. 1995. Transformation-based 
error-driven learning and natural 
language processing: A case study in part 
of speech tagging. Computational 
Linguistics, 21(4):543-566. 
Brown, Peter, Stephen Della Pietra, Vincent 
Della Pietra, and Robert Mercer. 1991. 
Word sense disambiguation using 
statistical methods. In Proceedings ofthe 
29th Meeting of the Association for 
Computational Linguistics (ACL-91), 
pages 264-270, Berkeley, CA. 
Bruce, Rebecca nd Louise Guthrie. 1992. 
Genus disambiguation: A study in 
weighted performance. In Proceedings of
the 14th International Conference on 
Computational Linguistics (COLING-92), 
pages 1187-1191, Nantes, France. 
Bruce, Rebecca nd Janyce Wiebe. 1994. 
Word-sense disambiguation using 
decomposable models. In Proceedings ofthe 
32nd Annual Meeting of the Association for 
Computational Linguistics (ACL-94), 
pages 139-145, Las Cruces, New Mexico. 
Burnard, Lou. 1995. Users Reference Guide for 
the British National Corpus. Oxford 
University Computing Services. 
Chapman, R. L. 1977. Roget's International 
Thesaurus Fourth Edition, Thomas Y. 
Crowell Company, New York, NY. 
Cost, Scott and Steven Salzberg. 1993. A 
weighted nearest neighbour algorithm for 
learning with symbolic features. Machine 
Learning, 10(1):57-78. 
Cottrell, Garrison. 1984. A model of lexical 
access of ambiguous words. In Proceedings 
of the National Conference on Artificial 
Intelligence (AAAI-84), pages 61-67, 
Austin, TX. 
Cowie, Jim, Louise Guthrie, and Joe 
Guthrie. 1992. Lexical disambiguation 
using simulated annealing. In Proceedings 
of the 14th International Conference on 
Computational Linguistics (COLING-92), 
pages 359-365, Nantes, France. 
Daelemans, Walter, Jakub Zavrel, Peter 
Berck, and Steven Gillis. 1996. MBT: A 
memory-based part of speech tagger 
generator. In Proceedings ofthe Fourth 
Workshop on Very Large Corpora, 
pages 14-27, Copenhagen. 
Daelemans, Walter, Jakub Zavrel, Ko van 
der Sloot, and Antal van den Bosch. 1998. 
TiMBL: Tilburg memory based learner 
version 1.0. Technical report, University 
of Tilburg Technical Report 98-03. 
Daelemans, Walter, Jakub Zavrel, Ko van 
der Sloot, and Antal van den Bosch. 1999. 
TiMBL: Tilburg memory based learner, 
version 2.0, reference guide. Technical 
346 
Stevenson and Wilks Interaction of Knowledge Sources in WSD 
report, University of Tilburg Technical 
Report 99-01. Available from ht tp : / / i l k .  
kub. nl/~ ilk/papers/ilk990 I. ps. 
Fellbaum, Christiane, Joachim Grabowski, 
Shari Landes, and A. Baumann. 1998. 
Matching words to senses in WordNet: 
Naive vs. expert differentiation of senses. 
In Christiane Fellbaum, editor, WordNet: 
An Electronic Lexical Database and Some 
Applications. MIT Press, Cambridge, MA. 
Gaizauskas, Robert, Takahiro Wakao, Kevin 
Humphreys, Hamish Cunningham, and 
Yorick Wilks. 1996. Description of the 
LaSIE system as used for MUC-6. In 
Proceedings ofthe Sixth Message 
Understanding Conference (MUC-6), 
pages 207-220, San Francisco, CA. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992a. Estimating upper and 
lower bounds on the performance of
word sense disambiguation programs. In 
Proceedings ofthe 30th Annual Meeting of the 
Association for Computational Linguistics 
(ACL-92), pages 249-256, Newark, DE. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992b. A method for 
disambiguating word senses in a large 
corpus. Computers and the Humanities, 
26:415-439. 
Gale, William, Kenneth Church, and David 
Yarowsky. 1992c. One sense per discourse. 
In Proceedings ofthe DARPA Speech and 
Natural Language Workshop, pages 233-237, 
Harriman, NY. 
Guo, Cheng-Ming. 1989. Constructing a 
Machine Tractable Dictionary from 
Longman Dictionary of Contemporary 
English. Technical Report MCCS-89-156, 
Computing Research Laboratory, New 
Mexico State University. 
Harley, Andrew and Dominic Glennon. 
1997. Sense tagging in action: Combining 
different ests with additive weights. In 
Proceedings ofthe SIGLEX Workshop 
"Tagging Text with Lexical Semantics", 
pages 74-78, Washington, DC. 
Hirst, Graeme. 1987. Semantic Interpretation 
and the Resolution of Ambiguity. Cambridge 
University Press, Cambridge, UK. 
Hirst, Graeme. 1995. Near-synonymy and 
the structure of lexical knowledge. In 
American Association for Artificial Intelligence 
Spring Symposium on Lexicons, pages 51-56. 
Ide, Nancy and Jean V4ronis. 1998. 
