
In this paper I argue in favour of a col-
location extraction approach to the acquisi-
tion of relational nouns in German. We an-
notated frequency-based best lists of noun-
preposition bigrams and subsequently trained
different classifiers using (combinations of)
association metrics, achieving a maximum F-
measure of 69.7 on a support vector machine
(Platt, 1998). Trading precision for recall, we
could achieve over 90% recall for relational
noun extraction, while still halving the anno-
tation effort.
1 Mining relational nouns: almost a MWE
extraction problem
A substantial minority of German nouns are char-
acterised by having an internal argument structure
that can be expressed as syntactic complements. A
non-negligeable number of relational nouns are de-
verbal, inheriting the semantic argument structure of
the verbs they derive from. In contrast to verbs, how-
ever, complements of nouns are almost exclusively
optional.
The identification of relational nouns is of great
importance for a variety of content-oriented applica-
tions: first, precise HPSG parsing for German can-
not really be achieved, if a high number of noun
complements is systematically analysed as modi-
fiers. Second, recent extension of Semantic Role La-
beling to the argument structure of nouns (Meyers
et al, 2004) increases the interest in lexicographic
methods for the extraction of noun subcategorisa-
tion information. Third, relational nouns are also
a valuable resource for machine translation, sepa-
rating the more semantic task of translating modi-
fying prepositions from the more syntactic task of
translating subcategorised for prepositions. Despite
its relevance for accurate deep parsing, the German
HPSG grammar developed at DFKI (Mu?ller and
Kasper, 2000; Crysmann, 2003; Crysmann, 2005)
currently only includes 107 entries for proposition
taking nouns, and lacks entries for PP-taking nouns
entirely.
In terms of subcategorisation properties, rela-
tional nouns in German can be divided up into 3
classes:
? nouns taking genitival complements (e.g., Be-
ginn der Vorlesung ?beginning of the lecture?,
Zersto?rung der Stadt ?destruction of the city? )
? nouns taking propositional complements, ei-
ther a complementiser-introduced finite clause
(der Glaube, da? die Erde flach ist ?the belief
that earth is flat?), or an infinitival clause (die
Hoffnung, im Lotto zu gewinnen ?the hope to
win the lottery?), or both
? nouns taking PP complements
In this paper, I will be concerned with nouns tak-
ing prepositional complements, although the method
described here can also be easily applied to the case
of complementiser-introduced propositional com-
plements.1
1In fact, I expect the task of mining relational nouns tak-
ing finite propositional complements to be far easier, owing to
a reduced ambiguity of the still relatively local complementiser
65
The prepositions used with relational nouns all
come from a small set of basic prepositions, mostly
locative or directional.
A characteristic of these prepositions when used
as a noun?s complement, is that their choice becomes
relatively fixed, a property shared with MWEs in
general. Furthermore, choice of preposition is of-
ten arbitrary, sometimes differing between relational
nouns and the verbs they derive from, e.g., Interesse
an ?lit: interest at? vs. interessieren fu?r ?lit: to inter-
est for?. Owing to the lack of alternation, the prepo-
sition by itself does not compositionally contribute
to sentence meaning, its only function being the en-
coding of a thematic property of the noun. Thus, in
syntacto-semantic terms, we are again dealing with
prototypical MWEs.
The fact that PP complements of nouns, like mod-
ifiers, are syntactically optional, together with the
fact that their surface form is indistinguishable from
adjunct PPs, makes the extraction task far from triv-
ial. It is clear that grammar-based error mining tech-
niques (van Noord, 2004; Cholakov et al, 2008)
that have been highly successful in other areas of
deep lexical acquisition (e.g., verb subcategorisa-
tion) cannot be applied here: first, given that an al-
ternative analysis as a modifier is readily available
in the grammar, missing entries for relational nouns
will never incur any coverage problems. Further-
more, since PP modifiers are highly common we
cannot expect a decrease in tree probability either.
Instead, I shall exploit the MWE-like properties of
relational nouns, building on the expectation that the
presence of a subcategorisation requirement towards
a fixed, albeit optional, prepositional head should
leave a trace in frequency distributions. Thus, build-
ing on previous work in MWE extraction, I shall
pursue a data-driven approach that builds on a va-
riety of association metrics combined in a proba-
bilistic classifier. Despite the difference of the task,
da?. Although complement that-clauses in German can indeed
can be extraposed, corpus studies on relative clause extraposi-
tion (Uszkoreit et al, 1998) have shown that the great majority
of extrapositions operates at extremely short surface distance,
typically crossing the verb or verb particle in the right sentence
bracket. Since locality conditions on complement clause extra-
position are more strict than those for relative clause extrapo-
sition (Kiss, 2005; Crysmann, to appear), I conjecture that the
actual amount of non-locality found in corpora will be equally
limited.
the approach suggested here shares some significant
similarity to previous classifier-based approaches to
MWE (Pecina, 2008).
2 Data
2.1 Data preparation
As primary data for relational noun extraction, I
used the deWaC corpus (Baroni and Kilgariff, 2006),
a 1.6 billion token corpus of German crawled from
the web. The corpus is automatically tagged and
lemmatised by TreeTagger (Schmid, 1995). From
this corpus, I extracted all noun (NN) and prepo-
sition (APPR) unigrams and noun?preposition bi-
grams. Noun unigrams occuring less than ten times
in the entire corpus were subsequently removed. In
addition to the removal of hapaxes, I also filtered out
any abbreviations.
Frequency counts were lemma-based, a deci-
sion that was motivated by the intended applica-
tion, namely mining of relational noun entries for
a lemma-based HPSG lexicon.
From the corpus, I extracted a best-list, based
on bigram frequency, a well-established heuristical
measure for collocational status (Krenn, 2000). Us-
ing a frequency based best list not only minimises
initial annotation effort, but also ensures the quick-
est improvement of the target resource, the gram-
mar?s lexicon. Finally, the use of ranked best lists
will also ensure that we will always have enough
positive items in our training data.
2.2 Annotation
The ranked best list was subsequently annotated by
two human annotators (A1,A2) with relatively little
prior training in linguistics. In order to control for
annotation errors, the same list was annotated a sec-
ond time by a third year student of linguistics (A3).
In order to operationalise the argument/modifier
annotators were asked to take related verbs into
consideration, as well as to test (local and tempo-
ral) prepositions for paradigmatic interchangeabil-
ity. Furthermore, since we are concerned with logi-
cal complements of nouns but not possessors, which
can be added quite freely, annotators were advised to
further distinguish whether a von-PP was only pos-
sible as a possessor or also as a noun complement.
An initial comparison of annotation decisions
66
showed an agreement of .82 between A1 and A3,
and an agreement of .84 between A2 and A3. In
a second round discrepancies between annotators
were resolved, yielding a gold standard annotation
of 4333 items, out of which 1179 (=27.2%) were
classified as relational nouns.
3 Experiments
All experiments reported here were carried out us-
ing WEKA, a Java platform for data exploration
and experimentation developed at the University of
Waikato (Hall et al, 2009).
Since our task is to extract relational nouns and
since we are dealing with a binary decision, per-
formance measures given here report on relational
nouns only. Thus, we do not provide figures for the
classification of non-relational nouns or any uninfor-
mative (weighted) averages of the two.2
3.1 Learners
In a pre-study, we conducted experiments with a sin-
gle feature set, but different classifiers in order to de-
termine which ones performed best on our data set.
Amongst the classifiers we tested were 2 Bayesian
classifiers (Naive Bayes and Bayesian Nets), a Sup-
port Vector Machine, a Multilayer Perceptron clas-
sifier, as well as the entire set of decision tree clas-
sifiers offered by WEKA 3.6.4 (cf. the WEKA doc-
umentation for an exhaustive list of references). All
test runs were performed with default settings. Un-
less otherwise indicated, all tests were carried out
using 10-fold cross-validation.
Among these, decision tree classifiers perform
quite well in general, with NBTree, a hybrid de-
cision tree classifier using Naive Bayes classifiers
at leave nodes producing optimal results. Perfor-
mance of the Naive Bayes classifier was subopti-
mal, with respect to both precision and recall. Over-
all performance of the Bayesian Net classifier (with
a K2 learner) was competitive to average decision
tree classifiers, delivering particularly good recall,
but fell short of the best classifiers in terms of preci-
sion and F-measure.
2A base-line classifier that consistently choses the majority
class (non-relational) and therefore does not detect a single re-
lational noun, already achieves an F-measure for non-relational
nouns of 84.3, and a weighted F-measure of 61.3%.
Thus, for further experimentation, we concen-
trated on the two best-performing classifiers, i.e.,
NBTree (Kohavi, 1996), which achieved the high-
est F-score and the second best precision, and SMO
(Platt, 1998), a support vector machine, which pro-
duced the best precision value.
After experimentation regarding optimal feature
selection (see next section), we re-ran our experi-
ments with the modified feature set, in order to con-
firm that the classifiers we chose were still optimal.
The results of these runs are presented in table 1.
Prec. Rec. F-meas.
ADTree 68.3 61.1 64.5
BFTree 75.0 51.7 61.2
DecisionStump 52.5 80.2 63.5
FT 73.8 59.1 65.7
J48 72.9 58.4 64.8
J48graft 72.6 58.4 64.7
LADTree 70.5 57.5 63.3
LMT 74.9 59.8 66.5
NBTree 74.9 62.8 68.7
RandomForest 67.4 63.4 65.3
RandomTree 61.8 61.1 61.4
REPTree 74.5 61.2 67.2
Naive Bayes 70.5 53.9 61.1
Bayes Net 60.6 71.4 65.6
SMO 76.5 57.7 65.8
MultilayerPerceptron 67.5 64.5 65.9
Bagging (RepTree) 75.9 62.4 68.5
Voting (maj) 72.7 66.3 69.4
Voting (av) 71.3 68.4 69.8
Table 1: Performance of different classifiers
Finally, we did some sporadic test using a vot-
ing scheme incorporating 3 classifiers with high pre-
cision values (SMO, NBTree, Bagging(REPTree)
(Breiman, 1996)), as well as two classifiers with
high recall (BayesNet, recall-oriented SMO, see be-
low). Using averaging, we managed to bring the F-
measure up to 69.8, the highest value we measured
in all our experiments.
3.2 Features
For NBTree, our best-performing classifier, we sub-
sequently carried out a number of experiments to as-
sess the influence and predictive power of individual
association measures and to study their interactions.
67
Essentially, we make use of two basic types of
features: string features, like the form of the preposi-
tion or the prefixes and suffixes of the noun, and as-
sociation measures. As for the latter, we drew on the
set of measures successfully used in previous studies
on collocation extraction:
Mutual information (MI) An information theo-
retic measure proposed by (Church and Hanks,
1990) which measures the joint probability of
the bigram in relation to the product of the
marginal probabilities, i.e., the expected proba-
bility.
MI =
p(noun, prep)
p(noun) ? p(prep)
MI2 A squared variant of mutal information, previ-
ously suggested by (Daille, 1994). Essentially,
the idea behind squaring the joint probability is
to counter the negative effect of extremely low
marginal probabilities yielding high MI scores.
MI2 =
(p(noun, prep))2
p(noun) ? p(prep)
Likelihood ratios A measure suggested by (Dun-
ning, 1993) that indicates how much more
likely the cooccurence is than mere coinci-
dence.
LR = logL(pi, k1, n1) + logL(p2, k2, n2)
? logL(p, k1, n1) ? logL(p, k2, n2)
where
logL(p, n, k) = k log p+ (n? k) log(1 ? p)
and
p1 =
k1
n1
, p2 =
k2
n2
, p =
k1 + k2
n1 + n2
t-score The score of Fisher?s t-test. Although the
underlying assumption regarding normal distri-
bution is incorrect (Church and Mercer, 1993),
the score has nevertheless been used with re-
peated success in collocation extraction tasks
(Krenn, 2000; Krenn and Evert, 2001; Evert
and Krenn, 2001).
tscore =
p(noun, prep) ? (p(noun) ? p(prep))
?
?2
N
As suggested by (Manning and Schu?tze, 1999)
we use p as an approximation of ?2.
Association Strength (Smadja, 1993)
A factor indicating how many times the stan-
dard deviation a bigram frequency differs from
the average.
Strength =
freqi ? f?
?
Best Indicates whether a bigram is the most fre-
quent one for the given noun or not.
Best-Ratio A relative version of the previous fea-
ture indicating the frequency ratio between the
current noun?preposition bigram and the best
bigram for the given noun.
In addition to the for,m of the preposition, we in-
cluded information about the noun?s suffixes or pre-
fixes:
Noun suffix We included common string suffixes
that may be clues as to the relational nature of
the noun, as, e.g., the common derviational suf-
fixes -ion, -schaft, -heit, -keit as well as the end-
ings -en, which are found inter alia with nom-
inalised infinitives, and -er, which are found,
inter alia with agentive nominals. All other suf-
fixes were mapped to the NONE class.
Noun prefix Included were prefixes that commonly
appear as verb prefixes. Again, this was used
as a shortcut for true lexical relatedness.
As illustrated by the diagrams in Figure 1, the
aforementioned association measures align differ-
ently with the class of relational nouns (in black):
The visually discernible difference in alignment
between association metrics and relational nouns
was also confirmed by testing single-feature classi-
fiers: as detailed in Table 2, MI, MI2, and t-score
all capable to successfully identify relational nouns
by themselves, whereas best, best-ratio and strength
68
Figure 1: Distribution of relational and non-relational nouns across features (created with WEKA 3.6.4)
are entirely unable to partition the data appropri-
ately. LR assumes an intermediate position, suffer-
ing mainly from recall problems.
Prec. Rec. F-meas.
MI 65.2 45.2 53.4
MI2 62.2 50.7 55.9
LR 60 23.5 33.8
T-score 66.4 42 51.5
Strength 0 0 0
Best 0 0 0
Best-Ratio 0 0 0
Table 2: Classification by a single association metric
The second experiment regarding features differs
from the first by the addition of form features:
Two things are worth noting here: first, the values
achieved by MI and T-score now come very close to
the values obtained with much more elaborate fea-
ture sets, confirming previous results on the useful-
ness of these metrics. Second, all association mea-
sures now display reasonable performance. Both
Prec. Rec. F-meas.
MI 74.2 61.2 67.1
MI2 72.5 56.4 63.5
LR 73.1 54.4 62.4
T-score 74.9 60.6 67
Strength 72.5 52.4 60.9
Best 69.7 48.7 57.3
Best-Ratio 72.1 53.4 61.3
Table 3: Classification by a single association metric +
form features (preposition, noun prefix, noun suffix)
these effects can be traced to a by-category sampling
introduced by the form features. The most clear-cut
case is probably the best feature: as shown in Fig-
ure 1, there is a clear increase in relational nouns in
the TRUE category of the Boolean best feature, yet,
they still do not represent a majority. Thus, a clas-
sifier with a balanced cost function will always pre-
fer the majority vote. However, for particular noun
classes (and prepositions for that matter) majorities
can be tipped.
69
Figure 2: MI-values of relational nouns relative to preposition
As depicted by the preposition-specific plot of MI
values in Figure 2, some prepositions have a clear
bias for their use with relational nouns (e.g., von
?of?) or against it (e.g., ab ?from?), while others ap-
pear non-commital (e.g., fu?r ?for?). Similar observa-
tions can be made for noun suffixes and prefixes.
The next set of experiments were targetted at op-
timisation. Assuming that the candidate sets se-
lected by different metrics will not stand in a sub-
set relation I explored which combination of met-
rics yielded the best results. To do this, I started
out with a full set of features and compared this to
the results obtained with one feature left out. In a
second and third step of iteration, I tested whether
simultaneously leaving out some features for which
we observed some gain would produce an even more
optimised classifier.
Table 4 presents the result of the first step. Here,
two outcomes are of particular interest: deleting
information about the noun suffix is detrimental,
Prec. Rec. F-meas.
All 74.4 61.2 67.2
?T-score 75.3 62.4 68.3
?MI 72.8 62.3 67.1
?MI2 75.1 61.6 67.7
?LR 74.1 60.1 66.3
?Strength 73.4 62 67.2
?Best 73.7 60.7 66.6
?Best-Ratio 74.2 61.8 67.4
?Prep 74.7 61.1 67.2
?Noun-Prefix 74.7 61.1 67.2
?Noun-Suffix 71.3 55.3 62.3
Table 4: Effects of leaving one feature out
whereas ignoring the t-score value appears to be
beneficial to overall performance.
In a second (and third) iteration, I tested whether
any additional feature deletion apart from t-score
would give rise to any further improvements.
70
?t-score Prec. Rec. F-meas.
75.3 62.4 68.3
?MI 74.4 57.6 64.9
?LR 74.8 61.3 67.4
?MI2 74.1 61.7 67.4
?Strength 75.1 62.8 68.4
?Best 74.1 61.5 67.2
?Best-Ratio 75.4 62.6 68.4
?Best-Ratio ?Strength 74.9 63.4 68.7
Table 5: Effects of leaving two or more features out
In fact, removal of the Strength feature provided
good results, whether taken out individually or in
combination, which may be due to this feature?s in-
herently poor statistical properties (cf. Figure 1). Ig-
noring best-ratio was also beneficial, probably due
to the fact that most of its benefical properties are al-
ready covered by the best feature and that non-best
noun-preposition combinations hardly ever give rise
to positive hits.
As a matter of fact, simultaneous removal of best-
ratio and strength, in addition to the removal of t-
score of course, yielded best overall results. As a
consequence, all remaining test runs were based on
this feature set. In separate test runs with the SMO
classifier, I finally confirmed that the optimality of
this feature set was not just an artifact of the classi-
fier, but that it generalises to SVMs as well.
3.3 Trade-offs
Since our main aim in relational noun mining is
the improvement of the accuracy of our grammar?s
lexicon, and since the quickest improvement are
expected for highly frequent noun-preposition bi-
grams, I tested whether I could bring the recall of our
classifiers up, at the expense of moderate losses in
precision. For this evaluation, I used again our best-
performing classifier (NBTree), as well as SMO,
which had the highest head-room in terms of preci-
sion, while already providing satisfactory recall. To
this end, I manipulated the classifier?s cost matrix
during training and testing, gradually increasing the
costs for false negatives compared to false positives.
The results of this evaluation are given in Figure
3. First, we obtained a new optimal f-measure for
the SMO classifier: at a cost factor of 2.1 for false
negatives, the f-measure peaks at 69.7, with a recall
of 75.1% and precision still acceptable (65.1%). At
this level, we still save more than two thirds of the
annotation effort.
By way of penalising false negatives 6 times more
than false positives, the suppport vector machine
was able to detect over 90% of all relational nouns,
at a precision of 50%. At these levels, we can still
save more than half of the entire annotation effort.
Going further down the Zipf distribution, we ex-
pect the savings in terms of annotation effort to go
further up, since our bigram frequency ranking en-
sures that relational nouns are overrepresented at the
top of the list, a rate that will gradually go down.
Finally, including false positives in the data to
be annotated will also ensure that we always have
enough positive and negative training data for learn-
ing a classifier on an extended data set.
3.4 Outlook
Although results are already useful at this point, I
hope to further improve precision and recall rates
by means of additional features. Evaluating the
NBTree classifier on the training data, we observe
an F-measure of only 74.7%, which suggests that
the current set of features models the training data
still quite imperfectly. Thus, one needs to incorpo-
rate further independent evidence in order to predict
relation nouns more reliably. Owing to the seman-
tic nature of the relational vs. non-relational dis-
tinction one type of additional evidence could come
from multilingual resources: as a first step, I en-
visage incorporating the classification of nouns in
the English Resource Grammar (ERG; (Copestake
and Flickinger, 2000)) as prior information regard-
ing relational status. In a second step I shall explore
whether one can exploit information from parallel
corpora, using in particular item-specific divergence
of preposition choice to detect whether we are deal-
ing with a contentful or rather a functional prepo-
sition.3 The intuition behind using cross-linguistic
evidence to try and boost the performance of the
learner is based on the observation that predicate ar-
gument structure in closely related languages such
as English and German tends to be highly similar,
with differences mostly located in syntactic proper-
3I expect that arbitrary divergence in the choice of preposi-
tion provides an indicator of grammaticalisation.
71
0 2 4 6 8 10 12 14 16
40
50
60
70
80
90
100
PrecisionRecallF-measurePrecisionRecallF-measure
Figure 3: Effect of trading precision for recall (NBTree: white; SMO: black)
ties such as selection for case or choice of preposi-
tion. As a consequence, I do not expect to be able to
predict the actual form of the German preposition,
but rather gain additional evidence as to whether a
given noun has some relational use at all or not.
The second type of information that I plan to use
more systematically in the future is morphological
and lexical relatedness which is only approximated
at present by the noun sufix and noun prefix fea-
tures which hint at the derived (deverbal) nature
of the noun under discussion. In addition to these
brute-force features, I plan to incorporate the HPSG
grammar?s verb subcategorisation lexicon, pairing
nouns and verbs by means of minimum edit dis-
tance.4 In essence, we hope to provide a more
general approach to lexical relatedness between re-
lational nouns and the non-unary verbal predicates
they derive from: in the current feature set, this was
only suboptimally approximated by the use of noun
suffix and prefix features, resulting in most nouns
being mapped to the unpredictive class NONE.5
Finally, I plan to apply the current approach to
the extraction of nouns taking propositional comple-
ments. Given the comparative ease of that task com-
pared to the extraction of PP-taking nouns, I shall in-
vestigate whether we can exploit the fact that many
4Being aware of the fact that lexical derivation may give rise
to arbitrary changes in syntactic subcategorisation, I minimally
expect to gather evidence regarding the arity of the derived noun
predicate. To what extent actual selectional properties as to the
shape of the functional preposition are maintained by deriva-
tional processes remains a matter of empirical research.
5The inclusion of noun prefixes, which are actually verb pre-
fixes, is inherently limited to mimick lexical relatedness to pre-
fix verbs.
relational nouns taking propositional complements
(e.g., der Glaube, da? ... ?the belief that?) also take
PP-complements (der Glaube an ?the belief in?) in
order to further improve our present classifier. In a
similar vein, I shall experiment whether it is possible
to extrapolate from relational nouns taking von-PPs
to genitive complements.
4 Conclusion
In this paper I have suggested to treat the task of
mining relational nouns in German as a MWE ex-
traction problem. Based on the first 4333 hand-
annotated items of a best-list ranked by bigram fre-
quencies, several classifiers have been trained in or-
der to determine which learner and which (combina-
tion of) association measures performed best for the
task.
Testing different classifiers and different metrics,
we found that optimal results were obtained us-
ing a support vector machine (Platt, 1998), includ-
ing Mutual Information (MI), its squared variant
(MI2), and Likelihood Ratios (LR) as association
measures, together with information about the iden-
tity of the preposition and the noun?s prefix and suf-
fix. The second best classifier, a hybrid decision tree
with Naive Bayes classifiers at the leaves produced
highly competitive results. T-scores, while being a
good predictor on its own, however, led to a slight
decrease in performance, when a full feature set was
used. Likewise, performance suffered when Associ-
ation Strength (Smadja, 1993) was included. Overall
performance of the best individual classifier figured
at an F-score of 69.7.
72
References
Marco Baroni and Adam Kilgariff. 2006. Large
linguistically-processed web corpora for multiple lan-
guages. In Proceedings of EACL 2006.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Kostadin Cholakov, Valia Kordoni, and Yi Zhang. 2008.
Towards domain-independent deep linguistic process-
ing: Ensuring portability and re-usability of lexicalised
grammars. In Coling 2008: Proceedings of the work-
shop on Grammar Engineering Across Frameworks,
pages 57?64, Manchester, England, August. Coling
2008 Organizing Committee.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Kenneth Church and Robert Mercer. 1993. 