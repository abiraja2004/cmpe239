
Disambiguating concepts and entities in a con-
text sensitive way is a fundamental problem
in natural language processing. The compre-
hensiveness of Wikipedia has made the on-
line encyclopedia an increasingly popular tar-
get for disambiguation. Disambiguation to
Wikipedia is similar to a traditional Word
Sense Disambiguation task, but distinct in that
the Wikipedia link structure provides addi-
tional information about which disambigua-
tions are compatible. In this work we analyze
approaches that utilize this information to ar-
rive at coherent sets of disambiguations for a
given document (which we call ?global? ap-
proaches), and compare them to more tradi-
tional (local) approaches. We show that previ-
ous approaches for global disambiguation can
be improved, but even then the local disam-
biguation provides a baseline which is very
hard to beat.
1 