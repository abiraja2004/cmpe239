
Naively collecting translations by crowd-
sourcing the task to non-professional trans-
lators yields disfluent, low-quality results if
no quality control is exercised. We demon-
strate a variety of mechanisms that increase
the translation quality to near professional lev-
els. Specifically, we solicit redundant transla-
tions and edits to them, and automatically se-
lect the best output among them. We propose a
set of features that model both the translations
and the translators, such as country of resi-
dence, LM perplexity of the translation, edit
rate from the other translations, and (option-
ally) calibration against professional transla-
tors. Using these features to score the col-
lected translations, we are able to discriminate
between acceptable and unacceptable transla-
tions. We recreate the NIST 2009 Urdu-to-
English evaluation set with Mechanical Turk,
and quantitatively show that our models are
able to select translations within the range of
quality that we expect from professional trans-
lators. The total cost is more than an order of
magnitude lower than professional translation.
1 