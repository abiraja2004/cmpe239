
We describe a computational framework for
language learning and parsing in which dy-
namical systems navigate on fractal sets. We
explore the predictions of the framework in
an artificial grammar task in which humans
and recurrent neural networks are trained on
a language with recursive structure. The re-
sults provide evidence for the claim of the
dynamical systems models that grammatical
systems continuously metamorphose during
learning. The present perspective permits
structural comparison between the recursive
representations in symbolic and neural net-
work models.
1 