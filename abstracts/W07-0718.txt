
This paper evaluates the translation quality
of machine translation systems for 8 lan-
guage pairs: translating French, German,
Spanish, and Czech to English and back.
We carried out an extensive human evalua-
tion which allowed us not only to rank the
different MT systems, but also to perform
higher-level analysis of the evaluation pro-
cess. We measured timing and intra- and
inter-annotator agreement for three types of
subjective evaluation. We measured the cor-
relation of automatic evaluation metrics with
human judgments. This meta-evaluation re-
veals surprising facts about the most com-
monly used methodologies.
1 