
Accuracy of content have not been fully uti-
lized in the previous studies on automated
speaking assessment. Compared to writing
tests, responses in speaking tests are noisy
(due to recognition errors), full of incomplete
sentences, and short. To handle these chal-
lenges for doing content-scoring in speaking
tests, we propose two new methods based
on information extraction (IE) and machine
learning. Compared to using an ordinary
content-scoring method based on vector anal-
ysis, which is widely used for scoring written
essays, our proposed methods provided con-
tent features with higher correlations to human
holistic scores.
1 