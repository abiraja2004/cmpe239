ion. A possible explanation for the different perplexity results
among the LMs could be the specific content of the corpora used to compile the LMs.
For example, onewould expect texts translated fromDutch to exhibit higher frequencies
of words such asAmsterdam or even canal. This, indeed, is reflected by the lower (usually
lowest) number of OOV items in language models compiled from texts translated from
the source language.
As a specific example, the top five words that occur in the T-FR corpus and the
evaluation set, but are absent from the O-EN corpus, are: biarritz, meat-and-bone,
808
Lembersky, Ordan, and Wintner Language Models for Machine Translation
armenian, ievoli, and ivorian. The top five words that occur in the O-EN corpus, but
are absent from the T-FR corpus, are: duhamel, paciotti, ivoirian, coke, and spds. Of
those, biarritz seems to be French-specific, but the other items seem more arbitrary.
To rule out the possibility that the perplexity results are due to specific content
phenomena, and to further emphasize that the corpora are indeed structurally different,
we conduct more experiments, in which we gradually abstract away from the domain-
and content-specific features of the texts and emphasize their syntactic structure. We
focus on French-to-English, but the results are robust and consistent (we repeated the
same experiments for all language pairs, with very similar outcomes).
First, we remove all punctuation to eliminate possible bias due to differences in
punctuation conventions.3 Then, we use the Stanford Named Entity Recognizer (Finkel,
Grenager, and Manning 2005) to identify named entities, which we replace with a
unique token (?NE?). Next, we replace all nouns with their part-of-speech (POS) tag;
we use the Stanford POS Tagger (Toutanova and Manning 2000). Finally, for full lexical
abstraction, we replace all words with their POS tags, retaining only abstract syntactic
structures devoid of lexical content.
At each step, we train six language models on O- and T-texts and apply them
to the reference set (which is adapted to the same level of abstraction, of course).
As the abstraction of the text increases, we also increase the order of the LMs: From
4-grams for text without punctuation and NE abstraction, to 5-grams for noun abstrac-
tion, to 8-grams for full POS abstraction. In all cases we fix the LM vocabulary to only
contain tokens that appear more than once in the ?abstracted? reference set. The results,
depicted in Table 9, consistently show that the T-based LM is a better fit to the reference
set, albeit to a lesser extent. The rightmost column specifies the improvement, in terms
of perplexity, of each language model, compared with the worst-performing model.
Although we do not show the details here, the same pattern is persistent in all the other
Europarl languages we experiment with.
4.1.3 More Language Pairs. To further test the robustness of these phenomena, we repeat
these experiments with the Hebrew-to-English corpus and reference set, reflecting a
different language family, a smaller corpus, and a different domain. We train two
4-gram language models on the O-EN and T-HE corpora. We then apply the two LMs
to the reference set and compute the perplexity. The results are presented in Table 10.
Again, the T-based LM is a better fit to the translated text than the O-based LM: Its
perplexity is lower by 12.8%. We also repeat the abstraction experiments on the Hebrew
scenario. The results, depicted in Table 11, consistently show that the T-based LM is a
better fit to the reference set.
Clearly, then, translated LMs better fit the references than original ones, and the
differences can be traced back not just to (trivial) specific lexical choice, but also to
syntactic structure, as evidenced by the POS abstraction experiments.
We further test our findings on other target languages, specifically English?German
and English?French. We train several 4-gram language models on the corpora specified
in Table 2. We then compute the perplexity of the German-translated-from-English and
French-translated-from-English reference sets (see Section 3.4) with respect to these
language models. Table 12 depicts the results; they are in complete agreement with our
hypothesis.
3 In fact, there is reason to assume that punctuation constitutes part of the translationese effect. Removing
punctuation therefore harms our cause of identifying this effect.
809
Computational Linguistics Volume 38, Number 4
Table 9
Fitness of O- vs. T-based LMs to the reference set (FR-EN), reflecting different abstraction levels.
No Punctuation
Orig. Lang. Perplexity Improvement (%)
MIX 105.91 19.73
O-EN 131.94
T-DE 122.50 7.16
T-FR 99.52 24.58
T-IT 112.71 14.58
T-NL 126.44 4.17
NE Abstraction
Orig. Lang. Perplexity Improvement (%)
MIX 93.88 18.51
O-EN 115.20
T-DE 107.48 6.70
T-FR 88.96 22.77
T-IT 99.17 13.91
T-NL 110.72 3.89
Noun Abstraction
Orig. Lang. Perplexity Improvement (%)
MIX 36.02 11.34
O-EN 40.62
T-DE 38.67 4.81
T-FR 34.75 14.46
T-IT 36.85 9.30
T-NL 39.44 2.91
POS Abstraction
Orig. Lang. Perplexity Improvement (%)
MIX 7.99 2.66
O-EN 8.20
T-DE 8.08 1.47
T-FR 7.89 3.77
T-IT 8.00 2.47
T-NL 8.11 1.11
Boldface = best fit; italics = worst fit.
4.1.4 Larger Language Models. Can these phenomena be attributed to the relatively small
size of the corpora we use?Will the perplexity of O texts converge to that of T texts when
more data become available, or will the differences persist? To address these questions,
we use the (much larger) Hansard corpus and the (even larger) Gigaword corpus. We
train 4-gram language models for each Hansard and Gigaword subcorpus described in
Section 3.2. We apply the LMs to the Hansard reference set, but also to the Europarl
reference set, to examine the effect on out-of-domain (but similar genre) texts. In both
cases we report perplexity (Table 13).
810
Lembersky, Ordan, and Wintner Language Models for Machine Translation
Table 10
Fitness of O- vs. T-based LMs to the reference set (HE-EN).
Hebrew to English translations
Orig. Lang. Perplexity Improvement (%)
O-EN 187.26
T-HE 163.23 12.83
The results are fully consistent with our previous findings: In the case of the
Hansard reference set, a language model based on original texts must be up to ten
times larger to retain the low perplexity level of translated texts. For example, whereas a
languagemodel compiled from 10million English-translated-from-French tokens yields
a perplexity of 42.70 on the Hansard reference set, a LM compiled from original English
texts requires 100 million words to yield a similar perplexity of 43.70 on the same
reference set. The Gigaword LMs, which are trained on texts representing completely
different domains and genres, produce much higher (i.e., worse) perplexity in this
scenario. In the case of the Europarl reference set, a language model based on original
texts must be approximately five times larger (and a Gigaword language model approxi-
mately twenty times larger) than a language model based on original texts to yield similar
perplexity.
Table 11
Fitness of O- vs. T-based LMs to the reference set (HE-EN), reflecting different abstraction levels.
No Punctuation
Orig. Lang. Perplexity Improvement (%)
O-EN 401.44
T-HE 335.30 16.48
NE Abstraction
Orig. Lang. Perplexity Improvement (%)
O-EN 298.16
T-HE 251.39 15.69
Noun Abstraction
Orig. Lang. Perplexity Improvement (%)
O-EN 81.92
T-HE 72.34 11.70
POS Abstraction
Orig. Lang. Perplexity Improvement (%)
O-EN 11.47
T-HE 10.76 6.20
811
Computational Linguistics Volume 38, Number 4
4.2 Original vs. Translated LMs for Machine Translation
4.2.1 SMT Experiments. The last hypothesis we test is whether a better fitting lan-
guage model yields a better machine translation system. In other words, we expect
the T-based LMs to outperform the O-based LMs when used as part of machine
translation systems. We construct German-to-English, English-to-German, French-to-
English, French-to-German, Italian-to-English, and Dutch-to-English MT systems using
the Moses phrase-based SMT toolkit (Koehn et al 2007). The systems are trained on
the parallel corpora described in Section 3.3. We use the reference sets (Section 3.4) as
follows: 1,000 sentences are randomly extracted for minimum error-rate training (Och
2003), and another, disjoint set of 1,000 randomly selected sentences is used for evalu-
ation. Each system is built and tuned with six different LMs: MIX, O-based, and four
T-based models (Section 3.2). We use Bleu (Papineni et al 2002) to evaluate translation
quality. The results are listed in Tables 14 and 15.
The results are consistent and fully confirm our hypothesis. Across all language
pairs, MT systems using LMs compiled from translated-from-source texts consistently
outperform all other systems. Systems that use LMs compiled from texts originally
written in the target language always perform worst or second worst. We test the statis-
tical significance of the differences between the results using the bootstrap resampling
method (Koehn 2004). In all experiments, the best system (translated-from-source LM)
is significantly better than the system that uses the O-based LM (p < 0.01).
We now repeat the experiment with Hebrew to English translation. We construct a
Hebrew-to-English MT system with Moses, using a factored translation model (Koehn
and Hoang 2007). Every token in the training corpus is represented as two factors:
surface form and lemma. The Hebrew input is fully segmented (Itai and Wintner 2008).
The system is built and tuned with O- and T-based LMs. The O-based LM yields a
Bleu score of 11.94, whereas using the T-based LM results in somewhat higher Bleu
Table 12
Fitness of O- vs. T-based LMs to the reference set (EN-DE and EN-FR).
English to German translations
Orig. Lang. Perplexity Improvement (%)
Mix 106.37 20.24
O-DE 133.37
T-EN 99.39 25.47
T-FR 119.21 10.61
T-IT 123.35 7.51
T-NL 119.99 10.03
English to French translations
Orig. Lang. Perplexity Improvement (%)
Mix 58.71 3.20
O-FR 60.65
T-EN 49.44 18.47
T-DE 55.41 8.63
T-IT 57.75 4.77
T-NL 54.23 10.57
812
Lembersky, Ordan, and Wintner Language Models for Machine Translation
Table 13
The effect of LM training corpus size on the fitness of LMs to the reference sets.
Hansard Reference Set
Hansard T-FR
Size Perplexity
1M 64.68
5M 47.63
10M 42.70
Hansard O-EN
Size Perplexity
1M 91.40
5M 66.95
10M 59.19
25M 51.59
50M 47.02
100M 43.70
Gigaword
Size Perplexity
100M 165.03
500M 151.00
1000M 145.88
Europarl Reference Set
Hansard T-FR
Size Perplexity
1M 169.66
5M 137.72
10M 128.65
Hansard O-EN
Size Perplexity
1M 198.93
5M 162.08
10M 150.05
25M 137.31
50M 129.43
100M 123.10
Gigaword
Size Perplexity
100M 136.72
500M 121.88
1000M 116.55
score, 12.07, but the difference is not statistically significant (p = 0.18). Presumably, the
low quality of both systems prevents the better LM frommaking a significant difference.
4.2.2 Larger Language Models. Again, the LMs used in the MT experiments reported here
are relatively small. To assess whether the benefits of using translated LMs carry over
to scenarios where larger original corpora exist, we build yet another set of French-to-
English MT systems. We use the Hansard SMT translation model and Hansard LMs
to train nine MT systems, three with varying sizes of translated texts and six with
varying sizes of original texts. We train additional MT systems with several subsets
of the Gigaword LM. We tune and evaluate on the Hansard reference set. In another
set of experiments we use the Europarl French-to-English scenario (using Europarl
813
Computational Linguistics Volume 38, Number 4
Table 14
Machine translation with various LMs; English target language.
DE to EN
LM Bleu
MIX 21.43
O-EN 21.10
T-DE 21.90
T-FR 21.16
T-IT 21.29
T-NL 21.20
FR to EN
LM Bleu
MIX 28.67
O-EN 27.98
T-DE 28.01
T-FR 29.14
T-IT 28.75
T-NL 28.11
IT to EN
LM Bleu
MIX 25.41
O-EN 24.69
T-DE 24.62
T-FR 25.37
T-IT 25.96
T-NL 24.77
NL to EN
LM Bleu
MIX 24.20
O-EN 23.40
T-DE 24.26
T-FR 23.56
T-IT 23.87
T-NL 24.52
Table 15
Machine translation with various LMs; non-English target language.
EN to DE
LM Bleu
MIX 13.00
O-DE 12.47
T-EN 13.10
T-FR 12.46
T-IT 12.65
T-NL 12.86
EN to FR
LM Bleu
MIX 24.83
O-FR 24.70
T-EN 25.31
T-DE 24.58
T-IT 24.89
T-NL 25.20
corpora for the translation model as well as for tuning and evaluation), but we use
the Hansard and Gigaword LMs to see whether our findings are consistent also when
LMs are trained on out-of-domain material.
Table 16 again demonstrates that language models compiled from original texts
must be up to ten times larger in order to yield translation quality similar to that of
LMs compiled from translated texts.4 In other words, much smaller translated LMs
perform better than much larger original ones, and this holds for various LM sizes,
both in-domain and out-of-domain. For example, on the Hansard corpus, a 10-million-
token T-FR language model yields a Bleu score of 34.67, whereas an O-EN language
model of 100 million tokens is required in order to yield a similar Bleu score of 34.44.
The systems that use the Gigaword LMs perform much worse in-domain, even with a
language model compiled from 1000M tokens. Out-of-domain, the Gigaword systems
are better than O-EN, but they require approximately five times more data to match the
performance of T-FR systems.
4.2.3 Enjoying Both Worlds. The previous section established the fact that language mod-
els compiled from translated texts are better for MT than ones compiled from original
texts, even when the original LMs are much larger. In many real-world scenarios,
however, one has access to texts of both types. Our results do not imply that original
4 The table only specifies three subsets of the Gigaword corpus, but the graphs show more data points.
Note that the x-axis is logarithmic. Incidentally, the graphs show that increases in (Gigaword) corpus size
do not monotonically translate to better MT quality.
814
Lembersky, Ordan, and Wintner Language Models for Machine Translation
Table 16
The effect of LM size on MT performance.
Hansard TM and Test
Hansard T-FR
Size Bleu
1M 33.03
5M 34.25
10M 34.67
Hansard O-EN
Size Bleu
1M 31.91
5M 33.27
10M 33.43
25M 33.49
50M 34.29
100M 34.44
Gigaword
Size Bleu
100M 31.77
500M 32.31
1000M 32.51
Europarl TM and Test
Hansard T-FR
Size Bleu
1M 26.36
5M 27.06
10M 27.22
Hansard O-EN
Size Bleu
1M 26.06
5M 26.03
10M 26.72
25M 26.72
50M 27.01
100M 27.04
Gigaword
Size Bleu
100M 27.47
500M 27.71
1000M 27.69
texts are useless, and that only translated ones should be used. In this section we explore
various ways to combine original and translated texts, thereby yielding even better
language models.
For these experiments we use 10 million English-translated-from-French tokens
from the Hansard corpus (T-FR) and another 100 million original-English tokens from
the same source (O-EN). We combine them in five different ways: straightforward
concatenation of the corpora; a concatenation of the original-English corpus with the
translated corpus, upweighted by a factor of 10 and then of 20; log-linear modeling;
and an interpolated language model. In each experiment we report both the fitness of
the LM to the reference set, in terms of perplexity, and the quality of machine translation
815
Computational Linguistics Volume 38, Number 4
Table 17
Various combinations of original and translated texts and their effect on perplexity (PPL) and
translation quality (Bleu).
Hansard TM, LM and Test
Combination PPL Bleu
O-EN 43.70 34.44
T-FR 42.70 34.67
Concatenation 38.43 34.62
Concatenation x10 41.15 35.09
Concatenation x20 45.07 34.67
Log-Linear LM ? 35.26
Interpolated LM 36.69 35.35
Europarl TM and Test; Hansard LM
Combination PPL Bleu
O-EN 123.10 27.04
T-FR 128.65 27.22
Concatenation 116.71 27.14
Concatenation x10 135.09 27.29
Concatenation x20 152.02 27.09
Log-Linear LM ? 27.30
Interpolated LM 107.82 27.48
that uses this LM, in terms of Bleu.5 We execute each experiment twice, once (in-domain)
with the Hansard reference set and once (out-of-domain) where the translation model,
tuning corpus, and reference set al come from the Europarl FR-EN subcorpus, as above.
The results are listed in Table 17; we now provide a detailed explanation of these
experiments.
Concatenation of O and T texts. We train three language models by concatenating the
T-FR and O-EN corpora. First, we simply concatenate the corpora obtaining 110 million
tokens. Second, we upweight the T-FR corpus by a factor of 10 before the concatenation;
and finally, we upweight the T-FR corpus by a factor of 20 before the concatenation.
In the ?in-domain? scenario, the LM trained on a simple concatenation of the corpora
reduces the perplexity by more than 10%. The best translation quality is obtained
when the T-FR corpus is upweighted by a factor of 10. It improves by 0.42 Bleu points
compared to the MT system that uses T-FR (p = 0.074), and, more significantly, by 0.65
Bleu points compared to O-EN (p < 0.05). In the ?out-of-domain? scenario, there is a
small reduction in perplexity (about 5%) with a language model that is trained on a
simple concatenation of the corpora. There is also a very small improvement in the
translation quality (0.07 Bleu points compared to the T-FR system and 0.25 Bleu points
compared to O-EN).
Log-Linear combination of language models. The MOSES decoder uses log-linear model-
ing (Och and Ney 2001) to discriminate between better and worse hypotheses dur-
ing decoding. A log-linear model is defined as a combination of N feature functions
hi(t, s), 1 ? i ? N, that map input (s), output (t), or a pair of input and output strings
to a numeric value. Each feature function is associated with a model parameter ?i, its
feature weight, which determines the contribution of the feature to the overall value of
P(t|s). Formally, decoding based on a log-linear model is defined by:
t? = argmax
t
P(t|s) = argmax
t
{
N
?
i=1
?ihi(t, s)
}
(2)
5 Except log-linear models, for which we only report the quality of machine translation, because there are
two language models in this case and perplexity is harder to compute.
816
Lembersky, Ordan, and Wintner Language Models for Machine Translation
We train two language models, based on T-FR and O-EN. Then, we combine these
models by including them as different feature functions. The feature weight of each LM
is set by minimum error-rate tuning, optimizing the translation quality; this is the same
technique that Koehn and Schroeder (2007) employ for domain adaptation. In-domain,
this combination is better by 0.82 Bleu points compared with an MT system that uses
O-EN (p < 0.001), 0.59 Bleu points compared with the one that uses T-FR (p < 0.05).
Out of domain, this combination is again not significantly better than using T-FR only
(improvement of 0.08 Bleu points, p = 0.255).
Interpolated language models. In the interpolated scenario, two language models are
mixed on a fixed proportion ?, according to the following equation (Weintraub et al
1996):
p(w|h) = (1? ?) ? p(w|h;LM1)+ ? ? p(w|h;LM2) (3)
where w is a word, h is its ?history,? and ? is the fixed interpolation weight. We use
SRILM to train an interpolated language model from LM1 = O-EN and LM2 = T-FR.
The interpolation weight is tuned to minimize the perplexity of the combined model
with respect to the tuning set; we use the EM algorithm provided as part of the SRILM
toolkit to establish the optimized weights. In the in-domain scenario ? = 0.46 and in the
out-of-domain scenario ? = 0.49. The interpolated language model yields additional
improvement in perplexity and translation quality compared to all other models. It is
significantly better (p < 0.05) than the T-FR system on the in-domain scenarios, but the
improvement is less significant (p = 0.075) out of domain.
In summary, LMs compiled from source-translated-to-target texts are almost as
good asmuch larger LMs that also include large corpora of texts originally written in the
target language. Clearly, ignoring the status (original or translated) of monolingual texts
and creating a single languagemodel from all of them (the concatenation scenario) is not
much better than using only translated texts. In order to benefit from (often much larger)
original texts, one must consider more creative ways of combining the two subcorpora.
Of the methods we explored here, interpolated LMs provide the greatest advantage.
More research is needed in order to find an optimal combination.
5. Discussion
We use language models computed from different types of corpora to investigate
whether their fitness to a reference set of translated sentences can differentiate between
them (and, hence, between the corpora on which they are based). Our main findings
are that LMs compiled from manually translated corpora are much better predictors of
translated texts than LMs compiled from original-language corpora of the same size.
The results are robust, and are sustainable even when the corpora and the reference
sentences are abstracted in ways that retain their syntactic structure but ignore spe-
cific word meanings. Furthermore, we show that translated LMs are better predictors
of translated sentences even when the LMs are compiled from texts translated from
languages other than the source language. LMs based on texts translated from the source
language still outperform LMs translated from other languages, however.
We also show that MT systems based on translated-from-source-language LMs out-
perform MT systems based on originals LMs or LMs translated from other languages.
Again, these results are robust and the improvements are statistically significant. This
effect seems to be amplified as translation quality improves. Furthermore, our results
817
Computational Linguistics Volume 38, Number 4
Table 18
MT system performance as measured by METEOR and TER.
DE to EN
Orig. Lang. METEOR TER
O-EN 28.26 64.56
T-DE 28.64 63.57
FR to EN
Orig. Lang. METEOR TER
O-EN 33.05 54.45
T-FR 33.30 53.65
IT to EN
Orig. Lang. METEOR TER
O-EN 31.03 58.30
T-IT 31.16 57.63
NL to EN
Orig. Lang. METEOR TER
O-EN 29.97 60.29
T-NL 30.40 59.63
show that original LMs require five to ten times more data to exhibit the same fitness
to the reference set and the same translation quality as translated LMs.
More generally, this study confirms that insights drawn from the field of theoretical
Translation Studies, namely, the dual claim according to which translations as such
differ from originals, and translations from different source languages differ from each
other, can be verified experimentally and contribute to the performance of machine
translation.
One question, however, requires further investigation: Do MT systems based on
translated-from-source-language LMs produce better translations, or do they merely
generate sentences that are directly adapted to the reference set, thereby only improving
a specific evaluation metric, such as Bleu? We address this issue in three ways, showing
that the former is indeed the case. First, we use two automated evaluation metrics other
than Bleu, and show that the T-based LMs yield better MT systems even with different
metrics. Second, we perform a manual evaluation of a portion of the evaluation set. The
results show that human evaluators prefer translations produced by an MT system that
uses a T-based LM over translations produced by a system built with an O-based LM.
Finally, we provide a detailed analysis of the differences between O- and T-based LMs,
explaining these differences in terms of insights from Translation Studies.
5.1 Automatic Evaluation
First, we use two alternative automatic evaluation metrics, METEOR6 (Denkowski and
Lavie 2011) and TER (Snover et al 2006), to assess the quality of the MT systems
described in Section 4.2. We focus on four translation tasks: From German, French,
Italian, and Dutch to English.7 For each task we report the performance of two MT
systems: One that uses a language model compiled from original-English texts, and one
that uses a language model trained on texts translated from the source language. The
results, which are reported in Table 18, fully support our previous findings (recall that
lower TER is better): MT systems that use T-based LMs significantly outperform systems
that use O-based LMs.
6 More precisely, we use METEOR-RANK, the configuration used for WMT-2011.
7 All MT systems were tuned using Bleu.
818
Lembersky, Ordan, and Wintner Language Models for Machine Translation
5.2 Human Evaluation
To further establish the qualitative difference between translations produced with an
English-original language model and translations produced with a LM created from
French-translated-to-English texts, we conducted a human evaluation campaign, using
Amazon?sMechanical Turk as an inexpensive, reliable, and accessible pool of annotators
(Callison-Burch and Dredze 2010). We created a small evaluation corpus of 100 sen-
tences, selected randomly among all (Europarl) reference sentences whose length is
between 15 and 25 words. Each instance of the evaluation task includes two English
sentences, obtained from the two MT systems that use the O-EN and the T-FR language
models, respectively. Annotators are presented with these two translations, and are re-
quested to determine which one is better. The definition given to annotators is: ?A better
translation is more fluent, reflecting better use of English.? Observe that because the
only variable that distinguishes between the two MT systems is the different language
model, we only have to evaluate the fluency of the target sentence, not its faithfulness
to the source. Consequently, we do not present the source or the reference translation
to the annotators. All annotators were located in the United States (and, therefore, are
presumably English speakers).
As a control set, we added a set of 10 sentences produced with the O-based LM,
which were paired with their (manually created) reference translations, and 10 sen-
tences produced with the T-based LM, again paired with their references. Each of the
120 evaluation instances was assigned to 10 different Mechanical Turk annotators. We
report two evaluation metrics: score and majority. The score of a given sentence pair
?e1, e2? is i/j, where i is the number of annotators who preferred e1 over e2, and j = 10? i
is the number of annotators preferring e2. For such a sentence pair, the majority is e1 if
i > j, e2 if i < j, and undefined otherwise.
The average score of the 10 sentences in the O-vs.-reference control set is 22/78,
and the majority is the reference translation in all but one of the instances. As for the
T-vs.-reference control set, the average score is 18/82, and the majority is the reference
in all of the instances. This indicates that the annotators are reliable, and also that it
is unrealistic to expect a clear-cut distinction even between human translations and
machine-generated output.
As for the actual evaluation set, the average score of O-EN vs. T-FR is 38/62, and
the majority is T-FR in 75% of the cases, O-EN in only 25% of the sentence pairs. We
take these results as a very strong indication that English sentences generated by an
MT system whose language model is compiled from translated texts are perceived
by humans as more fluent than ones generated by a system built with an O-based
language model. Not only is the improvement reflected in significantly higher Bleu
(and METEOR, TER) scores, but it is undoubtedly also perceived as such by human
annotators.
5.3 Analysis
In order to look into the differences between T and O qualitatively, rather than quantita-
tively, we turn now to study several concrete examples. To do so, we extracted approx-
imately 200 sentences from the French?English Europarl evaluation set; we chose all
sentences of length between 15 and 25. In addition, we extracted the 100 most frequent
n-grams, for 1 ? n ? 5, from both English-original and English-translated-from-French
Europarl corpora. As both corpora include approximately the same number of tokens,
we report counts in the following rather than frequencies.
819
Computational Linguistics Volume 38, Number 4
The differences between O and T texts are consistent with well-established observa-
tions of translation scholars. Consider the explicitation hypothesis (Blum-Kulka 1986),
which Se?guinot (1998, page 108) spells out thus:
1. ?something which was implied or understood through presupposition in
the source text is overtly expressed in the translation?
2. ?something is expressed in the translation which was not in the original?
3. ?an element in the source text is given greater importance in the
translation through focus, emphasis, or lexical choice?
Blum-Kulka (1986) uses the term cohesive markers to refer to items that are utilized by
the translator which cannot be found overtly in the source text. One would expect such
markers to be much more prevalent in translationese.
An immediate example of (1) is the case of acronyms: these tend to be spelled out
in translated texts. Indeed, the acronym EU is ranked 77 among the O-EN bigrams,
whereas in T-FR it does not appear in the top 100. On the other hand, the explicit trigram
The European Union occurs more frequently in T than in O.
Similarly, an instance of (2) is the cohesivemarker because in the following example,
which appears in T but neither appears in O nor can it be traced back to the original
source sentence:
Source Enfin, ce qui est grave dans le rapport de M. Olivier Tautologie, c?est qu?il
propose une constitution tripotage.
O Finally, which is serious in the report of Mr Olivier Tautologie, is that it proposes a
constitution tripotage.
T Finally, and this is serious in the report by Mr olivier Tautologie, it is because it
proposes a constitution tripotage.
Another cohesive marker, nevertheless, is correctly generated only in the T-based trans-
lation in the following example:
Source C?est quand me?me quelque chose de pre?cieux qui a e?te? souligne? par tous les
membres du conseil europe?en.
O Even when it is something of valuable which has been pointed out by all the mem-
bers of the European Council.
T It is nevertheless something of a valuable which has been pointed out by all the
members of the European Council.
Other cohesive markers discussed by Blum-Kulka (1986) are over-represented in
T compared with O. These include: therefore (3,187 occurrences in T, 1,983 in O); for
example (863 occurrences in T, 701 in O); in particular (1336 vs. 1068); first of all (601 vs.
266); in fact (1014 vs. 441); in other words (553 vs. 87); with regard to (1137 vs. 310); in
order to (2,016 vs. 603); in this respect (363 vs. 94); on the one hand (288 vs. 72); on the
other hand (428 vs. 76); and with a view to (213 vs. 51). A similar list of markers have
been shown to be excellent discriminating features between original and translated texts
(from several European languages, including French) in an independent study (Koppel
and Ordan 2011).
820
Lembersky, Ordan, and Wintner Language Models for Machine Translation
Another phenomenon we notice is that the T-based language model does a much
better job translating verbs than the O-based language model. In two very large corpora
of French and English (Ferraresi et al 2008), verbs are much more frequent in French
than in English (0.124 vs. 0.091). Human translations from French to English, therefore,
provide many more examples of verbs from which to model. Indeed, we encounter
several examples in which the O-based translation system fails to use a verb at all, or to
use one correctly, compared with the T-based system:
Source Une telle Europe serait un gage de paix et marquerait le refus de tout national-
isme ethnique.
O Such a Europe would be a show of peace and would the rejection of any ethnic
nationalism.
T Such a Europe would be a show of peace and would mark the refusal of all ethnic
nationalism.
Source Votre rapport, madame Sudre, met l?accent, a` juste titre, sur la ne?cessite? d?agir
dans la dure?e.
O Your report, Mrs Sudre, its emphasis, quite rightly, on the need to act in the long
term.
T Your report, Mrs Sudre, places the emphasis, quite rightly, on the need to act in the
long term.
Source Cette proposition, si elle constitue un pas dans la bonne direction n?en comporte
pas moins de nombreuses lacunes auxquelles le rapport evans reme?die.
O This proposal, if it is a step in the right direction do not least in contains many
shortcomings which the evans report resolve.
T This proposal, if it is a step in the right direction it contains no less many shortcom-
ings which the evans report resolve.
Last, there are several cases of interference, which Toury (1995, page 275) defines as
follows: ?Phenomena pertaining to the make-up of the source text tend to be transferred
to the target text.? In the following example, do not say nothing more is a literal
translation of the French construction On ne dit rien non plus. The T-based translation
is much more fluent:
Source On ne dit rien non plus sur la responsabilite? des fabricants, notamment en
grande-bretagne, qui ont e?te? les premiers responsables.
O We do not say nothing more on the responsibility of the manufacturers, particularly
in Britain, which were the first responsible.
T We do not say anything either on the responsibility of the manufacturers, particu-
larly in great Britain, who were the first responsible.
Incidentally, there are also some cultural differences between O and T that we
deem less important, because they are not part of the ?translationese dialect? but rather
indicate differences pertaining to the culture from which the speaker arrives. Most
notable is the form ladies and gentlemen, which is the tenth most frequent trigram in T,
but does not even rank among the top 100 in O. This is already noted by van Halteren
821
Computational Linguistics Volume 38, Number 4
(2008), according to whom this form is significantly more frequent in translations from
five European languages as opposed to original English.
In terms of (shallow) syntactic structure, we observe that part-of-speech n-grams
are distributed somewhat differently in O and in T (we use the POS-tagged Europarl
corpus of Section 4.1.2 for the following analysis). For example, proper nouns are more
frequent in O (ranking 7 among all POS 1-grams) than in T (rank 9). This has influence
on longer n-grams: For example, the 3-gram PRP MD VB is 20% more frequent in O
than in T. The sequence <S> PRP VBP is almost twice as frequent in O. The 4-gram IN
DT NN </S> is 25% more frequent in O. In contrast, the 4-gram IN DT NNS IN is 15%
more frequent in T than in O. A full analysis of such patterns is beyond the scope of
this article.
Summing up, T-based language models are more fluent and therefore yield better
translation results for the following reasons: They are more cohesive, less influenced by
structural differences between the languages, such as the under-representation of verbs
in original English texts, and less prone to interference (i.e., they can break away from
the original towards a more coherent model of the target language).
5.4 Future Research
This work is among the first to use insights from Translation Studies in order to improve
machine translation, and to use computational linguistic methodologies to corroborate
Translation Studies hypotheses. We believe that there are still vast opportunities for
fertile cross-disciplinary research in these directions. First, we only address the language
model in the present work. Kurokawa, Goutte, and Isabelle (2009) investigate the rela-
tions between the direction of translation and the quality of the translation model used
by SMT systems. There are various ways in which the two approaches can be extended
and combined, and we are actively pursuing such research directions now (Lembersky,
Ordan, and Wintner 2012).
This work also bears on language typology: We conjecture that LMs compiled
from texts translated not from the original language, but from a closely related one,
can be better than LMs compiled from texts translated from a more distant language.
Some of our results support this hypothesis, but more research is needed in order to
establish it.
The fact that translations seem to make do with fewer words (cf. also Laviosa 2008)
call into question certain norms in comparing corpora in the field of machine transla-
tion. Translated and original texts can be expected to either have the same number of
sentences or the same number of tokens, but not both. Similarly, theymay have the same
number of tokens or the same number of types, but not both.
Another interesting question that arises from this study is whether the perplexity
of a language model on a reference set is a good predictor of a translation quality
measure, such as Bleu. Although our results show a certain correlation between the
perplexity and Bleu, we acknowledge the fact that these results need further corrob-
oration. Chen, Beeferman, and Rosenfeld (1998) examine the ability of perplexity to
estimate the performance of speech recognition. They find that perplexity often does
not correlate well with word-error rates. As it is extremely important to have a reliable
measure capable of estimating the effect of language model improvements on transla-
tion quality without requiring expensive decoding resources, we believe that finding
correspondences between perplexity and the quality of MT is a valuable topic for future
research.
822
Lembersky, Ordan, and Wintner Language Models for Machine Translation
Acknowledgments
We are grateful to Cyril Goutte, George
Foster, and Pierre Isabelle for providing us
with an annotated version of the Hansard
corpus. Alon Lavie has been instrumental
in stimulating some of the ideas reported
in this article, as well as in his long-term
support and advice. We benefitted greatly
from several constructive suggestions by the
three anonymous Computational Linguistics
referees. This research was supported by the
Israel Science Foundation (grant no. 137/06)
and by a grant from the Israeli Ministry of
Science and Technology.
References
Al-Shabab, Omar S. 1996. Interpretation and
the Language of Translation: Creativity and
Conventions in Translation. Janus,
Edinburgh.
Bahl, Lalit R., Frederick Jelinek, and
Robert L. Mercer. 1983. A maximum
likelihood approach to continuous speech
recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence,
5(2):179?190.
Baker, Mona. 1993. Corpus linguistics and
translation studies: Implications and
applications. In Gill Francis Mona Baker
and Elena Tognini-Bonelli, editors, Text and
Technology: In Honour of John Sinclair. John
Benjamins, Amsterdam, pages 233?252.
Baker, Mona. 1995. Corpora in translation
studies: An overview and some
suggestions for future research.
Target, 7(2):223?243.
Baker, Mona. 1996. Corpus-based translation
studies: The challenges that lie ahead.
In Gill Francis Mona Baker and Elena
Tognini-Bonelli, editors, Terminology,
LSP and Translation. Studies in Language
Engineering in Honour of Juan C. Sager. John
Benjamins, Amsterdam, pages 175?186.
Baroni, Marco and Silvia Bernardini.
2006. A new approach to the study of
Translationese: Machine-learning the
difference between original and translated
text. Literary and Linguistic Computing,
21(3):259?274.
Blum-Kulka, Shoshana. 1986. Shifts of
cohesion and coherence in translation.
In Juliane House and Shoshana Editors
Blum-Kulka, editors, Interlingual and
Intercultural Communication Discourse and
Cognition in Translation and Second Language
Acquisition Studies, volume 35. Gunter
Narr Verlag, Berlin, pages 17?35.
Brants, Thorsten and Peng Xu. 2009.
Distributed language models. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, Companion
Volume: Tutorial Abstracts, pages 3?4,
Boulder, CO.
Callison-Burch, Chris and Mark Dredze.
2010. Creating speech and language
data with Amazon?s Mechanical Turk.
In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk,
pages 1?12, Los Angeles, CA.
Chen, Stanley, Douglas Beeferman, and
Ronald Rosenfeld. 1998. Evaluation
metrics for language models. In
Proceedings of the DARPA Broadcast
News Transcription and Understanding
Workshop (BNTUW), Landsdowne, PA.
Chen, Stanley F. 1998. An empirical study
of smoothing techniques for language
modeling. Technical report 10-98,
Computer Science Group, Harvard
University, Cambridge, MA.
Denkowski, Michael and Alon Lavie. 2011.
Meteor 1.3: Automatic metric for reliable
optimization and evaluation of machine
translation systems. In Proceedings of the
Sixth Workshop on Statistical Machine
Translation, pages 85?91, Edinburgh.
Ferraresi, Adriano, Silvia Bernardini, Picci
Giovanni, and Marco Baroni. 2008. Web
corpora for bilingual lexicography. a pilot
study of English/French collocation
extraction and translation. In Proceedings
of The International Symposium on Using
Corpora in Contrastive and Translation
Studies, Hangzhou.
Finkel, Jenny Rose, Trond Grenager, and
Christopher Manning. 2005. Incorporating
non-local information into information
extraction systems by Gibbs sampling.
In ACL ?05: Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 363?370, Morristown, NJ.
Frawley, William. 1984. Prolegomenon to a
theory of translation. In William Frawley,
editor, Translation. Literary, Linguistic and
Philosophical Perspectives. University of
Delaware Press, Newark, pages 159?175.
Gellerstam, Martin. 1986. Translationese in
Swedish novels translated from English.
In Lars Wollin and Hans Lindquist,
editors, Translation Studies in Scandinavia.
CWK Gleerup, Lund, pages 88?95.
Graff, David and Christopher Cieri. 2007.
English Gigaword. Linguistic Data
823
Computational Linguistics Volume 38, Number 4
Consortium, Philadelphia, PA, third
edition. LDC Catalog No. LDC2007T07.
Ilisei, Iustina, Diana Inkpen, Gloria
Corpas Pastor, and Ruslan Mitkov. 2010.
Identification of translationese:
A machine learning approach.
In Alexander F. Gelbukh, editor,
Proceedings of CICLing-2010: 11th
International Conference on Computational
Linguistics and Intelligent Text Processing,
volume 6008 of Lecture Notes in Computer
Science, pages 503?511. Springer, Berlin.
Itai, Alon and Shuly Wintner. 2008.
Language resources for Hebrew. Language
Resources and Evaluation, 42(1):75?98.
Jelinek, Frederick, Robert L. Mercer, Lalit R.
Bahl, and J. K. Baker. 1977. Perplexity?
A measure of the difficulty of speech
recognition tasks. Journal of the Acoustical
Society of America, 62:S63.
Jurafsky, Daniel and James H. Martin.
2008. Speech and Language Processing:
An 