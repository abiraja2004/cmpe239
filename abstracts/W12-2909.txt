 
American Sign Language (ASL) synthesis software can improve the accessibility of in-formation and services for deaf individuals with low English literacy. The synthesis com-ponent of current ASL animation generation and scripting systems have limited handling of the many ASL verb signs whose movement path is inflected to indicate 3D locations in the signing space associated with discourse refer-ents. Using motion-capture data recorded from human signers, we model how the mo-tion-paths of verb signs vary based on the lo-cation of their subject and object.  This model yields a lexicon for ASL verb signs that is pa-rameterized on the 3D locations of the verb?s arguments; such a lexicon enables more real-istic and understandable ASL animations.  A new model presented in this paper, based on identifying the principal movement vector of the hands, shows improvement in modeling ASL verb signs, including when trained on movement data from a different human signer. 1 