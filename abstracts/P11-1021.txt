
This paper presents an attempt at building
a large scale distributed composite language
model that simultaneously accounts for local
word lexical information, mid-range sentence
syntactic structure, and long-span document
semantic content under a directed Markov ran-
dom field paradigm. The composite language
model has been trained by performing a con-
vergent N-best list approximate EM algorithm
that has linear time complexity and a follow-
up EM algorithm to improve word prediction
power on corpora with up to a billion tokens
and stored on a supercomputer. The large
scale distributed composite language model
gives drastic perplexity reduction over n-
grams and achieves significantly better trans-
lation quality measured by the BLEU score
and ?readability? when applied to the task of
re-ranking the N-best list from a state-of-the-
art parsing-based machine translation system.
1 