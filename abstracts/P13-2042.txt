 
In this paper, we explore the use of distance 
and co-occurrence information of word-pairs 
for language modeling. We attempt to extract 
this information from history-contexts of up to 
ten words in size, and found it complements 
well the n-gram model, which inherently suf-
fers from data scarcity in learning long histo-
ry-contexts. Evaluated on the WSJ corpus, bi-
gram and trigram model perplexity were re-
duced up to 23.5% and 14.0%, respectively. 
Compared to the distant bigram, we show that 
word-pairs can be more effectively modeled in 
terms of both distance and occurrence. 
1 