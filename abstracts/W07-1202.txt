
This paper investigates perceptron training
for a wide-coverage CCG parser and com-
pares the perceptron with a log-linear model.
The CCG parser uses a phrase-structure pars-
ing model and dynamic programming in the
form of the Viterbi algorithm to find the
highest scoring derivation. The difficulty in
using the perceptron for a phrase-structure
parsing model is the need for an efficient de-
coder. We exploit the lexicalized nature of
CCG by using a finite-state supertagger to
do much of the parsing work, resulting in
a highly efficient decoder. The perceptron
performs as well as the log-linear model; it
trains in a few hours on a single machine;
and it requires only a few hundred MB of
RAM for practical training compared to 20
GB for the log-linear model. We also inves-
tigate the order in which the training exam-
ples are presented to the online perceptron
learner, and find that order does not signifi-
cantly affect the results.
1 