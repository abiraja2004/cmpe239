
Part-of-speech language modeling is com-
monly used as a component in statistical ma-
chine translation systems, but there is mixed
evidence that its usage leads to significant im-
provements. We argue that its limited effec-
tiveness is due to the lack of lexicalization.
We introduce a new approach that builds a
separate local language model for each word
and part-of-speech pair. The resulting mod-
els lead to more context-sensitive probabil-
ity distributions and we also exploit the fact
that different local models are used to esti-
mate the language model probability of each
word during decoding. Our approach is evalu-
ated for Arabic- and Chinese-to-English trans-
lation. We show that it leads to statistically
significant improvements for multiple test sets
and also across different genres, when com-
pared against a competitive baseline and a sys-
tem using a part-of-speech model.
1 