 
In a multimodal conversation, user refer-
ring patterns could be complex, involving 
multiple referring expressions from 
speech utterances and multiple gestures. 
To resolve those references, multimodal 
integration based on semantic constraints 
is insufficient. In this paper, we describe a 
graph-based probabilistic approach that 
simultaneously combines both semantic 
and temporal constraints to achieve a high 
performance.  
1 
                                                          
