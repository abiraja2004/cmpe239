 
Latent Semantic Analysis (LSA) is 
based on the Singular Value Decompo-
sition (SVD) of a term-by-document 
matrix for identifying relationships 
among terms and documents from co-
occurrence patterns. Among the multi-
ple ways of computing the SVD of a 
rectangular matrix X, one approach is to 
compute the eigenvalue decomposition 
(EVD) of a square 2 ? 2 composite ma-
trix consisting of four blocks with X and 
XT in the off-diagonal blocks and zero 
matrices in the diagonal blocks. We 
point out that significant value can be 
added to LSA by filling in some of the 
values in the diagonal blocks (corre-
sponding to explicit term-to-term or 
document-to-document associations) 
and computing a term-by-concept ma-
trix from the EVD.  For the case of mul-
tilingual LSA, we incorporate 
information on cross-language term 
alignments of the same sort used in Sta-
tistical Machine Translation (SMT). 
Since all elements of the proposed 
EVD-based approach can rely entirely 
on lexical statistics, hardly any price is 
paid for the improved empirical results. 
In particular, the approach, like LSA or 
SMT, can still be generalized to virtu-
ally any language(s); computation of the 
EVD takes similar resources to that of 
the SVD since all the blocks are sparse; 
 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
and the results of EVD are just as eco-
nomical as those of SVD. 
1 