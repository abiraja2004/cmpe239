
Multi-task learning is the problem of maxi-
mizing the performance of a system across a
number of related tasks. When applied to mul-
tiple domains for the same task, it is similar to
domain adaptation, but symmetric, rather than
limited to improving performance on a target
domain. We present a more principled, better
performing model for this problem, based on
the use of a hierarchical Bayesian prior. Each
domain has its own domain-specific parame-
ter for each feature but, rather than a constant
prior over these parameters, the model instead
links them via a hierarchical Bayesian global
prior. This prior encourages the features to
have similar weights across domains, unless
there is good evidence to the contrary. We
show that the method of (Daume? III, 2007),
which was presented as a simple ?prepro-
cessing step,? is actually equivalent, except
our representation explicitly separates hyper-
parameters which were tied in his work. We
demonstrate that allowing different values for
these hyperparameters significantly improves
performance over both a strong baseline and
(Daume? III, 2007) within both a conditional
random field sequence model for named en-
tity recognition and a discriminatively trained
dependency parser.
1 