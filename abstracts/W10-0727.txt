
We investigate human factors involved in de-
signing effective Human Intelligence Tasks
(HITs) for Amazon?s Mechanical Turk1. In
particular, we assess document relevance to
search queries via MTurk in order to evaluate
search engine accuracy. Our study varies four
human factors and measures resulting experi-
mental outcomes of cost, time, and accuracy
of the assessments. While results are largely
inconclusive, we identify important obstacles
encountered, lessons learned, related work,
and interesting ideas for future investigation.
Experimental data is also made publicly avail-
able for further study by the community2.
1 