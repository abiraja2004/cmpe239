
We investigate paradigmatic representations
of word context in the domain of unsupervised
syntactic category acquisition. Paradigmatic
representations of word context are based on
potential substitutes of a word in contrast to
syntagmatic representations based on prop-
erties of neighboring words. We compare
a bigram based baseline model with several
paradigmatic models and demonstrate signif-
icant gains in accuracy. Our best model based
on Euclidean co-occurrence embedding com-
bines the paradigmatic context representation
with morphological and orthographic features
and achieves 80% many-to-one accuracy on a
45-tag 1M word corpus.
1 