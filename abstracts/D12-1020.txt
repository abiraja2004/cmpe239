
Topic models traditionally rely on the bag-
of-words assumption. In data mining appli-
cations, this often results in end-users being
presented with inscrutable lists of topical un-
igrams, single words inferred as representa-
tive of their topics. In this article, we present
a hierarchical generative probabilistic model
of topical phrases. The model simultane-
ously infers the location, length, and topic of
phrases within a corpus and relaxes the bag-
of-words assumption within phrases by using
a hierarchy of Pitman-Yor processes. We use
Markov chain Monte Carlo techniques for ap-
proximate inference in the model and perform
slice sampling to learn its hyperparameters.
We show via an experiment on human subjects
that our model finds substantially better, more
interpretable topical phrases than do compet-
ing models.
1 