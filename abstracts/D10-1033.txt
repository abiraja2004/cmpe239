
Information-extraction (IE) research typically
focuses on clean-text inputs. However, an IE
engine serving real applications yields many
false alarms due to less-well-formed input.
For example, IE in a multilingual broadcast
processing system has to deal with inaccu-
rate automatic transcription and translation.
The resulting presence of non-target-language
text in this case, and non-language mate-
rial interspersed in data from other applica-
tions, raise the research problem of making
IE robust to such noisy input text. We ad-
dress one such IE task: entity-mention de-
tection. We describe augmenting a statistical
mention-detection system in order to reduce
false alarms from spurious passages. The di-
verse nature of input noise leads us to pursue
a multi-faceted approach to robustness. For
our English-language system, at various miss
rates we eliminate 97% of false alarms on in-
puts from other Latin-alphabet languages. In
another experiment, representing scenarios in
which genre-specific training is infeasible, we
process real financial-transactions text con-
taining mixed languages and data-set codes.
On these data, because we do not train on data
like it, we achieve a smaller but significant im-
provement. These gains come with virtually
no loss in accuracy on clean English text.
1 