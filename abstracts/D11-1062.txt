
We address the creation of cross-lingual tex-
tual entailment corpora by means of crowd-
sourcing. Our goal is to define a cheap and
replicable data collection methodology that
minimizes the manual work done by expert
annotators, without resorting to preprocess-
ing tools or already annotated monolingual
datasets. In line with recent works empha-
sizing the need of large-scale annotation ef-
forts for textual entailment, our work aims to:
i) tackle the scarcity of data available to train
and evaluate systems, and ii) promote the re-
course to crowdsourcing as an effective way
to reduce the costs of data collection without
sacrificing quality. We show that a complex
data creation task, for which even experts usu-
ally feature low agreement scores, can be ef-
fectively decomposed into simple subtasks as-
signed to non-expert annotators. The resulting
dataset, obtained from a pipeline of different
jobs routed to Amazon Mechanical Turk, con-
tains more than 1,600 aligned pairs for each
combination of texts-hypotheses in English,
Italian and German.
1 