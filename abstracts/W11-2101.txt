
The Workshop on Statistical Machine
Translation (WMT) has become one of
ACL?s flagship workshops, held annually
since 2006. In addition to soliciting pa-
pers from the research community, WMT
also features a shared translation task for
evaluating MT systems. This shared task
is notable for having manual evaluation as
its cornerstone. The Workshop?s overview
paper, playing a descriptive and adminis-
trative role, reports the main results of the
evaluation without delving deep into ana-
lyzing those results. The aim of this paper
is to investigate and explain some interest-
ing idiosyncrasies in the reported results,
which only become apparent when per-
forming a more thorough analysis of the
collected annotations. Our analysis sheds
some light on how the reported results
should (and should not) be interpreted, and
also gives rise to some helpful recommen-
dation for the organizers of WMT.
1 