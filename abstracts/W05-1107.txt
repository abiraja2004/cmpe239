
We describe the implementation steps re-
quired to scale high-order character lan-
guage models to gigabytes of training
data without pruning. Our online models
build character-level PAT trie structures on
the fly using heavily data-unfolded imple-
mentations of an mutable daughter maps
with a long integer count interface. Ter-
minal nodes are shared. Character 8-gram
training runs at 200,000 characters per
second and allows online tuning of hy-
perparameters. Our compiled models pre-
compute all probability estimates for ob-
served n-grams and all interpolation pa-
rameters, along with suffix pointers to
speedup context computations from pro-
portional to n-gram length to a constant.
The result is compiled models that are
larger than the training models, but exe-
cute at 2 million characters per second on
a desktop PC. Cross-entropy on held-out
data shows these models to be state of the
art in terms of performance.
1 