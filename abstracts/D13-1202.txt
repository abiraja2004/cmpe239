
Traditional distributional semantic models ex-
tract word meaning representations from co-
occurrence patterns of words in text cor-
pora. Recently, the distributional approach has
been extended to models that record the co-
occurrence of words with visual features in
image collections. These image-based models
should be complementary to text-based ones,
providing a more cognitively plausible view
of meaning grounded in visual perception. In
this study, we test whether image-based mod-
els capture the semantic patterns that emerge
from fMRI recordings of the neural signal.
Our results indicate that, indeed, there is a
significant correlation between image-based
and brain-based semantic similarities, and that
image-based models complement text-based
ones, so that the best correlations are achieved
when the two modalities are combined. De-
spite some unsatisfactory, but explained out-
comes (in particular, failure to detect differ-
ential association of models with brain areas),
the results show, on the one hand, that image-
based distributional semantic models can be a
precious new tool to explore semantic repre-
sentation in the brain, and, on the other, that
neural data can be used as the ultimate test set
to validate artificial semantic models in terms
of their cognitive plausibility.
1 