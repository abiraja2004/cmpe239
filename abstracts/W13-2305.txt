
We explore the use of continuous rat-
ing scales for human evaluation in the
context of machine translation evaluation,
comparing two assessor-intrinsic quality-
control techniques that do not rely on
agreement with expert judgments. Ex-
periments employing Amazon?s Mechan-
ical Turk service show that quality-control
techniques made possible by the use of
the continuous scale show dramatic im-
provements to intra-annotator agreement
of up to +0.101 in the kappa coefficient,
with inter-annotator agreement increasing
by up to +0.144 when additional standard-
ization of scores is applied.
1 