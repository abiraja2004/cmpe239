
We define a new learning task, minimum average
lookahead grammar induction, with strong poten-
tial implications for incremental parsing in NLP and
cognitive models. Our thesis is that a suitable learn-
ing bias for grammar induction is to minimize the
degree of lookahead required, on the underlying
tenet that language evolution drove grammars to be
efficiently parsable in incremental fashion. The in-
put to the task is an unannotated corpus, plus a non-
deterministic constraining grammar that serves as
an abstract model of environmental constraints con-
firming or rejecting potential parses. The constrain-
ing grammar typically allows ambiguity and is it-
self poorly suited for an incremental parsing model,
since it gives rise to a high degree of nondetermin-
ism in parsing. The learning task, then, is to in-
duce a deterministic LR (k) grammar under which
it is possible to incrementally construct one of the
correct parses for each sentence in the corpus, such
that the average degree of lookahead needed to do
so is minimized. This is a significantly more dif-
ficult optimization problem than merely compiling
LR (k) grammars, since k is not specified in ad-
vance. Clearly, na??ve approaches to this optimiza-
tion can easily be computationally infeasible. How-
ever, by making combined use of GLR ancestor ta-
bles and incremental LR table construction meth-
ods, we obtain an O(n3 + 2m) greedy approxima-
tion algorithm for this task that is quite efficient in
practice.
1 