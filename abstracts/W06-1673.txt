
The end-to-end performance of natural
language processing systems for com-
pound tasks, such as question answering
and textual entailment, is often hampered
by use of a greedy 1-best pipeline archi-
tecture, which causes errors to propagate
and compound at each stage. We present
a novel architecture, which models these
pipelines as Bayesian networks, with each
low level task corresponding to a variable
in the network, and then we perform ap-
proximate inference to find the best la-
beling. Our approach is extremely sim-
ple to apply but gains the benefits of sam-
pling the entire distribution over labels at
each stage in the pipeline. We apply our
method to two tasks ? semantic role la-
beling and recognizing textual entailment
? and achieve useful performance gains
from the superior pipeline architecture.
1 