
To date, few attempts have been made to de-
velop new methods and validate existing ones
for automatic evaluation of discourse coher-
ence in the noisy domain of learner texts.
We present the first systematic analysis of
several methods for assessing coherence un-
der the framework of automated assessment
(AA) of learner free-text responses. We ex-
amine the predictive power of different coher-
ence models by measuring the effect on per-
formance when combined with an AA system
that achieves competitive results, but does not
use discourse coherence features, which are
also strong indicators of a learner?s level of at-
tainment. Additionally, we identify new tech-
niques that outperform previously developed
ones and improve on the best published result
for AA on a publically-available dataset of En-
glish learner free-text examination scripts.
1 