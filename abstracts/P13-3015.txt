
Identifying complex words (CWs) is an
important, yet often overlooked, task
within lexical simplification (The process
of automatically replacing CWs with sim-
pler alternatives). If too many words are
identified then substitutions may be made
erroneously, leading to a loss of mean-
ing. If too few words are identified then
those which impede a user?s understand-
ing may be missed, resulting in a com-
plex final text. This paper addresses the
task of evaluating different methods for
CW identification. A corpus of sentences
with annotated CWs is mined from Sim-
ple Wikipedia edit histories, which is then
used as the basis for several experiments.
Firstly, the corpus design is explained and
the results of the validation experiments
using human judges are reported. Exper-
iments are carried out into the CW identi-
fication techniques of: simplifying every-
thing, frequency thresholding and training
a support vector machine. These are based
upon previous approaches to the task and
show that thresholding does not perform
significantly differently to the more na??ve
technique of simplifying everything. The
support vector machine achieves a slight
increase in precision over the other two
methods, but at the cost of a dramatic trade
off in recall.
1 