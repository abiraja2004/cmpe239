
A central topic in natural language process-
ing is the design of lexical and syntactic fea-
tures suitable for the target application. In this
paper, we study convolution dependency tree
kernels for automatic engineering of syntactic
and semantic patterns exploiting lexical simi-
larities. We define efficient and powerful ker-
nels for measuring the similarity between de-
pendency structures, whose surface forms of
the lexical nodes are in part or completely dif-
ferent. The experiments with such kernels for
question classification show an unprecedented
results, e.g. 41% of error reduction of the for-
mer state-of-the-art. Additionally, semantic
role classification confirms the benefit of se-
mantic smoothing for dependency kernels.
1 