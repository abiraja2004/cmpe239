
Current parameters of accurate unlexical-
ized parsers based on Probabilistic Context-
Free Grammars (PCFGs) form a two-
dimensional grid in which rewrite events
are conditioned on both horizontal (head-
outward) and vertical (parental) histories.
In Semitic languages, where arguments
may move around rather freely and phrase-
structures are often shallow, there are ad-
ditional morphological factors that govern
the generation process. Here we pro-
pose that agreement features percolated up
the parse-tree form a third dimension of
parametrization that is orthogonal to the pre-
vious two. This dimension differs from
mere ?state-splits? as it applies to a whole
set of categories rather than to individual
ones and encodes linguistically motivated
co-occurrences between them. This paper
presents extensive experiments with exten-
sions of unlexicalized PCFGs for parsing
Modern Hebrew in which tuning the param-
eters in three dimensions gradually leads to
improved performance. Our best result in-
troduces a new, stronger, lower bound on the
performance of treebank grammars for pars-
ing Modern Hebrew, and is on a par with
current results for parsing Modern Standard
Arabic obtained by a fully lexicalized parser
trained on a much larger treebank.
1 Dimensions of Unlexicalized Parsing
Probabilistic Context Free Grammars (PCFGs) are
the formal backbone of most high-accuracy statisti-
cal parsers for English, and a variety of techniques
was developed to enhance their performance rela-
tive to the na??ve treebank implementation ? from
unlexicalized extensions exploiting simple category
splits (Johnson, 1998; Klein and Manning, 2003)
to fully lexicalized parsers that condition events be-
low a constituent upon the head and additional lexi-
cal content (Collins, 2003; Charniak, 1997). While
it is clear that conditioning on lexical content im-
proves the grammar?s disambiguation capabilities,
Klein and Manning (2003) demonstrate that a well-
crafted unlexicalized PCFG can close the gap, to a
large extent, with current state-of-the-art lexicalized
parsers for English.
The factor that sets apart vanilla PCFGs (Char-
niak, 1996) from their unlexicalized extensions pro-
posed by, e.g., (Johnson, 1998; Klein and Manning,
2003), is the choice for statistical parametrization
that weakens the independence assumptions implicit
in the treebank grammar. Studies on accurate unlex-
icalized parsing models outline two dimensions of
parametrization. The first, proposed by (Johnson,
1998), is the annotation of parental history, and the
second encodes a head-outward generation process
(Collins, 2003). Johnson (1998) augments node la-
bels with the label of their parent, thus incorporat-
ing a dependency on the node?s grandparent. Collins
(2003) proposes to generate the head of a phrase first
and then generate its sisters using Markovian pro-
cesses, thereby exploiting head/sister-dependencies.
156
Klein and Manning (2003) systematize the dis-
tinction between these two forms of parametrization
by drawing them on a horizontal-vertical grid: par-
ent encoding is vertical (external to the rule) whereas
head-outward generation is horizontal (internal to
the rule). By varying the value of the parame-
ters along the grid, Klein and Manning (2003) tune
their treebank grammar to achieve improved perfor-
mance. This two-dimensional parametrization has
been instrumental in devising parsing models that
improve disambiguation capabilities for English as
well as other languages, such as German (Dubey and
Keller, 2003) Czech (Collins et al, 1999) and Chi-
nese (Bikel and Chiang, 2000). However, accuracy
results for parsing languages other than English still
lag behind.1
We propose that for various languages includ-
ing the Semitic family, e.g. Modern Hebrew (MH)
and Modern Standard Arabic (MSA), a third di-
mension of parametrization is necessary for encod-
ing linguistic information relevant for breaking false
independence assumptions. In Semitic languages,
arguments may move around rather freely and the
phrase-structure of clause-level categories is often
shallow. For such languages agreement features play
a role in disambiguation at least as important as the
vertical and horizontal conditioning. We propose a
third dimension of parameterizations that encodes
morphological features such as those realizing syn-
tactic agreement. These features are percolated from
surface forms in a bottom-up fashion and express
information that is complementary to the horizon-
tal and vertical generation histories proposed before.
Such morphological information refines syntactic
categories based on their morpho-syntactic role, and
captures linguistically motivated co-occurrences and
dependencies manifested via, e.g., morpho-syntactic
agreement.
This work aims at parsing MH and explores the
empirical contribution of the three dimensions of
parameters specified above. We present extensive
experiments that gradually lead to improved perfor-
mance as we extend the degree to which the three
dimensions are exploited. Our best model uses all
three dimensions of parametrization, and our best re-
1The learning curves over increasing training data (e.g., for
German (Dubey and Keller, 2003)) show that treebank size can-
not be the sole factor to account for the inferior performance.
sult is on a par with those achieved for MSA using a
fully lexicalized parser and a much larger treebank.
The remainder of this document is organized as fol-
lows. In section 2 we review characteristic aspects
of MH (and other Semitic languages) and illustrate
the special role of morphology and dependencies
displayed by morpho-syntactic processes using the
case of syntactic definiteness in MH. In section 3 we
define our three-dimensional parametrization space.
In section 4 we spell out the method and procedure
for the empirical evaluation of one, two and three
parametrization dimensions, and in section 5 we re-
port and analyze results for different parametrization
choices. Finally, section 6 discusses related work
and in section 7 we summarize and conclude.
2 Dimensions of Modern Hebrew Syntax
Parsing MH is in its infancy. Although a syntacti-
cally annotated corpus has been available for quite
some time (Sima?an et al, 2001), we know of only
two studies attempting to parse MH using statistical
methods (see section 6). One reason for the sparse-
ness in this field is that the adaptation of existing
models to parsing MH is technically involved yet
does not guarantee to yield comparable results as
the processes that license grammatical structures of
phrases and sentences in MH differ from those as-
sumed for English. This section outlines differences
between English and MH and discusses their reflec-
tion in the MH treebank annotation scheme. We
argue that on top of syntactic processes exploited
by current parsers there is an orthogonal morpho-
syntactic dimension which is invaluable for syntac-
tic disambiguation, and it can be effectively learned
using simple treebank grammars.
2.1 Modern Hebrew Structure
Phrases and sentences in MH, as well as in Arabic
and other Semitic languages, have a relatively flexi-
ble phrase structure. Subjects, verbs and objects can
be inverted and prepositional phrases, adjuncts and
verbal modifiers can move around rather freely. The
factors that affect word-order in the language are not
exclusively syntactic and have to do with rhetorical
and pragmatic factors as well.2
2See, for instance, (Melnik, 2002) for an Information
Structure-syntactic account of verb initial sentences.
157
(a) S
NP.MP-SBJ
CD.MP
sni
two.MP
N.MP
hildim
the-children.MP
VP.MP
V.MP
aklw
ate.MP
NP.FS-OBJ
N.FS
ewgh
cake.FS
(b) S
NP.FS-OBJ
N.FS
ewgh
cake.FS
VP.MP
V.MP
aklw
ate.MP
NP.MP-SBJ
CD.MP
sni
two.MP
N.MP
hildim
the-children.MP
Figure 1: Word Order and Agreement Features in MH
Phrases: Agreement on MP features reveals the subject-
predicate dependency between surface forms and their dom-
inating constituents in a variable phrase-structure (marking
M(asculine), F(eminine), S(ingular), P(lural).)
It would be too strong a claim, however, to clas-
sify MH (and similar languages) as a free-word-
order language in the canonical sense. The level of
freedom in the order and number of internal con-
stituents varies between syntactic categories. Within
a verb phrase or a sentential clause, for instance,
the order of constituents obeys less strict rules than
within, e.g., a noun phrase.3 Figure 1 illustrates two
syntactic structures that express the same grammat-
ical relations yet vary in their internal order of con-
stituents. Within the noun phrase constituents, how-
ever, determiners always precede nouns.
Within the flexible phrase structure it is typically
morphological information that provides cues for the
grammatical relations between surface forms. In
figure 1, for example, it is agreement on gender
and number that reveals the subject-predicate depen-
dency between surface forms. Figure 1 also shows
that agreement features help to reveal such relations
between higher levels of constituents as well.
Determining the child constituents that contribute
each of the features is not a trivial matter either. To
illustrate the extent and the complexity of that matter
let us consider definiteness in MH, which is morpho-
logically marked (as an h prefix to the stem, glossed
here explicitly as ?the-?) and behaves as a syntactic
3See (Wintner, 2000) and (Goldberg et al, 2006) for formal
and statistical accounts (respectively) of noun phrases in MH.
(a) NP.FS.D
NP.FS.D
sganit hmnhl
deputy.FS the-manager.MS.D
ADJP.FS.D
hmswrh
the-dedicated.FS.D
(a) S
NP.FS.D
sganit hmnhl
deputy.FS the-manager.MS.D
PREDP.FS
mswrh
dedicated.FS
Figure 2: Definiteness in MH as a Phrase-Level Agreement
Feature: Agreement on definiteness helps to determine the in-
ternal structure of a higher level NP (a), and the absence thereof
helps to determine the attachment to a predicate in a verb-less
sentence (b) (marking D(efiniteness))
(a) S
NP.FS.D
NNT.FS
sganit
deputy.FS
N.MS.D
hmnhl
the-manager.MS.D
VP.FS
V.FS
htpjrh
resigned.FS
(b) S?V?
NP?NNT?.FS.D
NNT.FS
sganit
deputy.FS
N.MS.D
hmnhl
the-manager.MS.D
VP?V?).FS
V.FS
htpjrh
resigned.FS
Figure 3: Phrase-Level Agreement Features and Head-
Dependencies in MH: The direction of percolating definiteness
in MH is distinct of that of the head (marking ?head-tag?)
property (Danon, 2001). Definite noun-phrases ex-
hibit agreement with other modifying phrases, and
such agreement helps to determine the internal struc-
ture, labels, and the correct level of attachment as
illustrated in figure 2. The agreement on definite-
ness helps to determine the internal structure of noun
phrases 2(a), and the absence thereof helps in de-
termining the attachment to predicates in verb-less
sentences, as in 2(b). Finally, definiteness may be
percolated from a different form than the one deter-
mining the gender and number of a phrase. In figure
3(a), for instance, the definiteness feature (marked
as D) percolates from ?hmnhl ? (the-manager.MS.D)
while the gender and number are percolated from
?sganit ? (deputy.FS). The direction of percolation
of definiteness may be distinct of that of percolat-
ing head information, as can be seem in figure 3(b).
(The direction of head-dependencies in MH typi-
cally coincides with that of percolating gender.)
To summarize, agreement features are helpful in
analyzing and disambiguating syntactic structures in
MH, not only at the lexical level, but also at higher
levels of constituency. In MH, features percolated
from different surface forms jointly determine the
features of higher-level constituents, and such fea-
tures manifest multiple dependencies, which in turn
cannot be collapsed onto a single head.
158
2.2 The Modern Hebrew Treebank Scheme
The annotation scheme of version 2.0 of the MH
treebank (Sima?an et al, 2001)4 aims to capture the
morphological and syntactic properties of MH just
described. This results in several aspects that dis-
tinguish the MH treebank from, e.g., the WSJ Penn
treebank annotation scheme (Marcus et al, 1994).
The MH treebank is built over word segments.
This means that the yields of the syntactic trees do
not correspond to space delimited words but rather
to morphological segments that carry distinct syn-
tactic roles, i.e., each segment corresponds to a sin-
gle POS tag. (This in turn means that prefixes
marking determiners, relativizers, prepositions and
definite articles are segmented away and appear as
leaves in a syntactic parse tree.) The POS categories
assigned to segmented words are decorated with fea-
tures such as gender, number, person and tense, and
these features are percolated higher up the tree ac-
cording to pre-defined syntactic dependencies (Kry-
molowski et al, 2007). Since agreement features
of non-terminal constituents may be contributed by
more than one child, the annotation scheme defines
multiple dependency labels that guide the percola-
tion of the different features higher up the tree. Def-
initeness in the MH treebank is treated as a segment
at the POS tags level and as a feature at the level of
non-terminals. As any other feature, it is percolated
higher up the tree according to marked dependency
labels. Table 1 lists the features and values annotated
on top of syntactic categories and table 2 describes
the dependencies according to which these features
are percolated from child constituents to their par-
ents.
In order to comply with the flexible phrase struc-
ture in MH, clausal categories (S, SBAR and FRAG
and their corresponding interrogatives SQ, SQBAR
and FRAGQ) are annotated as flat structures. Verbs
(VB tags) always attach to a VP mother, however
only non-finite VBs can accept complements un-
der the same VP parent, meaning that all inflected
verb forms are represented as unary productions
under an inflected VP. NP and PP are annotated
4Version 2.0 of the MH treebank is publicly available
at http://mila.cs.technion.ac.il/english/
index.html along with a complete overview of the MH
annotation scheme and illustrative examples (Krymolowski et
al., 2007).
Feature:Value Value Encoded
gender:Z masculine
gender:N feminine
gender:B both
number:Y singular
number:R plural
number:B both
definiteness:H definite
definiteness:U underspecified
Table 1: Features and Values in the MH Treebank
Dependency Type Features Percolated
DEP HEAD all
DEP MAJOR at least gender
DEP NUMBER number
DEP DEFINITE definiteness
DEP ACCUSATIVE case
DEP MULTIPLE all (e.g., conjunction)
Table 2: Dependency Labels in the MH Treebank
as nested structures capturing the recursive struc-
ture of construct-state nouns, numerical expressions
and possession. An additional category, PREDP, is
added in the treebank scheme to account for sen-
tences in MH that lack a copular element, and it may
also be decorated with inflectional features agreeing
with the subject. The MH treebank scheme also fea-
tures null elements that mark traces and additional
labels that mark functional features (e.g., SBJ,OBJ)
which we strip off and ignore throughout this study.
Morphological features percolated up the tree
manifest dependencies that are marked locally yet
have a global effect. We propose to learn treebank
grammars in which the syntactic categories are aug-
mented with morphological features at all levels of
the hierarchy. This allows to learn finer-grained
categories with subtle differences in their syntactic
behavior and to capture non-independence between
certain parts of the syntactic parse-tree.
3 Refining the Parameter Space
(Klein and Manning, 2003) argue that parent en-
coding on top of syntactic categories and RHS
markovization of CFG productions are two instances
of the same idea, namely that of encoding the gener-
ation history of a node to a varying degree. They
subsequently describe two dimensions that define
their parameters? space. The vertical dimension (v),
capturing the history of the node?s ancestors in a top-
159
down generation process (e.g., its parent and grand-
parent), and the horizontal dimension (h), capturing
the previously generated horizontal ancestors of a
node (effectively, its sisters) in a head-outward gen-
eration process. By varying the value of h and v
along this two-dimensional grid they improve per-
formance of their induced treebank grammar.
Formally, the probability of a parse tree pi is cal-
culated as the probability of its derivation, the se-
quential application of rewrite rules. This in turn
is calculated as the product of rules? probabilities,
approximated by assuming independence between
them P (pi) = ?i P (ri|r1 ? ... ? ri?1) ?
?
i P (ri).
The vertical dimension v can be thought of as a func-
tion ?0 selecting features from the generation his-
tory of the constituent thus restoring selected depen-
dencies:
P (ri) = P (ri|?0(r1 ? .. ? ri?1))
The horizontal dimension h can be thought of as two
functions ?1,?2 over decomposed rules, where ?1
selects hidden internal features of the parent, and
?2 selects previously generated sisters in a head-
outward Markovian process (we retain here the as-
sumption that the head child H always matters).
P (ri) = Ph(H|?1(LHS(ri)))
?
?
C?RHS(ri)?H
PC(C|?2(RHS(ri)),H)
The fact that the default notion of a treebank
grammar takes v = 1 (i.e., ?0(r1 ? .. ? ri?1) = ?)
and h = ? (RHS cannot decompose) is, according
to Klein and Manning (2003), a historical accident.
We claim that languages with freeer word order
and richer morphology call for an additional dimen-
sion of parametrization. The additional parameter
shows to what extent morphological features en-
coded in a specialized structure back up the deriva-
tion of the tree. This dimension can be thought of
as a function ?3 selecting aspects of morphological
orthogonal analysis of the rules, where MA denotes
morphological analysis of the syntactic categories in
both LHS and RHS of the rule.
P (ri) = P (ri|?3(MA(ri)))
The fact that in current parsers ?3(MA(ri)) = ? is,
we claim, another historical accident. Parsing En-
glish is quite remarkable in that it can be done with
Figure 4: The Three-Dimensional Parametrization Space
impoverished morphological treatment, but for lan-
guages in which morphological processes are more
pertinent, we argue, bi-dimensional parametrization
shall not suffice.
The emerging picture is as follows. Bare-category
skeletons reside in a bi-dimensional parametrization
space (figure 3(a)) in which the vertical (figure 3(b))
and horizontal (figure 3(c)) parameter instantiations
elaborate the generation history of a non-terminal
node. Specialized structures enriched with (an in-
creasing amount of) morphological features reside
deeper along a third dimension we refer to as depth
(d). Figure 4 illustrates an instantiation of d = 1
with a single definiteness feature. Higher d values
would imply adding more (accumulating) features.
Klein and Manning (2003) view the vertical
and horizontal parametrization dimensions as im-
plementing external and internal annotation strate-
gies respectively. External parameters indicate fea-
tures of the external environment that influence the
node?s expansion possibilities, and internal parame-
ters mark aspects of hidden internal content which
influence constituents? external distribution. We
view the third dimension of parametrization as im-
plementing a relational strategy of annotation en-
coding the way different constituents may combine
to form phrases and sentences. In a bottom up pro-
cess this annotation strategy imposes soft constraints
on a the top-down head-outward generation process.
Figure 6(a) focuses on a selected NP node high-
lighted in figure 4 and shows its expansion possibil-
ities in three dimensions. Figure 6(b) illustrates how
the depth expansion interacts with both parent anno-
160
(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimension
Figure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003)
tation and neighbor dependencies thereby affecting
both distributions.
3.1 A Note on State-Splits
Recent studies (Klein and Manning, 2003; Mat-
suzaki et al, 2005; Prescher, 2005; Petrov et al,
2006) suggest that category-splits help in enhanc-
ing the performance of treebank grammars, and a
previous study on MH (Tsarfaty, 2006) outlines spe-
cific POS-tags splits that improve MH parsing ac-
curacy. Yet, there is a major difference between
category-splits, whether manually or automatically
acquired, and the kind of state-splits that arise from
agreement features that refine phrasal categories.
While category-splits aim at each category in iso-
lation, agreement features apply to a whole set
of categories all at once, thereby capturing refine-
ment of the categories as well as linguistically mo-
tivated co-occurrences between them. Individual
category-splits are viewed as taking place in a two-
dimensional space and it is hard to analyze and em-
pirically evaluate their interaction with other annota-
tion strategies. Here we propose a principled way to
statistically model the interaction between different
linguistic processes that license grammatical struc-
tures and empirically contrast their contribution.
3.2 A Note on Stochastic AV grammars
The practice of having morphological features or-
thogonal to a constituency structure is not a new
one and is familiar from formal theories of syntax
such as HPSG (Sag et al, 2003) and LFG (Ka-
plan and Bresnan, 1982). Here we propose to re-
frame systematic morphological decoration of syn-
tactic categories at all levels of the hierarchy as
(a) (b)
Figure 6: The Expansion Possibilities of a Non-Terminal
Node: Expanding the NP from figure 4 in a three-dimensional
parameterization Space
an additional dimension of statistical estimation for
learning unlexicalized treebank PCFGs. Our pro-
posal deviates from various stochastic extensions of
such constraints-based grammatical formalisms (cf.
(Abney, 1997)) and has the advantage of elegantly
bypassing the issue of loosing probability mass to
failed derivations due to unification failures. To the
best of our knowledge, this proposal has not been
empirically explored before.
4 Experimental Setup
Our goal is to determine the optimal strategy for
learning treebank grammars for MH and to contrast
it with bi-dimensional strategies explored for En-
glish. The methodology we use is adopted from
(Klein and Manning, 2003) and our procedure is
identical to the one described in (Johnson, 1998).
We define transformations over the treebank that ac-
cept as input specific points in the (h, v, d) space de-
picted in figure 7. We use the transformed training
sets for learning different treebank PCFGs which we
then used to parse unseen sentences, and detrans-
form the parses for the purpose of evaluation.5
5Previous studied on MH used different portions of the tree-
bank and its annotation scheme due to its gradual development
161
Data We use version 2.0 of the MH treebank
which consists of 6501 sentences from the daily
newspaper ?Ha?aretz?. We employ the syntactic cat-
egories, POS categories and morphological features
annotated therein. The data set is split into 13 sec-
tions consisting of 500 sentences each. We use the
first section (section 0) as our development set and
the last section (section 12) as our test set. The re-
maining sentences (sections 1?11) are all used for
training. After removing empty sentences, sentences
with uneven bracketing and sentences that do not
match the annotation scheme6 we remain with a de-
vset of 483 sentences (average length in word seg-
ments 48), a trainset of 5241 sentences (53) and
a testset of 496 sentences (58). Since this work
is only the first step towards the development of a
broad-coverage statistical parser for MH (and other
Semitic languages) we use the development set for
parameter-tuning and error analysis and use the test
set only for confirming our best results.
Models The models we implement use one-, two-
or three-dimensional parametrization and different
instantiation of values thereof. (Due to the small
size of our data set we only use the values {0, 1}
as possible instantiations.)
The v dimension is implemented using a trans-
form as in (Johnson, 1998) where v = 0 corresponds
to bare syntactic categories and v = 1 augments
node labels with the label of their parent node.
The h dimension is peculiar in that it distinguishes
PCFGs (h = ?), where RHS cannot decompose,
from their head-driven unlexicalized variety. To im-
plement h 6= ? we use a PCFG transformation em-
ulating (Collins, 2003)?s first model, in which sisters
are generated conditioned on the head tag and a sim-
ple ?distance? function (Hageloh, 2007).7 The in-
process. As the MH treebank is approaching maturity we feel
that the time is ripe to standardize its use for MH statistical
parsing. The software we implemented will be made available
for non-commercial use upon request to the author(s) and the
feature percolation software by (Krymolowski et al, 2007) is
publicly available through the Knowledge Center for Process-
ing Hebrew. By this we hope to increase the interest in MH
within the parsing community and to facilitate the application
of more sophisticated models by cutting down on setup time.
6Marked as ?NO MATCH? in the treebank.
7A formal overview of the transformation and its corre-
spondence to (Collins, 2003)?s models is available at (Hageloh,
2007). We use the distance function defined therein, marking
the direction and whether it is the first node to be generated.
stantiated value of h then selects the number of pre-
viously generated (non-head) sisters to be taken into
account when generating the next sister in a Marko-
vian process (?2 in our formal exposition).
The d dimension we proposed is implemented us-
ing a transformation that augments syntactic cate-
gories with morphological features percolated up the
tree. We use d = 0 to select bare syntactic cate-
gories and instantiate d = 1 with the definiteness
feature. The decision to select definiteness (rather
than, e.g., gender or number) is rather pragmatic as
its direction of percolation may be distinct of head
information and the question remains whether the
combination of such non-overlapping dependencies
is instrumental for parsing MH.
Our baseline model is a vanilla treebank PCFG
as described in (Charniak, 1996) which we locate
on the (?, 0, 0) point of our coordinates-system.
In a first set of experiments we implement simple
PCFG extensions of the treebank trees based on se-
lected points on the (?, v, d) plain. In a second
set of experiments we use an unlexicalized head-
driven baseline a` la (Collins, 2003) located on the
(0, 0, 0) coordinate. We transform the treebank trees
in correspondence with different points in the three-
dimensional space defined by (h, v, d). The models
we implement are marked in the coordinate-system
depicted in figure 7. The implementation details of
the transformations we use are spelled out in tables
3?4.
Procedure We implement different models that
correspond to different instantiations of h, v and d.
For each instantiation we transform the training set
and learn a PCFG using Maximum Likelihood es-
timates, and we use BitPar (Schmidt, 2004), an ef-
ficient general-purpose parser, to parse unseen sen-
tences. The input to the parser is a sequence of word
segments where each segment corresponds to a sin-
gle POS tag, possibly decorated with morphologi-
cal features. This setup assumes partial morpholog-
ical disambiguation (namely, segmentation) but cru-
cially we do not disambiguate their respective POS
categories. This setup is more appropriate for us-
ing general-purpose parsing tools and it makes our
results comparable to studies in other languages.8
8Our working assumption is that better performance of a
parsing model in our setup will improve performance also
162
Transliterate The lexical items (leaves) in the MH treebank are written left-to-write and are encoded
in utf8. A transliteration software is used to convert the utf encoding into Latin characters and to reverse
their order, essentially allowing for standard left-to-right processing.
Correct The manual annotation resulted in unavoidable errors in the annotation scheme, such as typos
(e.g., SQBQR instead of SQBAR) wrong delimiters (e.g., ?-? instead of ? ?) or wrong feature order (e.g.,
number-gender instead of gender-number). We used an automatic script to detect these error, we manually
determine their correction. Then we created an automatic script to apply all fixes (57 errors in 1% sentences).
Re-attach VB elements are attached by convention to a VP which inherits its morphological features.
9 VB instances in the treebank are mistakenly attached to an S parent without an intermediate VP level.
Our software re-attaches those VB elements to a VP parent and percolates its morphological features.
Disjoint Due to recursive processes of generating noun phrases and numerical expression (smixut)
in MH the sets of POS and syntactic categories are not disjoint. This is a major concern for PCFG parsers
that assume disjoint sets of pre- and non-terminals. The overlap between the sets also introduces additional
infinite derivations to which we loose probability mass. Our software takes care to decorate POS categories
used as non-terminal with an additional ?P?, creating a new set of categories encoding partial derivations.
Lexicalize A pre-condition for applying horizontal parameterizations a` la Collins is the annotation of
heads of syntactic phrases. The treebank provided by the knowledge center does not define unique heads,
but rather, mark multiple dependencies for some categories and none for others. Our software uses rules
for choosing the syntactic head according to specified dependencies and a head table when none are specified.
Linearize In order to implement the head-outward constituents? generation process we use software made
available to us by (Hageloh, 2007) which converts PCFG production such as the generation of a head is followed by left and right
markovized derivation processes. We used two versions of Markovization, one which conditions only on the
head and a distance function, and another which conditions also on immediately neighboring sister(s).
Decorate Our software implements an additional general transform which selects the features that are to be
annotated on top of syntactic categories to implement various parametrization decisions. This transform can be
used for, e.g., displaying parent information, selecting morphological features, etc.
Table 3: Transforms over the MH Treebank: We clean and correct the treebank using Transliterate, Correct, Re-attach and
Disjoint, and transform the training set according to certain parametrization decisions using Lexicalize, Linearize and Decorate.
Smoothing pre-terminal rules is done explicitly by
collecting statistics on ?rare word? occurrences and
providing the parser with possible open class cat-
egories and their corresponding frequency counts.
The frequency threshold defining ?rare words? was
tuned empirically and set to 1. The resulting test
parses are detransformed and to skeletal constituent
structures, and are compared against the gold parses
to evaluate parsing accuracy.
Evaluation We evaluate our models using EVALB
in accordance with standard PARSEVAL evaluation
metrics. The evaluation of all models focuses on
Labeled Precision and Recall considering bare syn-
tactic categories (stripping off all morphological or
parental features and removing intermediate nodes
for linearization). We report the average F-measure
for sentences of length up to 40 and for all sentences
(F?40 and FAll respectively). We report the results
within an integrated model for morphological and syntactic dis-
ambiguation in the spirit of (Tsarfaty, 2006). We conjecture
that the kind of models developed here which takes into account
morphological information is more appropriate for the morpho-
logical disambiguation task defined therein.
for two evaluation options, once including punctua-
tion marks (WP ) and once excluding them (WOP ).
5 Results
Our baseline for the first set of experiments is
a vanilla PCFG as described in (Charniak, 1996)
(without a preceding POS tagging phase and without
right branching corrections). We transform the tree-
bank trees based on various points in the (?, v, d)
two-dimensional space to evaluate the performance
of the resulting PCFG extensions.
Table 5 reports the accuracy results for all models
on section 0 (devset) of the treebank. The accuracy
results for the vanilla PCFG are approximately 10%
lower than reported by (Charniak, 1996) for English
demonstrating that parsing MH using the currently
available treebank is a harder task. For all unlexical-
ized extensions learned from the transfromed tree-
banks, the resulting grammars show enhanced dis-
ambiguation capabilities and improved parsing ac-
curacy. We observe that the vertical dimension con-
tributes the most from both one-dimensional mod-
163
Name Params Description Transforms used
DIST h = 0 0-order Markov process Lexicalize(category), Linearize(distance)
MRK h = 1 1-order Markov process Lexicalize(category), Linearize(distance, neighbor)
PA v = 1 Parent Annotation Decorate(parent)
DEF d = 1 Definiteness feature percolation Decorate(definiteness)
Table 4: Implementing Different Parametrization Options using Transforms
Implementation (h, v, d) FALL F?40 FALL F?40
WP WP WOP WOP
PCFG (?, 0, 0) 65.17 66.63 66.17 67.7
PA (?, 0, 1) 70.6 71.96 70.96 72.18
DEF (?, 1, 0) 67.53 68.78 68.82 70.06
PA+DEF (?, 1, 1) 72.63 73.89 73.01 74.11
Table 5: PCFG Two-Dimensional Extensions: Accuracy re-
sults for parsing the devest (section 0)
els. A qualitative error analysis reveals that parent
annotation strategy distinguishes effectively various
kinds of distributions clustered together under a sin-
gle category. For example, S categories that appear
under TOP tend to be more flat than S categories ap-
pearing under SBAR (SBAR clauses typically gen-
erate a non-finite VP node under which additional
PP modifiers can be attached).
Orthogonal morphological marking provide addi-
tional information that is indicative of the kind of
dependencies that exist between a category and its
various child constituents, and we see that the d di-
mension instantiated with definiteness not only con-
tribute more than 2% to the overall parsing accuracy
of a vanilla PCFG, but also contributes as much to
the improvement obtained from a treebank already
annotated with the vertical dimension. The contribu-
tions are thus additive providing preliminary empir-
ical support to our claim that these two dimensions
provide information that is complementary.
In our next set of experiments we evaluate the
contribution of the depth dimension to extensions of
the head-driven unlexicalized variety a` la (Collins,
2003). We set our baseline at the (0, 0, 0) coordi-
nate and evaluate models that combine one, two and
three dimensions of parametrization. Table 6 shows
the accuracy results for parsing section 0 using the
resulting models.
The first outcome of these experiments is that our
new baseline improves on the accuracy results of
a simple treebank PCFG. This result indicates that
head-dependencies which play a role in determin-
ing grammatical structures in English are also in-
strumental for parsing MH. However, the marginal
contribution of the head-driven variation is surpris-
ingly low. Next we observe that for one-dimensional
models the vertical dimension still contributes the
most to parsing accuracy. However, morphologi-
cal information represented by the depth dimension
contributes more to parsing accuracy than informa-
tion concerning immediately preceding sisters on
the horizontal dimension. This outcome is consis-
tent with our observation that the grammar of MH
puts less significance on the position of constituents
relative to one others and that morphological in-
formation is more indicative of the kind of syntac-
tic relations that appear between them. For two-
dimensional models, incorporating the depth dimen-
sion (orthogonal morphological marking) is better
than not doing so, and relying solely on horizon-
tal/vertical parameters performs slightly worse than
the vertical/depth combination. The best performing
model for two-dimensional head-driven extensions
is the one combining vertical history and morpho-
logical depth. This is again consistent with the prop-
erties of MH highlighted in section 2 ? parental in-
formation gives cues about the possible expansion
on the current node, and morphological information
indicates possible interrelation between child con-
stituents that may be generated in a flexible order.
Our second set of experiments shows that a three-
dimensional annotation strategy strikes the best bal-
ance between bias and variance and achieves the best
accuracy results among all models. Different dimen-
sions provide different sorts of information which
are complementary, resulting in a model that is ca-
pable of generalizing better. The total error reduc-
tion from a plain PCFG is more than 20%, and our
best result is on a par with those achieved for other
languages (e.g., 75% for MSA).
164
Implementation Params FALL F?40 FALL F?40
(h, v, d) WP WP WOP WOP
DIST (0, 0, 0) 66.56 68.20 67.59 69.24
MRK (1, 0, 0) 66.69 68.14 67.93 69.37
PA (0, 1, 0) 68.87 70.48 69.64 70.91
DEF (0, 0, 1) 68.85 69.92 70.42 71.45
PA+MRK (1, 1, 0) 69.97 71.48 70.69 71.98
MRK+DEF (1, 0, 1) 69.46 70.79 71.05 72.37
PA+DEF (0, 1, 1) 71.15 72.34 71.98 72.91
PA+MRK+DEF (1, 1, 1) 72.34 73.63 73.27 74.41
Table 6: Head-Driven Three-Dimensional Extensions: Ac-
curacy results for parsing the devest (section 0)
Implementation Params FALL F?40 FALL F?40
(h, v, d) WP WP WOP WOP
PCFG (?, 0, 0) 65.08 67.31 65.82 68.22
PCFG+PA+DEF (?, 1, 1) 72.26 74.46 72.42 74.52
DIST (0, 0, 0) 66.33 68.79 67.06 69.47
PA+MRK+DEF (1, 1, 1) 72.64 74.64 73.21 75.25
Table 7: PCFG and Head-Driven Unlexicalized Models:
Accuracy Results for parsing the testst (section 12)
Figure 8 shows the FAll(WOP ) results for all
models we implemented. In general, we see that for
parsing MH higher dimensionality is better. More-
over, we see that for all points on the (v, h, 0) plain
the corresponding models on the (v, h, 1) plain al-
ways perform better. We further see that the contri-
bution of the depth dimension to a parent annotated
PCFG can compensate, to a large extent on the lack
of head-dependency information. These accumula-
tive results, then, provide empirical evidence to the
importance of morphological and morpho-syntactic
processes such as definiteness for syntactic analysis
and disambiguation as argued for in section 2.
We confirm our results on the testset and report
in table 7 our results on section 12 of the treebank.
The performance has slightly increased and we ob-
tain better results for our best strategy. We retain the
high error-reduction rate and propose our best result,
75.25% for sentences of length ? 40, as an empiri-
cally established string baseline on the performance
of treebank grammars for MH.
6 Related Work
The MH treebank (Sima?an et al, 2001), a mor-
phologically and syntactically annotated corpus, has
been successfully used for various NLP tasks such as
morphological disambiguation, POS tagging (Bar-
Haim et al, 2007) and NP chunking (Goldberg et
al., 2006). However its use for statistical parsing has
been more scarce and less successful. The only pre-
vious studies attempting to parse MH we know of
are (Sima?an et al, 2001), applying a variation of the
DOP tree-gram model to 500 sentences, and (Tsar-
faty, 2006), using a treebank PCFG in an integrated
system for morphological and syntactic disambigua-
tion.9 The adaptation of state-of-the-art parsing
models to MH is not immediate as the flat variable
structures of phrases are hard to parse and a plen-
tiful of morphological features that would facilitate
disambiguation are not exploited by currently avail-
able parsers. Also, the MH treebank is much smaller
than the ones for, e.g., English (Marcus et al, 1994)
and Arabic (Maamouri and Bies, 2004), making it
hard to apply data-intensive methods such as the all-
subtrees approach (Bod, 1992) or full lexicalization
(Collins, 2003). Our best performing model incor-
porates three dimensions of parametrization and our
best result (75.25%) is similar to the one obtained
by the parser of (Bikel, 2004) for Modern Standard
Arabic (75%) using a fully lexicalized model and
a training corpus about three times as large as our
newest MH treebank.
This work has shown that devising an adequate
baseline for parsing MH requires more than sim-
ple category-splits and sophisticated head-driven ex-
tensions, and our results provide preliminary evi-
dence for the variation in performance of different
parametrization strategies relative to the properties
and structure of a given language. The compari-
son with parsing accuracy for MSA suggests that
parametrizing an orthogonal depth dimension may
be able to compensate, to some extent, on the lack
of sister-dependencies, lexical information, and per-
haps even the lack of annotated data, but establish-
ing empirically its contribution to parsing MSA is a
matter for further research. In the future we intend
to further investigate the significance of the depth di-
mension by extending our models to include more
morphological features, more variation in the pa-
9Both studies acheived between 60%?70% accuracy, how-
ever the results are not comparable to our study because of the
use of different training sets, different annotation conventions,
and different evaluation schemes.
165
Figure 7: All Models: Locating Unlexicalized Parsing Models
in a Three-Dimensional Parametrization Space
Figure 8: All Results: Parsing Results for Unlexicalized Mod-
els in a Three-Dimensional Parametrization Space
rameter space, and applications to more languages.
7 Conclusion
Morphologically rich languages introduce a new di-
mension into the expansion possibilities of a non-
terminal node in a syntactic parse tree. This di-
mension is orthogonal to the vertical (Collins, 2003)
and horizontal (Johnson, 1998) dimensions previ-
ously outlined by Klein and Manning (2003), and
it cannot be collapsed into any one of the previous
two. These additional dependencies exist alongside
the syntactic head dependency and are attested using
morphosyntactic phenomena such as long distance
agreement. We demonstrate using syntactic defi-
niteness in MH that incorporating morphologically
marked features as a third, orthogonal dimension
for annotating syntactic categories is invaluable for
weakening the independence assumptions implicit
in a treebank PCFG and increasing the model?s dis-
ambiguation capabilities. Using a three-dimensional
model we establish a new, stronger, lower bound on
the performance of unlexicalized parsing models for
Modern Hebrew, comparable to those achieved for
other languages (Czech, Chinese, German and Ara-
bic) with much larger corpora.
Tuning the dimensions and value of the parame-
ters for learning treebank grammars is largely an em-
pirical matter, and we do not wish to claim here that
a three-dimensional annotation strategy is the best
for any given language. Rather, we argue that for
different languages different optimal parametriza-
tion strategies may apply. MH is not a free-word-
order language in the canonical sense, and our qual-
itative analysis shows that all dimensions contribute
to the models? disambiguation capabilities. Orthog-
onal dimensions provide complementary informa-
tion that is invaluable for the parsing process to the
extent that the relevant linguistic phenomena license
grammatical structures in the language. Our results
point out a principled way to quantitatively charac-
terizing differences between languages, thus guid-
ing the selection of parameters for the development
of annotated resources, custom parsers and cross-
linguistic robust parsing engines.
Acknowledgments We thank the Knowledge
Center for Processing Hebrew and Dalia Bojan for
providing us with the newest version of the MH
treebank. We are particularly grateful to the devel-
opment team of version 2.0, Adi Mile?a and Yuval
Krymolowsky, supervised by Yoad Winter for con-
tinued collaboration and technical support. We fur-
ther thank Felix Hageloh for allowing us to use the
software resulting from his M.Sc. thesis work. We
also like to thank Remko Scha, Jelle Zuidema, Yoav
Seginer and three anonymous reviewers for helpful
comments on the text, and Noa Tsarfaty for techni-
cal help in the graphical display. The work of the
first author is funded by the Netherlands Organiza-
tion for Scientific Research (NWO), grant number
017.001.271, for which we are grateful.
166
References
S. Abney. 1997. Stochastic Attribute-Value Grammars.
Computational Linguistics, 23 (4):597?618.
R. Bar-Haim, K. Sima?an, and Y. Winter. 2007. Part-of-
Speech Tagging of Modern Hebrew Text. Journal of
Natural Language Engineering.
D. Bikel and D. Chiang. 2000. Two Statistical Parsing
Models Applied to the Chinese Treebank. In Second
Chinese Language Processing Workshop, Hong Kong.
D. Bikel. 2004. Intricacies of Collins? Parsing Model.
Computational Linguistics, 4(30).
R. Bod. 1992. Data Oriented Parsing. In Proceedings of
COLING.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2, pages 1031?1036.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In AAAI/IAAI,
pages 598?603.
M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann. 1999.
A Statistical Parser for Czech. In Proceedings of ACL,
College Park, Maryland.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics, 29(4).
G. Danon. 2001. Syntactic Definiteness in the Grammar
of Modern Hebrew. Linguistics, 6(39):1071?1116.
A. Dubey and F. Keller. 2003. Probabilistic Parsing for
German using Sister-Head Dependencies. In Proceed-
ings of ACL.
Y. Goldberg, M. Adler, and M. Elhadad. 2006. Noun
Phrase Chunking in Hebrew: Influence of Lexical and
Morphological Features. In Proceedings of COLING-
ACL.
F. Hageloh. 2007. Parsing using Transforms over Tree-
banks. Master?s thesis, University of Amsterdam.
M. Johnson. 1998. PCFG Models of Linguistic
Tree Representations. Computational Linguistics,
24(4):613?632.
R. Kaplan and J. Bresnan. 1982. Lexical-Functional
Grammar: A formal system for grammatical represen-
tation. In J. Bresnan, editor, The Mental Representa-
tion of Grammatical Relations, Cambridge, MA. The
MIT Press.
D. Klein and C. Manning. 2003. Accurate Unlexicalized
Parsing. In Proceedings of ACL, pages 423?430.
Y. Krymolowski, Y. Adiel, N. Guthmann, S. Kenan,
A. Milea, N. Nativ, R. Tenzman, and P. Veisberg.
2007. Treebank Annotation Guide. MILA, Knowl-
edge Center for Hebrew Processing.
M. Maamouri and A. Bies. 2004. Developing an Ara-
bic Treebank: Methods, Guidelines, Procedures, and
Tools. In Proceedings of COLING.
M. Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,
A. Bies, M. Ferguson, K. Katz, and B. Schasberger.
1994. The Penn Treebank: Annotating Predicate-
Argument Structure.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with Latent Annotations. In Proceedings of
ACL?05.
N. Melnik. 2002. Verb-Initial Constructions in Modern
Hebrew. Ph.D. thesis, Berkeley University of Califor-
nia.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings of ACL-COLING, pages
433?440, Sydney, Australia, July.
D. Prescher. 2005. Head-Driven PCFGs with Latent-
Head Statistics. In In Proceedings of the International
Workshop on Parsing Technologies.
I. A. Sag, T. Wasow, and E. M. Bender. 2003. Syntactic
Theory: A Formal 