
This paper proposes a framework of super-
vised model learning that realizes feature
grouping to obtain lower complexity mod-
els. The main idea of our method is to
integrate a discrete constraint into model
learning with the help of the dual decom-
position technique. Experiments on two
well-studied NLP tasks, dependency pars-
ing and NER, demonstrate that our method
can provide state-of-the-art performance
even if the degrees of freedom in trained
models are surprisingly small, i.e., 8 or
even 2. This significant benefit enables us
to provide compact model representation,
which is especially useful in actual use.
1 