
Concept-to-text generation refers to the task of
automatically producing textual output from
non-linguistic input. We present a joint model
that captures content selection (?what to say?)
and surface realization (?how to say?) in
an unsupervised domain-independent fashion.
Rather than breaking up the generation pro-
cess into a sequence of local decisions, we de-
fine a probabilistic context-free grammar that
globally describes the inherent structure of the
input (a corpus of database records and text
describing some of them). We represent our
grammar compactly as a weighted hypergraph
and recast generation as the task of finding the
best derivation tree for a given input. Experi-
mental evaluation on several domains achieves
competitive results with state-of-the-art sys-
tems that use domain specific constraints, ex-
plicit feature engineering or labeled data.
1 