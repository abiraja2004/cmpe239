
We present a data-driven approach to learn
user-adaptive referring expression gener-
ation (REG) policies for spoken dialogue
systems. Referring expressions can be dif-
ficult to understand in technical domains
where users may not know the techni-
cal ?jargon? names of the domain entities.
In such cases, dialogue systems must be
able to model the user?s (lexical) domain
knowledge and use appropriate referring
expressions. We present a reinforcement
learning (RL) framework in which the sys-
tem learns REG policies which can adapt
to unknown users online. Furthermore,
unlike supervised learning methods which
require a large corpus of expert adaptive
behaviour to train on, we show that effec-
tive adaptive policies can be learned from
a small dialogue corpus of non-adaptive
human-machine interaction, by using a RL
framework and a statistical user simula-
tion. We show that in comparison to
adaptive hand-coded baseline policies, the
learned policy performs significantly bet-
ter, with an 18.6% average increase in
adaptation accuracy. The best learned pol-
icy also takes less dialogue time (average
1.07 min less) than the best hand-coded
policy. This is because the learned poli-
cies can adapt online to changing evidence
about the user?s domain expertise.
1 