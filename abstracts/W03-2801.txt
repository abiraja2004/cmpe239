
Although there is an increasing shift
towards evaluating Natural Language
Generation (NLG) systems, there are
still many NLG-specific open issues that
hinder effective comparative and quan-
titative evaluation in this field. The pa-
per starts off by describing a task-based,
i.e., black-box evaluation of a hyper-
text NLG system. Then we examine the
problem of glass-box, i.e., module spe-
cific, evaluation in language generation,
with focus on evaluating machine learn-
ing methods for text planning.
1 