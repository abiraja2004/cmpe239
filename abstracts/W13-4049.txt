 Unsupervised dialogue act modeling holds great promise for decreasing the develop-ment time to build dialogue systems. Work to date has utilized manual annota-tion or a synthetic task to evaluate unsu-pervised dialogue act models, but each of these evaluation approaches has substan-tial limitations. This paper presents an in-context evaluation framework for an un-supervised dialogue act model within tuto-rial dialogue. The clusters generated by the model are mapped to tutor responses by a handcrafted policy, which is applied to unseen test data and evaluated by hu-man judges. The results suggest that in-context evaluation may better reflect the performance of a model than comparing against manual dialogue act labels. 1 