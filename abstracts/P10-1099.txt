
Most supervised language processing sys-
tems show a significant drop-off in per-
formance when they are tested on text
that comes from a domain significantly
different from the domain of the training
data. Semantic role labeling techniques
are typically trained on newswire text, and
in tests their performance on fiction is
as much as 19% worse than their perfor-
mance on newswire text. We investigate
techniques for building open-domain se-
mantic role labeling systems that approach
the ideal of a train-once, use-anywhere
system. We leverage recently-developed
techniques for learning representations of
text using latent-variable language mod-
els, and extend these techniques to ones
that provide the kinds of features that are
useful for semantic role labeling. In exper-
iments, our novel system reduces error by
16% relative to the previous state of the art
on out-of-domain text.
1 