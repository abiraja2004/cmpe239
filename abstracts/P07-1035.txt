
Historically, unsupervised learning tech-
niques have lacked a principled technique
for selecting the number of unseen compo-
nents. Research into non-parametric priors,
such as the Dirichlet process, has enabled in-
stead the use of infinite models, in which the
number of hidden categories is not fixed, but
can grow with the amount of training data.
Here we develop the infinite tree, a new infi-
nite model capable of representing recursive
branching structure over an arbitrarily large
set of hidden categories. Specifically, we
develop three infinite tree models, each of
which enforces different independence as-
sumptions, and for each model we define a
simple direct assignment sampling inference
procedure. We demonstrate the utility of
our models by doing unsupervised learning
of part-of-speech tags from treebank depen-
dency skeleton structure, achieving an accu-
racy of 75.34%, and by doing unsupervised
splitting of part-of-speech tags, which in-
creases the accuracy of a generative depen-
dency parser from 85.11% to 87.35%.
1 