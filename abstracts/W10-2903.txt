
Current approaches to semantic parsing,
the task of converting text to a formal
meaning representation, rely on annotated
training data mapping sentences to logi-
cal forms. Providing this supervision is
a major bottleneck in scaling semantic
parsers. This paper presents a new learn-
ing paradigm aimed at alleviating the su-
pervision burden. We develop two novel
learning algorithms capable of predicting
complex structures which only rely on a
binary feedback signal based on the con-
text of an external world. In addition we
reformulate the semantic parsing problem
to reduce the dependency of the model on
syntactic patterns, thus allowing our parser
to scale better using less supervision. Our
results surprisingly show that without us-
ing any annotated meaning representations
learning with a weak feedback signal is ca-
pable of producing a parser that is compet-
itive with fully supervised parsers.
1 