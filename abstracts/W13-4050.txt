 We explore the plausibility of using automated spoken dialog systems (SDS) for administer-ing survey interviews. Because the goals of a survey dialog system differ from more tradi-tional information-seeking and transactional applications, different measures of task accu-racy and success may be warranted. We report a large-scale experimental evaluation of an SDS that administered survey interviews with questions drawn from government and social scientific surveys. We compare two dialog confirmation strategies: (1) a traditional strate-gy of explicit confirmation on low-confidence recognition; and (2) no confirmation. With ex-plicit confirmation, the small percentage of re-sidual errors had little to no impact on survey data measurement. Even without confirmation, while there are significantly more errors, im-pact on the substantive conclusions of the sur-vey is still very limited. 1 