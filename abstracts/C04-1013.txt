
Regular languages are widely used in NLP to-
day in spite of their shortcomings. Efficient
algorithms that can reliably learn these lan-
guages, and which must in realistic applications
only use positive samples, are necessary. These
languages are not learnable under traditional
distribution free criteria. We claim that an ap-
propriate learning framework is PAC learning
where the distributions are constrained to be
generated by a class of stochastic automata with
support equal to the target concept. We discuss
how this is related to other learning paradigms.
We then present a simple learning algorithm
for regular languages, and a self-contained proof
that it learns according to this partially distri-
bution free criterion.
1 