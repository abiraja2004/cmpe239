
Standard phrase-based translation models do
not explicitly model context dependence be-
tween translation units. As a result, they rely
on large phrase pairs and target language mod-
els to recover contextual effects in translation.
In this work, we explore n-gram models over
Minimal Translation Units (MTUs) to explic-
itly capture contextual dependencies across
phrase boundaries in the channel model. As
there is no single best direction in which con-
textual information should flow, we explore
multiple decomposition structures as well as
dynamic bidirectional decomposition. The
resulting models are evaluated in an intrin-
sic task of lexical selection for MT as well
as a full MT system, through n-best rerank-
ing. These experiments demonstrate that ad-
ditional contextual modeling does indeed ben-
efit a phrase-based system and that the direc-
tion of conditioning is important. Integrating
multiple conditioning orders provides consis-
tent benefit, and the most important directions
differ by language pair.
1 