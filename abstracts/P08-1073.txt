
We address two problems in the field of au-
tomatic optimization of dialogue strategies:
learning effective dialogue strategies when no
initial data or system exists, and evaluating the
result with real users. We use Reinforcement
Learning (RL) to learn multimodal dialogue
strategies by interaction with a simulated envi-
ronment which is ?bootstrapped? from small
amounts of Wizard-of-Oz (WOZ) data. This
use of WOZ data allows development of op-
timal strategies for domains where no work-
ing prototype is available. We compare the
RL-based strategy against a supervised strat-
egy which mimics the wizards? policies. This
comparison allows us to measure relative im-
provement over the training data. Our results
show that RL significantly outperforms Super-
vised Learning when interacting in simulation
as well as for interactions with real users. The
RL-based policy gains on average 50-times
more reward when tested in simulation, and
almost 18-times more reward when interacting
with real users. Users also subjectively rate
the RL-based policy on average 10% higher.
1 