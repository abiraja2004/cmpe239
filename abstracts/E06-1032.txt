
We argue that the machine translation
community is overly reliant on the Bleu
machine translation evaluation metric. We
show that an improved Bleu score is nei-
ther necessary nor sufficient for achieving
an actual improvement in translation qual-
ity, and give two significant counterex-
amples to Bleu?s correlation with human
judgments of quality. This offers new po-
tential for research which was previously
deemed unpromising by an inability to im-
prove upon Bleu scores.
1 