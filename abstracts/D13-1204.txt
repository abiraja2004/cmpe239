
Many statistical learning problems in NLP call
for local model search methods. But accu-
racy tends to suffer with current techniques,
which often explore either too narrowly or too
broadly: hill-climbers can get stuck in local
optima, whereas samplers may be inefficient.
We propose to arrange individual local opti-
mizers into organized networks. Our building
blocks are operators of two types: (i) trans-
form, which suggests new places to search, via
non-random restarts from already-found local
optima; and (ii) join, which merges candidate
solutions to find better optima. Experiments
on grammar induction show that pursuing dif-
ferent transforms (e.g., discarding parts of a
learned model or ignoring portions of train-
ing data) results in improvements. Groups of
locally-optimal solutions can be further per-
turbed jointly, by constructing mixtures. Us-
ing these tools, we designed several modu-
lar dependency grammar induction networks
of increasing complexity. Our complete sys-
tem achieves 48.6% accuracy (directed depen-
dency macro-average over all 19 languages in
the 2006/7 CoNLL data) ? more than 5%
higher than the previous state-of-the-art.
1 