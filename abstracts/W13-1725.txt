
Our efforts in the 2013 NLI shared task fo-
cused on the potential benefits of external cor-
pora. We show that including training data
from multiple corpora is highly effective at ro-
bust, cross-corpus NLI (i.e. open-training task
1), particularly when some form of domain
adaptation is also applied. This method can
also be used to boost performance even when
training data from the same corpus is available
(i.e. open-training task 2). However, in the
closed-training task, despite testing a number
of new features, we did not see much improve-
ment on a simple model based on earlier work.
1 