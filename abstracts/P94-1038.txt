 
In many applications of natural language processing it 
is necessary to determine the likelihood of a given word 
combination. For example, a speech recognizer may 
need to determine which of the two word combinations 
"eat a peach" and "eat a beach" is more likely. Statis- 
tical NLP methods determine the likelihood of a word 
combination according to its frequency in a training cor- 
pus. However, the nature of language is such that many 
word combinations are infrequent and do not occur in a 
given corpus. In this work we propose a method for es- 
t imating the probability of such previously unseen word 
combinations using available information on "most sim- 
ilar" words. 
We describe a probabilistic word association model 
based on distributional word similarity, and apply it 
to improving probability estimates for unseen word bi- 
grams in a variant of Katz's back-off model. The 
similarity-based method yields a 20% perplexity im- 
provement in the prediction of unseen bigrams and sta- 
tistically significant reductions in speech-recognition er-
ror. 
