
We propose a succinct randomized language
model which employs a perfect hash func-
tion to encode fingerprints of n-grams and
their associated probabilities, backoff weights,
or other parameters. The scheme can repre-
sent any standard n-gram model and is easily
combined with existing model reduction tech-
niques such as entropy-pruning. We demon-
strate the space-savings of the scheme via ma-
chine translation experiments within a dis-
tributed language modeling framework.
1 