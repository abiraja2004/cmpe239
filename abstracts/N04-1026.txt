
We examine the utility of multiple types of
turn-level and contextual linguistic features for
automatically predicting student emotions in
human-human spoken tutoring dialogues. We
first annotate student turns in our corpus for
negative, neutral and positive emotions. We
then automatically extract features represent-
ing acoustic-prosodic and other linguistic in-
formation from the speech signal and associ-
ated transcriptions. We compare the results of
machine learning experiments using different
feature sets to predict the annotated emotions.
Our best performing feature set contains both
acoustic-prosodic and other types of linguistic
features, extracted from both the current turn
and a context of previous student turns, and
yields a prediction accuracy of 84.75%, which
is a 44% relative improvement in error reduc-
tion over a baseline. Our results suggest that
the intelligent tutoring spoken dialogue system
we are developing can be enhanced to automat-
ically predict and adapt to student emotions.
1 