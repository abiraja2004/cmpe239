
This paper investigates several augmented mixture
models that are competitive alternatives to standard
Bayesian models and prove to be very suitable to
word sense disambiguation and related classifica-
tion tasks. We present a new classification correc-
tion technique that successfully addresses the prob-
lem of under-estimation of infrequent classes in the
training data. We show that the mixture models are
boosting-friendly and that both Adaboost and our
original correction technique can improve the re-
sults of the raw model significantly, achieving state-
of-the-art performance on several standard test sets
in four languages. With substantially different out-
put to Na?ve Bayes and other statistical methods, the
investigated models are also shown to be effective
participants in classifier combination.
1 