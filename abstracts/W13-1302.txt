
We present a holistic data-driven technique
that generates natural-language descriptions
for videos. We combine the output of state-of-
the-art object and activity detectors with ?real-
world? knowledge to select the most proba-
ble subject-verb-object triplet for describing a
video. We show that this knowledge, automat-
ically mined from web-scale text corpora, en-
hances the triplet selection algorithm by pro-
viding it contextual information and leads to
a four-fold increase in activity identification.
Unlike previous methods, our approach can
annotate arbitrary videos without requiring the
expensive collection and annotation of a sim-
ilar training video corpus. We evaluate our
technique against a baseline that does not use
text-mined knowledge and show that humans
prefer our descriptions 61% of the time.
1 