 
 
In this paper we present experiments related to 
the validation of spoken language understand-
ing capabilities in a language and culture train-
ing system.  In this application, word-level 
recognition rates are insufficient to character-
ize how well the system serves its users.  We 
present the results of an annotation exercise 
that distinguishes instances of non-recognition 
due to learner error from instances due to poor 
system coverage.  These statistics give a more 
accurate and interesting description of system 
performance, showing how the system could 
be improved without sacrificing the instruc-
tional value of rejecting learner utterances 
when they are poorly formed. 
1 