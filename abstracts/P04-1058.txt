
We compare two approaches for describing and gen-
erating bodies of rules used for natural language
parsing. In today?s parsers rule bodies do not ex-
ist a priori but are generated on the fly, usually with
methods based on n-grams, which are one particu-
lar way of inducing probabilistic regular languages.
We compare two approaches for inducing such lan-
guages. One is based on n-grams, the other on min-
imization of the Kullback-Leibler divergence. The
inferred regular languages are used for generating
bodies of rules inside a parsing procedure. We com-
pare the two approaches along two dimensions: the
quality of the probabilistic regular language they
produce, and the performance of the parser they
were used to build. The second approach outper-
forms the first one along both dimensions.
1 