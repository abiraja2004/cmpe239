
Coreference resolution is governed by syntac-
tic, semantic, and discourse constraints. We
present a generative, model-based approach in
which each of these factors is modularly en-
capsulated and learned in a primarily unsu-
pervised manner. Our semantic representation
first hypothesizes an underlying set of latent
entity types, which generate specific entities
that in turn render individual mentions. By
sharing lexical statistics at the level of abstract
entity types, our model is able to substantially
reduce semantic compatibility errors, result-
ing in the best results to date on the complete
end-to-end coreference task.
1 