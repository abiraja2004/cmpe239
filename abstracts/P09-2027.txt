
In the context of the Document Understand-
ing Conferences, the task of Query-Focused
Multi-Document Summarization is intended to
improve agreement in content among human-
generated model summaries. Query-focus also
aids the automated summarizers in directing
the summary at specific topics, which may re-
sult in better agreement with these model sum-
maries. However, while query focus corre-
lates with performance, we show that high-
performing automatic systems produce sum-
maries with disproportionally higher query
term density than human summarizers do. Ex-
perimental evidence suggests that automatic
systems heavily rely on query term occurrence
and repetition to achieve good performance.
1 