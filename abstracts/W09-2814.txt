
This work describes first steps towards
building a system that synchronously gen-
erates multimodal (textual and visual)
route directions for pedestrians. We pur-
sue a corpus-based approach for building a
generation model that produces natural in-
structions in multiple languages. We con-
ducted an empirical study to collect ver-
bal route directions, and annotated the ac-
quired texts on different levels. Here we
describe the experimental setting and an
analysis of the collected data.
1 