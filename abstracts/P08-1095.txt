
When multiple conversations occur simultane-
ously, a listener must decide which conversa-
tion each utterance is part of in order to inter-
pret and respond to it appropriately. We refer
to this task as disentanglement. We present a
corpus of Internet Relay Chat (IRC) dialogue
in which the various conversations have been
manually disentangled, and evaluate annota-
tor reliability. This is, to our knowledge, the
first such corpus for internet chat. We pro-
pose a graph-theoretic model for disentangle-
ment, using discourse-based features which
have not been previously applied to this task.
The model?s predicted disentanglements are
highly correlated with manual annotations.
1 Motivation
Simultaneous conversations seem to arise naturally
in both informal social interactions and multi-party
typed chat. Aoki et al (2006)?s study of voice con-
versations among 8-10 people found an average of
1.76 conversations (floors) active at a time, and a
maximum of four. In our chat corpus, the average is
even higher, at 2.75. The typical conversation, there-
fore, is one which is interrupted? frequently.
Disentanglement is the clustering task of dividing
a transcript into a set of distinct conversations. It is
an essential prerequisite for any kind of higher-level
dialogue analysis: for instance, consider the multi-
party exchange in figure 1.
Contextually, it is clear that this corresponds to
two conversations, and Felicia?s1 response ?excel-
1Real user nicknames are replaced with randomly selected
(Chanel) Felicia: google works :)
(Gale) Arlie: you guys have never worked
in a factory before have you
(Gale) Arlie: there?s some real unethical
stuff that goes on
(Regine) hands Chanel a trophy
(Arlie) Gale, of course ... thats how they
make money
(Gale) and people lose limbs or get killed
(Felicia) excellent
Figure 1: Some (abridged) conversation from our corpus.
lent? is intended for Chanel and Regine. A straight-
forward reading of the transcript, however, might in-
terpret it as a response to Gale?s statement immedi-
ately preceding.
Humans are adept at disentanglement, even in
complicated environments like crowded cocktail
parties or chat rooms; in order to perform this task,
they must maintain a complex mental representation
of the ongoing discourse. Moreover, they adapt their
utterances to some degree to make the task easier
(O?Neill and Martin, 2003), which suggests that dis-
entanglement is in some sense a ?difficult? discourse
task.
Disentanglement has two practical applications.
One is the analysis of pre-recorded transcripts in
order to extract some kind of information, such as
question-answer pairs or summaries. These tasks
should probably take as as input each separate con-
versation, rather than the entire transcript. Another
identifiers for ethical reasons.
834
application is as part of a user-interface system for
active participants in the chat, in which users target a
conversation of interest which is then highlighted for
them. Aoki et al (2003) created such a system for
speech, which users generally preferred to a conven-
tional system? when the disentanglement worked!
Previous attempts to solve the problem (Aoki et
al., 2006; Aoki et al, 2003; Camtepe et al, 2005;
Acar et al, 2005) have several flaws. They clus-
ter speakers, not utterances, and so fail when speak-
ers move from one conversation to another. Their
features are mostly time gaps between one utterance
and another, without effective use of utterance con-
tent. Moreover, there is no framework for a prin-
cipled comparison of results: there are no reliable
annotation schemes, no standard corpora, and no
agreed-upon metrics.
We attempt to remedy these problems. We present
a new corpus of manually annotated chat room data
and evaluate annotator reliability. We give a set of
metrics describing structural similarity both locally
and globally. We propose a model which uses dis-
course structure and utterance contents in addition
to time gaps. It partitions a chat transcript into dis-
tinct conversations, and its output is highly corre-
lated with human annotations.
2 Related Work
Two threads of research are direct attempts to solve
the disentanglement problem: Aoki et al (2006),
Aoki et al (2003) for speech and Camtepe et al
(2005), Acar et al (2005) for chat. We discuss
their approaches below. However, we should em-
phasize that we cannot compare our results directly
with theirs, because none of these studies publish re-
sults on human-annotated data. Although Aoki et al
(2006) construct an annotated speech corpus, they
give no results for model performance, only user sat-
isfaction with their conversational system. Camtepe
et al (2005) and Acar et al (2005) do give perfor-
mance results, but only on synthetic data.
All of the previous approaches treat the problem
as one of clustering speakers, rather than utterances.
That is, they assume that during the window over
which the system operates, a particular speaker is
engaging in only one conversation. Camtepe et al
(2005) assume this is true throughout the entire tran-
script; real speakers, by contrast, often participate
in many conversations, sequentially or sometimes
even simultaneously. Aoki et al (2003) analyze each
thirty-second segment of the transcript separately.
This makes the single-conversation restriction some-
what less severe, but has the disadvantage of ignor-
ing all events which occur outside the segment.
Acar et al (2005) attempt to deal with this prob-
lem by using a fuzzy algorithm to cluster speakers;
this assigns each speaker a distribution over conver-
sations rather than a hard assignment. However, the
algorithm still deals with speakers rather than utter-
ances, and cannot determine which conversation any
particular utterance is part of.
Another problem with these approaches is the in-
formation used for clustering. Aoki et al (2003) and
Camtepe et al (2005) detect the arrival times of mes-
sages, and use them to construct an affinity graph be-
tween participants by detecting turn-taking behavior
among pairs of speakers. (Turn-taking is typified by
short pauses between utterances; speakers aim nei-
ther to interrupt nor leave long gaps.) Aoki et al
(2006) find that turn-taking on its own is inadequate.
They motivate a richer feature set, which, however,
does not yet appear to be implemented. Acar et
al. (2005) adds word repetition to their feature set.
However, their approach deals with all word repe-
titions on an equal basis, and so degrades quickly
in the presence of noise words (their term for words
which shared across conversations) to almost com-
plete failure when only 1/2 of the words are shared.
To motivate our own approach, we examine some
linguistic studies of discourse, especially analysis of
multi-party conversation. O?Neill and Martin (2003)
point out several ways in which multi-party text chat
differs from typical two-party conversation. One key
difference is the frequency with which participants
mention each others? names. They hypothesize that
mentioning is a strategy which participants use to
make disentanglement easier, compensating for the
lack of cues normally present in face-to-face dia-
logue. Mentions (such as Gale?s comments to Ar-
lie in figure 1) are very common in our corpus, oc-
curring in 36% of comments, and provide a useful
feature.
Another key difference is that participants may
create a new conversation (floor) at any time, a pro-
cess which Sacks et al (1974) calls schisming. Dur-
835
ing a schism, a new conversation is formed, not
necessarily because of a shift in the topic, but be-
cause certain participants have refocused their atten-
tion onto each other, and away from whoever held
the floor in the parent conversation.
Despite these differences, there are still strong
similarities between chat and other conversations
such as meetings. Our feature set incorporates infor-
mation which has proven useful in meeting segmen-
tation (Galley et al, 2003) and the task of detect-
ing addressees of a specific utterance in a meeting
(Jovanovic et al, 2006). These include word rep-
etitions, utterance topic, and cue words which can
indicate the bounds of a segment.
3 Dataset
Our dataset is recorded from the IRC (Internet Re-
lay Chat) channel ##LINUX at freenode.net, using
the freely-available gaim client. ##LINUX is an un-
official tech support line for the Linux operating sys-
tem, selected because it is one of the most active chat
rooms on freenode, leading to many simultaneous
conversations, and because its content is typically
inoffensive. Although it is notionally intended only
for tech support, it includes large amounts of social
chat as well, such as the conversation about factory
work in the example above (figure 1).
The entire dataset contains 52:18 hours of chat,
but we devote most of our attention to three anno-
tated sections: development (706 utterances; 2:06
hr) and test (800 utts.; 1:39 hr) plus a short pilot sec-
tion on which we tested our annotation system (359
utts.; 0:58 hr).
3.1 Annotation
Our annotators were seven university students with
at least some familiarity with the Linux OS, al-
though in some cases very slight. Annotation of the
test dataset typically took them about two hours. In
all, we produced six annotations of the test set2.
Our annotation scheme marks each utterance as
part of a single conversation. Annotators are in-
structed to create as many, or as few conversations as
they need to describe the data. Our instructions state
that a conversation can be between any number of
2One additional annotation was discarded because the anno-
tator misunderstood the task.
people, and that, ?We mean conversation in the typ-
ical sense: a discussion in which the participants are
all reacting and paying attention to one another. . . it
should be clear that the comments inside a conver-
sation fit together.? The annotation system itself is a
simple Java program with a graphical interface, in-
tended to appear somewhat similar to a typical chat
client. Each speaker?s name is displayed in a differ-
ent color, and the system displays the elapsed time
between comments, marking especially long pauses
in red. Annotators group sentences into conversa-
tions by clicking and dragging them onto each other.
3.2 Metrics
Before discussing the annotations themselves, we
will describe the metrics we use to compare differ-
ent annotations; these measure both how much our
annotators agree with each other, and how well our
model and various baselines perform. Comparing
clusterings with different numbers of clusters is a
non-trivial task, and metrics for agreement on su-
pervised classification, such as the ? statistic, are not
applicable.
To measure global similarity between annota-
tions, we use one-to-one accuracy. This measure de-
scribes how well we can extract whole conversations
intact, as required for summarization or information
extraction. To compute it, we pair up conversations
from the two annotations to maximize the total over-
lap3, then report the percentage of overlap found.
If we intend to monitor or participate in the con-
versation as it occurs, we will care more about lo-
cal judgements. The local agreement metric counts
agreements and disagreements within a context k.
We consider a particular utterance: the previous
k utterances are each in either the same or a dif-
ferent conversation. The lock score between two
annotators is their average agreement on these k
same/different judgements, averaged over all utter-
ances. For example, loc1 counts pairs of adjacent
utterances for which two annotations agree.
836
Mean Max Min
Conversations 81.33 128 50
Avg. Conv. Length 10.6 16.0 6.2
Avg. Conv. Density 2.75 2.92 2.53
Entropy 4.83 6.18 3.00
1-to-1 52.98 63.50 35.63
loc 3 81.09 86.53 74.75
M-to-1 (by entropy) 86.70 94.13 75.50
Table 1: Statistics on 6 annotations of 800 lines of chat
transcript. Inter-annotator agreement metrics (below the
line) are calculated between distinct pairs of annotations.
3.3 Discussion
A statistical examination of our data (table 1) shows
that that it is eminently suitable for disentanglement:
the average number of conversations active at a time
is 2.75. Our annotators have high agreement on
the local metric (average of 81.1%). On the 1-to-
1 metric, they disagree more, with a mean overlap
of 53.0% and a maximum of only 63.5%. This level
of overlap does indicate a useful degree of reliabil-
ity, which cannot be achieved with naive heuristics
(see section 5). Thus measuring 1-to-1 overlap with
our annotations is a reasonable evaluation for com-
putational models. However, we feel that the major
source of disagreement is one that can be remedied
in future annotation schemes: the specificity of the
individual annotations.
To measure the level of detail in an annotation, we
use the information-theoretic entropy of the random
variable which indicates which conversation an ut-
terance is in. This quantity is non-negative, increas-
ing as the number of conversations grow and their
size becomes more balanced. It reaches its maxi-
mum, 9.64 bits for this dataset, when each utterance
is placed in a separate conversation. In our anno-
tations, it ranges from 3.0 to 6.2. This large vari-
ation shows that some annotators are more specific
than others, but does not indicate how much they
agree on the general structure. To measure this, we
introduce the many-to-one accuracy. This measure-
ment is asymmetrical, and maps each of the conver-
sations of the source annotation to the single con-
3This is an example of max-weight bipartite matching, and
can be computed optimally using, eg, max-flow. The widely
used greedy algorithm is a two-approximation, although we
have not found large differences in practice.
(Lai) need money
(Astrid) suggest a paypal fund or similar
(Lai) Azzie [sic; typo for Astrid?]: my
shack guy here said paypal too but i have
no local bank acct
(Felicia) second?s Azzie?s suggestion
(Gale) we should charge the noobs $1 per
question to [Lai?s] paypal
(Felicia) bingo!
(Gale) we?d have the money in 2 days max
(Azzie) Lai: hrm, Have you tried to set
one up?
(Arlie) the federal reserve system conspir-
acy is keeping you down man
(Felicia) Gale: all ubuntu users .. pay up!
(Gale) and susers pay double
(Azzie) I certainly would make suse users
pay.
(Hildegard) triple.
(Lai) Azzie: not since being offline
(Felicia) it doesn?t need to be ?in state?
either
Figure 2: A schism occurring in our corpus (abridged):
not all annotators agree on where the thread about charg-
ing for answers to techical questions diverges from the
one about setting up Paypal accounts. Either Gale?s or
Azzie?s first comment seems to be the schism-inducing
utterance.
versation in the target with which it has the great-
est overlap, then counts the total percentage of over-
lap. This is not a statistic to be optimized (indeed,
optimization is trivial: simply make each utterance
in the source into its own conversation), but it can
give us some intuition about specificity. In partic-
ular, if one subdivides a coarse-grained annotation
to make a more specific variant, the many-to-one
accuracy from fine to coarse remains 1. When we
map high-entropy annotations (fine) to lower ones
(coarse), we find high many-to-one accuracy, with a
mean of 86%, which implies that the more specific
annotations have mostly the same large-scale bound-
aries as the coarser ones.
By examining the local metric, we can see even
more: local correlations are good, at an average of
81.1%. This means that, in the three-sentence win-
dow preceding each sentence, the annotators are of-
837
ten in agreement. If they recognize subdivisions of
a large conversation, these subdivisions tend to be
contiguous, not mingled together, which is why they
have little impact on the local measure.
We find reasons for the annotators? disagreement
about appropriate levels of detail in the linguistic
literature. As mentioned, new conversations of-
ten break off from old ones in schisms. Aoki et
al. (2006) discuss conversational features associated
with schisming and the related process of affiliation,
by which speakers attach themselves to a conversa-
tion. Schisms often branch off from asides or even
normal comments (toss-outs) within an existing con-
versation. This means that there is no clear begin-
ning to the new conversation? at the time when it
begins, it is not clear that there are two separate
floors, and this will not become clear until distinct
sets of speakers and patterns of turn-taking are es-
tablished. Speakers, meanwhile, take time to ori-
ent themselves to the new conversation. An example
schism is shown in Figure 2.
Our annotation scheme requires annotators to
mark each utterance as part of a single conversation,
and distinct conversations are not related in any way.
If a schism occurs, the annotator is faced with two
options: if it seems short, they may view it as a mere
digression and label it as part of the parent conver-
sation. If it seems to deserve a place of its own, they
will have to separate it from the parent, but this sev-
ers the initial comment (an otherwise unremarkable
aside) from its context. One or two of the annota-
tors actually remarked that this made the task con-
fusing. Our annotators seem to be either ?splitters?
or ?lumpers?? in other words, each annotator seems
to aim for a consistent level of detail, but each one
has their own idea of what this level should be.
As a final observation about the dataset, we test
the appropriateness of the assumption (used in pre-
vious work) that each speaker takes part in only one
conversation. In our data, the average speaker takes
part in about 3.3 conversations (the actual number
varies for each annotator). The more talkative a
speaker is, the more conversations they participate
in, as shown by a plot of conversations versus utter-
ances (Figure 3). The assumption is not very accu-
rate, especially for speakers with more than 10 utter-
ances.
0 10 20 30 40 50 60
Utterances
0
1
2
3
4
5
6
7
8
9
10
T
h
r
e
a
d
s
Figure 3: Utterances versus conversations participated in
per speaker on development data.
4 Model
Our model for disentanglement fits into the general
class of graph partitioning algorithms (Roth and Yih,
2004) which have been used for a variety of tasks in
NLP, including the related task of meeting segmen-
tation (Malioutov and Barzilay, 2006). These algo-
rithms operate in two stages: first, a binary classifier
marks each pair of items as alike or different, and
second, a consistent partition is extracted.4
4.1 Classification
We use a maximum-entropy classifier (Daume? III,
2004) to decide whether a pair of utterances x and
y are in same or different conversations. The most
likely class is different, which occurs 57% of the
time in development data. We describe the classi-
fier?s performance in terms of raw accuracy (cor-
rect decisions / total), precision and recall of the
same class, and F-score, the harmonic mean of pre-
cision and recall. Our classifier uses several types
of features (table 2). The chat-specific features yield
the highest accuracy and precision. Discourse and
content-based features have poor accuracy on their
own (worse than the baseline), since they work best
on nearby pairs of utterances, and tend to fail on
more distant pairs. Paired with the time gap fea-
ture, however, they boost accuracy somewhat and
produce substantial gains in recall, encouraging the
model to group related utterances together.
The time gap, as discussed above, is the most
widely used feature in previous work. We exam-
4Our first attempt at this task used a Bayesian generative
model. However, we could not define a sharp enough posterior
over new sentences, which made the model unstable and overly
sensitive to its prior.
838
Chat-specific (Acc 73: Prec: 73 Rec: 61 F: 66)
Time The time between x and y in sec-
onds, bucketed logarithmically.
Speaker x and y have the same speaker.
Mention x mentions y (or vice versa),
both mention the same name, ei-
ther mentions any name.
Discourse (Acc 52: Prec: 47 Rec: 77 F: 58)
Cue words Either x or y uses a greeting
(?hello? &c), an answer (?yes?,
?no? &c), or thanks.
Question Either asks a question (explicitly
marked with ???).
Long Either is long (> 10 words).
Content (Acc 50: Prec: 45 Rec: 74 F: 56)
Repeat(i) The number of words shared
between x and y which have
unigram probability i, bucketed
logarithmically.
Tech Whether both x and y use tech-
nical jargon, neither do, or only
one does.
Combined (Acc 75: Prec: 73 Rec: 68 F: 71)
Table 2: Feature functions with performance on develop-
ment data.
ine the distribution of pauses between utterances in
the same conversation. Our choice of a logarithmic
bucketing scheme is intended to capture two char-
acteristics of the distribution (figure 4). The curve
has its maximum at 1-3 seconds, and pauses shorter
than a second are less common. This reflects turn-
taking behavior among participants; participants in
the same conversation prefer to wait for each others?
responses before speaking again. On the other hand,
the curve is quite heavy-tailed to the right, leading
us to bucket long pauses fairly coarsely.
Our discourse-based features model some pair-
0 10 100 1000
0
20
40
seconds
Fr
eq
ue
nc
y
Figure 4: Distribution of pause length (log-scaled) be-
tween utterances in the same conversation.
wise relationships: questions followed by answers,
short comments reacting to longer ones, greetings at
the beginning and thanks at the end.
Word repetition is a key feature in nearly every
model for segmentation or coherence, so it is no sur-
prise that it is useful here. We bucket repeated words
by their unigram probability5 (measured over the en-
tire 52 hours of transcript). The bucketing scheme
allows us to deal with ?noise words? which are re-
peated coincidentally.
The point of the repetition feature is of course to
detect sentences with similar topics. We also find
that sentences with technical content are more likely
to be related than non-technical sentences. We label
an utterance as technical if it contains a web address,
a long string of digits, or a term present in a guide
for novice Linux users 6 but not in a large news cor-
pus (Graff, 1995)7. This is a light-weight way to
capture one ?semantic dimension? or cluster of re-
lated words, in a corpus which is not amenable to
full LSA or similar techniques. LSA in text corpora
yields a better relatedness measure than simple rep-
etition (Foltz et al, 1998), but is ineffective in our
corpus because of its wide variety of topics and lack
of distinct document boundaries.
Pairs of utterances which are widely separated
in the discourse are unlikely to be directly related?
even if they are part of the same conversation, the
link between them is probably a long chain of in-
tervening utterances. Thus, if we run our classifier
on a pair of very distant utterances, we expect it to
default to the majority class, which in this case will
be different, and this will damage our performance
in case the two are really part of the same conver-
sation. To deal with this, we run our classifier only
on utterances separated by 129 seconds or less. This
is the last of our logarithmic buckets in which the
classifier has a significant advantage over the major-
ity baseline. For 99.9% of utterances in an ongoing
conversation, the previous utterance in that conver-
sation is within this gap, and so the system has a
5We discard the 50 most frequent words entirely.
6
?