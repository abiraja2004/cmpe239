
One style of Multi-Engine Machine
Translation architecture involves choos-
ing the best of a set of outputs from
different systems. Choosing the best
translation from an arbitrary set, even
in the presence of human references, is
a difficult problem; it may prove better
to look at mechanisms for making such
choices in more restricted contexts.
In this paper we take a classification-
based approach to choosing between
candidates from syntactically informed
translations. The idea is that using
multiple parsers as part of a classifier
could help detect syntactic problems in
this context that lead to bad transla-
tions; these problems could be detected
on either the source side?perhaps sen-
tences with difficult or incorrect parses
could lead to bad translations?or on
the target side?perhaps the output
quality could be measured in a more
syntactically informed way, looking for
syntactic abnormalities.
We show that there is no evidence that
the source side information is useful.
However, a target-side classifier, when
used to identify particularly bad trans-
lation candidates, can lead to signifi-
cant improvements in Bleu score. Im-
provements are even greater when com-
bined with existing language and align-
ment model approaches.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
1 