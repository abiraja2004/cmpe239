
The ever growing amount of web images
and their associated texts offers new op-
portunities for integrative models bridging
natural language processing and computer
vision. However, the potential benefits of
such data are yet to be fully realized due
to the complexity and noise in the align-
ment between image content and text. We
address this challenge with contributions
in two folds: first, we introduce the new
task of image caption generalization, for-
mulated as visually-guided sentence com-
pression, and present an efficient algo-
rithm based on dynamic beam search with
dependency-based constraints. Second,
we release a new large-scale corpus with
1 million image-caption pairs achieving
tighter content alignment between images
and text. Evaluation results show the in-
trinsic quality of the generalized captions
and the extrinsic utility of the new image-
text parallel corpus with respect to a con-
crete application of image caption transfer.
1 