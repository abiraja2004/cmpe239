
Abstractive summarization has been a long-
standing and long-term goal in automatic sum-
marization, because systems that can generate
abstracts demonstrate a deeper understanding
of language and the meaning of documents
than systems that merely extract sentences
from those documents. Genest (2009) showed
that summaries from the top automatic sum-
marizers are judged as comparable to manual
extractive summaries, and both are judged to
be far less responsive than manual abstracts,
As the state of the art approaches the limits
of extractive summarization, it becomes even
more pressing to advance abstractive summa-
rization. However, abstractive summarization
has been sidetracked by questions of what
qualifies as important information, and how do
we find it? The Guided Summarization task
introduced at the Text Analysis Conference
2010 attempts to neutralize both of these prob-
lems by introducing topic categories and lists
of aspects that a responsive summary should
address. This design results in more similar
human models, giving the automatic summa-
rizers a more focused target to pursue, and also
provides detailed diagnostics of summary con-
tent, which can can help build better meaning-
oriented summarization systems.
1 