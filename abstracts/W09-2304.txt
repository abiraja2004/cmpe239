 We argue that learning word alignments through a compositionally-structured, joint process yields higher phrase-based transla-tion accuracy than the conventional heuris-tic of intersecting conditional models. Flawed word alignments can lead to flawed phrase translations that damage translation accuracy. Yet the IBM word alignments usually used today are known to be flawed, in large part because IBM models (1) model reordering by allowing unrestricted movement of words, rather than con-strained movement of compositional units, and therefore must (2) attempt to compen-sate via directed, asymmetric distortion and fertility models. The conventional heuris-tics for attempting to recover from the re-sulting alignment errors involve estimating two directed models in opposite directions and then intersecting their alignments ? to make up for the fact that, in reality, word alignment is an inherently joint relation. A natural alternative is provided by Inversion Transduction Grammars, which estimate the joint word alignment relation directly, eliminating the need for any of the conven-tional heuristics. We show that this align-ment ultimately produces superior translation accuracy on BLEU, NIST, and METEOR metrics over three distinct lan-guage pairs. 1 