
This paper describes the development of
QuestionBank, a corpus of 4000 parse-
annotated questions for (i) use in training
parsers employed in QA, and (ii) evalua-
tion of question parsing. We present a se-
ries of experiments to investigate the ef-
fectiveness of QuestionBank as both an
exclusive and supplementary training re-
source for a state-of-the-art parser in pars-
ing both question and non-question test
sets. We introduce a new method for
recovering empty nodes and their an-
tecedents (capturing long distance depen-
dencies) from parser output in CFG trees
using LFG f-structure reentrancies. Our
main findings are (i) using QuestionBank
training data improves parser performance
to 89.75% labelled bracketing f-score, an
increase of almost 11% over the base-
line; (ii) back-testing experiments on non-
question data (Penn-II WSJ Section 23)
shows that the retrained parser does not
suffer a performance drop on non-question
material; (iii) ablation experiments show
that the size of training material provided
by QuestionBank is sufficient to achieve
optimal results; (iv) our method for recov-
ering empty nodes captures long distance
dependencies in questions from the ATIS
corpus with high precision (96.82%) and
low recall (39.38%). In summary, Ques-
tionBank provides a useful new resource
in parser-based QA research.
1 