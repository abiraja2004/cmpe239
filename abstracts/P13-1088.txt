
Modelling the compositional process by
which the meaning of an utterance arises
from the meaning of its parts is a funda-
mental task of Natural Language Process-
ing. In this paper we draw upon recent
advances in the learning of vector space
representations of sentential semantics and
the transparent interface between syntax
and semantics provided by Combinatory
Categorial Grammar to introduce Com-
binatory Categorial Autoencoders. This
model leverages the CCG combinatory op-
erators to guide a non-linear transforma-
tion of meaning within a sentence. We use
this model to learn high dimensional em-
beddings for sentences and evaluate them
in a range of tasks, demonstrating that
the incorporation of syntax allows a con-
cise model to learn representations that are
both effective and general.
1 