
We challenge the NLP community to par-
ticipate in a large-scale, distributed effort
to design and build resources for devel-
oping and evaluating solutions to new and
existing NLP tasks in the context of Rec-
ognizing Textual Entailment. We argue
that the single global label with which
RTE examples are annotated is insufficient
to effectively evaluate RTE system perfor-
mance; to promote research on smaller, re-
lated NLP tasks, we believe more detailed
annotation and evaluation are needed, and
that this effort will benefit not just RTE
researchers, but the NLP community as
a whole. We use insights from success-
ful RTE systems to propose a model for
identifying and annotating textual infer-
ence phenomena in textual entailment ex-
amples, and we present the results of a pi-
lot annotation study that show this model
is feasible and the results immediately use-
ful.
1 