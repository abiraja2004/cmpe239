
This paper presents a case study of a
difficult and important categorical anno-
tation task (word sense) to demonstrate
a probabilistic annotation model applied
to crowdsourced data. It is argued that
standard (chance-adjusted) agreement lev-
els are neither necessary nor sufficient
to ensure high quality gold standard la-
bels. Compared to conventional agree-
ment measures, application of an annota-
tion model to instances with crowdsourced
labels yields higher quality labels at lower
cost.
1 