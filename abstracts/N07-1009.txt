
Conditional Random Fields (CRFs) have shown
great success for problems involving structured out-
put variables. However, for many real-world NLP
applications, exact maximum-likelihood training is
intractable because computing the global normal-
ization factor even approximately can be extremely
hard. In addition, optimizing likelihood often does
not correlate with maximizing task-specific evalu-
ation measures. In this paper, we present a novel
training procedure, structured local training, that
maximizes likelihood while exploiting the benefits
of global inference during training: hidden vari-
ables are used to capture interactions between lo-
cal inference and global inference. Furthermore,
we introduce biased potential functions that empir-
ically drive CRFs towards performance improve-
ments w.r.t. the preferred evaluation measure for
the learning task. We report promising experimen-
tal results on two coreference data sets using two
task-specific evaluation measures.
1 