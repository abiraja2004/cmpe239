
In this paper we show how to auto-
matically induce non-linear features for
machine translation. The new features
are selected to approximately maximize
a BLEU-related objective and decompose
on the level of local phrases, which guar-
antees that the asymptotic complexity of
machine translation decoding does not in-
crease. We achieve this by applying gra-
dient boosting machines (Friedman, 2000)
to learn newweak learners (features) in the
form of regression trees, using a differen-
tiable loss function related to BLEU. Our
results indicate that small gains in perfor-
mance can be achieved using this method
but we do not see the dramatic gains ob-
served using feature induction for other
important machine learning tasks.
1 