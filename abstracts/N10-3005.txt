
Vector-based distributional models
of semantics have proven useful
and adequate in a variety of natural
language processing tasks. How-
ever, most of them lack at least
one key requirement in order to
serve as an adequate representa-
tion of natural language, namely
sensitivity to structural information
such as word order. We propose a
novel approach that offers a poten-
tial of integrating order-dependent
word contexts in a completely un-
supervised manner by assigning to
words characteristic distributional
matrices. The proposed model is
applied to the task of free associa-
tions. In the end, the first results as
well as directions for future work
are discussed.
1 